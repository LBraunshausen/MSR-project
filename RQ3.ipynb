{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "killing-squad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded csv data\n"
     ]
    }
   ],
   "source": [
    "# Load csv data\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy \n",
    "from spacy.matcher import Matcher\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example\n",
    "from spacy.tokens import Doc\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import copy\n",
    "import csv\n",
    "\n",
    "filepath = os.path.join(os.getcwd(), 'QueryResults_sample.csv')\n",
    "\n",
    "stack_posts = pd.read_csv(filepath, sep = \",\")\n",
    "\n",
    "print(\"loaded csv data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "neither-purchase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encog', 'math', 'pybrain', 'search', 'knn', 'ensemble-learning', 'q-learning', 'euclidean-distance', 'feature-extraction', 'document-classification', 'scala', 'decision-tree', 'kaggle', 'missing-data', 'stanford-nlp', 'numpy', 'supervised-learning', 'smo', 'vectorization', 'svm', 'arff', 'perceptron', 'scipy', 'cryptography', 'metrics', 'php', 'collaborative-filtering', 'nlp', 'keyword-search', 'cascade-classifier', 'biometrics', 'dirichlet', 'datumbox', 'pam', 'apache-spark', 'twitter', 'kernel', 'compilation', 'e-commerce', 'dlib', 'oauth', 'haar-classifier', 'simplecv', 'confusion-matrix', 'ranking', 'test-data', 'object-recognition', 'dbn', 'pos-tagger', 'entity-framework', 'cuda', 'data-visualization', 'r-caret', 'arrays', 'bayesian-networks', 'cluster-analysis', 'matlab', 'neural-network', 'vgg-net', 'matplotlib', 'azure-machine-learning-studio', 'rbm', 'security', 'image-processing', 'node.js', 'tokenize', 'object-detection', 'io', 'java', 'python', 'genetic-programming', 'sparse-matrix', 'measurement', 'google-analytics', 'mahout', 'regression', 'precision-recall', 'mixture-model', 'tic-tac-toe', 'feature-selection', 'porter-stemmer', 'evolutionary-algorithm', 'lda', 'multilabel-classification', 'data-analysis', 'prediction', 'gps', 'wordnet', 'elasticsearch', 'hadoop', 'c++', 'predictionio', 'cors', 'pom.xml', 'google-analytics-api', 'pmml', 'quadratic-programming', 'large-files', 'hierarchical-clustering', 'indices', 'deep-learning', 'projection', 'computer-vision', 'implementation', 'time-series', 'logistic-regression', 'pandas', 'gate', 'single-sign-on', 'image-recognition', 'distributed-computing', 'probability-density', 'calibration', 'text-analysis', 'cross-validation', 'cluster-computing', 'caffe', 'training-data', 'lstm', 'pylearn', 'classification', 'text-classification', 'genetic-algorithm', 'apriori', 'speech-recognition', 'treemodel', 'parameter-passing', 'mp3', 'mallet', 'random', 'cloudera', 'matrix', 'amazon-web-services', 'grouping', 'sift', 'python-3.x', 'conv-neural-network', 'nearest-neighbor', 'r', 'algorithm', 'fuzzy-search', 'signal-processing', 'pattern-matching', 'mfcc', 'reservoir-sampling', 'statistics', 'opennlp', 'detection', 'linear-regression', 'convolution', 'filesize', 'id3', 'weka', 'face-recognition', 'apache-spark-mllib', 'glm', 'unsupervised-learning', 'csv', 'prolog', 'normalization', 'probability', 'pattern-recognition', 'naivebayes', 'data-manipulation', 'image', 'data-mining', 'mysql', 'mathematical-optimization', 'text-mining', 'gaussian', 'gradient-descent', 'differentiation', 'azure', 'cvx', 'information-retrieval', 'plot', 'tagged-corpus', 'tf-idf', 'ruby', 'c#', 'topic-modeling', 'audio', 'optimization', 'black-box', 'linear-algebra', 'scientific-computing', 'data-structures', 'regex', 'contour', 'reduction', 'vector', 'office365', 'computer-science', 'svmlight', 'orange', 'summarization', 'backpropagation', 'liblinear', 'gradient', 'viola-jones', 'pruning', 'text', 'sampling', 'linux', 'artificial-intelligence', 'logarithm', 'recommendation-engine', 'random-forest', 'auc', 'algorithmic-trading', 'feature-detection', 'opencv', 'pca', 'standardized', 'mex', 'kernel-density', 'hashmap', 'maven', 'api', 'authentication', 'bayesian', 'scikit-learn', 'reinforcement-learning', 'gpu', 'anpr', 'python-2.7', 'k-means', 'word2vec', 'microsoft-account', 'nltk', 'libsvm', 'theano', 'ssas', 'mapreduce', 'gearman', 'gnuplot', 'macos', 'hessian-matrix', 'chess', 'evaluation', 'elki'}\n"
     ]
    }
   ],
   "source": [
    "# drop all duplicates in posts\n",
    "df = pd.DataFrame(stack_posts[0:500])\n",
    "df = df.drop_duplicates([\"QuestionId\"])\n",
    "df = df.sort_values(by=[\"QuestionId\"])\n",
    "#df.duplicated([\"QuestionId\"])\n",
    "\n",
    "\n",
    "# get all tags of questions\n",
    "tag_set = set()\n",
    "tag_list = []\n",
    "for tags in df[\"Tags\"]:\n",
    "    # clean tags from '>' and '<' occurences\n",
    "    tags = re.sub('><', ' ', tags) \n",
    "    tags = re.sub('<|>', '', tags)\n",
    "    # add single tag of tags and add it to lists and sets\n",
    "    for tag in tags.split():               \n",
    "        tag_list.append(tag)\n",
    "\n",
    "#tag_set = set(tag_list)\n",
    "# filter term 'machine-learning', because sql export filters for this term\n",
    "tag_set = set(filter(lambda a: a != 'machine-learning', tag_list))\n",
    "tag_Counter = Counter(tag_list)\n",
    "\n",
    "print(tag_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hindu-apparel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "# clean posts and match words\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "technology_pattern1 = [{'POS': 'PROPN', 'OP': '+'},\n",
    "                       {'POS': 'NUM', 'OP': '?'}\n",
    "                      ]\n",
    "\n",
    "technology_pattern2 = [{'OP': '+', 'POS': 'PROPN'},\n",
    "                       {'TEXT': '-', 'OP': '+'},\n",
    "                       {'POS': 'VERB', 'OP': '+'}\n",
    "                      ]\n",
    "\n",
    "technology_pattern3 = [{'OP': '+', 'POS': 'NOUN'},\n",
    "                       {'TEXT': '-', 'OP': '?'},\n",
    "                       {'POS': 'PROPN', 'OP': '+'}\n",
    "                      ]\n",
    "\n",
    "ml_pattern1 = [{'LOWER': 'machine', 'OP': '!'},\n",
    "                       #{'TEXT': '-', 'OP': '!'},\n",
    "                       {'LOWER': 'learning', 'OP': '!'}\n",
    "                      ]\n",
    "\n",
    "\n",
    "word_set = set()\n",
    "regex_pattern = '(<(pre|code|blockquote|a|strike)(.|\\n)*?\\/(pre|code|blockquote|a|strike)>)*?|<(p|b|br|br(.|\\n)*?\\/|sub|sup|em|strong|hr|s|i|ol|ul|li|code)*?>|<\\/(p|b|br|sub|sup|em|strong|s|i|ol|ul|li|div|pre|blockquote|a|code)>|<h(.|\\n)*?>(.|\\n)*?<\\/h(.|\\n)*?>*?|(<(img|div|ol|ul|li)(.|\\n)*?\\/*?>)|\\n'\n",
    "matcher.add(\"match_technology1\", [technology_pattern1])\n",
    "matcher.add(\"match_technology2\", [technology_pattern2])\n",
    "matcher.add(\"match_technology3\", [technology_pattern3])\n",
    "#matcher.add(\"unmatch_ml_pattern\", [ml_pattern1])\n",
    "\n",
    "for text in stack_posts[\"AnswerBody\"][500:2000]:\n",
    "    text = re.sub(regex_pattern, '', text, flags=re.I)\n",
    "    text = re.sub('\\(|\\)', ' ', text, flags=re.I)    \n",
    "    doc = nlp(text)    \n",
    "    \n",
    "    matches = matcher(doc)\n",
    "        \n",
    "    for match_id, start, end in matches:\n",
    "        word_set.add(doc[start:end])       \n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecological-export",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-3141c5bde907>:13: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  if tag_doc.similarity(span) >= 0.8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "# Term similarity\n",
    "\n",
    "# loop through relavant words and get their vectors\n",
    "technology_list = []\n",
    "technology_string_list = []\n",
    "technology_counter = Counter()\n",
    "for tag in tag_set:\n",
    "    term_vector = []    \n",
    "    tag_doc = nlp(str(tag))\n",
    "    #tag_vector = tag_doc.vector    \n",
    "    \n",
    "    for i,span in enumerate(word_set):        \n",
    "        if tag_doc.similarity(span) >= 0.8:\n",
    "        # very slow\n",
    "        # if cosine_similarity([tag_vector], [span_vector])[0][0] >= 0.7:\n",
    "            if span.text not in technology_string_list:\n",
    "                technology_list.append(span)\n",
    "                technology_string_list.append(span.text)\n",
    "            technology_counter[span.text] = technology_counter[span.text] + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "for technology in technology_list:\n",
    "    print(technology)\n",
    "    with open('technology_list.txt', 'a', newline='') as myfile:\n",
    "        wr = csv.writer(myfile)\n",
    "        wr.writerow([technology])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
