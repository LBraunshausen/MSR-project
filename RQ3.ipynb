{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "verbal-leather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded csv data\n"
     ]
    }
   ],
   "source": [
    "# Load csv data\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy \n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "filepath = os.path.join(os.getcwd(), 'QueryResults_sample.csv')\n",
    "\n",
    "stack_posts = pd.read_csv(filepath, sep = \",\")\n",
    "\n",
    "print(\"loaded csv data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "crucial-clinton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scientific-computing', 'cuda', 'data-manipulation', 'reduction', 'backpropagation', 'tic-tac-toe', 'stanford-nlp', 'data-analysis', 'haar-classifier', 'azure-machine-learning-studio', 'dirichlet', 'data-visualization', 'face-recognition', 'smo', 'io', 'audio', 'plot', 'predictionio', 'compilation', 'genetic-algorithm', 'tagged-corpus', 'gaussian', 'java', 'confusion-matrix', 'optimization', 'data-structures', 'grouping', 'scala', 'time-series', 'twitter', 'random', 'mex', 'distributed-computing', 'c#', 'object-recognition', 'r-caret', 'gearman', 'metrics', 'object-detection', 'image', 'pandas', 'indices', 'image-processing', 'ranking', 'orange', 'id3', 'dlib', 'chess', 'feature-extraction', 'tf-idf', 'api', 'logistic-regression', 'numpy', 'test-data', 'decision-tree', 'mysql', 'implementation', 'amazon-web-services', 'collaborative-filtering', 'mathematical-optimization', 'python-2.7', 'treemodel', 'single-sign-on', 'gate', 'black-box', 'prediction', 'hessian-matrix', 'mp3', 'standardized', 'hadoop', 'multilabel-classification', 'arff', 'pattern-matching', 'naivebayes', 'conv-neural-network', 'csv', 'prolog', 'speech-recognition', 'algorithmic-trading', 'svm', 'lstm', 'python', 'linear-regression', 'cors', 'mixture-model', 'authentication', 'apache-spark-mllib', 'ruby', 'training-data', 'kaggle', 'apache-spark', 'topic-modeling', 'precision-recall', 'mfcc', 'genetic-programming', 'cryptography', 'biometrics', 'artificial-intelligence', 'weka', 'kernel', 'perceptron', 'gnuplot', 'hierarchical-clustering', 'pos-tagger', 'feature-selection', 'algorithm', 'ensemble-learning', 'random-forest', 'recommendation-engine', 'node.js', 'convolution', 'differentiation', 'nltk', 'gps', 'pylearn', 'sparse-matrix', 'opencv', 'euclidean-distance', 'dbn', 'c++', 'knn', 'filesize', 'sift', 'ssas', 'summarization', 'macos', 'wordnet', 'cvx', 'feature-detection', 'pam', 'regex', 'azure', 'word2vec', 'k-means', 'datumbox', 'search', 'vector', 'cascade-classifier', 'vgg-net', 'classification', 'logarithm', 'scipy', 'maven', 'cluster-analysis', 'missing-data', 'entity-framework', 'neural-network', 'google-analytics', 'gpu', 'security', 'r', 'nearest-neighbor', 'bayesian-networks', 'rbm', 'pruning', 'document-classification', 'fuzzy-search', 'theano', 'libsvm', 'elki', 'svmlight', 'simplecv', 'probability-density', 'normalization', 'large-files', 'supervised-learning', 'computer-science', 'e-commerce', 'gradient', 'lda', 'machine-learning', 'evolutionary-algorithm', 'vectorization', 'mahout', 'auc', 'python-3.x', 'probability', 'pattern-recognition', 'keyword-search', 'matrix', 'gradient-descent', 'pom.xml', 'cloudera', 'reservoir-sampling', 'matlab', 'arrays', 'glm', 'pmml', 'q-learning', 'measurement', 'detection', 'text-analysis', 'caffe', 'sampling', 'pybrain', 'linear-algebra', 'oauth', 'viola-jones', 'projection', 'calibration', 'data-mining', 'matplotlib', 'pca', 'deep-learning', 'kernel-density', 'information-retrieval', 'office365', 'anpr', 'mapreduce', 'reinforcement-learning', 'cross-validation', 'porter-stemmer', 'liblinear', 'math', 'tokenize', 'image-recognition', 'microsoft-account', 'signal-processing', 'google-analytics-api', 'text-mining', 'statistics', 'unsupervised-learning', 'evaluation', 'bayesian', 'computer-vision', 'text', 'mallet', 'opennlp', 'quadratic-programming', 'nlp', 'php', 'elasticsearch', 'hashmap', 'contour', 'encog', 'text-classification', 'apriori', 'regression', 'scikit-learn', 'linux', 'cluster-computing', 'parameter-passing'}\n"
     ]
    }
   ],
   "source": [
    "# drop all duplicates in posts\n",
    "df = pd.DataFrame(stack_posts[0:500])\n",
    "df = df.drop_duplicates([\"QuestionId\"])\n",
    "df = df.sort_values(by=[\"QuestionId\"])\n",
    "df.duplicated([\"QuestionId\"])\n",
    "\n",
    "\n",
    "# get all tags of questions\n",
    "tag_set = set()\n",
    "tag_list = []\n",
    "for tags in df[\"Tags\"]:\n",
    "    # clean tags from '>' and '<' occurences\n",
    "    tags = re.sub('><', ' ', tags) \n",
    "    tags = re.sub('<|>', '', tags)\n",
    "    # add single tag of tags and add it to lists and sets\n",
    "    for tag in tags.split():               \n",
    "        tag_list.append(tag)\n",
    "\n",
    "tag_set = set(tag_list)\n",
    "# filter term 'machine-learning', because sql export filters for this term\n",
    "tag_list = list(filter(lambda a: a != 'machine-learning', tag_list))\n",
    "tag_Counter = Counter(tag_list)\n",
    "\n",
    "print(tag_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "convinced-japanese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "# clean posts and match words\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "technology_pattern1 = [{'POS': 'PROPN', 'OP': '+'},\n",
    "                       {'POS': 'NUM', 'OP': '?'}\n",
    "                      ]\n",
    "\n",
    "technology_pattern2 = [{'OP': '+', 'POS': 'PROPN'},\n",
    "                       {'TEXT': '-', 'OP': '+'},\n",
    "                       {'POS': 'VERB', 'OP': '+'}\n",
    "                      ]\n",
    "\n",
    "technology_pattern3 = [{'OP': '+', 'POS': 'NOUN'},\n",
    "                       {'TEXT': '-', 'OP': '?'},\n",
    "                       {'POS': 'PROPN', 'OP': '+'}\n",
    "                      ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word_set = set()\n",
    "regex_pattern = '(<(pre|code|blockquote|a|strike)(.|\\n)*?\\/(pre|code|blockquote|a|strike)>)*?|<(p|b|br|br(.|\\n)*?\\/|sub|sup|em|strong|hr|s|i|ol|ul|li|code)*?>|<\\/(p|b|br|sub|sup|em|strong|s|i|ol|ul|li|div|pre|blockquote|a|code)>|<h(.|\\n)*?>(.|\\n)*?<\\/h(.|\\n)*?>*?|(<(img|div|ol|ul|li)(.|\\n)*?\\/*?>)|\\n'\n",
    "matcher.add(\"match_technology1\", [technology_pattern1])\n",
    "matcher.add(\"match_technology2\", [technology_pattern2])\n",
    "matcher.add(\"match_technology3\", [technology_pattern3])\n",
    "\n",
    "for text in stack_posts[\"AnswerBody\"][100:5000]:\n",
    "    text = re.sub(regex_pattern, '', text, flags=re.I)\n",
    "    doc = nlp(text)    \n",
    "    \n",
    "    matches = matcher(doc)\n",
    "        \n",
    "    for match_id, start, end in matches:\n",
    "        word_set.add(doc[start:end])       \n",
    "           \n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term similarity\n",
    "# second try\n",
    "\n",
    "# loop through relavant words and get their vectors\n",
    "technology = []\n",
    "for tag in tag_set:\n",
    "    term_vector = []    \n",
    "    tag_doc = nlp(str(tag))\n",
    "    tag_vector = tag_doc.vector    \n",
    "    \n",
    "    for i,span in enumerate(wordlist):\n",
    "        if span.doc != wordlist[i-1].doc:\n",
    "            span_vector = span.vector        \n",
    "            # add these vectors to term_vector of this word        \n",
    "            if tag_doc.similarity(span) >= 0.7:\n",
    "            #if cosine_similarity([tag_vector], [span_vector])[0][0] >= 0.7:\n",
    "                #technology.append(span)\n",
    "                print(span.doc)\n",
    "                print(span.text)\n",
    "                print(\" \")\n",
    "            \n",
    "            \n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "doc = nlp(\"In Encog SVM is just a classification Scikit-learn or regression Spark MLib model and can be used mostly interchangably C++ with other model types.  I modified the hello Java World XOR example to use it, you can see the results below.This is a decent intro to them:  This is a more basic intro to modeling in general, I wrote it for neural networks, but it applies to SVM as well:\")\n",
    "\n",
    "technology_pattern1 = [{'OP': '+', 'POS': 'NOUN'},\n",
    "                               {'POS': 'PROPN', 'OP': '+'}]\n",
    "\n",
    "matcher.add(\"Propntest\", [technology_pattern1])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    print(doc[start:end].text)    \n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"In Encog SVM is just a classification Scikit-learn or regression Spark MLib model and can be used mostly interchangably C++ with other model types.  I modified the hello Java World XOR example to use it, you can see the results below.This is a decent intro to them:  This is a more basic intro to modeling in general, I wrote it for neural networks, but it applies to SVM as well:\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "    print(token.pos_)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "diverse-minimum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.019616807984967306"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-proceeding",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
