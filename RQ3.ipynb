{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "piano-specialist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded csv data\n"
     ]
    }
   ],
   "source": [
    "# Load csv data\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy \n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "filepath = os.path.join(os.getcwd(), 'QueryResults_sample.csv')\n",
    "\n",
    "stack_posts = pd.read_csv(filepath, sep = \",\")\n",
    "\n",
    "print(\"loaded csv data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "disturbed-brown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scientific-computing', 'cuda', 'data-manipulation', 'reduction', 'backpropagation', 'tic-tac-toe', 'stanford-nlp', 'data-analysis', 'haar-classifier', 'azure-machine-learning-studio', 'dirichlet', 'data-visualization', 'face-recognition', 'smo', 'io', 'audio', 'plot', 'predictionio', 'compilation', 'genetic-algorithm', 'tagged-corpus', 'gaussian', 'java', 'confusion-matrix', 'optimization', 'data-structures', 'grouping', 'scala', 'time-series', 'twitter', 'random', 'mex', 'distributed-computing', 'c#', 'object-recognition', 'r-caret', 'gearman', 'metrics', 'object-detection', 'image', 'pandas', 'indices', 'image-processing', 'ranking', 'orange', 'id3', 'dlib', 'chess', 'feature-extraction', 'tf-idf', 'api', 'logistic-regression', 'numpy', 'test-data', 'decision-tree', 'mysql', 'implementation', 'amazon-web-services', 'collaborative-filtering', 'mathematical-optimization', 'python-2.7', 'treemodel', 'single-sign-on', 'gate', 'black-box', 'prediction', 'hessian-matrix', 'mp3', 'standardized', 'hadoop', 'multilabel-classification', 'arff', 'pattern-matching', 'naivebayes', 'conv-neural-network', 'csv', 'prolog', 'speech-recognition', 'algorithmic-trading', 'svm', 'lstm', 'python', 'linear-regression', 'cors', 'mixture-model', 'authentication', 'apache-spark-mllib', 'ruby', 'training-data', 'kaggle', 'apache-spark', 'topic-modeling', 'precision-recall', 'mfcc', 'genetic-programming', 'cryptography', 'biometrics', 'artificial-intelligence', 'weka', 'kernel', 'perceptron', 'gnuplot', 'hierarchical-clustering', 'pos-tagger', 'feature-selection', 'algorithm', 'ensemble-learning', 'random-forest', 'recommendation-engine', 'node.js', 'convolution', 'differentiation', 'nltk', 'gps', 'pylearn', 'sparse-matrix', 'opencv', 'euclidean-distance', 'dbn', 'c++', 'knn', 'filesize', 'sift', 'ssas', 'summarization', 'macos', 'wordnet', 'cvx', 'feature-detection', 'pam', 'regex', 'azure', 'word2vec', 'k-means', 'datumbox', 'search', 'vector', 'cascade-classifier', 'vgg-net', 'classification', 'logarithm', 'scipy', 'maven', 'cluster-analysis', 'missing-data', 'entity-framework', 'neural-network', 'google-analytics', 'gpu', 'security', 'r', 'nearest-neighbor', 'bayesian-networks', 'rbm', 'pruning', 'document-classification', 'fuzzy-search', 'theano', 'libsvm', 'elki', 'svmlight', 'simplecv', 'probability-density', 'normalization', 'large-files', 'supervised-learning', 'computer-science', 'e-commerce', 'gradient', 'lda', 'machine-learning', 'evolutionary-algorithm', 'vectorization', 'mahout', 'auc', 'python-3.x', 'probability', 'pattern-recognition', 'keyword-search', 'matrix', 'gradient-descent', 'pom.xml', 'cloudera', 'reservoir-sampling', 'matlab', 'arrays', 'glm', 'pmml', 'q-learning', 'measurement', 'detection', 'text-analysis', 'caffe', 'sampling', 'pybrain', 'linear-algebra', 'oauth', 'viola-jones', 'projection', 'calibration', 'data-mining', 'matplotlib', 'pca', 'deep-learning', 'kernel-density', 'information-retrieval', 'office365', 'anpr', 'mapreduce', 'reinforcement-learning', 'cross-validation', 'porter-stemmer', 'liblinear', 'math', 'tokenize', 'image-recognition', 'microsoft-account', 'signal-processing', 'google-analytics-api', 'text-mining', 'statistics', 'unsupervised-learning', 'evaluation', 'bayesian', 'computer-vision', 'text', 'mallet', 'opennlp', 'quadratic-programming', 'nlp', 'php', 'elasticsearch', 'hashmap', 'contour', 'encog', 'text-classification', 'apriori', 'regression', 'scikit-learn', 'linux', 'cluster-computing', 'parameter-passing'}\n"
     ]
    }
   ],
   "source": [
    "# drop all duplicates in posts\n",
    "df = pd.DataFrame(stack_posts[0:500])\n",
    "df = df.drop_duplicates([\"QuestionId\"])\n",
    "df = df.sort_values(by=[\"QuestionId\"])\n",
    "df.duplicated([\"QuestionId\"])\n",
    "\n",
    "\n",
    "# get all tags of questions\n",
    "tag_set = set()\n",
    "tag_list = []\n",
    "for tags in df[\"Tags\"]:\n",
    "    # clean tags from '>' and '<' occurences\n",
    "    tags = re.sub('><', ' ', tags) \n",
    "    tags = re.sub('<|>', '', tags)\n",
    "    # add single tag of tags and add it to lists and sets\n",
    "    for tag in tags.split():               \n",
    "        tag_list.append(tag)\n",
    "\n",
    "tag_set = set(tag_list)\n",
    "# filter term 'machine-learning', because sql export filters for this term\n",
    "tag_list = list(filter(lambda a: a != 'machine-learning', tag_list))\n",
    "tag_Counter = Counter(tag_list)\n",
    "\n",
    "print(tag_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acceptable-statement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "# clean posts and match words\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "technology_pattern1 = [{'POS': 'PROPN', 'OP': '+'},\n",
    "                       {'POS': 'NUM', 'OP': '?'}\n",
    "                      ]\n",
    "\n",
    "technology_pattern2 = [{'OP': '+', 'POS': 'PROPN'},\n",
    "                       {'TEXT': '-', 'OP': '+'},\n",
    "                       {'POS': 'VERB', 'OP': '+'}\n",
    "                      ]\n",
    "\n",
    "technology_pattern3 = [{'OP': '+', 'POS': 'NOUN'},\n",
    "                       {'TEXT': '-', 'OP': '?'},\n",
    "                       {'POS': 'PROPN', 'OP': '+'}\n",
    "                      ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word_set = set()\n",
    "regex_pattern = '(<(pre|code|blockquote|a|strike)(.|\\n)*?\\/(pre|code|blockquote|a|strike)>)*?|<(p|b|br|br(.|\\n)*?\\/|sub|sup|em|strong|hr|s|i|ol|ul|li|code)*?>|<\\/(p|b|br|sub|sup|em|strong|s|i|ol|ul|li|div|pre|blockquote|a|code)>|<h(.|\\n)*?>(.|\\n)*?<\\/h(.|\\n)*?>*?|(<(img|div|ol|ul|li)(.|\\n)*?\\/*?>)|\\n'\n",
    "matcher.add(\"match_technology1\", [technology_pattern1])\n",
    "matcher.add(\"match_technology2\", [technology_pattern2])\n",
    "matcher.add(\"match_technology3\", [technology_pattern3])\n",
    "\n",
    "for text in stack_posts[\"AnswerBody\"][100:5000]:\n",
    "    text = re.sub(regex_pattern, '', text, flags=re.I)\n",
    "    doc = nlp(text)    \n",
    "    \n",
    "    matches = matcher(doc)\n",
    "        \n",
    "    for match_id, start, end in matches:\n",
    "        word_set.add(doc[start:end])       \n",
    "           \n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "downtown-arkansas",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-ca259d902f33>:15: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  if tag_doc.similarity(span) >= 0.7:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For anyone else that finds this. The CUDA dev kit is only needed if you have an Nvidia card on the machines you'll use this on. For most Mac users this likely isn't the case (check your hardware specs). If you don't have that kind of graphics card, you can't use CUDA. Just disable it before compiling Caffe:In your caffe dir, edit the Makefile.configUncomment this line:  to indicate CUDA won't be needed.\n",
      "CUDA\n",
      " \n",
      "CUDA, and GPU computation in general, is fast only if all threads do the same thing (well, technically every 32 threads need to be doing the exact same on most hardware). GPUs have many computation units, but little control flow capabilities.in 3D graphics, you apply the exact same projection to all vertices, then apply the same shaders for all pixels, etc.matrix multiplications: you do the exact same order of operations, just on different parts of the data.This is what the hardware was optimized for. Decision trees (and thus random forests) are not of this kind. You will be taking different if/else branches in each tree. So this is not a good fit for GPUs. You will get horrible branch efficiency. Your performance will drop to less than 1/32 (i.e. 3%) of the theoretical capabilities of your hardware.From Wikipedia CUDA limitations:Same execution path = all 32 threads do the \"if true\" branch, or all 32 do the \"else\" branch.(There is a \"hack\" around this, but it is fairly expensive IMHO, and I don't think it will be fully competitive with approaches that are designed for GPUs e.g. via matrix multiplications right away. I'm too lazy to type it in - you'll find it in GPU literature for sure. It's fairly common to speed up single if operations. While this should be possible for multiple levels of if's, it will be much less effective because the cost is growing exponentially with the depth. Sometimes, either the CUDA compiler or the CPU may be able to optimize this automatically for simple/shallow if cases.)\n",
      "CUDA\n",
      " \n",
      "There's a  that uses the CUDA framework. will never support GPU. From the :\n",
      "CUDA\n",
      " \n",
      "I could resolve the issue (almost) by upgrading from CUDA 6.5 to CUDA 7.5 on the machine which I was doing the above experiments. Now, for most of the time when the program crashes while running the GPU memory gets released. However, still sometimes it does not happen and I have to restart the machine.Also, I would do the followings in order to make sure the program clears the GPU memory when the program successfully runs:\n",
      "CUDA\n",
      " \n",
      "CUDA is for NVidia graphics cards. Therefore you cannot make use of the CUDA built of Caffe in your PC.Still, you could make use of the CPU_ONLY flag during built and get Caffe working on your system. But you can expect the training to be too slow compared to what you should get when using a graphics card.There are various fork projects of Caffe which are made to make it work on other graphics card and are available on Git. But most of these projects are seriously outdated when compared with the master branch of original Caffe.An OpenCL implementation of Caffe can be found .\n",
      "CUDA\n",
      " \n",
      "That's a tough questions. Generally this kinds of machine predict based on Data Mining result form previous data. These data can be so huge that human won't be able to find any patter on that data. In that case machine is far batter than human as they will implement its algorithm and  predict intelligently. But if you ask that do the machines know what is human's choice as human himself is unpredictable :) Yes there are so many researcher working on data mining, autonomous agent or human-agent teamwork. We can see some of by searching on these topics fat Google school-er. Best of luck  \n",
      "Data\n",
      " \n",
      "You can just configure that by using the modules under Data Format Conversions. Have a look  and . Documentation is in progress, unluckily. \n",
      "Data\n",
      " \n",
      "You can read  about Data sources in Power BI.You can connect your Power Bi to: Azure Blob, Azure SQL server, Azure SQL Data Warehouse, Azure Table Storage, or Azure HDInsight.All these PBI sources are also available as outputs to the Writer module in Azure ML. so you can use them to write your results from Azure ML and later read them as input for Power BI\n",
      "Data\n",
      " \n",
      "Q: How should we deal with unreliable data in data scienceA: Use feature engineering to fix unreliable data (make some transformations on unreliable data to make it reliable) or drop them out completely - bad features could significantly decrease the quality of the modelQ: Is there any way to figure out these misstatements and then report the top 10% rich people with better accuracy using Machine Learning algorithms?A: ML algorithms are not magic sticks, they can't figure out anything unless you tell them what you are looking for. Can you describe what means 'unreliable'? If yes, you can, as I mentioned, use feature engineering or write a code which will fix the data. Otherwise no ML algorithm will be able to help you, without the description of what exactly you want to achieveQ: Is there any idea or application in Machine Learning which tries to improve the quality of collected data?A: I don't think so just because the question itself is too open-ended. What means 'the quality of the data'?Generally, here are couple of things for you to consider:1) Spend some time on googling feature engineering guides. They cover how to prepare your data for you ML algorithms, refine it, fix it. Good data with good features dramatically increase the results.2) You don't need to use all of features from original data. Some of features of original dataset are meaningless and you don't need to use them. Try to run gradient boosting machine or random forest classifier from scikit-learn on your dataset to perform classification (or regression, if you do regression). These algorithms also evaluate importance of each feature of original dataset. Part of your features will have extremely low importance for classification, so you may wish to drop them out completely or try to combine unimportant features together somehow to produce something more important.\n",
      "data scienceA\n",
      " \n",
      "This research field is called \"Data Matching\" or \"Record Linkage\". There is a  He also goes deep down into machine learning models and how to improve them from the basic approach like simple string distances (as other answers already suggested).To give you a head start, you can try to compute character n-grams of your titles.For n = 3 and Hugo Boss, you would get Now you can compute the  between two sets of these ngrams. Here, for example between  and :If you don't want to go down the route of implementing all of these things yourself, use . It's also very fast and scales well to billions of documents. \n",
      "Data\n",
      " \n",
      "2>Don't expect to find an algorithm that exactly does what you need.Customize algorithms as adequate for your problem. That is the very story of the Data Science buzz, the need to experiment and customize instead of hoping for a turnkey solution.You have avery specific idea of what you need. You will have to put this idea into code and plug it into some algorithm. For example, consider complete linkage clustering with maximum norm. It probably is what you explained above, but I don't think it will be useful.\n",
      "Data\n",
      " \n",
      "If there are not so many blobs, you could just add multiple readers with each map to one of your input blobs. Then use modules under \"Data Transformation\" -> \"Manipulation\" to do things like \"Add Rows\" or \"Join\".\n",
      "Data\n",
      " \n",
      "A good recap can be found , section 1 on Data Augmentation: so namely flips, random crops and color jittering and also lighting noise: proposed fancy PCA when training the famous Alex-Net in 2012. Fancy PCA alters the intensities of the RGB channels in training images.Alternatively you can also have a look at the Kaggle Galaxy Zoo challenge: the winners wrote a . It covers the same kind of techniques:rotation,translation,zoom,flips,color perturbation.As stated they also do it \"in realtime, i.e. during training\".For example here is a practical   by Facebook (for  training).\n",
      "Data\n",
      " \n",
      "When you say \"normalize\" labels, it is not clear what you mean (i.e. whether you mean this in a statistical sense or something else). Can you please provide an example? On Making labels uniform in data analysisIf you are trying to neaten labels for use with the  function, you could try the  function to shorten them, or the  function to align them better. The  function works well for rounding labels on plot axes. For instance, the base function  for drawing histograms calls on Sturges or other algorithms and then uses  to choose nice bin sizes.The  function will standardize values by subtracting their mean and dividing by the standard deviation, which in some circles is referred to as normalization. On the reasons for scaling in regression (in response to comment by questor). Suppose you regress Y on covariates X1, X2, ... The reasons for scaling covariates Xk depend on the context. It can enable comparison of the coefficients (effect sizes) of each covariate. It can help ensure numerical accuracy (these days not usually an issue unless covariates on hugely different scales and/or data is big). For a readable intro see . For a mathematically intense discussion see .In particular, in Bayesian regression, rescaling is advisable to ensure convergence of MCMC estimation; e.g. see . \n",
      "data analysisIf\n",
      " \n",
      "There has been many sharedTasks in NLP community on textual similarity/entailment (STS 2015, 2014, 2013, RTE 2010, ...) etc. This is the latest competition:Some of them release the submitted systems or the baseline to use which I assume you can use for your task as well. Have a look at this:\n",
      "NLP\n",
      " \n",
      "The idea I'm suggesting is originated in text-processing, NLP and information retrieval and very widely used in situations where you have sequences of characters/information like Genetic information. Because you have to preserve the sequence, we can use the concepts of n-grams. I'm using bi-grams in the following example, though you can generalize for higher order grams. N-grams help in preserving the sequential pattern in the data. Don't worry - we keep borrowing ideas from other fields of science - even edit distance and dynamic programming were not originally computer science concepts.There are many possible approaches to tackle this problem - each one unique, and there's no right one - atleast there's no sufficient research that proves which one is right. Here's my take on it. So the goal is to create a Bag-Of-Words like vector out of your data strings - and these vectors can be easily fed to any machine learning tool or library for clustering. A quick summary of steps:-Collect bigrams (and unigrams etc)Create a dictionary to get Bag Of Words (Code attached)Create functionality to get vector from a stringLet's get startedSo here These functions would convert the given string to a set of two characters (and single character if there's only one). You can modify them to capture 3-grams or even complete set of all possible uni-, bi- and tri-grams. Feel free to experiment.Now how to convert strings to vectors? We'll define a function that will convert string to vectors taking care of how many times a particular n-gram appears. This is called BAG OF WORDS. Here, these are BAGS OF SCREENS. Following two functions help you with that:Voila! We're done. Now feed your data vectors to any K-Means implementation of your choice. I used SKLearn. You should rather choose to read strings from a fileAnd then I finally see what my clusters look like using km.labels_ property of KMeans class.Here're your clusters. Look at the console (bottom) window - There are ten clusters.Now you can modify feature generation I wrote in the code and see how your modifications performs. Rather than just bigrams, extract all the possible unigrams, bigrams and trigrams and use them to create BOW. There will be significant difference. You can also use length of the string as a feature. You can also experiment with other algorithms including hierarchical clustering. Be sure to send me updated results after your modifications.Enjoy! \n",
      "NLP\n",
      " \n",
      "It depends what do you mean by term. If - as usual - term is just a word, then a probability model will work the same as... simple tf weighting (even without idf!). Why? Beacause empirical estimator of  is just , and as  is constant, then the weight becomes just , which is simple term frequency. So in this sense, scikit does what you need.Ok, so maybe you want to consider context? Then what kind of context? Do you want to analyze independently  and use it as a weighted sum for ? Then why not ? Why not  etc.? Why not to include some reweighting based on unigrams when bigrams are not available? The answer is quite simple, once you go into using language models as a weighting schemes, amount of possible introductions grows exponentialy, and there is no \"typical\" approach, which is worth implementing as a \"standard\" for a library which is not a NLP library.\n",
      "NLP\n",
      " \n",
      "NLP generally does this with real-time interpretation.  Set a match threshold; when a sequence of motions resolves to a unique gesture and meets the threshold, you interpret that as a gesture.This is simple in description.  In practice, there is a lot of feedback, especially if some gestures are subsets of others, or if the matches are not quite as crisp as we'd like.If you want to use HMM, can you seed it after some training with markers for terminal states?\n",
      "NLP\n",
      " \n",
      "2>Sampled, in both cases means you don't calculate it for all of what's possible as an output (e.g.: if there are too many words in a dictionary to take all of them at each derivation, so we take just a few samples and learn on that for NLP problems). 3>This is the cross entropy and receives logits as inputs and yields what can be used as a loss.3>This is a sampled softmax_cross_entropy_with_logits, so it takes just a few samples before using the cross entropy rather than using the full cross entropy: \n",
      "NLP\n",
      " \n",
      "NLP stand for Natural Language Processing. Thus everything that involves processing natural language can be described as NLP. This is not any strict science, this is just a bag term for everything related to this field. Even if you just map your text to vector space and forget about any language and semantics, you are still processing natural language, thus - NLP.\n",
      "NLP\n",
      " \n",
      "That's a tough questions. Generally this kinds of machine predict based on Data Mining result form previous data. These data can be so huge that human won't be able to find any patter on that data. In that case machine is far batter than human as they will implement its algorithm and  predict intelligently. But if you ask that do the machines know what is human's choice as human himself is unpredictable :) Yes there are so many researcher working on data mining, autonomous agent or human-agent teamwork. We can see some of by searching on these topics fat Google school-er. Best of luck  \n",
      "Data\n",
      " \n",
      "You can just configure that by using the modules under Data Format Conversions. Have a look  and . Documentation is in progress, unluckily. \n",
      "Data\n",
      " \n",
      "You can read  about Data sources in Power BI.You can connect your Power Bi to: Azure Blob, Azure SQL server, Azure SQL Data Warehouse, Azure Table Storage, or Azure HDInsight.All these PBI sources are also available as outputs to the Writer module in Azure ML. so you can use them to write your results from Azure ML and later read them as input for Power BI\n",
      "Data\n",
      " \n",
      "Q: How should we deal with unreliable data in data scienceA: Use feature engineering to fix unreliable data (make some transformations on unreliable data to make it reliable) or drop them out completely - bad features could significantly decrease the quality of the modelQ: Is there any way to figure out these misstatements and then report the top 10% rich people with better accuracy using Machine Learning algorithms?A: ML algorithms are not magic sticks, they can't figure out anything unless you tell them what you are looking for. Can you describe what means 'unreliable'? If yes, you can, as I mentioned, use feature engineering or write a code which will fix the data. Otherwise no ML algorithm will be able to help you, without the description of what exactly you want to achieveQ: Is there any idea or application in Machine Learning which tries to improve the quality of collected data?A: I don't think so just because the question itself is too open-ended. What means 'the quality of the data'?Generally, here are couple of things for you to consider:1) Spend some time on googling feature engineering guides. They cover how to prepare your data for you ML algorithms, refine it, fix it. Good data with good features dramatically increase the results.2) You don't need to use all of features from original data. Some of features of original dataset are meaningless and you don't need to use them. Try to run gradient boosting machine or random forest classifier from scikit-learn on your dataset to perform classification (or regression, if you do regression). These algorithms also evaluate importance of each feature of original dataset. Part of your features will have extremely low importance for classification, so you may wish to drop them out completely or try to combine unimportant features together somehow to produce something more important.\n",
      "data scienceA\n",
      " \n",
      "This research field is called \"Data Matching\" or \"Record Linkage\". There is a  He also goes deep down into machine learning models and how to improve them from the basic approach like simple string distances (as other answers already suggested).To give you a head start, you can try to compute character n-grams of your titles.For n = 3 and Hugo Boss, you would get Now you can compute the  between two sets of these ngrams. Here, for example between  and :If you don't want to go down the route of implementing all of these things yourself, use . It's also very fast and scales well to billions of documents. \n",
      "Data\n",
      " \n",
      "2>Don't expect to find an algorithm that exactly does what you need.Customize algorithms as adequate for your problem. That is the very story of the Data Science buzz, the need to experiment and customize instead of hoping for a turnkey solution.You have avery specific idea of what you need. You will have to put this idea into code and plug it into some algorithm. For example, consider complete linkage clustering with maximum norm. It probably is what you explained above, but I don't think it will be useful.\n",
      "Data\n",
      " \n",
      "If there are not so many blobs, you could just add multiple readers with each map to one of your input blobs. Then use modules under \"Data Transformation\" -> \"Manipulation\" to do things like \"Add Rows\" or \"Join\".\n",
      "Data\n",
      " \n",
      "A good recap can be found , section 1 on Data Augmentation: so namely flips, random crops and color jittering and also lighting noise: proposed fancy PCA when training the famous Alex-Net in 2012. Fancy PCA alters the intensities of the RGB channels in training images.Alternatively you can also have a look at the Kaggle Galaxy Zoo challenge: the winners wrote a . It covers the same kind of techniques:rotation,translation,zoom,flips,color perturbation.As stated they also do it \"in realtime, i.e. during training\".For example here is a practical   by Facebook (for  training).\n",
      "Data\n",
      " \n",
      "When you say \"normalize\" labels, it is not clear what you mean (i.e. whether you mean this in a statistical sense or something else). Can you please provide an example? On Making labels uniform in data analysisIf you are trying to neaten labels for use with the  function, you could try the  function to shorten them, or the  function to align them better. The  function works well for rounding labels on plot axes. For instance, the base function  for drawing histograms calls on Sturges or other algorithms and then uses  to choose nice bin sizes.The  function will standardize values by subtracting their mean and dividing by the standard deviation, which in some circles is referred to as normalization. On the reasons for scaling in regression (in response to comment by questor). Suppose you regress Y on covariates X1, X2, ... The reasons for scaling covariates Xk depend on the context. It can enable comparison of the coefficients (effect sizes) of each covariate. It can help ensure numerical accuracy (these days not usually an issue unless covariates on hugely different scales and/or data is big). For a readable intro see . For a mathematically intense discussion see .In particular, in Bayesian regression, rescaling is advisable to ensure convergence of MCMC estimation; e.g. see . \n",
      "data analysisIf\n",
      " \n",
      "I'm of the opinion that you need to train some Haar detector per traffic sign's shape (one for triangular warning signs, an other one for circular signs, etc.). As a result of detection you will have some candidates for further processing and should be decided whether a candidate is true positive or not.If it is true positive: additional classification needs to recognize the type of a known shape. This classification can be an ANN algorithm or SVMs.Answers to your questions:It's strongly depends on the positive/negative database, the features used for training (Haar, LBP, HoG), but I think this cascade structure can be useful for your purposes.Partly answered above. For the negatives: you should use a very different set of images. E.g. landscapes, animals, etc. It's important to collect a large database because most of the negatives will be rejected during the first steps of the training.You need to use same scale (for positives) during the training and it is recommended to use some global transformation for reducing the effect of different lighting conditions. But you don't need to remove the background, just crop the images along the border of signs.\n",
      "Haar\n",
      " \n",
      "Haar is better for human face. Hog with SVM is classic for human detection and there've been lots of source and blogs about them, it's not hard to train a classifier. For your scene, I think 'head and shoulder' is better than 'head alone'. But your multi-view samples increase the difficulty. A facing cam would be better. Add more hard neg samples if you always have much more false positive alarms.This paper may help:\n",
      "Haar\n",
      " \n",
      "Normally, with Haar cascade, the result is very different when we change the parameters when we train the classifier. In my case, that object is very simple, but it cannot detect too:When I changed the parameters, it can detect very nice.You can have a look here: Or more special and more professional, you can research about Bag of Visual Words (SIFT/SURF + SVM or SGD).Personally, I think you don't need to use the method complex for person detection.Regards,\n",
      "Haar\n",
      " \n",
      "If I got it right, you could use a regular expression with no problem. For example, with the input samples you gave, you could use a regex like:The first part gets the DF- or df-, which may or may not occur: ([A-Z|a-z]{2,2}-){0,1}The second part gets the first group of digits: \\d{6,6}Then, we say that it could have a dash: \\-{0,1}Finally, we get the last group of digits: \\d{4,4}This would cover the values you provided as sample, but you also could write other expressions to fetch other values.Or, maybe, you could use something like . From what I know, this could help you too. \n",
      "A-Z|a\n",
      " \n",
      "I started to improve the solution by transforming the  into a smarter, dichotomous way of finding the maximumThen I realized, after 2 hours of work, that getting all the accuracies were far more cheaper than just finding the maximum !! (Yes it is totally counter-intuitive).I wrote a lot of comments here below to explain my code. Feel free to delete all these to make the code more readable.The all process is just a single loop, and the algorithm is just trivial.In fact, the stupidly simple function is 10 times faster than the solution proposed before me (commpute the accuracies for ) and 30 times faster than my previous smart-ass-dychotomous-algorithm...You can then easily compute ANY KPI you want, for example :If you want to test it :Enjoy ;)\n",
      "-intuitive).I\n",
      " \n",
      "I've been messing around with this same problem lately, and think I came up with a decent solution. Check out  and let me know if it helps. By specifying that continuous features need to have a prefix that follow the regex '[A-Z]+=', I was able to set up a pipe that (loosely) sets  to the double that follows it. I think I still have some tests to do to verify, but maybe it'll give you some inspiration. \n",
      "A-Z]+=\n",
      " \n",
      "Majority of machine learning algorithms work with numbers, so you can to transform your categorical values and string into numbers.Popular python machine-learning library scikit-learn has the . With 'yes/no' everything is easy - just put 0/1 instead of it.Among many other important things it explains the process of  using their .When you work with text, you also have to transform your data in a suitable way. One of the common feature extraction strategy for text is a  score, and I wrote a .\n",
      "scikit-learn\n",
      " \n",
      "I'm not 100% sure, but I think scikit-learn.naive_bayes requires a purely numeric feature vector instead of a mixture of text and numbers. It looks like it crashes when trying to \"divide\" a unicode string by a long integer.I can't be much help with finding numeric representations for text, but  might be a good start.\n",
      "scikit-learn.naive_bayes\n",
      " \n",
      "1>Previous answers do not specify how to handle the multi-label case so here is such a version implementing three types of multi-label f1 score in tensorflow: micro, macro and weighted (as per scikit-learn)Update (06/06/18): I wrote a  about how to compute the streaming multilabel f1 score in case it helps anyone (it's a longer process, don't want to overload this answer)3>outputs:\n",
      "scikit-learn)Update\n",
      " \n",
      "You didn't show which kind of model you use to me, but I assume that you initialized your model as . In a  model you can only stack one layer after another - so adding a &quot;short-cut&quot; connection is not possible.For this reason authors of Keras added option of building &quot;graph&quot; models. In this case you can build a graph (DAG) of your computations. It's a more complicated than designing a stack of layers, but still quite easy.Check the documentation  to look for more details.\n",
      "quot;short-cut&quot\n",
      " \n",
      "take a simple linear classification problem-y={0 if 5x-3>=0 else 1}here y is class, x is feature, 5,3 are parameters.\n",
      "problem-y={0\n",
      " \n",
      "I am not sure if I understand the question but if you are looking for methods that can capture more (or less) than a predetermined number of clusters, I would suggest you look into a non-parametric clustering algorithm such as Dirichlet Process Mixture.\n",
      "Dirichlet\n",
      " \n",
      "That's a tough questions. Generally this kinds of machine predict based on Data Mining result form previous data. These data can be so huge that human won't be able to find any patter on that data. In that case machine is far batter than human as they will implement its algorithm and  predict intelligently. But if you ask that do the machines know what is human's choice as human himself is unpredictable :) Yes there are so many researcher working on data mining, autonomous agent or human-agent teamwork. We can see some of by searching on these topics fat Google school-er. Best of luck  \n",
      "Data\n",
      " \n",
      "You can just configure that by using the modules under Data Format Conversions. Have a look  and . Documentation is in progress, unluckily. \n",
      "Data\n",
      " \n",
      "You can read  about Data sources in Power BI.You can connect your Power Bi to: Azure Blob, Azure SQL server, Azure SQL Data Warehouse, Azure Table Storage, or Azure HDInsight.All these PBI sources are also available as outputs to the Writer module in Azure ML. so you can use them to write your results from Azure ML and later read them as input for Power BI\n",
      "Data\n",
      " \n",
      "Q: How should we deal with unreliable data in data scienceA: Use feature engineering to fix unreliable data (make some transformations on unreliable data to make it reliable) or drop them out completely - bad features could significantly decrease the quality of the modelQ: Is there any way to figure out these misstatements and then report the top 10% rich people with better accuracy using Machine Learning algorithms?A: ML algorithms are not magic sticks, they can't figure out anything unless you tell them what you are looking for. Can you describe what means 'unreliable'? If yes, you can, as I mentioned, use feature engineering or write a code which will fix the data. Otherwise no ML algorithm will be able to help you, without the description of what exactly you want to achieveQ: Is there any idea or application in Machine Learning which tries to improve the quality of collected data?A: I don't think so just because the question itself is too open-ended. What means 'the quality of the data'?Generally, here are couple of things for you to consider:1) Spend some time on googling feature engineering guides. They cover how to prepare your data for you ML algorithms, refine it, fix it. Good data with good features dramatically increase the results.2) You don't need to use all of features from original data. Some of features of original dataset are meaningless and you don't need to use them. Try to run gradient boosting machine or random forest classifier from scikit-learn on your dataset to perform classification (or regression, if you do regression). These algorithms also evaluate importance of each feature of original dataset. Part of your features will have extremely low importance for classification, so you may wish to drop them out completely or try to combine unimportant features together somehow to produce something more important.\n",
      "data scienceA\n",
      " \n",
      "This research field is called \"Data Matching\" or \"Record Linkage\". There is a  He also goes deep down into machine learning models and how to improve them from the basic approach like simple string distances (as other answers already suggested).To give you a head start, you can try to compute character n-grams of your titles.For n = 3 and Hugo Boss, you would get Now you can compute the  between two sets of these ngrams. Here, for example between  and :If you don't want to go down the route of implementing all of these things yourself, use . It's also very fast and scales well to billions of documents. \n",
      "Data\n",
      " \n",
      "2>Don't expect to find an algorithm that exactly does what you need.Customize algorithms as adequate for your problem. That is the very story of the Data Science buzz, the need to experiment and customize instead of hoping for a turnkey solution.You have avery specific idea of what you need. You will have to put this idea into code and plug it into some algorithm. For example, consider complete linkage clustering with maximum norm. It probably is what you explained above, but I don't think it will be useful.\n",
      "Data\n",
      " \n",
      "If there are not so many blobs, you could just add multiple readers with each map to one of your input blobs. Then use modules under \"Data Transformation\" -> \"Manipulation\" to do things like \"Add Rows\" or \"Join\".\n",
      "Data\n",
      " \n",
      "A good recap can be found , section 1 on Data Augmentation: so namely flips, random crops and color jittering and also lighting noise: proposed fancy PCA when training the famous Alex-Net in 2012. Fancy PCA alters the intensities of the RGB channels in training images.Alternatively you can also have a look at the Kaggle Galaxy Zoo challenge: the winners wrote a . It covers the same kind of techniques:rotation,translation,zoom,flips,color perturbation.As stated they also do it \"in realtime, i.e. during training\".For example here is a practical   by Facebook (for  training).\n",
      "Data\n",
      " \n",
      "When you say \"normalize\" labels, it is not clear what you mean (i.e. whether you mean this in a statistical sense or something else). Can you please provide an example? On Making labels uniform in data analysisIf you are trying to neaten labels for use with the  function, you could try the  function to shorten them, or the  function to align them better. The  function works well for rounding labels on plot axes. For instance, the base function  for drawing histograms calls on Sturges or other algorithms and then uses  to choose nice bin sizes.The  function will standardize values by subtracting their mean and dividing by the standard deviation, which in some circles is referred to as normalization. On the reasons for scaling in regression (in response to comment by questor). Suppose you regress Y on covariates X1, X2, ... The reasons for scaling covariates Xk depend on the context. It can enable comparison of the coefficients (effect sizes) of each covariate. It can help ensure numerical accuracy (these days not usually an issue unless covariates on hugely different scales and/or data is big). For a readable intro see . For a mathematically intense discussion see .In particular, in Bayesian regression, rescaling is advisable to ensure convergence of MCMC estimation; e.g. see . \n",
      "data analysisIf\n",
      " \n",
      "Now there is a pre-trained sentiment classifier for German text. Hugging Face has released two open-source APIs as follows.\n",
      "Face\n",
      " \n",
      "First, SMO is a fairly complicated algorithm - it is not one easy to debug in this kind of format. Second, you are starting too high up in your testing. Some advice to help you debug your problems. 1) First, switch to using the linear kernel. Its much easier for you to compute the exact linear solution with another algorithm and compare what you are getting with the exact solution. This way its only the weight vectors and bias term. If you stay in the dual space, you'll have to compare all the coefficients and make sure things stay in the same order. 2) Start with a much simpler 2D problem where you know what the general solution should look like. You can then visualize the solution, and watch as it changes at each step - this can be a visual tool to help you find where something goes wrong. \n",
      "SMO\n",
      " \n",
      "Using any (other) database would probably not help, as they are subject to same disk IO and memory limitations as ZODB. If you manage to offload computations to the database engine itself (PostgreSQL + using SQL scripts) it might help, as the database engine would have more information to make intelligent choices how to execute the code, but there is nothing magical here and same things can be most likely done with ZODB with quite ease.Some ideas what can be done: Have indexes of data instead of loading full objects (equal to SQL \"full table scan\"). Keep intelligent preprocesses copies of data: indexes, sums, partials.Make the objects themselves smaller (Python classes have  trick)Use transactions in intelligent fashion. Don't try to process all data in a single big chunk.Parallel processing - use all CPU cores instead of single threaded approachDon't use BTrees - maybe there is something more efficient for your use caseHaving some code samples of your script, actual RAM and Data.fs sizes, etc. would help here to give further ideas.\n",
      "disk IO\n",
      " \n",
      "In this context, cross-entropy is one particular form of a genetic algorithm. Its a much more specific thing than saying \"Genetic Algorithms\" as that covers a huge number of different algorithms. Put simply:Genetic Algorithms is a family of algorithms / one type of approach to optimizationCross-entropy is a specific genetic algorithm. \n",
      "Genetic\n",
      " \n",
      "I'm not an expert in this field but I think you can look at some of the heuristic search algorithms like Genetic Algorithm (GA).Also, every games are different for instance the super mario vs chess game. I'm not sure what game you are after but GA has been successfully implemented to play Super Mario smartly. I think for a start you can develop an AI to play the game first, then meantime collect some data for analytics so that it can play better the next time? Again, I dont know what kind of game you are after so it is  difficult for me to contribute ideas to help you.\n",
      "Algorithm\n",
      " \n",
      "Spiking neural networks are closer models to real neurons in brain. They have the ability of plasticity to change their characteristics over time. Therefore, they tend to mimic the synaptic plasticity in real neurons in brain and change their activations, architecture and outputs of neurons over time. As a small-scale example of these models you can take a look at the neural networks models whose architectures are learned  (Genetic Algorithms for instance). However, due to lack of enough computational power people have not been able to deploy these models in large-scale applications. There is a very small research going on for these models too.Sources    \n",
      "Genetic\n",
      " \n",
      "What you have here is a multi-class classification problem that can be solved with Genetic Programming and related techniques.I suppose that data are those from the well-known Iris data set: If you need a quick start, you can use the source code of my method: Multi Expression Programming (which is based on Genetic Programming) which can be downloaded from here: There is a C++ source name mep_multi_class.cpp in the src folder which can \"solve\" iris dataset. Just call the read_training_data function with iris.txt file (which can also be downloaded from dataset folder from github).Or, if you are not familiar with C++, you can try directly MEPX software which has a simple user-interface: . A project with iris dataset can also be downloaded from github.\n",
      "Genetic\n",
      " \n",
      "If your problem is mostly due to the varying number of samples, interpolation and than re-sampling could indeed be the way to go. However, I would use a less constrained interpolation technique. If you have a large number of points, even linear interpolation would work. Alternatively, you could use Gaussian Process regression which is nicely implemented in scikit learn. You don't need to appply a Gaussian filter first then, since you are using regression. Check this link out for an example of how to apply GPs for regression: \n",
      "Gaussian\n",
      " \n",
      "First of all, Since your data has features that are not informative (same value for all data), I cleaned it a bit:All the rest is not important, and cleaning your code will help you focus on what counts.Now, Gaussian Naive Bayes is all about probability: as you may notice, the classifier tries to tell you that:Because it assumes that both  and  have normal distribution, and the probabilities in this case are very low, the priors you gave it -(1,400) are negligible. You can see the Formula itself .By the way, you can get the exact probabilities:So the classifier is sure that 0 is the right class. Now, lets change a bit the test data:Now we have:So you did nothing wrong, it is all a matter of calculation. By the way, I challenge you to replace  with , and see how the priors change it all. Also, unless you have a very good reason to use here , I would consider using some kind of tree classification, as in my opinion it may suit your problem better.\n",
      "Gaussian\n",
      " \n",
      "In short - it is impossible (as you would achieve infinite noiseless compression). You have to either have varied length representation (or fixed length with length being proportional to maximum number of points) or dealing with \"collisions\" (as your mapping will not be injective). In the first scenario you simply can store coordinates of each point. In the second one you approximate your point clouds with more and more complex descriptors to balance collisions and memory usage, some posibilities are:storing mean and covariance (so basically perofming maximum likelihood estimation over Gaussian families)performing some fixed-complexity density estimation like Gaussian Mixture Model or training a generative Neural Networkuse set of simple geometrical/algebraical properties such as:number of pointsmean, max, min, median distance between each pair of pointsetc.\n",
      "Gaussian\n",
      " \n",
      "Even using a kernel you still work with features, you can simply exploit more complex relations of these features. Such as in your example - polynomial kernel gives you access to low-degree polynomial relations between features (such as squares, or products of features). Gaussian kernel maps your feature vector to the unnormalized Gaussian probability density function. In other words, you map each point onto a space of functions, where your point is now a Gaussian centered in this point (with variance corresponding to the hyperparameter gamma of the gaussian kernel). Kernel is always a dot product between vectors. In particular, in function spaces L2 we define classic dot product as an integral over the product, sowhere  are Gaussian distributions.Luckily, for two Gaussian densities, integral of their product is also a Gaussian, this is why gaussian kernel is so similar to the pdf function of the gaussian distribution.As mentioned before, kernel is a dot product, and dot product can be seen as a measure of similarity (it is maximized when two vectors have the same direction). However it does not work the other way around, you cannot use every similarity measure as a kernel, because not every similarity measure is a valid dot product.\n",
      "Gaussian\n",
      " \n",
      "In Java, you should use .\n",
      "Java\n",
      " \n",
      "I solved the problem by resorting to a completely different approach. Instead of using the already available Java SDK, I made an HTTP connection to the endpoint of URLGetRankedNamedEntities API, and retrieved the response.Here is a code sample that demonstrates how to do this-Similar endpoints are avaliable for other APIs as well, which can found .\n",
      "Java\n",
      " \n",
      "If you want to build this system in Java, I suggest you use Weka, which is a machine learning software similar to sklearn. Here is a simple tutorial about text classification with Weka:You can download Weka from:\n",
      "Java\n",
      " \n",
      "You can use the the PhP SDK to send events to PredictionIO. Here is a quickstart on how to setup the ecommerce engine with an app. eIn your case, each app would have its own access key and you need to setup an engine for each app if their data don't mix together - i.e. if they have different products and users. -Isabelle \n",
      "PhP SDK\n",
      " \n",
      "2>3>If your data is dense, a dense representation needs  bytes for your data (i.e. usually  bytes). A sparse representation usually needs . Depending on your programming language and code quality, it can also be much more due to memory management overhead. A typical Java implementation adds 8 bytes of overhead, and will round to 8 bytes size; so sparse vectors may easily use  bytes. then.If your sparsity is 1, this means a sparse representation needs 50% more memory. I guess the memory tradeoff in practise will be somewhere around 50% sparsity; and if your implementation isn't carefull optimized, maybe even 30% - so 1 out of 3 values should be a zero.Memory consumption is usually a key problem. The more memory you use, the more pagefaults and cache misses your CPU will have, which can have a big impact on performance (which is why e.g. BLAS perform large matrix multiplications in block sizes optimized for your CPU caches).3>Dense vector code (e.g. BLAS) is usually much better optimized than sparse operations. In particular, SIMD (single instruction, multiple data) CPU instructions usually only work with dense data.3>Many algorithms may need random access to vectors. If your data is represented as a  array, random access is . If your data is a sparse vector, random access usually is , i.e. you will have to scan the vector to check if there is a value present. It may thus be beneficial to transpose the matrix for some operations, and work with sparse columns instead of sparse rows.On the other hand, some algorithms may exactly benefit from this. But many implementations have such optimizations built in, and will take care of this automatically. Sometimes you also have different choices available. For example APRIORI works on rows, and thus will work well with row-sparse data. Eclat on the other hand is an algorithm to solve the same problem, but it first transforms all data into a row-sparse form, then even computes column differences to further optimize.3>Code to process sparse data usually is much more complex. In particular, it cannot make use of SSE and similar fast CPU instructions easily. It is one of the reasons why sparse matrix multiplications are much slower than dense operations - optimizing these operations without knowing certain characteristics of your data is surprisingly hard. :-(\n",
      "Java\n",
      " \n",
      "I agree with @lejlot that there is no silver bullet method to solve your problem.  However, I believe this answer can get you started thinking about to handle at least the numerical fields in your data set.For the numerical fields, you can make use of the Java  class and map a given number to another obfuscated value.  The trick here is to make sure that you map the same numbers to the same new obfuscated value.  As an example, consider your credit card data, and let's assume that each card number is 16 digits.  You can load your credit card data into a  and iterate over it, creating a new proxy for each number:After this, any time you need to use a credit card  you would access it via  to use the obfuscated value.You can follow a similar plan for the IP addresses.\n",
      "Java\n",
      " \n",
      "See here Java code with slides and textual dataset. It trains a machine, loads the model and classify real time unseen textual data. Very easy to follow\n",
      "Java\n",
      " \n",
      "If you want to apply a top-down clustering approach you could easily distribute it, related article: Long story short (quote from other article): After your first node split, each node created can be shipped to a distributed process to be split again and so on... Each distributed process needs only to be aware of the subset of the dataset it is splitting. Only the parent process is aware of the full dataset.Bottom-up approach is much harder to distribute and I won't try to suggest anything here.But hey, you don't need to write this in Java yourself, Mahout or MLLib libraries already have it, and they support java. And hadoopAnyway, here is your example in Java for hadoop if you want to write it yourself:Finally, a good and big work on comparison of different distributed approaches for hierarchical clustering:\n",
      "Java\n",
      " \n",
      "In Java arrays are reference types.outputs -10 7 6 5 4 3 2 1 8So i mean that you incorrectly creating arraysthey are just references to theta, and by changing their contents you change theta also, +mu-mu = 0. Use clone() methods while copying array.But remember that clone may not work as you expecting in some cases, with simple(non-reference) types it works ideal. Look at this\n",
      "Java\n",
      " \n",
      " (not scikit-learn, but Java) has a number of advanced methods that extract clusters from a hierarchical clustering. They are smarter than just cutting the tree at a particular height, but they can produce a hierarchy of clusters of a minimum size, for example.You could check if these methods work for you.\n",
      "Java\n",
      " \n",
      "Here is the link for the example in Java right from Datumbox, I used this and it was good.\n",
      "Java\n",
      " \n",
      "There is a Java fuzzy matching algorithm, that I found very useful for a similar problem. Where we have a rows of records where each column is a different type of element.For example a list of Users, and having attributes like (name, address, phone) and want to predict which users are similar looking at different formats in which each data is entered.This library groups similar rows together and gives a probability score of rows being similar (by bubbling up the score of each element being similar)This library is suited well for String similarity, but could be used to find similarity between numeric data too. I was able to pass in a list of phone number and was able to identify similar phone numbers.hope this helps\n",
      "Java\n",
      " \n",
      "Confusion Matrix tells us about the distribution of our predicted values across all the actual outcomes.Accuracy_scores, Recall(sensitivity), Precision, Specificity and other similar metrics are subsets of Confusion Matrix.F1 scores are the harmonic means of precision and recall.Support columns in Classification_report tell us about the actual counts of each class in test data.Well, rest is explained above beautifully.Thank you.\n",
      "Confusion Matrix\n",
      " \n",
      "if you have more than one classes in your classifier, you might want to use pandas-ml at that part. Confusion Matrix of pandas-ml give more detailed information. \n",
      "Confusion Matrix\n",
      " \n",
      "That's a tough questions. Generally this kinds of machine predict based on Data Mining result form previous data. These data can be so huge that human won't be able to find any patter on that data. In that case machine is far batter than human as they will implement its algorithm and  predict intelligently. But if you ask that do the machines know what is human's choice as human himself is unpredictable :) Yes there are so many researcher working on data mining, autonomous agent or human-agent teamwork. We can see some of by searching on these topics fat Google school-er. Best of luck  \n",
      "Data\n",
      " \n",
      "You can just configure that by using the modules under Data Format Conversions. Have a look  and . Documentation is in progress, unluckily. \n",
      "Data\n",
      " \n",
      "You can read  about Data sources in Power BI.You can connect your Power Bi to: Azure Blob, Azure SQL server, Azure SQL Data Warehouse, Azure Table Storage, or Azure HDInsight.All these PBI sources are also available as outputs to the Writer module in Azure ML. so you can use them to write your results from Azure ML and later read them as input for Power BI\n",
      "Data\n",
      " \n",
      "Q: How should we deal with unreliable data in data scienceA: Use feature engineering to fix unreliable data (make some transformations on unreliable data to make it reliable) or drop them out completely - bad features could significantly decrease the quality of the modelQ: Is there any way to figure out these misstatements and then report the top 10% rich people with better accuracy using Machine Learning algorithms?A: ML algorithms are not magic sticks, they can't figure out anything unless you tell them what you are looking for. Can you describe what means 'unreliable'? If yes, you can, as I mentioned, use feature engineering or write a code which will fix the data. Otherwise no ML algorithm will be able to help you, without the description of what exactly you want to achieveQ: Is there any idea or application in Machine Learning which tries to improve the quality of collected data?A: I don't think so just because the question itself is too open-ended. What means 'the quality of the data'?Generally, here are couple of things for you to consider:1) Spend some time on googling feature engineering guides. They cover how to prepare your data for you ML algorithms, refine it, fix it. Good data with good features dramatically increase the results.2) You don't need to use all of features from original data. Some of features of original dataset are meaningless and you don't need to use them. Try to run gradient boosting machine or random forest classifier from scikit-learn on your dataset to perform classification (or regression, if you do regression). These algorithms also evaluate importance of each feature of original dataset. Part of your features will have extremely low importance for classification, so you may wish to drop them out completely or try to combine unimportant features together somehow to produce something more important.\n",
      "data scienceA\n",
      " \n",
      "This research field is called \"Data Matching\" or \"Record Linkage\". There is a  He also goes deep down into machine learning models and how to improve them from the basic approach like simple string distances (as other answers already suggested).To give you a head start, you can try to compute character n-grams of your titles.For n = 3 and Hugo Boss, you would get Now you can compute the  between two sets of these ngrams. Here, for example between  and :If you don't want to go down the route of implementing all of these things yourself, use . It's also very fast and scales well to billions of documents. \n",
      "Data\n",
      " \n",
      "2>Don't expect to find an algorithm that exactly does what you need.Customize algorithms as adequate for your problem. That is the very story of the Data Science buzz, the need to experiment and customize instead of hoping for a turnkey solution.You have avery specific idea of what you need. You will have to put this idea into code and plug it into some algorithm. For example, consider complete linkage clustering with maximum norm. It probably is what you explained above, but I don't think it will be useful.\n",
      "Data\n",
      " \n",
      "If there are not so many blobs, you could just add multiple readers with each map to one of your input blobs. Then use modules under \"Data Transformation\" -> \"Manipulation\" to do things like \"Add Rows\" or \"Join\".\n",
      "Data\n",
      " \n",
      "A good recap can be found , section 1 on Data Augmentation: so namely flips, random crops and color jittering and also lighting noise: proposed fancy PCA when training the famous Alex-Net in 2012. Fancy PCA alters the intensities of the RGB channels in training images.Alternatively you can also have a look at the Kaggle Galaxy Zoo challenge: the winners wrote a . It covers the same kind of techniques:rotation,translation,zoom,flips,color perturbation.As stated they also do it \"in realtime, i.e. during training\".For example here is a practical   by Facebook (for  training).\n",
      "Data\n",
      " \n",
      "When you say \"normalize\" labels, it is not clear what you mean (i.e. whether you mean this in a statistical sense or something else). Can you please provide an example? On Making labels uniform in data analysisIf you are trying to neaten labels for use with the  function, you could try the  function to shorten them, or the  function to align them better. The  function works well for rounding labels on plot axes. For instance, the base function  for drawing histograms calls on Sturges or other algorithms and then uses  to choose nice bin sizes.The  function will standardize values by subtracting their mean and dividing by the standard deviation, which in some circles is referred to as normalization. On the reasons for scaling in regression (in response to comment by questor). Suppose you regress Y on covariates X1, X2, ... The reasons for scaling covariates Xk depend on the context. It can enable comparison of the coefficients (effect sizes) of each covariate. It can help ensure numerical accuracy (these days not usually an issue unless covariates on hugely different scales and/or data is big). For a readable intro see . For a mathematically intense discussion see .In particular, in Bayesian regression, rescaling is advisable to ensure convergence of MCMC estimation; e.g. see . \n",
      "data analysisIf\n",
      " \n",
      "I assume that the  is computed using  and counting is an action and will force the computation of all the dependent RDDs.You will actually make more than once the filtering in this case, once for the count and another time for each children dependence. You should cache your RDD to avoid double computation and you only need the last one so you can  the previous one at every stage.See  for more explanationSide note: Spark uses the lazy evaluation model, I think we don't say scala is a lazy language.\n",
      "scala\n",
      " \n",
      "Here is an example in , which I guess is straightforward to port to Scala - the key is the use of .First, we train the model as in the example: is the dimensionality of the word vectors - the higher the better (default value is 100), but you will need memory, and the highest number I could go with my machine was 220. (EDIT: Typical values in the relevant publications are between 300 and 1000)After we have trained the model, we can define a simple function as follows:Now, here are some examples with countries and their capitals:The results are not always correct - I'll leave it to you to experiment, but they get better with more training data and increased vector dimensionality .The  loop in the function removes entries that belong to the input query itself, as I noticed that frequently the correct answer was the second one in the returned list, with the first usually being one of the input terms.\n",
      "Scala\n",
      " \n",
      "You're looking at the wrong documentation. A simple fact that some operation is implemented in a Scala or Java API it doesn't mean it is exposed to PySpark. If you check  you'll see it doesn't provide requested method. and  have been introduced in PySpark 1.6 with .\n",
      "Scala\n",
      " \n",
      "Movies are an excellent use case for classic collaborative filtering: they're items people are interested in for a long time, there are relatively few of them, many people have overlapping interests, and star ratings make sense. News stories are completely different. Rather than collaborative filtering, look at content-based filtering. That's where people's interests align with content identifiers (which could be keywords about the news story, or the publisher, or metadata about time of day or region of the world). View counts are your best bet for information about people's preferences, and they also allow you to use some data mining techniques like association rule mining.While you'll still have the user cold start problem -- where a new user in your system has given you no information about her preferences, unless you bootstrap it from mining her tweets or Facebook interests or something of the sort -- you can avoid the item cold start problem. Instead of relying on news stories read through your community as the only way to get item similarities, you can use another corpus. In particular, try Wikipedia, and check out WikiBrain (). That's an API through which you can get the similarity of one concept to another, and apply it to your recommendation needs.\n",
      "Facebook\n",
      " \n",
      " - implementation from Facebook research, essentially help you achieve what you have been asking for. Since you were asking about gensim, I assume you might be aware of word2vec in gensim.Now word2vec was proposed Mikolov while at google. Mikolov and his team at Facebook ahs come up with fastText, which takes into consideration the word and sub-word information. It also allows for classification of text.\n",
      "Facebook\n",
      " \n",
      "I'm not sure machine learning algorithms will help you if the only data you have is the predictions you parse from predictions on Twitter. You'd need to know the length of previous games and have a set of attributes regarding those past games. That way your learning algorithm could learn from the games which you already know the length of.\n",
      "Twitter\n",
      " \n",
      "What you want is a geolocation system that works with informal text input. I have a previously used a  trained on Twitter data.To solve your problem, you need training data in the form of:If you have access to such data (e.g. using the addresses which can be geolocated) then you can train a text-based classifier that given a new informal address can predict where on the map it points to. In your case every village becomes a class label. You can use scikit-learn to train the classifier.\n",
      "Twitter\n",
      " \n",
      "Random Forest select the best features for your classification task using information gain. The classifier works for multiple feature sources and also types. For example, you can combine continuous attributes and discrete ones.The training time is a little longer since you have to iterate all features multiple times but the memory performance and the classification speed is pretty good. \n",
      "Random\n",
      " \n",
      "This isn't really a programming question, but anyway:If your goal is prediction, as opposed to text classification, usual methods are backoff models () and interpolation/smoothing, e.g. . More complicated models like Random Forests are AFAIK not absolutely necessary and may pose problems if you need to make predictions quickly. If you are using an interpolation model, you can still tune the model parameters (lambda) using a held out portion of the data.Finally, I agree with NEO on the reading part and would recommend \"Speech and Language Processing\" by Jurafsky and Martin.\n",
      "Random\n",
      " \n",
      "No decision trees approach or Random Forests for that matter don't really care whether they are dealing with continuous data  or discrete data. So even if you don't standardize it wont be a issue.\n",
      "Random\n",
      " \n",
      "I get a similar situation with the too-large Random Forest model. The problem was that trees were too deep, and take a lot of memory. To deal with it, I set  and it reduces the memory. I even write down about it in . In the article, I was using 32k rows dataset with 15 columns. Setting  decreases memory consumption 66 times and keep similar performance (in the article the performance even increases).\n",
      "Random\n",
      " \n",
      "The comparison between the two is a bit pointless because Random Forest is a method of combining multiple Random Trees (thus - Forest) into one big classifier using even more randomization (selection of random samples with replacement for training each tree plus random selection of features which tree can use to perform split). In other words - RF is an ensemble method usually applied to Random Tree. There is no point in comparing them as comepetetice methods because they are not. Random Forest should be compared with other ensemble methods such as AdaBoost etc. and Random Tree with basic, simple classifiers such as Perceptron (although it is from different family of models).\n",
      "Random\n",
      " \n",
      "1) Random Forests Random forests is a idea of the general technique of random decision forests that are an ensemble learning technique for classification, regression and other tasks, that control by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests accurate for decision trees' habit of overfitting to their training set. The first algorithm for random decision forests was created by Tin Kam Ho using the random subspace method which, in Ho's formulation, is a way to implement the \"stochastic discrimination\" approach to classification proposed by Eugene Kleinberg. An extension of the algorithm was developed by Leo Breiman[5] and Adele Cutler,[2] and \"Random Forests\" is their trademark[3].The extension combines Breiman's \"bagging\" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman[4] in order to construct a collection of decision trees with controlled variance. 2) Random Tree Random Tree is a supervised Classifier; it is an ensemble learning algorithm that generates lots of individual learners. It employs a bagging idea to construct a random set of data for constructing a decision tree. In standard tree every node is split using the best split among all variables. In a random forest, every node is split using the best among the subset of predicators randomly chosen at that node. Random trees have been introduced by Leo Breiman and Adele Cutler.The algorithm can deal with both classification and regression problems. Random trees is a group (ensemble) of tree predictors that is called forest. The classification mechanisms as follows: the random trees classifier gets the input feature vector, classifies it with every tree in the forest, and outputs the class label that received the majority of votes. In case of a regression, the classifier reply is the average of the responses over all the trees in the forest. Random Trees are essentially the combination of two existing algorithms in Machine Learning: single model trees are merged with Random Forest ideas. Model trees are decision trees where every single leaf holds a linear model which is optimised for the local subspace explained by this leaf. Random Forests have shown to improve the performance of single decision trees considerably: tree diversity is created by two ways of randomization[4,6,11]. First the training data is sampled with replacement for each single tree like in Bagging. Secondly, when growing a tree, instead of always computing the best possible split for each node only a random subset of all attributes is considered at every node, and the best split for that subset is computed. Such trees have been for classification Random model trees for the first time combine model trees and random forests. Random trees uses this produce for split selection and thus induce reasonably balanced trees where one global setting for the ridge value works across all leaves, thus simplifying the optimization procedure. [1] [2] [3] [4] [1]N. Landwehr, M. Hall, and E. Frank, Logistic model trees, Mach. Learn., vol. 59, no. 12, pp. 161-205, 2005. [2]Breiman Leo (2001). \"Random Forests\". Machine Learning 45 (1): 532.[3] Liaw, Andy (16 October 2012). Documentation for R package random forest. Retrieved 15 March 2013.  [4]  U.S. trademark registration number 3185828, registered 2006/12/19. [5]Wikipedia contributors, Random tree, Wikipedia, The Free Encyclopedia. Wikimedia Foundation, 13-Jul- 2014 \n",
      "Random\n",
      " \n",
      "I only have a basic knowledge of the type of classifiers and normalization techniques you're using, but the general rule, that I think applies to what you're doing as well, is to do the following.Your classifier is not a Random Forest Classifier. That is only one step of the pipeline that acts as your actual classifier. This pipeline / actual classifier is what you describe:Use a Random Forest Classifier on what you get from the first 2 steps.This pipeline, that encompasses 3 things, is what you're actually using as your classifier.Now, how does a classifier work?You build some state based on the training data.You use that state to make predictions on the test data.So:Your classifier normalizes the incoming features for the training data, so it will normalize those for unseen instances too. To do this, it must use the state it has built during training.For example, if you were doing min-max scaling on your features, your state would store a  and  for each feature . Then, during testing / prediction, you would do min-max scaling for each feature  using the stored  and  values.I'm not sure what you mean by \"normalize continuous numeric features\". Do you mean ? If you build some state for this discretization during training, then you need to find a way to factor that in.Don't you know how many values each category can have beforehand? Usually you do (since categoricals are things like nationality, continent etc. - things you know in advance). If you can get a value for a categorical feature that you haven't seen during training, it begs the question if you should even care about it. What good is a categorical value you've never trained on?Maybe add an \"unknown\" category. I think expanding for a single one should be fine, what good are more going to do if you've never trained on them?What kind of categoricals do you have?I could be wrong, but do you really need one-hot encoding? AFAIK, tree-based classifiers don't seem to benefit that much from it.\n",
      "Random\n",
      " \n",
      "Random Forest and Naive Bayes should be able to handle this for you.  Given the sparsity, I'd go for the Naive Bayes first.  Random Forest would be better if you're looking for combinations.\n",
      "Random\n",
      " \n",
      "If you have to make predictions at each time stamp, then this doesn't become a a time series problem (unless you plan to use the sequence of previous observations to make your next prediction, in which case you will need to train a sequence based model). Assuming you can only train a model based on the final data you observe, there can be many approaches, but I'd recommend you use Random Forest with large number of trees and 3 or 4 variables in each tree. That way even if some variables don't give you the desired input other trees can still make predictions to a fair amount of accuracy. Besides this there can be many ensemble approaches.The way you're currently doing may be a very loose approximation and practical but doesn't make much statistical sense.\n",
      "Random\n",
      " \n",
      "2>As of this very moment, the class weighting for the Random Forest algorithm is still under development (see )But If you're willing to try other classifiers - this functionality . Consider a case where we have 80% positives (label == 1) in the dataset, so theoretically we want to \"under-sample\" the positive class. The logistic loss objective function should treat the negative class (label == 0) with higher weight. Here is an example in Scala of generating this weight, we add a new column to the dataframe for each record in the dataset:  Then, we create a classier as follow:For more details, watch here: 2>A different issue you should check - whether your features have a \"predictive power\" for the label you're trying to predict. In a case where after under-sampling you still have low precision, maybe that has nothing to do with the fact that your dataset is imbalanced by nature.I would do a exploratory data analysis  - If the classifier doesn't do better than a random choice, there is a risk that there simply is no connection between features and class. Perform correlation analysis for every feature with the label.Generating class specific histograms for features (i.e. plotting    histograms of the data for each class, for a givenfeature on the    same axis) can also be a good way to show if afeature discriminates    well between the two classes.Overfitting - a low error on your training set and a high error on your test set might be an indication that you overfit using an overly flexible feature set.Bias variance - Check whether your classifier suffers from a high bias or high variance problem.Training error vs. validation error -  graph the validation error and training set error, as a function of training examples (do incremental learning) If the lines seem to converge to the same value and are close at the end, then your classifier has high bias. In such case, adding more data won't help. Change the classifier for a one that has higher variance, or simply lower the regularization parameter of your current one.If on the other hand the lines are quite far apart, and you have a low training set error but high validation error, then your classifier has too high variance. In this case getting more data is very likely to help. If after getting more data the variance will still be too high, you can increase the regularization parameter.\n",
      "Random\n",
      " \n",
      "Random Forest can measure the relative importance of any feature in a classification task.Usually,  we measure the loss that would be done if we lose the true values of that feature. One feature at a time the values are scrambled and the loss in predictive accuracy is measured.Because this is done every time we build a new decision tree and a Random Forest is composed of several trees the values are reliable.Have a look at this The higher the numbers returned from forest.feature_importances_ mean they are more important in this classification task.However in your case this is not suitable. I suggest trying  and inspect feature_log_prob_ after training. That way you can see the probability of features given a class, P(x_i|y).\n",
      "Random\n",
      " \n",
      "I may have found the origin for this problem. In my dataset I have approx 500000 events and 30 variables, 10 of these variables are factors, and some of them have weakly populated levels in some cases having as little as 1 event. I built several Random Forest models, each time including and extra variable to the model. I started adding to the model the numerical variables without a problem to generate a PMML, the same happened for the categorical variables with all levels largely populated, when I tried to include categorical variables with levels weakly populated I got the error:I suppose that the origin of the problem is that in some situations when building a tree where the levels is weakly populated then there is no split as there is only one case and although the  package knows how to handle these cases, the  package does not.\n",
      "Random\n",
      " \n",
      "It is highly implementation specific but in general randomized algorithms, ran in parallel may behave differently when working with different number of cores (unless one forces synchronization of random number generators, which would slow down the process). So it is something that one should expect - the same applies to Random Forest model etc.\n",
      "Random\n",
      " \n",
      "If you are confident the clusters are meaningful for your particular needs, you could view it as a classification problem.One option would be to apply a feature selection algorithm to rank the features. You could use recursive feature elimination to identify a subset of features that are predictive for the cluster labels.Another good option for interpreting the clusters could be building a decision tree. With decision trees you can see what features are used to best separate the classes (clusters in your case). You could also use an ensemble like Random Forest and ask for feature importance scores.\n",
      "Random\n",
      " \n",
      "You could use Random Forest Classifier to give you a ranking of your features.  You could then select the top x features from this and use it for logistic regression, although Random Forest would work perfectly fine as well.Check out variable importance at \n",
      "Random\n",
      " \n",
      "You could probably use Random Forest for your classification problem. There are basically 3 parameters to deal with data imbalance. Class Weight, Samplesize and Cutoff.Class Weight-The higher the weight a class is given, the more its error rate is decreased.Samplesize- Oversample the minority class to improve class imbalance while sampling the defects for each tree[not sure if Sci-kit supports this, used to be param in R)Cutoff- If >x% trees vote for the minority class, classify it as minority class. By default x is 1/2 in Random forest for 2-class problem. You can set it to a lower value for the minority class.Check out balancing predict error at For the 2nd question if you are using Random Forest, you do not need to keep separate train/validation/test set. Random Forest does not choose any parameters based on a validation set, so validation set is un-necessary.Also during the training of Random Forest, the data for training each individual tree is obtained by sampling by replacement from the training data, thus each training sample is not used for roughly 1/3 of the trees. We can use the votes of these 1/3 trees to predict the out of box probability of the Random forest classification. Thus with OOB accuracy you just need a training set, and not validation or test data to predict performance on unseen data. Check Out of Bag error at  for further study.\n",
      "Random\n",
      " \n",
      "The clue is (at least partly) in the name.A Random Forest uses randomised decision trees, and as such, each time you fit, the result will change. \n",
      "Random\n",
      " \n",
      "This seems true conditioning on your learning procedure, thus in particular - selection of hyperparameters. Thus it does not mean that given different set of hyperparameters the same effect would occur. It only seems that given current setting - rate of convergence is relatively small thus getting to 95% would probably require significant amounts of data.Yes, in general - these kind of curves at least do not reject option to go for higher bias. You clearly overfit towards training set. On the other hand, trees usually do that, thus increasing bias might be hard without changing the model. One option that I would suggest is going for , which is nearly the same as Random Forest, but with randomly chosen threshold instead of full optimization. They have significantly bigger bias and should take these curves a bit closer to each other. Obviously there is no guarantee - as you said, this is data specific, but the general characteristic looks promising (however might require changing the model). \n",
      "Random\n",
      " \n",
      "There are plenty of models which give you ability to weight classes. This in general is better than just oversampling as it directly alternates the objective, not artificially tricks the model to overweight. If you use python, and like tree-based approach, Random Forest in scikit-learn has class-weight capabilities, simply overweight your minority class as long as the desired precision is not obtained.\n",
      "Random\n",
      " \n",
      "In the case of C-SVM, you should use a linear kernel and a very large C value (or nu = 0.999... for nu-SVM). If you still have slacks with this setting, probably your data is not linearly separable.Quick explanation: the C-SVM optimization function tries to find the hyperplane having maximum margin and lowest misclassification costs at the same time. The misclassification costs in the C-SVM formulation is defined by: distance from the misclassified point to its correct side of the hyperplane, multiplied by C. If you increase the C value (or nu value for nu-SVM), every misclassified point will be too costly and an hyperplane that separates the data perfectly will be preferable for the optimization function.\n",
      "C\n",
      " \n",
      "This is a quite vague definition of a problem. The only thing you have is 1000 high-quality photos and 1000 substandard photos. Your application however, is quite concrete, and I doubt (but I'm not sure) that you will find such a software.Without looking to your images and have some tests is also difficult to say if contrast/gamma would be enough to classify them properly.What you can do, if you know a bit of coding in matlab/python/C, is to use some existing libraries to try to solve your problem. I can't help you with that, as this itself is a quite tedious work, but, I can give you some insights.To define your problem you will need:Input: 1000 pro images, 1000 std imagesYou can represent this as 2000 images and a 2000 binary vector (1 for pro, 0 for std)FeaturesImages itself might not give you enough information. What you can do is extract features from images. This step is called feature extraction and is an open research field in Computer Vision. There several feature extractors out there, you can try a couple of the most used ones, such as HoG or SIFT (have a look ).This feature extractors will give you a  numerical vector for each image. With  images, you have a  matrix composed of  images and their descriptor.Classification:Once you managed to extract features from the image, having  and , you can use any machine learning algorithm, such as Deep Neural Networks, Random Forests, Supported Vector Machines, or any other one, to train your data, and classify it later.By putting everything together, you might be able to get decent results.\n",
      "C\n",
      " \n",
      "Pure python code can be really slow - that is why numpy etc. are written in C, Fortran, and Cython.For example an integer in pure python is stored using 12 bytes instead of 8. Builiding a  of integers via  is expected to be slow and expensive.To speed up, try toallocate a numpy vector of integer zeros as desiredinstead of appending 0s and 1s, ignore the zeros and only set the 1salso use the python profiler to identigy where your hotspots are.\n",
      "C\n",
      " \n",
      "-C parameter: C determines how many data samples are allowed to be placed in different classes. If the value of C is set to a low value, the probability of the outliers is increased, and the general decision boundary is found. If the value of C is set high, the decision boundary is found more carefully.C is used in the soft margin, which requires understanding of slack variables.-Soft margin classifier:-slack variables  determine how much margin to adjust.gamma parameter: gamma determines the distance a single data sample exerts influence. That is, the gamma parameter can be said to adjust the curvature of the decision boundary.\n",
      "C\n",
      " \n",
      "I don't know that library, so I can't tell you if this is correctly implemented, but it looks legit.I think your problem lies with activation function - tanh(500)=1 and tanh(1)=0.76. This difference seem too small for me. Try using -1 instead of 500 for testing purposes and normalize your real data to something about [-2, 2]. If you need full real numbers range, try using linear activation function. If you only care about positive half on real numbers, I propose softplus or ReLU. I've checked and all those functions are provided with Keras.You can try thresholding your output too - answer 0.75 when expecting 1 and 0.25 when expecting 0 are valid, but may impact you accuracy.Also, try tweaking your parameters. I can propose (basing on my own experience) that you'd use:learning rate = 0.1lambda in L2 = 0.2number of epochs = 250 and biggerbatch size around 20-30momentum = 0.1learning rate decay about 10e-2 or 10e-3I'd say that learning rate, number of epochs, momentum and lambda are the most important factors here - in order from most to least important.PS. I've just spotted that you're initializing your weights uniformly (is that even a word? I'm not a native speaker...). I can't tell you why, but my intuition tells me that this is a bad idea. I'd go with random initial weights.\n",
      "function - tanh(500)=1\n",
      " \n",
      "There is a technique where you can write you own loss function to focus on the ranking metrics like(AUC, Precision-Recall) rather than the classification losses like(hinge loss or log-loss).Refer section 4( Maximizing Recall at Fixed Precision) of the paperScalable Learning of Non-Decomposable Objectives () for more details.\n",
      "metrics like(AUC\n",
      " \n",
      "I don't know that library, so I can't tell you if this is correctly implemented, but it looks legit.I think your problem lies with activation function - tanh(500)=1 and tanh(1)=0.76. This difference seem too small for me. Try using -1 instead of 500 for testing purposes and normalize your real data to something about [-2, 2]. If you need full real numbers range, try using linear activation function. If you only care about positive half on real numbers, I propose softplus or ReLU. I've checked and all those functions are provided with Keras.You can try thresholding your output too - answer 0.75 when expecting 1 and 0.25 when expecting 0 are valid, but may impact you accuracy.Also, try tweaking your parameters. I can propose (basing on my own experience) that you'd use:learning rate = 0.1lambda in L2 = 0.2number of epochs = 250 and biggerbatch size around 20-30momentum = 0.1learning rate decay about 10e-2 or 10e-3I'd say that learning rate, number of epochs, momentum and lambda are the most important factors here - in order from most to least important.PS. I've just spotted that you're initializing your weights uniformly (is that even a word? I'm not a native speaker...). I can't tell you why, but my intuition tells me that this is a bad idea. I'd go with random initial weights.\n",
      "function - tanh(500)=1\n",
      " \n",
      "Segmentation, as in Image Segmentation means creating parts of an image into segments which are conceptually meaningful or simple for further analysis. Usually we want to locate objects and boundaries in the images. Simplest example is removing background from foreground. You can relate it with edge detection and countour detection. There are various methods to do this like clustering using K-Means, compressing image to reduce texture, edge detection or markov fields.Classification is entirely different. In classification, you want to find LABEL of the given data item. The Labels are usually predefined classes or categories - like whether an email is spam or not, or an image contains a human or animal. Decision tree is one of the approaches to do this.There have been experiments on segmenting images with the help of classification algorithms.  (link in reference) tries to create too many segments using cut algorithm and use classification to combine them based on whether it is a good segment based on human intervention or not. Their algorithm didn't perform very well but gave a sign that classification can be used in this task. p.s. Segmentation is more related to clustering than classification.\n",
      "Image\n",
      " \n",
      "Positive image setsYes you should definitely choose images which look more similar to the test images. Negative image setsIt can be any image set however, it is better to include images from the environment where this algorithm is going to be tested without object of interest.GenerallyPlease read my  to some other SO question, it would be useful. Discussion continued in comments, so that might be useful as well.\n",
      "image setsIt\n",
      " \n",
      "Try mapping your dataframe to a list of dictionaries (where each entry represents a column) that represent your data and then write a custom tokenizer function that accepts a dictionary as input and outputs a list of features.In the example below, I create a custom tokenizer that iterates through each of your columns so you can do whatever you want with them inside the function before appending them to your tokens list.  The data is then converted into a list of dictionaries using Pandas.\n",
      "Pandas\n",
      " \n",
      "Pandas version 0.18.0 implemented exactly what you're looking for: the  option. Here's an example:\n",
      "Pandas\n",
      " \n",
      "This is called indicator or dummy variables, and Pandas allows to easily encode such categorical values: Don't forget about , though - algorithms like linear regression rely on independence of variables, while in your case  definitely means . In this case simply remove one dummy variable (e.g. use only  var and not ).\n",
      "Pandas\n",
      " \n",
      "Generally speaking if you have data that can be processed using Pandas data frames and  using Spark seems to be a serious overkill. Still, if you do it probably make more sense to use Spark tools all the way. Lets start with indexing your features:Pipeline defined above will create following data frame:where  should be a valid input for  (see ).You can create label points out of it as follows:\n",
      "Pandas\n",
      " \n",
      "Using s within  should work. The problem here is likely related to the implementation of . Notice that it outputs a single dimensional structure upon each transformation; however, interfaces in scikit-learn generally expect input of a 2D shape, i.e. .Assuming the input to  is a pandas DataFrame, try changing the code to:which makes the transformed output to have a 2D shape.Internally,  uses  to perform combination of feature matrices. This is a minimal example that causes  to complain about dimension mismatch in the way as described in the question:However, this works:since now  has two dimensions.\n",
      "pandas DataFrame\n",
      " \n",
      "The trick is that right after you have trained your model, you know the order of the coefficients:This will print the coefficients and the correct feature. (Tested with pandas DataFrame)If you want to reuse the coefficients later you can also put them in a dictionary:(You can test it for yourself by training two models with the same features but, as you said, shuffled order of features.)\n",
      "pandas DataFrame)If\n",
      " \n",
      "This happened to me. I had a positive correlation but negative weights in linear regression with no possible explanation, as data didn't present collinearity and this was not possible to rationalize in the explanation. It simply didn't make sense.In my case, what was causing this issue was that Pandas dataframe index was messed. After I applied  I had an expected behaviour of variables and the problem was solved.\n",
      "Pandas\n",
      " \n",
      "In order to make your categorical variables usable by the classifier, one possibility is to use  from scikit-learn.You should watch out that no variable has levels with too few occurrences. If you don't want to or cannot check this manually, use a threshold on the variance of variables with .Other possibility if you were using pandas DataFrame as structure,  will build the dummy variables for you.\n",
      "pandas DataFrame\n",
      " \n",
      "Segmentation, as in Image Segmentation means creating parts of an image into segments which are conceptually meaningful or simple for further analysis. Usually we want to locate objects and boundaries in the images. Simplest example is removing background from foreground. You can relate it with edge detection and countour detection. There are various methods to do this like clustering using K-Means, compressing image to reduce texture, edge detection or markov fields.Classification is entirely different. In classification, you want to find LABEL of the given data item. The Labels are usually predefined classes or categories - like whether an email is spam or not, or an image contains a human or animal. Decision tree is one of the approaches to do this.There have been experiments on segmenting images with the help of classification algorithms.  (link in reference) tries to create too many segments using cut algorithm and use classification to combine them based on whether it is a good segment based on human intervention or not. Their algorithm didn't perform very well but gave a sign that classification can be used in this task. p.s. Segmentation is more related to clustering than classification.\n",
      "Image\n",
      " \n",
      "Positive image setsYes you should definitely choose images which look more similar to the test images. Negative image setsIt can be any image set however, it is better to include images from the environment where this algorithm is going to be tested without object of interest.GenerallyPlease read my  to some other SO question, it would be useful. Discussion continued in comments, so that might be useful as well.\n",
      "image setsIt\n",
      " \n",
      "Don't remove the class. Orange does it internally.If you use Test learners, don't use Data sampler. Test learners samples the data.You first line will run cross validation (or whatever you set up in Test Learners). Just remove Data sampler from this pipeline in you're OK.The second line needs to go like this: from data sample, give the Data Sample to Logistic regression and Remaining data to Predictions. Also connect Logistic regression to Predictions. Predictions will show you the predictions for the remaining data, using a model constructed from the data sample. Don't remove classes from Remaining data. Don't worry, Logistic regression won't use it to make predictions. And, besides, you'll be able to compare the actual and the predicted class in Predictions.I don't understand the third and the fourth line. Anyway, the first two represent the two common scenarios.\n",
      "Orange\n",
      " \n",
      "Your question is too general. Orange is just a tool. Yes, it's a friendly tool, but you will need to know something about machine learning before you can use it. There are plenty of free MOOCs on these topics, so pick one that you like.\n",
      "Orange\n",
      " \n",
      "The issue is that your input image (in ) has 3 channels (presumably Red, Green, and Blue), whereas the convolutional filter for layer  (in ) expects 1 input channel.There are at least two possibilities for how to solve this problem:Redefine  to accept 3 input channels:Convert your input image  to have 1 input channel using the  op:\n",
      "Green\n",
      " \n",
      "I'm not sure that it's the only differences between sklearn implementation and ID3 algo, but from what i know you have to change criterion from \"gini\" to \"entropy\" for ID3\n",
      "ID3\n",
      " \n",
      "The documentation says so you probably cannot emulate an ID3 as found in the textbooks.\n",
      "ID3\n",
      " \n",
      "If you use ID3, it will go through the same process for every new \"input\" in order to produce the output/state, where in an FSM you move from state to state in a single step. So, In your case it really seems that using ID3 adds unnecessary compelxity. I would opt for FSM. \n",
      "ID3\n",
      " \n",
      "This situation cannot occur with the ID3 decision-tree learner---regardless of whether it uses information gain or some other heuristic for split selection. (See, for example,  on Wikipedia.)The \"trained tree\" in your example above could not have been returned by the ID3 decision-tree learning algorithm.This is because when the algorithm selects a -valued attribute (i.e. an attribute with  possible values) on which to split the given leaf, it will create  new children (one per attribute value). In particular, in your example above, the node  would have three children, corresponding to attribute values ,, and .It follows from the previous paragraph (and, in general, from the way the ID3 algorithm works) that any well-formed vector---of the form , where  is a value of -th attribute and  is the class value---should be classifiable by the decision tree that the algorithm learns on a given train set.Would you mind providing a link to the software you used to learn the \"incomplete\" trees?To answer your questions:Not that I know of. It doesn't make sense to learn such \"incomplete trees.\" If we knew that some attribute values will never occur then we would not include them in the specification (the file where you list attributes and their values) in the first place.With the ID3 algorithm, you can prove---as I sketched in the answer---that every tree returned by the algorithm will cover all possible combinations.You're using the wrong algorithm. Data has nothing to do with it.There is no such thing as an unclassifiable instance in decision-tree learning. One usually defines a decision-tree learning problem as follows. Given a train set  of examples  of the form  where  is the value of the -th attribute and  is the class value in example , learn a function (represented by a decision tree) , where  is the space of all possible well-formed vectors (i.e. all possible combinations of attribute values) and  is the space of all possible class values, which minimizes an error function (e.g. the number of misclassified examples). From this definition, you can see that one requires that the function  is able to map any combination to a class value; thus, by definition, each possible instance is classifiable. \n",
      "ID3\n",
      " \n",
      "Use the public Api of , this is a .It extracts concepts from a text, so, in order to reduce dimentionality, you could use those concepts, instead of the bag-of-word paradigm.\n",
      "Api\n",
      " \n",
      "The bad news: I'm pretty sure that there is no such library/API, because the things you want are too complicated and (at least now) cannot be done automatically, especially in cases like C: there is too much domain-specific semantics that is very hard to be encoded.The good news: I suppose that 80/20 rule remains true for your case - most tables have clear structure like A or B and you can write simple scripts for extracting values for them, while others have to be done manually. I'd advise to develop such scripts incrementally: start for the case A, then launch program for all tables. For failed tables, choose the simplest cases and adapt the code for them; and so on. I believe this way to be the fastest, although not so exciting.A little more interesting approach for semi-automatic extraction of needed info from tables is described in  (sorry for self-citation). Unfortunately, there is no working library or API, but the idea is rather simple and can be easily coded, I guess.\n",
      "API\n",
      " \n",
      "You can use the the PhP SDK to send events to PredictionIO. Here is a quickstart on how to setup the ecommerce engine with an app. eIn your case, each app would have its own access key and you need to setup an engine for each app if their data don't mix together - i.e. if they have different products and users. -Isabelle \n",
      "PhP SDK\n",
      " \n",
      "Great question.Logistic Regression also assumes the following:That there isn't (or there is little) multicollinearity (high correlation) among the independent variables.Even though LR doesn't require the dependent and independent variables to be linearly related, it does however require that the independent variables to be linearly related to the log odds. The log odds function is simply .\n",
      "Regression\n",
      " \n",
      "There are a couple of things that make this code run slowly. is essentially just syntactic sugar for a  loop over the rows of a column. There's also an explicit  loop over a NumPy array in your function (the  part). Looping in this (non-vectorised) way is best avoided whenever possible as it impacts performance heavily. is looking up  across an entire column for each row of , then returning a sub-DataFrame and then creating a new NumPy array. Repeatedly looking for values in this way is expensive ( complexity each lookup). Creating new arrays is going to be expensive since memory has to be allocated and the data copied across each time.If I understand what you're trying to do, you could try the following approach to get your new column.First use  to group the rows of  by the values in 'session'.  is used to join up the strings for each value:where  is the column from  which contains the values you want to join together into a string.  means that only one pass through the DataFrame is needed and is pretty well-optimised in Pandas. The use of  to join the strings is unavoidable here, but only one pass through the grouped values is needed. is now a Series of strings indexed by the unique values in the 'session' column. This is useful because lookups to Pandas indexes are  in complexity.To match each string in  to the approach value in  you can use . Unlike ,  is fully vectorised and should be very fast:\n",
      "NumPy\n",
      " \n",
      "Not sure if this is your error at all but I got this when I was using regular arrays when I should have been using Numpy.\n",
      "Numpy\n",
      " \n",
      "Matplotlib is a very good library for that task. You can plot histograms, scatter plots and lot of other things. You just have to know what you want and then you can probably do it with that. I use that for similar tasks a lot.[UPDATE]As I said, you can do that with matplotlib. Here is an example from their gallery: It's not so pretty as with the answer in the comment of @lejlot, but still correct.\n",
      "Matplotlib\n",
      " \n",
      "Goal was to achieve similar results using Numpy and Tensorflow. The only change from original answer is  parameter for  api.Initial approach :  - This however does not provide intended results when dimensions are N.Modified approach:  - Always sum on the last dimension. This provides similar results as tensorflow's softmax function.\n",
      "Numpy\n",
      " \n",
      "It's possible to do what you've started in Numpy, but I think it's too low-level for this sort of stuff. I'd suggest you install . Then, you can do the following\n",
      "Numpy\n",
      " \n",
      "If you read the error message you can see that passing single dimensional arrays will soon not be supported. Instead you have to ensure that your single sample looks like a list of samples, in which there is just one. When dealing with NumPy arrays (which is recommended), you can use  however as you're using lists then the following will do: \n",
      "NumPy\n",
      " \n",
      "Not sure if I understand you correctly. Training and test set need to have the exact same format. To test, you just solve the equation for known weights and features (of your test set).In principle, you should generate test and training data together to ensure they're as equal as possible - and then split them into two sets. Generating test data (i.e. the labels) depending on how the decision boundary is set is a very bad idea: The main idea of the test set is to test the current trained boundary against data that follows an unknown, real boundary. By inducing knowledge into the system your test results will badly reflect the real accuracy. \n",
      "test set).In\n",
      " \n",
      "That's a tough questions. Generally this kinds of machine predict based on Data Mining result form previous data. These data can be so huge that human won't be able to find any patter on that data. In that case machine is far batter than human as they will implement its algorithm and  predict intelligently. But if you ask that do the machines know what is human's choice as human himself is unpredictable :) Yes there are so many researcher working on data mining, autonomous agent or human-agent teamwork. We can see some of by searching on these topics fat Google school-er. Best of luck  \n",
      "Data\n",
      " \n",
      "I made an example that contains both missing values in training and the test setsI just picked a strategy to replace missing data with the mean, using the  class. There are other strategies.\n",
      "test setsI\n",
      " \n",
      "You can just configure that by using the modules under Data Format Conversions. Have a look  and . Documentation is in progress, unluckily. \n",
      "Data\n",
      " \n",
      "You can read  about Data sources in Power BI.You can connect your Power Bi to: Azure Blob, Azure SQL server, Azure SQL Data Warehouse, Azure Table Storage, or Azure HDInsight.All these PBI sources are also available as outputs to the Writer module in Azure ML. so you can use them to write your results from Azure ML and later read them as input for Power BI\n",
      "Data\n",
      " \n",
      "Q: How should we deal with unreliable data in data scienceA: Use feature engineering to fix unreliable data (make some transformations on unreliable data to make it reliable) or drop them out completely - bad features could significantly decrease the quality of the modelQ: Is there any way to figure out these misstatements and then report the top 10% rich people with better accuracy using Machine Learning algorithms?A: ML algorithms are not magic sticks, they can't figure out anything unless you tell them what you are looking for. Can you describe what means 'unreliable'? If yes, you can, as I mentioned, use feature engineering or write a code which will fix the data. Otherwise no ML algorithm will be able to help you, without the description of what exactly you want to achieveQ: Is there any idea or application in Machine Learning which tries to improve the quality of collected data?A: I don't think so just because the question itself is too open-ended. What means 'the quality of the data'?Generally, here are couple of things for you to consider:1) Spend some time on googling feature engineering guides. They cover how to prepare your data for you ML algorithms, refine it, fix it. Good data with good features dramatically increase the results.2) You don't need to use all of features from original data. Some of features of original dataset are meaningless and you don't need to use them. Try to run gradient boosting machine or random forest classifier from scikit-learn on your dataset to perform classification (or regression, if you do regression). These algorithms also evaluate importance of each feature of original dataset. Part of your features will have extremely low importance for classification, so you may wish to drop them out completely or try to combine unimportant features together somehow to produce something more important.\n",
      "data scienceA\n",
      " \n",
      "This research field is called \"Data Matching\" or \"Record Linkage\". There is a  He also goes deep down into machine learning models and how to improve them from the basic approach like simple string distances (as other answers already suggested).To give you a head start, you can try to compute character n-grams of your titles.For n = 3 and Hugo Boss, you would get Now you can compute the  between two sets of these ngrams. Here, for example between  and :If you don't want to go down the route of implementing all of these things yourself, use . It's also very fast and scales well to billions of documents. \n",
      "Data\n",
      " \n",
      "2>Don't expect to find an algorithm that exactly does what you need.Customize algorithms as adequate for your problem. That is the very story of the Data Science buzz, the need to experiment and customize instead of hoping for a turnkey solution.You have avery specific idea of what you need. You will have to put this idea into code and plug it into some algorithm. For example, consider complete linkage clustering with maximum norm. It probably is what you explained above, but I don't think it will be useful.\n",
      "Data\n",
      " \n",
      "If there are not so many blobs, you could just add multiple readers with each map to one of your input blobs. Then use modules under \"Data Transformation\" -> \"Manipulation\" to do things like \"Add Rows\" or \"Join\".\n",
      "Data\n",
      " \n",
      "A good recap can be found , section 1 on Data Augmentation: so namely flips, random crops and color jittering and also lighting noise: proposed fancy PCA when training the famous Alex-Net in 2012. Fancy PCA alters the intensities of the RGB channels in training images.Alternatively you can also have a look at the Kaggle Galaxy Zoo challenge: the winners wrote a . It covers the same kind of techniques:rotation,translation,zoom,flips,color perturbation.As stated they also do it \"in realtime, i.e. during training\".For example here is a practical   by Facebook (for  training).\n",
      "Data\n",
      " \n",
      "When you say \"normalize\" labels, it is not clear what you mean (i.e. whether you mean this in a statistical sense or something else). Can you please provide an example? On Making labels uniform in data analysisIf you are trying to neaten labels for use with the  function, you could try the  function to shorten them, or the  function to align them better. The  function works well for rounding labels on plot axes. For instance, the base function  for drawing histograms calls on Sturges or other algorithms and then uses  to choose nice bin sizes.The  function will standardize values by subtracting their mean and dividing by the standard deviation, which in some circles is referred to as normalization. On the reasons for scaling in regression (in response to comment by questor). Suppose you regress Y on covariates X1, X2, ... The reasons for scaling covariates Xk depend on the context. It can enable comparison of the coefficients (effect sizes) of each covariate. It can help ensure numerical accuracy (these days not usually an issue unless covariates on hugely different scales and/or data is big). For a readable intro see . For a mathematically intense discussion see .In particular, in Bayesian regression, rescaling is advisable to ensure convergence of MCMC estimation; e.g. see . \n",
      "data analysisIf\n",
      " \n",
      "You need to use the predict method. After training the tree, you feed the X values to predict their output. output:To get details on the tree structure, we can use Tree structure translated into an \"ASCII art\" picturetree structure as an array.Where: The first node [0] is the root node. internal nodes have left_child and right_child refering to nodes with positive values, and greater than the current node. leaves have -1 value for the left and right child nodes. nodes 1,5,6, 8,10,11,14,15,16 are leaves. the node structure is built using the Depth First Search Algorithm. the feature field tells us which of the iris.data features was used in the node to determine the path for this sample. the threshold tells us the value used to evaluate the direction based on the feature. impurity reaches 0 at the leaves... since all the samples are in the same class once you reach the leaf. n_node_samples tells us how many samples reach each leaf. Using this information we could trivially track each sample X to the leaf where it eventually lands by following the classification rules and thresholds on a script. Additionally, the n_node_samples would allow us to perform unit tests ensuring that each node gets the correct number of samples.Then using the output of tree.predict, we could map each leaf to the associated class. \n",
      "Tree\n",
      " \n",
      "Standardizing the features isn't to make the data fit a normal distribution, it's to put the feature values in a known range that makes it easier for algorithms to learn from the data. This is because most algorithms are not scale/shift invariant. Decision Trees, for example, are both scale and shift invariant, and so doing the normalization has no impact on the performance of the tree. No. That's not a thing. The output is whatever the output is. You do have to make sure the activation function of the final layer of your network can make the predictions you want (i.e.: Sigmoid activation can't output negative values or values > 1). No, they would generally be considered parametric. Parametric / non-parametric doesn't really have a hard definition. People may mean slightly different things when talking about this. Those things have nothing to do with each other at all. That's the very first thing I mention, it's to make learning easier/possible.\n",
      "Decision Trees\n",
      " \n",
      "You can visualize the Tree by using the JFrame in your Java Code. You can find the more help Please share your code for more help. Thanks\n",
      "Tree\n",
      " \n",
      "R-Tree variants are a pretty good choice, but M-trees are a little better for your application, since you only need to compute one distance to determine how close a bounding sphere is to your target point:\n",
      "R-Tree\n",
      " \n",
      "Decision Tree algorithmThis algorithm works for both classifier and Regressor.For classifier demonstration\n",
      "Decision Tree\n",
      " \n",
      "I think you can use datumbox api:Common API URL: API doc PDF: As you can see it used REST  API, so i think it will be not hard to used it.EDIT:Also for this API exits PHP client:\n",
      "PHP\n",
      " \n",
      "You can try aho-corasick finite state machine and augment it with a wildcard. Another option is a suffix tree, i.e. a levensthein distance. You can try my PHP implementation of aho-corasick @ .\n",
      "PHP\n",
      " \n",
      "Currently, you'd have to implement the logic for inserting the query key in the client application that calls the web service. Support for parameterized SQL queries is in our feature backlog.\n",
      "SQL\n",
      " \n",
      "The SQL Database Driver name for Pyodbc should be  since some reasons below.The connection string for SQL Database shown on Azure portal, please see the Fig 1 &amp; Fig 2 below.Fig 1. On Azure old portalFig 2. On Azure new portal (the version 11.0 later then v10.0)According to , please see the Fig 3 below.Fig 3. \n",
      "SQL\n",
      " \n",
      "I got an answer from azure support:So currently it it impossible to connect to SQL server from execute python script module in Azure-ML. If you like to change it, please vote  \n",
      "SQL\n",
      " \n",
      "For my implementation scipy.optimize.fmin_cg also failed with the above-mentioned error in some initial guesses. Then I changed it to the BFGS method and converged. seems that this error in cg is inevitable still as,\n",
      "implementation scipy.optimize.fmin_cg\n",
      " \n",
      "If I got it right, you could use a regular expression with no problem. For example, with the input samples you gave, you could use a regex like:The first part gets the DF- or df-, which may or may not occur: ([A-Z|a-z]{2,2}-){0,1}The second part gets the first group of digits: \\d{6,6}Then, we say that it could have a dash: \\-{0,1}Finally, we get the last group of digits: \\d{4,4}This would cover the values you provided as sample, but you also could write other expressions to fetch other values.Or, maybe, you could use something like . From what I know, this could help you too. \n",
      "A-Z|a\n",
      " \n",
      "I started to improve the solution by transforming the  into a smarter, dichotomous way of finding the maximumThen I realized, after 2 hours of work, that getting all the accuracies were far more cheaper than just finding the maximum !! (Yes it is totally counter-intuitive).I wrote a lot of comments here below to explain my code. Feel free to delete all these to make the code more readable.The all process is just a single loop, and the algorithm is just trivial.In fact, the stupidly simple function is 10 times faster than the solution proposed before me (commpute the accuracies for ) and 30 times faster than my previous smart-ass-dychotomous-algorithm...You can then easily compute ANY KPI you want, for example :If you want to test it :Enjoy ;)\n",
      "-intuitive).I\n",
      " \n",
      "I've been messing around with this same problem lately, and think I came up with a decent solution. Check out  and let me know if it helps. By specifying that continuous features need to have a prefix that follow the regex '[A-Z]+=', I was able to set up a pipe that (loosely) sets  to the double that follows it. I think I still have some tests to do to verify, but maybe it'll give you some inspiration. \n",
      "A-Z]+=\n",
      " \n",
      "Majority of machine learning algorithms work with numbers, so you can to transform your categorical values and string into numbers.Popular python machine-learning library scikit-learn has the . With 'yes/no' everything is easy - just put 0/1 instead of it.Among many other important things it explains the process of  using their .When you work with text, you also have to transform your data in a suitable way. One of the common feature extraction strategy for text is a  score, and I wrote a .\n",
      "scikit-learn\n",
      " \n",
      "I'm not 100% sure, but I think scikit-learn.naive_bayes requires a purely numeric feature vector instead of a mixture of text and numbers. It looks like it crashes when trying to \"divide\" a unicode string by a long integer.I can't be much help with finding numeric representations for text, but  might be a good start.\n",
      "scikit-learn.naive_bayes\n",
      " \n",
      "1>Previous answers do not specify how to handle the multi-label case so here is such a version implementing three types of multi-label f1 score in tensorflow: micro, macro and weighted (as per scikit-learn)Update (06/06/18): I wrote a  about how to compute the streaming multilabel f1 score in case it helps anyone (it's a longer process, don't want to overload this answer)3>outputs:\n",
      "scikit-learn)Update\n",
      " \n",
      "You didn't show which kind of model you use to me, but I assume that you initialized your model as . In a  model you can only stack one layer after another - so adding a &quot;short-cut&quot; connection is not possible.For this reason authors of Keras added option of building &quot;graph&quot; models. In this case you can build a graph (DAG) of your computations. It's a more complicated than designing a stack of layers, but still quite easy.Check the documentation  to look for more details.\n",
      "quot;short-cut&quot\n",
      " \n",
      "take a simple linear classification problem-y={0 if 5x-3>=0 else 1}here y is class, x is feature, 5,3 are parameters.\n",
      "problem-y={0\n",
      " \n",
      "You can use GATE twitter pos model with stanford packageuse  for GATE\n",
      "GATE\n",
      " \n",
      "This is not a complete answer, but it can lead to what you want. The first suggestion is looking at . It provides a great annotation framework and as long as you don't want to program anything for it, it is easy to use. The second thing is to search for summarization plug-ins for GATE. GATE has been around for such a long time that I am sure someone has already implemented a summarization plug-in for it. \n",
      "GATE\n",
      " \n",
      "The following answers how attributes of instances and their positions are converted into machine learning feature vectors:The code that converts a document's text into machine learning features (GATE ML attributes) can be found in  in The code loops over all annotations of type INSTANCE-TYPE and produces a machine learning feature vector for each annotation using all the annotation features listed in the machine learning config xml file (config file creates  in the code).Prior to this process each of the features mentioned in the config file is 'multiplied' in the  creating one feature for all elements specified by . If for example the element  appears in a feature in the config then this is expanded to produce 7 features using a different  element for each.\n",
      "GATE\n",
      " \n",
      "I would suggest you to check file on which training process failed. It may happen that no features were produced by GATE (for example document content is empty and etc).\n",
      "GATE\n",
      " \n",
      "To use a regression model for forecasting, the X's (independent variables) have to be known or created based on assumptions.  If you do not know, or want to estimate, the X's, you should look at a time series based model such as Box Jenkins (a technique to apply an ARIMA model mentioned by @Roland).   \n",
      "Box\n",
      " \n",
      "Using the Prediction API is the most reliable and scaleable than making your own. There is no need to reinvent the wheel.If you were to code your own it would likely be a long complex process with lots of bumps in the road, unless you have an avid interest in learning and coding that system I'd suggest you use the existing tools.Here's an  from google themselves.Here's the .The  with Prediction API.\n",
      "Prediction\n",
      " \n",
      "It looks like you want to perform so-called \"one hot encoding\" of your Prediction factor variable by introducing dummy variables. One way to do so is using the  package. Suppose you have a data frame like this:First make sure you have the caret package installed and loaded.You can then use caret's dummyVars() function to create dummy variables.The first argument to dummyVars(), a formula, tells it to generate dummy variables for the Prediction factor in the date frame df. (levelsOnly = TRUE strips the variable name from the columns names, leaving just the level, which looks nicer in this case.)The dummy variables can then be passed to the predict() function to generate a matrix with the one hot encoded factors. You can then, for example, create a new data frame with the encoded variables instead of the original factor variable:This technique generalises easily to a mixture of numerical and categorical variables. Here's a more general example:All numerical variables remain unchanged, whereas all categorical variables get encoded. A typical situation where this is useful is to prepare data for a machine learning algorithm that only accepts numerical variables, not categorical variables.\n",
      "Prediction\n",
      " \n",
      "In this case you can apply an . The idea is that a sentencegets the following 3-grams:Then you think of it as a probability model .So your work would be:Get a lot of data of n words after each other (e.g. I think  has a download option)For a given set of words, find all n-grams which contain only those wordsFind the most likely combination.Please note:n should be at least 3the bigger n gets, the more likely it gets that you have to \"\" as you don't have data (but the n-gram might exist and make sense)even n=5 is already VERY much data\n",
      "download option)For\n",
      " \n",
      "Try the following-Download the Rtools file which usually contains the zip utility.-Copy all the files in the \"bin\" folder of \"Rtools\" -Paste them in \"~/RStudio/bin/x64\" folder\n",
      "Download\n",
      " \n",
      "Applying MapReduce Jobs on the data to do data analysis.HBase supports two types of read access: table scans by row key and MapReduce jobs. Table scans enable you to retrieve the exact subset of rows you are looking for, and MapReduce jobs enable you to perform analysis across a greater set of data. \n",
      "MapReduce\n",
      " \n",
      "You can write your own MapReduce programs and call the third party machine learning algorithms in mapper and reducer. There are several open source machine learning libraries are available like Weka, open nlp, ctakes, mallet,uima etc...Another best way is use Spark ML lib or H2O, Oryix on top of Hadoop.Another solution: use PMML and JPMML to integrate the machine learning tools like R, Weka, SAS with Hadoop.You can develop the model using any one of the tools (R, SAS, Weka) and use the model in MapReduce programs with help of JPMML.\n",
      "MapReduce\n",
      " \n",
      "As your model is taking too long to train, I think you should first try and understand how spark actually benefits the model training part. As per ,Many common machine learning algorithms apply a function repeatedly to the same dataset to optimize a parameter (e.g., through gradient descent). While each iteration can be expressed as a MapReduce/Dryad job, each job must reload the data from disk, incurring a significant performance penaltySpark mllib's libraries remove this performance penalty by caching the data in memory during the first iteration. So subsequent iterations are extremely quick compared to the first iteration and hence, there is a significant reduction in model training time. I think, in your case, the executor memory might be insufficient to load a partition of data in memory. Hence contents would be spilled to disk and would need to be fetched from disk again in every iteration, thus killing any performance benefits of spark. To make sure, this is actually the case, you should try and look at the executor logs which would contain some lines like \"Unable to store rdd_x_y in memory\".If this is indeed the case, you'll need to adjust --num-executors, --executor-memory and numPartitions to see which values of these parameters are able to load the entire data into memory. You can try out with a small data set, single executor and a small value of executor memory on your local machine and analyze logs while incrementally increasing executor memory to see at which config the data is totally cached in memory. Once you have the configs for the small data set, you can do the Maths to figure out how many executors with how much memory are required and what should be the number of partitions for the required partition size.I had faced a similar problem and managed to bring down model training time from around 4 hours to 20 minutes by following the above steps.\n",
      "MapReduce\n",
      " \n",
      "3>Classification is about determining a (categorial) class (or label) for an element in a datasetPrediction is about predicting a missing/unknown element(continuous value) of a dataset3>In classification, data is grouped into categories based on a training dataset.In prediction, a classification/regression model is built to predict the outcome(continuous value)3>In a hospital, the grouping of patients based on their medical record or treatment outcome is considered classification, whereas, if you use a classification model to predict the treatment outcome for a new patient, it is considered a prediction.\n",
      "Classification\n",
      " \n",
      "Classification is the prediction of a categorial variable within a predefined vocabulary based on training examples.The prediction of numerical (continuous) variables is called regression.In summary, classification is one kind of prediction, but there are others. Hence, prediction is a more general problem.\n",
      "Classification\n",
      " \n",
      "Classification is the process of identifying the category or class label of the new observation to which it belongs.Predication is the process of identifying the missing or unavailable numerical data for a new observation. That is the key difference between classification and prediction. The predication does not concern about the class label like in classification.\n",
      "Classification\n",
      " \n",
      "Restating, the problem is that the five binary models you've trained are not mutually exhaustive.  There are several possibilities.First of all, do you have a 100% clean classification for each of the five sentiments, or are there some acknowledged classification errors?You need a set that is mutually exclusive and exhaustive.  Your approach suggests, but hardly guarantees, this result.  You might consider an integrated solution that does make this guarantee.  Multi-class SVM is one such, but may not apply well to your situation.If the classes are not 100% accurate, you can easily have all five rejecting a particular observation.  This suggests that your classification algorithms need tuning, or that the data themselves are not as amenable to classification as you would like.You might also check that you've cleaned that data appropriately; a few errors can seriously move the class boundaries.What I suspect is happening is a small-boundary effect: each class, when compared against the combination of the other four, \"pulls in\" its boundaries, leaving unclaimed territory between the final sets.Do you have a way to check the classification parameters after training?  If so, can you visualize the five boundaries selected?  If you do find pathological gaps, are there training parameters you can tune, such as giving a larger epsilon to the training groups?I hope this helps.\n",
      "classification errors?You\n",
      " \n",
      "Classification examples:- Predicting whether a share of a company is good to buy or no given that the previous history of the company, along with the buyer's review on it saying yes or no for buying the share. (Discrete answer: Buy - Yes/No)Regression example:-Predicting the best price at which one should buy the share of a company given that the previous history of the company, along with the buyer's review of the price at which they bought the share in the past. (Continuous answer:- Price range)\n",
      "Classification examples:-\n",
      " \n",
      "An unsupervised clustering technique fails on scanned documents since it fails to grasp the underlying structure and ends up giving non nonsensical clusters. So the approach is fundamentally flawed. However Classification using deep convolutional neural networks, with sufficient data and carefully chosen distinct classes, can outperform OCR techniques if the documents have a distinct structure. \n",
      "Classification\n",
      " \n",
      "If you have a large number of categories, Classification algorithm does not work well. Instead, there is a better approach of doing this. You apply regression algorithm on data and then train offset on those output. It would give you better results. A sample code can be found . \n",
      "Classification\n",
      " \n",
      "I'd recommend you to experiment with both approaches. Dump the data into an ARFF file (similar to a CSV with some header) and open it in Weka (). You will be able to easily explore different scenarios, visualize the dimensionality reduction and even check some feature selection algorithms.\n",
      "ARFF\n",
      " \n",
      "I think that you need an ARFF file without the class attribute as test set. After that, you can use the  filter to add the class label to all the test set instances using a specific classifier.\n",
      "ARFF\n",
      " \n",
      "Feature selection is of crucial importance,it gives information of the relevance of features for your problem.Good theoretical explanation is given in Pattern Recognition by Sergios Theodoridis and Konstantinos Koutroumbas book.I found this simple code exampleResult You can read more [ examples.\n",
      "Pattern\n",
      " \n",
      "MLLib's implementation of SVM is limited to linear kernels, so you're not going to find anything related to kernels. There is some work related to this happening, though, for example .\n",
      "SVM\n",
      " \n",
      "Some implementations of the SVM algorithm do provide probability estimates. However, the SVM does not inherently provide probability estimates. It is a function that is \"tacked on\" after the algorithm was created. These probability estimates are not \"trustworthy\", and if I remember correctly, the ability to compute probability estimates was removed from the Scikit-Learn library a few releases ago for this reason. Still, if you insist on using SVM, look at  from LibSVM. It is the library that OpenCV calls. You can skip the math to get to the tips. The outputs of LibSVM, and hence OpenCV's SVM, are explained in the document. Alternatively, you can choose to use LibSVM instead. This will allow you to get to the probability estimates without recompiling OpenCV (as suggested in your link), but the downside is you will have to pass your data to the appropriate form for LibSVM (i.e., OpenCV's Mat is unlikely to work directly with LibSVM).If you are using a Linear SVM, i.e., SVM with the linear kernel, then you can try replacing it with a Logistic Regression classifier as empirically they behave similarly (both are linear classifiers, just that one uses hinge loss and the other, logistic loss). The probability estimates from Logistic Regression would work.Alternatively, consider using a Random Forest (or its variant, Extremely Randomized Trees) classifier. They also provide probability estimates as the proportion of training samples in a given leaf node reached by your test sample. Having said that, these two classifiers are not based on principled mathematics (although researchers are working on figuring out how they work theoretically), although they have been known to work superbly in many real-world settings (Kinect pose estimation is an example).The thing is coming up with probability estimates is a very hard if your classifier is not designed to do that from the beginning, i.e., not one of those you find from a standard statistical machine learning textbook. It is like pulling numbers out of one's ass. Most algorithm that perform classification simply compute a \"score\" for each category/label for each test sample and go with the one with the \"best\" score. That is much easier to do. For the SVM, it tries to \"translate\" this score to a \"probability\", but it is not \"calibrated\", which effectively makes it useless.You can take a look at this paper:  for more details on how the probabilities are computed for some of these classifiers, and why they need to be calibrated.In general, I would advice taking probability estimates returned by classifier with a grain of salt. If you want them, go with a statistical classifier, e.g., Logistic Regression, not SVM.As for libraries, while OpenCV do provide some machine learning algorithms, they are very limited. Try a proper ML library. I'm assuming you are using C++, so I will recommend taking a look at the free .If you are using Python, or just wish to take a look at tutorials on how to use machine learning algorithms, then check out the excellent .Some general advice on applying machine learning algorithms to industry problems (slides): .\n",
      "SVM\n",
      " \n",
      "If you want to use liblinear for multi class classification, you can use  technique. For more information Look at But if you have large database then use of SVM is not recommended. As Run time complexity of SVM is O(N * N * m) N = number of samples in data m = number of features in data So, alternatively You can use Neural Network. You can start with nntool available in MATLAB. \n",
      "SVM\n",
      " \n",
      "It depends on what you mean by \"the margins\". It also depends on what SVM version you are talking about (separable on non-separable), but since you mentioned libsvm I'll assume you mean the more general, non-separable version.The term \"margin\" can refer to the Euclidean distance from the separating hyperplane to the hyperplane defined by  (or ). This distance is given by . \"Margin\" can also refer to the margin of a specific sample , which is the Euclidean distance of  from the separating hyperplane. It is given by note that this is a signed distance, that is it is negative/positive, depending on which side of the hyperplane the point  resides. You can draw it as a line from the point, perpendicular to the hyperplane.Another interesting value is the slack variable , which is the \"algebraic\" distance (not Euclidean) of a support vector from the \"hard\" margin defined by  (or ). It is positive only for support vectors, and if a point is not a support vector, its  equals 0. More compactly:where  is the label.\n",
      "SVM\n",
      " \n",
      "You can extract features, or you can use pixel intensity values as the features. In this example, they have done the latter. In this case, you end up with a very high number of features that many of them may be not useful. This makes the convergence of the SVM training more difficult, but can be still possible. Based on my personal experience, SVM works better if you extract a lower number of \"good\" features that best describe your data. However, in recent years, it has been shown that state-of-the-art estimators like deep neural networks (when used instead of SVM) can perform very well with only using the pixel intensity values as features. This has eliminated the need for feature extraction in the methods that has led to state-of-the-art results on public data sets (like )\n",
      "SVM\n",
      " \n",
      "So, the vocabulary has been already created (e.g. by ) and you start to train you SVM classifier, right?At this point you have a feature detector, extractor, matcher and a BOW image descriptor extractor (to compute an image descriptor using the bag of visual words) such as:First of all we need to scour the training set for our histograms:Do the same for  except that  will have zero values, such as:Note how I build the samples and the labels, I marked the positive samples with labels '1', and then the negatives with label '0'. So we have the training data for each class (here for positives and negatives) in . Lets's get training:Then testing let's get testing the classifier:Is it working?Update #1:Here I made a simple example about BOW+SVM under OpenCV 3.0:This works me fine for classifying bottles of Coca Cola / Pepsi. I also published the binaries so you can have a try on your database. Hope it works :)\n",
      "SVM\n",
      " \n",
      "Well, it depends on exactly what one means by \"duplicating the data\". If one is exactly duplicating the whole data set a number of times, then methods based on maximum likelihood (as with many models in common use) must find exactly the same result since the log likelihood function of the duplicated data is exactly a multiple of the unduplicated data's log likelihood, and therefore has the same maxima. (This argument doesn't apply to methods which aren't based on the likelihood function; I believe that CART and other tree models, and SVM's,  are such models. In that case you'll have to work out a different argument.)However, if by duplicating, one means duplicating the positive examples in a classification problem (which is common enough, since there are often many more negative examples than positive), then that does make a difference, since the likelihood function is modified.Also if one means bootstrapping, then that, too, makes a difference.PS. Probably you'll get more interest in this question on stats.stackexchange.com.\n",
      "SVM\n",
      " \n",
      " has an SVM implementation (not the exact same algorithm used by LibSVM, but solves the same problem) as well as many other algorithms, GridSearch, and feature selection methods. bias note: I'm the author of the library. \n",
      "SVM\n",
      " \n",
      "The model selection process in SVM which helps you to select the best model, based on different parameters of function. In LibSVM library, model selection is done with the use of cross validation method. What it does is partitions your training data in several subsets and trains the model with different parameters every time with every data in order to improve the accuracy. this also eliminates the . \n",
      "SVM\n",
      " \n",
      "First, your approach is not good. SVM is trying to say if the image is the same (in some size, as you said), so not if your image is containing an object in some region. So you need detect the object in the image and then classify it with the SVM. In your second image your iPod is too small, and there is too much information around it. You'll need a detector ().Or you can train a BOW (see  for better understanding), and this will be able to say if your image contains the object or not. Or try to use the feature detector, descriptor extractor and matching for finding the same object (as ).\n",
      "SVM\n",
      " \n",
      "Chapter 2.1 Categorical Features:You'll find many more if you search for \"svm Categorical Features\"\n",
      "svm\n",
      " \n",
      "SVM is not an online algorithm. This means that it doesn't support incremental learning which is what you are trying to do. So if you want to add new points you must retrain the model again.There are some variations of SVM that support online learning (i.e Pegasos SVM), but I don't think OpenCV implement them.\n",
      "SVM\n",
      " \n",
      "So you want to grid-search the C in the SVM for each number of features in the RFE? Or for each CV iteration in the RFECV? From your last sentence, I guess it is the former.You can do  to achieve that,though I'm not sure that is actually a helpful thing to do.I don't think the second is possible right now (but soon). You could do , but that nests two sets of cross-validation inside each other.Update:GridSearchCV has no , so the first one fails.A simple fix:And then use that instead.\n",
      "SVM\n",
      " \n",
      "In a multi-class SVM that uses the one-vs-one strategy, the problem is divided into a set of smaller binary problems. For example, if you have three possible classes, using the one-vs-one strategy requires the creation of (n(n-1))/n binary classifiers. In your example, this would beEach of those will be specialized in the following problems:Distinguishing between class 1 and class 2 (let's call it svma).Distinguishing between class 1 and class 3 (let's call it svmb)Distinguishing between class 2 and class 3 (let's call it svmc)Now, I see that actually you have asked multiple questions in your original post, so I will ask them separately. First I will clarify how the decision process works, and then tell how you could detect which features are the most important.Since you mentioned Accord.NET, there are two ways this framework might be computing the multi-class decision. The default one is to use a Decision Directed Acyclic Graph (DDAG), that is nothing more but the sequential elimination of classes. The other way is by solving all binary problems and taking the class that won most of the time. You can configure them at the moment you are classifying a new sample by setting the  of the .Since the winning-most-of-the-time version is straightforward to understand, I will explain a little more about the default approach, the DDAG.1>In this algorithm, we test each of the SVMs and eliminate the class that lost at each round. So for example, the algorithm starts with all possible classes:Now it asks svma to classify x, it decides for class 2. Therefore, class 1 lost and is not considered anymore in further tests:Now it asks svmb to classify x, it decides for class 2. Therefore, class 3 lost and is not considered anymore in further tests:The final answer is thus 2.1>Now, since the one-vs-one SVM is decomposed into (n(n-1)/2) binary problems, the most straightforward way to analyse which features are the most important is by considering each binary problem separately. Unfortunately it might be tricky to globally determine which are the most important for the entire problem, but it will be possible to detect which ones are the most important to discriminate between class 1 and 2, or class 1 and 3, or class 2 and 3.However, here I can offer a suggestion if your are using DDAGs. Using DDAGs, it is possible to . This means that is it possible to estimate how many times each of the binary machines was used when classifying your entire database. If you can estimate the importance of a feature for each of the binary machines, and estimate how many times a machine is used during the decision process in your database, perhaps you could take their weighted sum as an indicator of how useful a feature is in your decision process.By the way, you might also be interested in trying one of the Logistic Regression Support Vector Machines using L1-regularization with a high C to perform sparse feature selection:\n",
      "SVM\n",
      " \n",
      "These .My assumption about the feature statistics is: maximal distances between means of values between the classes and minimal variance of values for one class classify a good feature. I start with small learning set, test this assumption and increase the learning set if results look promising.The final optimization is the histogram of means comparison. Features with similar histograms are removed. Those are redundant features which decrease (at least on SVM) the accuracy considerable (5-10%).With this approach I gain 95% of accuracy on my data-set of 5 classes, 600 instances. The training takes &lt; 1h. Manual training used to gained 98% with many days of experimenting.\n",
      "SVM\n",
      " \n",
      "Hi maybe your predict variable  is not a vector, commonly the  has the same size with your one train sample data.Here is my example code for opencv's SVM:\n",
      "SVM\n",
      " \n",
      "There is no big difference here between regression and classification. Positive values will always correlate with growing chance of classifying as positive class, and negative ones with chance becoming negative samples (I assume that all your features are positive, otherwise you loose any meaning in that sense). Be careful though, in both classification and regression it is not true that if one weight is bigger than the other then one feature is more important. There are many aspects which alter this behavior, in particular features values scale, variance, generaly - features values distribution.In short. If your features are positive values, then sign of the feature shows towards which class thich feature is more correlated. The general classification procedure in linear models (such as linear SVM, not RF) isSo you can see that if  are positive then the sign of  either rises (for positive ones) or lowers (for negative) chance of . However if you have for example  and  it does not mean that second feature is more important, it might be the result of the fact, that second feature values are simply smaller, for example feature 1 might be a person height in cm (so for example 180cm) while the other feature is binary (0 or 1) so The actual importance of each feature is a whole field of study in machine learning community called \"feature importance\" or \"feature selection methods\". There are dozens of such approaches, and none of them is simply \"the best\".\n",
      "SVM\n",
      " \n",
      "Probably it depends on the classification algorithm. I'm only familiar with SVM. Please see The type of feature (count of words) doesn't matter. The feature ranges should be more or less similar. If the count of e.g. \"dignity\" is 10 and the count of \"have\" is 100000000 in your texts, then (at least on SVM) the results of such features would be less accurate as when you scaled both counts to similar range.The cases, where no scaling is needed are those, where the data is scaled implicitly e.g. features are pixel-values in an image. The data is scaled already to the range 0-255.\n",
      "SVM\n",
      " \n",
      "During the training of an SVM you assign a label to each class of training data.When you classify a sample the returned result will match up with one of these labels telling you which class the sample is predicted to fall into.There's some more documentation here which might help:\n",
      "SVM\n",
      " \n",
      "You can tune parameters of your SVM by using  to maximize your precision. To do so, set the parameter \"scoring\" likeHere  is your SVC classifier and, of course, you you also need to set the grid of parameters . See examples  \n",
      "SVM\n",
      " \n",
      "To solve this specifically for linear SVM, we first have to understand the formulation of the SVM in sklearn and the differences that it has to MultinomialNB.The reason why the  works for MultinomialNB is because the output of the  is essentially the log probability of features given a class (and hence would be of size , due to the formulation of the naive bayes problem. But if we check the  for SVM, the  is not that simple. Instead  for (linear) SVM is  because each of the binary models are fitted to every possible class. If we do possess some knowledge on which particular coefficient we're interested in, we could alter the function to look like the following:This would work as intended and print out the labels and the top n features according to the coefficient vector that you're after. As for getting the correct output for a particular class, that would depend on the assumptions and what you aim to output. I suggest reading through the multi-class documentation within the SVM documentation to get a feel for what you're after. So using the   which was described in this , we can get some kind of output, though in this situation it isn't particularly descriptive or helpful to interpret. Hopefully this helps you.with output:\n",
      "SVM\n",
      " \n",
      "I personally prefer neural networks. They can be better implemented and are far more flexible than SVM in the sense that a separation (whether linear or not) is learned in the hidden layers without requiring from us to decide for a kernel (like in SVM). But again, everything depends on many factors such as the complexity of the data, the class balance, the quality of the targets (whether there are conflicting or not) and the like.\n",
      "SVM\n",
      " \n",
      "All types of classifiers perform differently in different scenarios: SVM, AdaBoost, RandomForest and even a (naive) Bayesian classifier. Just learn about some libraries for your favorite programming language containing a number of these algorithms and try them.What matters more are the features you train the classifiers with. In your case,  ...depth of the treebalance of the treeordersome property of the nodes weighted by the order of these nodes \n",
      "SVM\n",
      " \n",
      "For this scaling it probably doesn't matter much in practice, but in general you should not use your test data to estimate any parameters of the preprocessing. This can severely bias you results for more complex preprocessing steps.There is really no reason why you would want to concatenate the data here, the SVM will deal with it.If you would be using a model that needs positive values and your test data is not made positive, you might consider another strategy than the MinMaxScaler.\n",
      "SVM\n",
      " \n",
      "What do you want to know? Which service person to fire, what are the best hours to provide the service, or sth. else? I mean what are your classes?Provided, you what to evaluate the service person - the classes are thepersons.  In SVM (and I think for NN applies the same) I would split all not purely numerical data in boolean attributes. Age: unchanged, numberGender: male 1/0, female 1/0Date: 7 features for days of week, possibly the number of experience days of the service person. for each special date an attribute e.g. national holiday 1/0Time: split the time-span in reasonable ranges e.g. 15 min. Each range is a featureSatisfaction: unchanged - number 1-10With this model you could predict the satisfaction index for each service person for given date, time, gender, age.\n",
      "SVM\n",
      " \n",
      "Actually any  has such a property by design.As I understand, what you want to do is something like feature selection without cut-off of least useful ones. See  authors compare several methods for feature selection including usage of SVM weights and find the last to be the best.\n",
      "SVM\n",
      " \n",
      "Here's a step-by-step guide for how to train an SVM using your data and then evaluate using the same dataset. It's also available at . At the url you can also see the output of the intermediate data and the resulting accuracy (it's an )3>You need to install the following libraries:pandasscikit-learnFrom command line:3>We will use pandas to load our data.pandas is a library for easily loading data. For illustration, we first savesample data to a csv and then load it.We will train the SVM with  and get test labels with 3>We will convert our dataframe into numpy arrays which is a format that scikit-learn understands.We need to convert the labels \"B\", \"M\", \"C\",... to numbers also because svm doesnot understand strings.Then we will train a linear svm with the dataWe see here that the length of  (5) exactly matches how many rowswe have in . Each item in  corresponds to a row.3>3>3>Example code for how to load LinearSVC: Long list of scikit-learn examples: . I've found these mildly helpful butoften confusing myself.If you find that the SVM is taking a long time to train, try LinearSVCinstead: Here's another tutorial on getting familiar with machine learning models: You should be able to take this code and replace  with your training data,  with your testing data, and get predictions for your test data, along with accuracy results.Note that since you're evaluating using the data you trained on the accuracy will be unusually high.\n",
      "SVM\n",
      " \n",
      "I don't think you want to use SVM for image classification. The task you describe (detecting emotions on image) require you to feed extremely good features to your SVM to learn from, gabor filter won't do this.I suggest you to try a deep learning approach - for example you may wish to try a convolutional neural network for this. These thing is able to extract features out of raw image and then use them to classify the images.Check this out:Here they use DNN to find the facial key points on image (i.e. location of eyes, nose tip etc.). You may want to adjust the code a little so that it just classifies your images.Once again, the power of DNN is that it acts both as feature extractor and as a classifier. It is an incredibly powerful tool for image recognition tasks, unlike the SWM of any type.\n",
      "SVM\n",
      " \n",
      "It is very hard to answer this question without looking at the data in question.SVM does have a history of working better with text classification - but machine learning by definition is context dependent.Consider the parameters by which you are running the random forest algorithm. What are your number and depth of trees, are you pruning branches? Are you searching a larger parameter space for SVMs therefore are more likely to find a better optimum. \n",
      "SVM\n",
      " \n",
      "The ensemble of the random forest performs well across many domains and types of data.   They are excellent at reducing error from variance and don't over fit if trees are kept simple enough. I would expect a forest to perform comparably to a SVM with a linear kernel. The SVM will tend to overfit more because it does not benefit from being an ensemble.   If you are not using cross validation of some kind.   At minimum measuring performance on unseen data using a test/training regimen than i could see you obtaining this type of result. Go back and make sure performance is measured on unseen data and likelier you'll see the RF performing more comparably.Good luck. \n",
      "SVM\n",
      " \n",
      "It seems that you need to use a one-class classifier for each user, e.g. a SVM. It could be trained by using the descriptions of all items the user have seen. These can be ranked by its interior distance from the hypersphere enclosing them (the radius). The greater distance, the greater ranking. Thus, items outside the hypersphere wouldn't be recommended. If you had extra metadata indicating how interested the user has been in each seen item (e.g. observation time), it woud be possible using this information for downranking items or even discarding outliers.\n",
      "SVM\n",
      " \n",
      "In general, distance from the separating hyperplane is the exact same thing SVM is using for classification. In other words classification equation is simply a sign of the signed distance you are asking for.Given your kernel , support vectors , and alpha coefficients  (Lagrange multipliers) and threshold (bias)  the equation is simply, given labels associated with each support vector :This is signed distance, so you get positive value when it is on positive side and negative otherwise, if you want a \"true\" distance (without label) simply divide by label or take absolute value\n",
      "thing SVM\n",
      " \n",
      "As I answered to , weight vector of any  indicates feature importance: simply because final value is a linear combination of feature values with weights as coefficients, so the bigger weight, the more impact to the final value is caused by the corresponding summand.Thus, for linear classifier you can take features with biggest weights (not with biggest values of the feature itself, or the biggest product of weight and feature value). It also explains why SVM with non-linear kernels like RBF don't have such a property: both feature values and weights are transformed into another space and you can't say that the bigger weight leads to bigger impact, see .If you need to select most important features for non-linear SVM, use special methods for , namely .\n",
      "SVM\n",
      " \n",
      "As you have a multiclass case (>2 cases) an one-vs-rest strategy is applied. sklearn creates 4 classiefiers, not only 1. Hence you have 4 hypothesis and 4*coefficents. Note: I have no clue about the logistic regression classifier, but that is how the sklearn SVM work.\n",
      "sklearn SVM\n",
      " \n",
      "Simply apply non-linear SVM to discriminate between these classes. RBF kernel should do the trick.\n",
      "SVM\n",
      " \n",
      "There is no definite answer for such question as it is heavily data dependent factor. In other words, the only general answer is \"you have to check both and select the best\". You could for example gather smaller sample and test your hypothesis there.The only theoretical aspect that could guide you is the fact that ranking SVM is actually transforming your problem into quadratic (in terms of number of relations) size of binary classification problems. So if by \"10\" or \"20 graded documents\" you mean that for each query you get 10 or 20 ordered results, then there is more information in 100 queries with 20 documents, as it gives you  training samples, while the second approach only . However, in practise these 40,000 samples can be less informative than 20,000 (as they cover only half of the documents space, furthermore ordering of 20 documents can be simply useless, as most of them would be \"equally bad\").\n",
      "SVM\n",
      " \n",
      "As I found out - it is bug in sklearn SVM implementation, seems that it appears only when used together with Bagging. To reproduce it you just need to pass the sample_weights parameter into fit method, where weights of samples of some class entirely zeroed. That's what implicitly happening when Bagging algorithm randomly zeroes sample_weights to create subsets for basic estimators. It tends to exclude most of the classes, and in case with this SVM implementation it leads to bug.I'll report this bug. Instead of using SVM you can use another basic estimator, or use another ensemble method instead of bagging, it may help you.\n",
      "sklearn SVM\n",
      " \n",
      "If you want a single feature to discriminate your data, use a decision tree, and look at the root node.SVM by design looks at combinations of all features.\n",
      "SVM\n",
      " \n",
      "You seem to be confused how SVM works. In short, x has to be one, big two-dimensional array, while in your case it is a list of various matrices. SVM will not ever run on such data. First, find a meaningful (in your data sense) way to represent each image as a constant size vector, which is often called feature extraction. One of the basic approaches would be to represent each image as some  or as .\n",
      "SVM\n",
      " \n",
      "You can use SVM class of OpenCV instead of scikit's. It's easy to use.For more info, check .\n",
      "SVM\n",
      " \n",
      "In short - no. If you add new class it should be added to each of the \"old\" classifiers so \"one vs. all\" still makes sense. If you assume that new classes can appear with time consider using one class classifiers instead, such as one-class SVM. This way once you get new samples for a particular class you only retrain a particular model, or add a completely new one to the system.Furthermore, for large number of classes, 1 vs all SVM works quite badly, and one-class approach is usually much better.\n",
      "class SVM\n",
      " \n",
      "If you are working with linear model (like linear SVM) then simply put \"0\" for this feature. While -1 and +1 values lead to the use of a particular weight assigned by the model, using \"0\" means that it will ignore the weight. It becomes much more complex once you consider kernel spaces and I do not think you can make an easy solution for such problem then.\n",
      "SVM\n",
      " \n",
      "Precision and recall does not have to be poor. You should try and build a binary classifier (I would recommend SVM or decision tree for this purpose). I would recommend extracting features like the number of occurrences of each word in a sample (or tf-idf) or the length of the words and sentences. I guess that the question word in the sentence will have a major impact on the classification. In addition, please note that a good precision value is very easy to get when you do not care about recall. \n",
      "SVM\n",
      " \n",
      "The model retains only the support vectors needed to define the classes.  Frankly, I'm surprised that it retained so many of the original rows.Your terminology is not correct.  \"Dimension\" is the number of features (columns), not the number of rows.  The dimensionality has not been reduced.  One way to think of this is that it took 2233 of the observations to define the entire border between positive and negative.  The other 1694 points are \"behind\" other data points, farther from the border.For a really simple example, consider all integers as the data points.  We classify them simply: all points larger than pi (3.14159...) are in the positive set; all smaller ones are marked negative.  Feed this to the SVM algorithm -- and what you get back is only two rows: 3 is negative; 4 is positive.  All other points are \"behind\" one or the other.Does that help?\n",
      "SVM\n",
      " \n",
      "A stochastic gradient descent approach to SVM could help, as it scales well and avoids the  problem. An implementation available in R is , which was created by a team at Google and is discussed in . In the paper, they show that compared to a traditional SVM, the SGD approach significantly decreases the training time (this is due to 1, the pairwise learning method and 2, only a subset of the observations end up being used to train the model).Note that  is a little more bare bones than some of the other SVM packages available in R; for example, you need to do your own centering and scaling of features.As to your memory problem, it'd be a little surprising if you needed the entire dataset - I would expect that you'd be fine reading in a sample of your data and then training your model on that. To confirm this, you could train multiple models on different samples and then estimate performance on the same holdout set - the performance should be similar across the different models.\n",
      "SVM\n",
      " \n",
      "I haven't used this much myself, but I know that the SVM algorithm itself does not produce class probabilities, only the response function (distance from hyperplane). If you look at the documentation for svm function, the argument \"probability\" - \"logical indicating whether the model should allow for probability predictions\" - is FALSE by default and you did not set it equal to TRUE. Documentation for predict.svm says similarly, argument \"probability\" is a \"Logical indicating whether class probabilities should be computed and returned. Only possible if the model was fitted with the probability option enabled.\" Hope that's helpful.\n",
      "SVM\n",
      " \n",
      "It appears that although svm() allows you to specify your input using the either default or formula method, plot.svm() only allows a formula method. Also, by only giving x to plot.svm, you are not giving it all the info it needs. It also needs y. Try this:\n",
      "svm\n",
      " \n",
      "just a bit of introduction about svm before i start answering the question. This will help you get overview about svm. Svm task is to find the best margin-maximing hyperplane that best separates the data. We have soft margin representation of svm which is also known as primal form and its equivalent form is dual form of svm. Dual form of svm makes the use of kernel trick.kernel trick is partially replacing the feature engineering which is the most important step in machine learning when we have datasets that are not linear (eg. datasets in shape of concentric circles). Now you can transform this dataset from non-linear to linear by both FE and kernel trick. By FE you can square each of the features in this dataset and it will transform into linear dataset and then you can apply techniques like logisitic regression which work best for linear data.In kernel trick you can use the polynomial kernel whose general form is (a + x_i(transpose)x_j)^d where a and d are constants and d specifies the degree, suppose if the degree is 2 then we can say it is quadratic and likewise. now lets say we apply d =2 now our equation becomes (a + x_i(transpose)x_j)^2. lets the we 2 features in our original dataset (eg. for x_1 vector is [x_11,x__12] and x_2 the vector is [x_21,x_22]) now when we apply polynomial kernel on this we get we get 6-d vectors. Now we have transformed the features from 2-d to 6-d.Now you can see intuitively that higher your dimension of data better would svm work because it will eventually transform that features to a higher space. Infact the best case of svm is if you have high dimensionality, then go for svm.Now you can see both kernel trick and Feature Engineering solve and transforms the  dataset(concentric circle one)  but the difference is we are doing FE explicitly but kernel trick comes implicitly with svm. There is also a general purpose kernel know as Radial basis function kernel which can be used when you don't know the kernel in advance.RBF kernel has a parameter (sigma) if the value of sigma is set to 1, then you get a curve that looks like gaussian curve.You can consider kernel as a similarity metric only and can interpret if the distance between the points is less, higher will be the similarity. \n",
      "Svm\n",
      " \n",
      "Yes there is. Typically your model will provide a member function called  which returns the distance of the given sample to the separating hyperplane which you can interpret as confidence. Check out this  for SVM classification for usage.\n",
      "SVM\n",
      " \n",
      "This happens because the SVM solvers in VLFeat do not estimate the model and bias directly, but use the workaround of adding a constant component to the data (as mentioned in ) and return the corresponding model weight as the bias.The bias term is thus a part of the regularizer and models with higher bias are \"penalized\" in terms of energy. This effect is especially strong in your case, since your data are extremely low dimensional :)Therefore you need to choose a small value of the regularization parameter LAMBDA to lower the importance of the regularizer.\n",
      "SVM\n",
      " \n",
      "The specific error you're seeing actually has nothing to do with SVM. On this line in your  function:lowercase  (and ) are currently undefined, hence the . You  probably meant  and .\n",
      "SVM\n",
      " \n",
      "You're looking for the  parameter in scikit-learn's SVM implementation. This is the parameter usually called the penalty term or cost factor depending on the reference, and its value controls the extent to which the algorithm penalizes misclassification of training samples.\n",
      "SVM\n",
      " \n",
      "First of all, when you do classification using SVM, you usually extract a feature (like HOG) of an image, so that the dimensionality of space on which SVM has to operate gets reduced. You are using raw pixel values, which generates a 10304-D vector. That is not good. Use some standard feature.Secondly, you do not call  100 times. You call only once. The concept behind the optimization is, you want to find a weight vector  and a bias  such that it satisfies  for all images (i.e. all x_i). Note that  will go from 1 to the number of examples in your training set, but  and  stay the same. If you wanted a  that will satisfy only one , you do not need any fancy optimization. So stack your  in a big matrix of dimension ,  will be a vector of , and then call . It will take a longer time than 35 seconds, but you do it only once. This is called . While testing for a new image, you just extract its feature, and do  to get its class.Third, unless you are doing this as an exercise to understand SVMs,  is not the best way to solve this problem. You need to use some other techniques which work well with large data, for example, .How can one linear hyperplane classify more than 2 classes: For multi-class classification, SVMs usually use two popular approaches: SVM: For a  class problem, you train  classifiers and at test time, you test that many classifiers and choose the class which receives most votes. SVM: As name suggests, you train one classifier per class with positive samples from that class and negative samples from all other classes.From :However, also note that a naive implementation of one-vs-one may not be practical for large-scale problems. LIBSVM website also lists this shortcoming and provides .\n",
      "SVM\n",
      " \n",
      "First we need to clarify two things, hyperparameter  is actually the inverse of the regularization. Bigger the  - smaller the regularization. From your question I suppose that you are interested in upper bound of , which is actually lower bound of regularization. I am pretty sure that you cannot have such value based solely on the number of points. Why? Because SVM is not affine transformations invariant, so if I simply scale your whole dataset 2 times, you will need bigger  to get the very same results as before scaling. In other words - actual scale/position of your points is crucial. Unfortunately even for correctly scaled data - as far as I know - there is no upper bound of , there is however the lower one - you can compute the smallest  for which SVM leads to non-trivial model. However on the opposite end - it seems as a more complex optimization problem than SVM itself.\n",
      "SVM\n",
      " \n",
      "A classic SVM only does binary classification. This should make sense once you understand what's happening underneath the hood! Each training observation i is an n-dimensional vector x_i along with a binary classification y_i = 1 or y_i = 0. What SVM training tries to do is draw a hyperplane through n-dimensional space to spit your observations x into a half-space that's categorized as y_i=1 and a half-space that's category 0. A hyperplane splits a linear space into half spaces, so a classic SVM can only do binary classification.Linear regression with ordinary least squares is possibly the simplest thing to try. You can always add non-linear transformations of your data to get more right hand side variables for linear regression. Back on SVM stuff, there may be a way to represent your classification problem as a multiple binary classification problem, but I've never done that so I don't know how easy/hard etc... that is.\n",
      "SVM\n",
      " \n",
      "Yes; it will work fine, so long as you have enough data on each side to properly define the class.  The amount you need depends on the classification method you use.  In fact, I have a couple of SVM models that work very nicely, trained with nothing but +ve data -- no -ve data at all!For most methods, the lopsided input suggests that you could toss out the 80% of your +ve cases that aren't doing as much to define the boundary.  Which 80% will vary with the method.  For instance, spectral clustering and k-means will work well enough if you remove 80% evenly spaced (at random is likely to work).  Linear SVM works if you keep only the 10% nearest the boundary.  Naive Bayes and random forest can also work nicely with a random 80% removal, although any of these that work by successive refinement may converge a little more slowly.\n",
      "SVM\n",
      " \n",
      "Machine learning is a part of neural network? I'd be surprised because machine learning includes dozen of techniques that have nothing to do with neural network. It's most likely the other way around.The exact pattern recognition algorithm depends on your requirement and data set. There're many such algorithms, for example, SVM, linear models for classification, HMM, PCA etc. Note that the phase \"pattern recognition\" is a very general term, there is no algorithm that always work. It all depends on what patterns you are looking and what kind of assumption you can make.I recommend Dr Bishop's \"Pattern Recognition and Machine Learning\" book, you'll learn a lot from the book. \n",
      "SVM\n",
      " \n",
      "One main problem with the approach is that you do not set hyperparemeters of your SVM, while you use RBF, so probably  and  (or ) as these are default values in most implementations of SVM.While these are critical to build a valid model. In particular, if your  value is too low ( might be, depends on many features of data) then SVM builds a trivial model simply always predicting one of the classes.What you should do? You should check multiple values of both  and . What are the meanings of these parameters? (your ) is a weight of missclassification - greater the , SVM will try harder to learn training data exactly, possibly at the cost of overfitting. (your default) is the inverse of the 2 times variance of your RBF kernel. In other words - greater the gamma, smaller the Gaussians, and thus - your method is more \"local\" in the geometrical sense. Again - big gamma helps you minimize the training error (bias) but leads to higher testing error (variance).Correct selection of the tradeoff between variance-bias is crucial element of machine learning techniques. In case of RBF SVM - you can control it through the above. Play around with them, check both training set error and testing set error to see what is happening. If your training set error is big - increase C and/or gamma. Once your training set error is fine, look at the testing set - if it is too big - try to decrese values and so on. It is usually done in automatic manner through some internal cross validation with grid search of the paremeters.Check out materials on model selection and hyperparameter optimization.  Furthermore you fix number of iterationswhile for SVM you should never do that. Let it converge (or at least put something like 100,000), after just 100 steps it might be the case that SVM did not come even close to convergence (thus resulted in trivial model).\n",
      "SVM\n",
      " \n",
      "Apart from logistic regression, as @neerajkh suggested, I would try as well. This method use to work very well in multiclass problems (I assume you have many inputs, which are the marks of the students) and many outputs (the different colleges). To implement one vs all algorithm I would use  (SVM). It is one of the most powerful algorithms (until deep learning came into the scene, but you don't need deep learning here)If you could consider changing framework, I would suggest to use python libraries. In  it is very straightforward to compute very very fast the problem you are facing.\n",
      "SVM\n",
      " \n",
      "SVM is only actually capable of doing binary classification.  The multi-class adaptation uses several models and votes on what the class should be in a one-vs-one scheme.Quick example:would all be used in a 3-class SVM, then the models would vote on what class a observation should be. one-vs-all is another popular way to use SVM in a multiple classification scenario.  To answer your question, that's already kind of what is going on behind the scenes.  It is possible building even more models could help improve on your accuracy by a small margin, so its worth a shot if you're bored and want to see if it helps or not\n",
      "SVM\n",
      " \n",
      "Yes, it's a good idea. Check performance of other models on your data. Since SVM is a binary classifier. A multiclass SVM classifier internally uses either . What you are suggesting is one-vs-all. Since libsvm uses One-vs-One technique. you can use  but this usually doesn't increase accuracy performance as one-vs-one uses more number of classifier.  \n",
      "SVM\n",
      " \n",
      "First things first.SVM does not work on distance functions, it only accepts dot products. So your distance function (actually similarity, but usually 1-distance is similarity) has to:be symmetric be positive definite be linear in first argument  and This can be tricky to check, as you actually ask \"is there a function from my objects to some vector space, phi such that \" is a dot-product, thus leading to definition of so called kernel, . If your objects are themselves elements of vector space, then sometimes it is enough to put  thus , but it is not true in general.Once you have this kind of similarity nearly any SVM library (for example ) works with providing Gram matrix. Which is simply defined asThus requiring  memory and time. Consequently it does not matter what are your objects, as SVM only works on pairwise dot-products, nothing more.If you look appropriate mathematical tools to show this property, what can be done is to look for kernel learning from similarity. These methods are able to create valid kernel which behaves similarly to your similarity. \n",
      "SVM\n",
      " \n",
      "You should use a classifier instead of a regressor so either SVM or Logistic Regression would do the job. Instead you can use SGDClassifier where you can set the loss parameter to 'log' for Logistic Regression or 'hinge' for SVM.In SGDClassifier you can set the penalty to either of 'l1', 'l2' or 'elasticnet' which is a combination of both.You can find an opimum value of 'alpha' by either looping over different values of alpha and evaluating the performance over a validation set or you can use gridsearchcv as:This, searches over the range of values of alpha you have provided in tuned_parameters and then finds the best one. You can change the performance criteria from 'f1_macro' to 'f1_weighted' or other metrics.To address the skewness of your dataset in terms of labels use the class_weight parameter of  and set it to \"balanced\".To find the top 10 features contributing to the class labels you can find the indices as:Note 1: It is always good to keep some part of your dataset aside as validation/test set and after finding your optimum model evaluating it on the held out data.Note 2: It is usually good to play a little bit with different types of feature normalisation and sample normalisation by dividing a row or a column to 'l2' or 'l1' of the row or column to see its effect on the performance using Note 3: For elasticnet regularisation play a little bit with l1_ratio parameter.\n",
      "SVM\n",
      " \n",
      "As in any other machine learning problem, if you do not have a quality criterion, you suck.When people say \"classification\", they have supervised learning in mind: there is some ground truth against which you can train and check your algorithms. If new classes can appear, this ground truth is ambiguous. Imagine one class is \"horse\", and you see many horses: black horses, brown horses, even white ones. And suddenly you see a zebra. Whoa! Is it a new class or just an unusual horse? The answer will depend on how you are going to use your class labels. The SVM itself cannot decide, because SVM does not use these labels, it only produces them. The decision is up to a human (or to some decision-making algorithm which knows what is \"good\" and \"bad\", that is, has its own \"loss function\" or \"utility function\").So you need a supervisor. But how can you assist this supervisor? Two options come to mind: Anomaly detection. This can help you with early occurences of new classes. After the very first zebra your algorithm sees it can raise an alarm: \"There is something unusual!\". For example,  various algorithms from random forest to one-class SVM can be used to detect unusial observations. Then your supervisor can look at them and decide whether they deserve to form an entirely new class.Clustering. It can help you to make decision about splitting your classes. For example, after the first zebra, you decided it is not worth making a new class. But over time, your algorithm has accumulated dozens of their images. So if you run a clustering algorithm on all the observations labeled as \"horses\", you might end up with two well-separated clusters. And it will be again up to the supervisor to decide, whether the striped horses should be detached from the plain ones into a new class. If you want this decision to be purely authomatic, you can split classes if the ratio of within-cluster mean distance to between-cluster distance is low enough. But it will work well only if you have a good distance metric in the first place. And what is \"good\" is again defined by how you use your algorithms and what your ultimate goal is.\n",
      "SVM\n",
      " \n",
      "This means that using your parameters, optimal SVM is to do nothing (build a trivial model which simply answers one label). so yes, there is something wrong, as your model does not use your training data at all.What are the reasons? In order to use RBF-SVM you need to fit two hyperparameters:  and , in particular  is (probably) orders of magnitude too small to work well. Also - remember to normalize your data, because images are often represented as pixel intensitiy values (0-255) while SVM works better for a normally distributed values. Gamma also has to be carefully fitted (its default value in scikit-learn is 1/no. of features, thus it is really rough guess of a good value, rarely a good one). As an example refer to  and a plot showing relation of C and gamma values to the obtained accuracy on heart dataset\n",
      "SVM\n",
      " \n",
      "You have to read the docs (or at least docstrings through  command) of each method and decide which parameters to fit. In particular, many of them have infinite number of possible values (such as C) thus you cannot check all values. You will need some sampling.In particular noone besides you can decide whether to check many ways of weighting samples in SVM or not, whether or not to test multiple stopping tolerance parameters or not.\n",
      "SVM\n",
      " \n",
      "Answering your questions:1-  Should i learn on the plain coin-images or should i first extract features and learn on the features? (i think: features)For many object classification tasks it's better to extract the features first and then train a classifier using a learning algorithm. (e.g the features can be  and the learning algorithm can be something like SVM or Adaboost). It's mainly due to the fact that the features have more meaningful information compared to the pixel values. (They can describe edges,shapes, texture, etc.) However, the algorithms like deep learning will extract the useful features as a part of learning procedure.2 - How much positives and negatives per coin should be given?You need to answer this question depending on the variation in the classes you want to recognize and the learning algorithm you use. For SVM , if you use HOG features and want to recognize specific numbers on coins you won't need much. 3- Would i have to learn also on rotated coins or would this rotation be handled \"automagically\" by the SVM? So would the SVM recognize rotated coins, even if i only trained it on non-rotated coins?Again it depends on your final decision about the features(not SVM which is the learning algorithm) you're going to choose. HOG features are not rotation invariant but there are features like SIFT or SURF which are. 4-One of my picture-requirements above (\"DinA4\") limits the size of the coin to a certain size, e.g. 1/12 of the picture-height. Should i learn on coins of roughly the same size or different sizes? I think, that different sizes would result in different features, which would not help the learning process, what do you think? Again, choose your algorithm , some of them ask you for a fixed/similar width/height ratio. You can find out about the specific requirements in related papers.If you decide to use SVM take a look at  and also if you feel ok with Neural Network, using Tensorflow is a good idea.\n",
      "SVM\n",
      " \n",
      "Following paper is a good guide for SVM users. A Practical Guide to Support Vector ClassificationIn a nutshell, three points are essential to let SVM perform correctly.(1) feature preparation (feature scaling, feature categorization)(2) parameter tuning (coarse and fine-grained cross validation)(3) kernel selection (#features vs #instances)Basic idea for (3) is to select the linear kernel if #features >> #instances. With small #instances, SVMs with non-linear kernels can be overfit easily. \n",
      "SVM\n",
      " \n",
      "A complete answer to this question is not simple. Here is an example for getting started on this subject:Result of script above is:You can see the result variable importance of two models are similar.This is one of many methods of interpreting SVM results. See following paper for more information: \"An Introduction to Variable and Feature Selection\", \n",
      "SVM\n",
      " \n",
      "Faster is a bit of a weird question, in part because it is hard to compare apples to apples on this, and it depends on context. LR and SVM are very similar in the linear case. The TLDR for the linear case is that Logistic Regression and SVMs are both very fast and the speed difference shouldn't normally be too large, and both could be faster/slower in certain cases. From a mathematical perspective, Logistic regression is strictly convex [its loss is also smoother] where SVMs are only convex, so that helps LR be \"faster\" from an optimization perspective, but that doesn't always translate to faster in terms of how long you wait.Part of this is because, computationally, SVMs are simpler. Logistic Regression requires computing the  function, which is a good bit more expensive than just the  function used in SVMs, but computing these doesn't make the majority of the work in most cases. SVMs also have hard zeros in the dual space, so a common optimization is to perform \"shrinkage\", where you assume (often correctly) that a data point's contribution to the solution won't change in the near future and stop visiting it / checking its optimality. The hard zero of the SVM loss and the  regularization term in the soft margin form allow for this, where LR has no hard zeros to exploit like that.  However, when you want something to be fast, you usually don't use an exact solver. In this case, the issues above mostly disappear, and both tend to learn just as quick as the other in this scenario. In my own experience, I've found Dual Coordinate Descent based solvers to be the fastest for getting exact solutions to both, with Logistic Regression usually being faster in wall clock time than SVMs, but not always (and never by more than a 2x factor). However, if you try and compare different solver methods for LRs and SVMs you may get very different numbers on which is \"faster\", and those comparisons won't necessarily be fair. For example, the SMO solver for SVMs can be used in the linear case, but will be orders of magnitude slower because it is not exploiting the fact that you only care are Linear solutions. \n",
      "SVM\n",
      " \n",
      "I have had similar problems. Even when I have prior knowledge about the effect of a variable, I often just run an unbiased algorithm, like an SVM, to classify the data. I then check to see accurate my SVM was. I also check to see if the SVM agrees with what I already know about the variables. If the SVM fails, I then try a neural network. Hope this helped!\n",
      "SVM\n",
      " \n",
      "A 1-class SVM is simply looking for a boundary between two classes.  In theory, it doesn't matter which is the outlier; your inversion is logically sound.In practice, I've occasionally found better results when I use the better-defined class as the inlier, provided I'm using an advanced kernel (e.g. Gaussian).  That little bit of guidance seems to help the chosen algorithm to converge more reliably on a better solution.  This shouldn't be the case; perhaps my feeling is merely observation bias.Yes, evolving classification is a well-known problem with a wide variety of counter-measures.  The problem is called .  You'll need to educate yourself on the concept and research a approaches that address your application.Once you've done that, if you still have open issues, you should post a separate question in this group.  It's a different topic, and your paradigm is presently too broad for a StackOverflow question.Given that you're trying to identify malicious samples as being of a well-defined class, you may well get the better result with your desired method.  However, note that the benign code is also well-defined: absence of any trigger phrase.  To me, this seems the better definition: your malicious code class is actually the union of disjoint or loosely-connected classes, each characterized by a particular trigger phrase.I would base this decision on which visualization is most convenient for the person doing the work: you.  Since you seem to like having the \"any hot\" side be the distinguishing factor, I suggest that you go with it.  If you have any doubts, try training your model each way, and see if there are differences with the implementation you're using.\n",
      "class SVM\n",
      " \n",
      "Yes you can use it but:One-class SVM is an outlier detection method and unsupervised technique. Meaning it seperates an area of your training data INCLUDING outliers (anomalies/malicious instances). This  means that to work you should have a quite \"pure\" dataset,preferable use only the \"good\" data.  Also keep in mind that ANY unsupervised method will underperform a supervised one (more knowledge == reduced entropy for  possibile outliers )\n",
      "class SVM\n",
      " \n",
      "Do you mean negative of the bias term instead of inverse? The decision function of the SVM is , where  is the bias term ,  is the weight vector, and  is the input. But thats in the primal space / linear form.  is replaced by our kernel function, which in this case is the RBF kernel. The RBF kernel is defined as . So if the distance between two things is very large, then it gets squared - we get a huge number.  is a positive number, so we are making our huge giant value a huge giant negative value.  is already on the order of 5*10^-5, so for far away points the RBF kernel is going to become essentailly zero. If sample is far aware from all of your training data, than all of the kernel products will be nearly zero. that means  will be nearly zero. And so what you are left with is essentially , ie: the negative of your bias term.  \n",
      "SVM\n",
      " \n",
      "Mathematically, optimizing an SVM is a convex optimization problem, usually with a unique minimizer. This means that there is only one solution to this mathematical optimization problem.The differences in results come from several aspects:  and  are supposed to optimize the same problem, but in fact all  estimators penalize the intercept, whereas  ones don't (IIRC). This leads to a different mathematical optimization problem and thus different results. There may also be other subtle differences such as scaling and default loss function (edit: make sure you set  in ). Next, in multiclass classification,  does one-vs-rest by default whereas  does one-vs-one. is different from the other two in the sense that it uses stochastic gradient descent and not exact gradient descent and may not converge to the same solution. However the obtained solution may generalize better.Between  and , one important decision criterion is that  tends to be faster to converge the larger the number of samples is. This is due to the fact that the linear kernel is a special case, which is optimized for in Liblinear, but not in Libsvm.\n",
      "SVM\n",
      " \n",
      "The actual problem is in the problem with scikit approach, where they call SVM something which is not SVM. LinearSVC is actually minimizing squared hinge loss, instead of just hinge loss, furthermore, it penalizes size of the bias (which is not SVM), for more details refer to other question:So which one to use? It is purely problem specific. As due to no free lunch theorem it is impossible to say \"this loss function is best, period\". Sometimes squared loss will work better, sometimes normal hinge.\n",
      "SVM\n",
      " \n",
      "I recommend you to visit this awesome map on choosing the right estimator by the scikit-learn team As describing the specifics of your own case would be an enormous task (I totally understand you didn't do it!) I encourage you to ask yourself several questions. Thus, I think the map on 'choosing the right estimator' is a good start.Literally, go to the 'start' node in the map and follow the path:is my number of samples > 50?And so on. In the end you might end at some point and see if your results match with the recommendations in the map (i.e. did I end up in a SVM, which gives me better results?). If so, go deeper into the documentation and ask yourself why is that one classifier performing better on text data or whatever insight you get.As I told you, we don't know the specifics of your data, but you should be able to ask such questions: what type of data do I have (text, binary, ...), how many samples, how many classes to predict, ... So ideally your data is going to give you some hints about the context of your problem, therefore why some estimators perform better than others.But yeah, your question is really broad to grasp in a single answer (and specially without knowing the type of problem you are dealing with). You could also check if there might by any of those approaches more inclined to overfit, for example.The list of recommendations could be endless, this is why I encourage you to start defining the type of problem you are dealing with and your data (plus to the number of samples, is it normalized? Is it disperse? Are you representing text in sparse matrix, are your inputs floats from 0.11 to 0.99).Anyway, if you want to share some specifics on your data we might be able to answer more precisely. Hope this helped a little bit, though ;)\n",
      "SVM\n",
      " \n",
      "This is way more complex than that. In short - you do not map your data directly into feature space. You simply change the dot product to the one induced by the kernel. What happens \"inside\" SVM when you work with polynomial kernel, each point is actually (indirectly) transformed to  dimensional space (where d-input data dimension, p-degree of polynomial kernel). From mathematical perspective you work with some (often unknown) projection  which has the property that , and nothing more. In SVM implementations, you do not need actual data representation (as  is usually huge, sometimes even infinite, like in RBF case) but instead it needs vector of dot product of your point will each element of the training set.Thus what you do (in implementations, not from math perspective) is you provide:During training whole Gram matrix,  defined as  where  is i'th training sampleDuring testing, when you get new point  you provide it to SVM as a vector of dot products  such that , where again  are your training points (in fact you just need values for support vectors, but many implementations, like libsvm, actually require vector of the size of the training set - you can simply put 0's for  if  is not a training vector)Just remember, that this is not the same as training linear SVM \"on top\" of the above representation. This is just a way implementations usually accept your data, as they need a definition of dot product (function) and it is often easier to pass numbers than functions (but some of them, like scikit-learn SVC module, actually accepts functions as kernel parameter).So what is RBF kernel? It is actually a mapping from points to functions space of normal distributions with means in your training points. And then dot product is just an integral from -inf to +inf from the product of such two functions. Sounds complex? It is  at first sight, but it is a really nice trick, worth understanding!\n",
      "SVM\n",
      " \n",
      "Not sure I fully understood your problem, but regarding SVM and its view as convex optimization problem - you could read this short paper:Why ANNs cost function is non-convex is answered here (especially good answer from mr. Roland):\n",
      "SVM\n",
      " \n",
      "In supervised learning, such as SVMs, the dataset should be composed as follows:  where  goes from 1 to the number of patterns (also examples or observations) in your training set so this represents a single record in your training set which can be used to train the SVM classifier.So you basically have a set composed by such tuples and if you do have just 2 labels (binary classification problem) you can easily use a SVM.  Indeed the SVM model will be trained thanks to the training set and the training labels and once the training phase has finished you can use another set (called Validation Set or Test Set), which is structured in the same way as the training set, to test the accuracy of your SVMs.In other words the SVM workflow should be structured as follows:train the SVM using the training set and the training labelspredict the labels for the validation set using the model trained in the previous stepif you know what the actual validation labels are, you can match the predicted labels with the actual labels and check how many labels have been correctly predicted. The ratio between the number of correctly predicted labels and the total number of labels in the validation set returns a scalar between [0;1] and it's called the accuracy of your SVM model.if you're interested in the ROI, you might want to check the trained SVM parameters (mainly the weights and bias) to reconstruct the separation hyperplaneIt is also important to know that the training set records should be correctly, a priori labelled: if the training labels are not correct, the SVM will never be able to correctly predict the output for previously unseen patterns. You do not have to label your data according to the ROI you want to extract, the data must be correctly labelled a priori: the SVM will have the entire set of type A pictures and the set of type B pictures and will learn the decision boundary to separate pictures of type A and pictures of type B. You do not have to trick the labels: if you do, you're not doing classification and/or machine learning and/or pattern recognition. You're basically tricking the results.\n",
      "SVM\n",
      " \n",
      "You must always retrain considering the entire, newly concatenated, training set.The support vectors from the \"old\" model might not be support vectors anymore in case some \"new points\" are closest to the decision boundary. Behind the SVM there is an optimization problem that must be solved, keep that in mind. With a given training set, you find the optimal solution (i.e. support vectors) for that training set. As soon as the dataset changes, such solution might not be optimal anymore.The SVM training is nothing more than a maximization problem where the geometrical and functional margins are the objective function. Is like maximizing a given function f(x)...but then you change f(x): by adding/removing points from the training set you have a better/worst understanding of the decision boundary since such decision boundary is known via sampling where the samples are indeed the patterns from your training set.I understand your concerned about time and memory efficiency, but that's a common problem: indeed training the SVMs for the so-called big data is still an open research topic (there are some hints regarding backpropagation training) because such optimization problem (and the heuristic regarding which Lagrange Multipliers should be pairwise optimized) are not easy to parallelize/distribute on several workers.LibSVM uses the well-known Sequential Minimal Optimization algorithm for training the SVM:  you can find John Pratt's article regarding the SMO algorithm, if you need further information regarding the optimization problem behind the SVM.\n",
      "SVM\n",
      " \n",
      "When you are using SVM, you are necessarily using one of the kernels: linear, polynomial or RBF=Radial Base Function (also called Gaussian Kernel) or anything else . The latter iswhich explicitly contains your gamma. The larger the gamma, the narrower the gaussian \"bell\" is.I believe, as you go with the course, you will learn more about such \"kernel trick\". \n",
      "SVM\n",
      " \n",
      "Sounds like a binary classification problem.There are three basic approaches you might use:Collect packages you can manually label by \"browsing activity\" and \"others\" and train binary classifier on top (like SVM etc.)Collect just packages which are \"browsing activity\" and train one-class classifier on top (like one class SVM)Just collect all the data you can and try to cluster it into two clusters, there is a (very small unfortunately!) chance that the division found will be the one you are looking forIn each of the above cases you will need to prepare set of features to represent your data. So either a constant set of some features, or you might try to simply use packet header as a raw text and traing some text-based model, like some convolutional neural network etc.\n",
      "SVM\n",
      " \n",
      "The classification rule of your SVM is (no matter if you trained it in soft or hard margin rule):where w_i are your coefficients and b is a bias.Just take your test set, pass it through this rule and calculate the fraction of correct predictions (accuracy).\n",
      "SVM\n",
      " \n",
      "SVM is internally a binary classifier. This means that it can only separate two labels. Thus it is well suited for problems where the question can be expressed in the form \"Does given vector has property X?\" and the answer is either yes (+1) or no (-1).But there are so many problems in real life which require more labels, what to do about it? In order to solve problems with m labels, like for example \"What type of animal is on the picture: cat, dog or horse?\" you basiccally create ensemble of SVMs, which vote for particular labels. There are many possibilities, but two of them are most popular:One vs. all approach. Given m labels you create m models, i'th SVM is trained to recognize if the object has label \"i\" or not, thus you train it on label \"i\" as +1, and with all other samples as -1. Then, when it comes to classification - you ask each SVM if it recognizes the sample - if there are any ties - you use SVM decision function (distance to hyperplane) to break it (the most distant one wins)One vs one approach. Now you create m(m-1)/2 SVMs. Each answers the question \"is this sample more i-labeled or j-labeled\", for each pair (i, j) of labels (and there are m(m-1)/2 ways of select two labels among m ones). Thus again - during classification you ask each such SVM, and they \"vote\", you sum votes and the label with most votes - wins (again in case of ties you can use decision functions).\n",
      "SVM\n",
      " \n",
      "Are you using ipython notebook and Python 2.x? If yes then multiprocessing module doesn't work with this combination. You can export (save as) your ipython notebook as a regular .py file and run it with regular python interpreter. Then you can use n_jobs=-1\n",
      "Python\n",
      " \n",
      "In Python, Modules (=Packages in other languages) oftentimes define routines that are interdependent. In these cases, you cannot only download one .py file and put it into your Workspace (i.e. the directory where your sources are located). Instead, download the entire package into that folder, and import relatively, i.e. like this:\n",
      "Python\n",
      " \n",
      "List comprehensions to the rescue!Totally possible, even using very similar syntax to what you had before. Python has a construct known as a  made exactly for this application. Basically, it's a functional syntax for inline for loops, but tend to be cleaner, more robust implementations with slightly lower overhead.Example:This will even scale to slightly more complex data structures pretty easily, for instance:Not sure if that's exactly what you're looking for, but it should generalize to whatever you are looking to do similar to this.If it's useful I asked a question about grabbing all related synsets  a while ago.\n",
      "Python\n",
      " \n",
      "I don't know much about , though I vaguely recall some earlier issue triggered by a switch to using sparse matricies.  Internally some of the matrices had to replaced by  or .But to give you an idea of what the error message was about, consider usually is used in Python to count the number of 1st level terms of a list.  When applied to a 2d array, it is the number of rows.  But  is a better way of counting the rows. And  is the same.  In this case you aren't interested in , which is the number of nonzero terms of a sparse matrix.   doesn't have this method, though can be derived from .\n",
      "Python\n",
      " \n",
      "There is no need for external libraries, such as , as Python provides the required functionality easily with dictionary methods. Under the hood, any external library will be following a similar algorithm, described in the next paragraph.First you'll want to generate a list of all the headers that are distributed over those different files. Then, it would be a cleaner idea to switch the internal structure into a , such that you can easily retrieve the filling value  when the header is not present in a specific file:You'll want to change the last  function into a  form, if your eventual goal is to present an array of arrays (a 2D matrix) to some other function in . I'll leave that fairly easy challenge to you.Mind you, a very similar functionality, intended for writing to a csv file (or any object which has a  method), is provided by Python's , which you could use like this:If there are duplicate headers in the files, then you'll want to get rid of the duplicates, e.g. by calling , just before the writing/printing code blocks.\n",
      "Python\n",
      " \n",
      "You can use the Python random forest package  as follows:Note that I imported  here, but you could have also used  in case you wanted run random forests in regression mode instead.\n",
      "Python\n",
      " \n",
      "I agree that the most likely reason why adding more features produces worse results is overfitting, and that the main solution is feature selection. Now, there are different techniques to verify and measure this intuition. One of the best tools is to produce the learning curves for the model given training and validation subsets. A good example of this can be seen in  tutorials for the sklearn library (Python). Also, I strongly recommend you to have a look at the  from the Machine Learning course by Andrew Ng in Coursera.\n",
      "Python\n",
      " \n",
      "I use Python for date format normalization. You have to change to string before returning a dataFrame because underlying R will thrown an exceptionThen I use a Metadata Editor to change the type to DateTime\n",
      "Python\n",
      " \n",
      "For simple off-the-shelf sentiment analysis, take a look at , a Python library developed at the University of Antwerp. It can return the polarity and subjectivity of English sentences in the following way: \n",
      "Python\n",
      " \n",
      "In the specified example the bag of words contains 5000 features; this requires significant memory. So, one solution is to reduce the number of features, but doing this may affect the model performance.Another solution is to switch from 32-bit Python to 64-bit.\n",
      "bit Python\n",
      " \n",
      "That definitely sounds like a problem where machine learning could give good results. I recommend that you look into , a powerful and easy to use toolkit for machine learning in Python. There are many introductory examples and tutorials available.For your use case, I would say that random forests could give good results, although it's hard to say without knowing more about the structure of the data. They are available in the class  in sklearn. Again, there are many tutorials and examples to be found.Since your training data is unlabeled, you may want to look into unsupervised learning methods. A simple class of such methods are clustering algorithms. In sklearn, you can find, for instance,  along other such algorithms. The idea would be to let the algorithm split your data into different cluster and see if there is any correlation between cluster membership and observed effect.\n",
      "Python\n",
      " \n",
      "Interpreted Python code is slow. Really slow.That is why the good python toolkits contain plenty of Cython code and even C and Fortran code (e.g. matrix operations in numpy), and only use Python for driving the overall process.You may be able to speed up your code substantially if you try to use as much  as possible. Or if you use Cython instead.Instead of fighting centroids, consider using an distance-based clustering algorithm:Hierarchical Agglomerative Clustering (HAC), which expects a distance matrixDBSCAN, which can work with arbitrary distances. It does not even need a distance matrix, only a list of similar items for some threshold.K-medoids/PAM of course is worth a try, too; but usually not very fast.\n",
      "Python\n",
      " \n",
      "If you don't have to use scipy/numpy, you might checkout , which is a Python wrapper around Google's excellent  Voice Activity Detection code. WebRTC uses Gaussian Mixture Models (GMMs), works well, and is very fast.Here's an example of how you might use it:\n",
      "Python\n",
      " \n",
      "I have recently added a  on how I did this using Python.Read after going through the tutorial:If you are doing something at a large scale I suggest you to check out . The Sparse Matrix can be used as an input to . There will be Python support that soon, I guess so.\n",
      "Python\n",
      " \n",
      "You are mixing symbolic and non-symbolic operations but this doesn't work.For example,  returns a non-executable symbolic expression representing the idea of comparing two things for equality but it doesn't actually do the comparison there and then.  actually returns a Python object that represents the equality comparison and since a non- object reference is considered the same as  in Python, the execution will always continue inside the if statement.If you need to construct a Theano computation involving conditionals then you need to use one of its two symbolic conditional operations:  or . .You are also using Python loops which is probably not what you need. To construct a Theano computation that explicitly loops you need to use the  module. However, if you can express your computation in terms of matrix operations (dot products, reductions, etc.) then it will run much, much, faster than something using scans.I suggest you work through some more  before trying to implement something complex from scratch.\n",
      "Python\n",
      " \n",
      "Can you check if the file paths are correct, according to , the Python code cannot access most directories on the machine it runs on, the exception being the current directory and its sub-directories. Is \"Script Bundle\" a sub-directory? You can also try to use inputs rather than reading the csv files from the script. The Execute Python Scripts module accepts 3 inputs, the first 2 being data frames, the third is reserved for python library files. You can, for example, use input 1 for actual data, and input 2 for the words originally in csv. Its needed to have a mechanism to bundle 3 csv files into a single data frame for input 2.\n",
      "Python\n",
      " \n",
      "Although the approach to use multiple  modules will work, it become wildly difficult when there are many inputs, or the number of inputs is varied.Instead, you can use the  module to directly access blob storage. Doing so, however, is exceedingly painful if you've never done it before.  Here's are the issues:The  Python package is not loaded by default into Azure ML.  However, this can be created manually, or downloaded from the link below (correct version as of Feb 11, 2016).The default usage of  uses HTTPS, which is not currently supported in Azure ML blob storage access. For this, you can pass in  during the BlobService creation to force the use of HTTP: Here are the steps to get it working:Download  which provides the required  libraries: Upload them as a DataSet to the Azure ML StudioConnect them to the Zip input on an  module, which is the 3rd input.Write your script as you would normally, being sure to create your  object with Run the Experiment - you should now be able to read and write to blob storage.Some example code can be found here: Here is the code to make it work for a single file. This can be extended to work with numerous files by accessing a container and filtering, but that will depend on your business logic.For further information on limitations, why HTTP, and other notes, see \n",
      "Python\n",
      " \n",
      "You have to shuffle your data and create indices for the training and test folds. For example, if you have 2000 training examples and you want to use 10 folds, then you will have:Here is a sample code in Python:\n",
      "Python\n",
      " \n",
      "NOTE: This is not an answer, only a hint on possible solutions.I encountered a similar problem recently in my project. My goal is to extract the corresponding chain of decisions for some particular samples. I think your problem is a subset of mine, since you just need to record the last step in the decision chain.Up to now, it seems the only viable solution is to write a custom  method in Python to keep track of the decisions along the way. The reason is that the  method provided by scikit-learn cannot do this out-of-box (as far as I know). And to make it worse, it is a wrapper for C implementation which is pretty hard to customize.Customization is fine for my problem, since I'm dealing with a unbalanced dataset, and the samples I care about (positive ones) are rare. So I can filter them out first using sklearn  and then get the decision chain using my customization.However, this may not work for you if you have a large dataset. Because if you parse the tree and do predict in Python, it will run slow in Python speed and will not (easily) scale. You may have to fallback to customizing the C implementation.\n",
      "Python\n",
      " \n",
      "You can try something like below. It depends heavily on  but if you prefer to avoid external dependencies you can easily replace it with some standard Python libraries.Since you know all possible transitions I would recommend using a sparse representation and ignore zeros. Moreover you can replace dictionaries with sparse vectors to reduce memory footprint. \n",
      "Python\n",
      " \n",
      "I tend toward Python in AzureML, but the concepts should still apply to R. Categorical inputs are used to categorize data into several, well defined buckets. In the data you're passing in, it appears that you are restricting the allowable values, rather than providing default values, which seems to be your intent.Can you pass in the allowed parameters in a tuple-like construct?\n",
      "Python\n",
      " \n",
      "1>Unfortunately, the most common error in scientific python programming that causes code to run slow is... to use pure python. Python loops are slow, period.  Even assuming that eveything is perfectly correct, you will end up with extremely slow optimizer unless you do one of the following:switch to  (gives you access to the beauty of typed loops)use  (basic numeric library)try to run code through  (it is not a library, it is different interpreter, faster one)1>There is no  afterSo it will not even run.Next, after fixing it and runing on  it crasheswhich is caused by your invalid reading function. libsvm (svmlight) dataformat is sparse, so some of the dimensions might be missing - your code assumes that it does not.You even read your data as stringsIt should be (after you preallocate your sample lists so they are big enough to fit data, or use defaultdicts ).The same applies to reading labels. Consequently you have dozens of completely redundant type conversions in your code (all your current  and  calls are redundant).There is also error, in final building of :In testing code you add intercept () multiple timeswhile it should beI would not be suprised if one would find also many errors in SMO itself, which might cause the algorithm to not converge at all, but for now I only managed to find the above. 1>after fixing all of the above, removing all debug printing messages and running with  I get following model:while scikit-learn givesSo up to sign it is the same model.When using  both codes end up with training set accuracy 1>using your code [with type casting] and pure pythonusing your code [with type casting] and using your code [without type casting] and using scikit-learn SMO (C code)1>It appears that most of your errors are located in data reading utilities. Furthermore, as stated at the begining - \"classic\" python interpreter has extremely slow loops, so you either have to use  (and lack of support for many libraries), or  (and more complex development) or at least numerical libraries such as  and .\n",
      "Python\n",
      " \n",
      "The following steps work perfectly in my Notebook:step 1 : get Python files from github :step 2 : append these files in my Python path : step 3 : load the MNIST data with 'input_data' fonctionThat's all !\n",
      "Python\n",
      " \n",
      "Thanks to disco4ever answer. I was trying to build xgboost for Python Anaconda environment in my windows 10 64 bit machine. Used Git, mingw64 and basic windows cmd.Everthing worked for me till the copy step: cp make/mingw64.mk config.mk, as I was using windows cmd I modified it to copy c:\\xgboost\\make\\mingw64.mk c:\\xgboost\\config.mkwhen I proceeded to the next step : make -j4, I got error that build failed. At this stage after so much frustration just tired something different by clicking on build.sh (shell script). It started executing and auto finished. Then I executed the same step make -j4, to my awe build was successful. I have seen the most awaited xgboost.exe file in my xgboost folder. I then proceeded with further steps and executed python setup.py install. finally everything installed perfectly. Then I went to my spyder and checked whether it is working or not. But I was one step away to my happiness because I was still seeing the import error. Closed all command prompts (Anaconda, Git bash, Windows CMD, cygwin terminal) then again opened spyder and typed 'import xgboost'. SUCCESS, No ERROR. Once again thank you for everyone.\n",
      "Python\n",
      " \n",
      "My environment: Python 3.6, Tensorflow 1.3.0Though there have been many solutions, most of them is based on . When we load a  saved by , we have to either redefine the tensorflow network or use some weird and hard-remembered name, e.g. ,. Here I recommend to use , one simplest example given below, your can learn more from :Save the model:Load the model:\n",
      "Python\n",
      " \n",
      "As lejilot said, you have a lot of biases there. You don't need a bias in the last layer, it's an output layer and a bias must be connected to its input, but not to its output. Take a look at the following image:In this image you can see that there is just one bias per layer, except for the last one, where there is no need of a bias. a very intuitive approach to neural networks. It is in Python, but it can help you to understand some concepts of neural networks better.\n",
      "Python\n",
      " \n",
      "It looks like you have an instance of 3.2.0.9 already running and the Python package, which is 3.6.0.8, is complaining due to incompatibility.I recommend you kill the current running instance of H2O (the 3.2.0.9 version that you see in the error message) and run the same commands (import h2o, h2o.init) to drive H2O from Python cleanly.\n",
      "Python\n",
      " \n",
      "Check out the following:: a lightweight library that provides lots of functionality.: a very popular toolkit that is used both in industry and academia.Apart from these, you can also use Python packages, but import them from C++.\n",
      "Python\n",
      " \n",
      "I experienced a similar problem in Python when training a Word2Vec model. Looking at the PySpark docs for word2vec , it reads: Sets number of iterations  (default: 1), which should be smaller than or equal to number of  partitions.    New in version 1.2.0.    Sets number of partitions  (default: 1). Use a small number for accuracy.    New in version 1.2.0.My word2vec model stopped hanging, and Spark stopped running out of memory when I increased the number of partitions used by the model so that I suggest you set  or .\n",
      "Python\n",
      " \n",
      "In your code,  is a Python list. TensorFlow's  expects a single tensor, not a Python list.where size is the number of values you're taking a softmax over should do what you want. See But, one thing I notice in your code that seems a bit odd, is that you have a list of placeholders for your inputs, whereas the code in  uses an order 3 tensor for inputs. Your input is a list of order 2 tensors. I recommend looking over the code in the tutorial, because it does almost exactly what you're asking about. One of the main files in that tutorial is . In particular, line 139 is where they create their cost. Regarding your input, lines 90 and 91 are where the input and target placeholders are setup. The main takeaway in those 2 lines is that an entire sequence is passed in in a single placeholder rather than with a list of placeholders.See line 120 in the ptb_word_lm.py file to see where they do their concatenation.\n",
      "Python\n",
      " \n",
      "In Python you can use , which according to the  containsSimply do a  on the feature importances and you get a feature ranking (ties are not accounted for).You can look at the  ( in the graphviz output) to get a first idea. Lower is better. However, be aware that you will need a way to combine impurity values if a feature is used in more than one split. Typically, this is done by taking the average information gain (or 'purity gain') over all splits on a given feature. This is done for you if you use .Edit:I see the problem goes deeper than I thought. The graphviz thing is merely a graphical representation of the tree. It shows the tree and every split of the tree in detail. This is a representation of the tree, not of the features. Informativeness (or importance) of the features does not really fit into this representation because it accumulates information over multiple nodes of the tree.The variable  contains importance information for every feature. If you get for example [0, 0.2, 0, 0.1, ...] the first feature has an importance of 0, the second feature has an importance of 0.2, the third feature has an importance of 0, the fourth feature an importance of 0.1, and so on.Let's sort features by their importance (most important first): Now rank contains the indices of the features, starting with the most important one: [1, 3, 0, 1, ...]Want to see the five most important features?This prints the indices. What index corresponds to what feature? That's something you should know yourself because you supposedly constructed the feature matrix. Chances are, that this works:Or maybe this:\n",
      "Python\n",
      " \n",
      "We've started a cookiecutter-data-science project designed for Python data scientists that might be of interest to you, check it out . Structure is explained .Would love feedback if you have it! Feel free to respond here, open PRs or file issues.In response to your issue about re-using code by importing .py files into notebooks, the most effective way that our team has found is to append to the system path. This may make some people cringe, but it seems like the cleanest way of importing code into a notebook without lots of module boilerplate and a pip -e install.One tip is to use the  and   with the above. Here's an example:The above code comes from  for some context.\n",
      "Python\n",
      " \n",
      "If you can \"restore the model and do predictions without difficulty in Python\", then you can find out the names/labels of the input nodes or tensors using their \"name\" property, search for \".name\" in either:or:  All nodes have names/labels whether you have explicitly named them or not.\n",
      "Python\n",
      " \n",
      "You are going down the correct path. The  module is meant for custom needs just like this. Your real issue is how to import existing Python script modules. The complete directions can be found here, but I will summarize for SO. You will want to take the Azure Python SDK and zip it up, upload, then import into your module. I can look into why this is not there by default... Importing existing Python script modulesA common use-case for many data scientists is to incorporate existing Python scripts into Azure Machine Learning experiments. Instead of concatenating and pasting all the code into a single script box, the Execute Python Script module accepts a third input port to which a zip file that contains the Python modules can be connected. The file is then unzipped by the execution framework at runtime and the contents are added to the library path of the Python interpreter. The azureml_main entry point function can then import these modules directly.As an example, consider the file Hello.py containing a simple Hello, World function. Figure 4. User-defined function.Next, we can create a file Hello.zip containing Hello.py: Figure 5. Zip file containing user-defined Python code.Then, upload this as a dataset into Azure Machine Learning Studio. If we then create and run a simple experiment a uses the module:  Figure 6. Sample experiment with user-defined Python code uploaded as a zip file.The module output shows that the zip file has been unpackaged and the function print_hello has indeed been run.  Figure 7. User-defined function in use inside the Execute Python Script module.\n",
      "Python\n",
      " \n",
      "As I know, you can use other packages via a zip file which you provide to the third input. The comments in the Python template script in Azure ML say:So you can package  as a zip file thru click New, click Dataset, and then select From local file and the Zip file option to upload a ZIP file to your workspace.As reference, you can see more information at the section  of the doc .\n",
      "Python\n",
      " \n",
      "You can use Python library  (), which allows you to cluster your users based on clickstream data with a simple command. You can also specify any target events you are interested in your clusters and explore obtained graphs using interactive graphs.Next you can explore obtained behavioral clusters with interactive graph:\n",
      "Python\n",
      " \n",
      "At the moment, you can do much more things in mxnet using Python than using R. I am using Gluon API, which makes writing code even simpler, and it allows to load pretrained models. The model that is used in the tutorial you refer to is an . The list of all available pretrained models can be found .The rest of actions in the tutorial is data normalization and augmentation. You can do the normalization of the new data similar to how they normalize it on the API page:The list of possible augmentation is available . Here is the runnable example for you. I did only one augmentation, and you can add more parameters to  if you want to do more of them:\n",
      "Python\n",
      " \n",
      "Python has a very strong support from the Data Science Community, you have very good packages like Pandas and Python has a very good integration with Spark\n",
      "Python\n",
      " \n",
      "Python allows you to create extension modules using C and C++, interfacing with native code, and still getting the advantages that Python gives you.TensorFlow uses Python, yes, but it also contains large amounts of .This allows a simpler interface for experimentation with less human-thought overhead with Python, and add performance by programming the most important parts in C++.\n",
      "Python\n",
      " \n",
      "Python doesn't impose memory limit beyond what the OS imposes. Make sure you're not limiting the process's memory usage with ulimit or equivalent. Also run top and see if the process uses all available memory.How is that your documents near 2GB each? Is it a concatenation of multiple documents? If so, maybe split the documents more.I would suggest to set-up Spark installation and check the code there. You already have the files, so the only thing that is left is running tfidf part. It has Python interface so it will be fairly painless. Spark is highly optimized for working with big files - may it will overcome scikit-learn's bugs.\n",
      "Python\n",
      " \n",
      "Here are the details I came up with...There doesn't seem to be an implementation in Python as yet. Though there was  about some people desiring to implement it in sklearn a few years ago. However, based on , it might be possible to modify the criterion parameter of the sklearn  to achieve the desired effect.Alternatively, if you know R, the original implementation also exists .\n",
      "Python\n",
      " \n",
      "Please see the difference between the two code snippets below, the first of which is correct and the second produces an error:First:Second:Edit:you should run the lines from the first code snippets line by line in Python interpreter (or in a cell as a whole in Ipython notebook) and you'll get the line in the output after you have executed  statement.\n",
      "Python\n",
      " \n",
      "From the R doc.Now, I think this is particular for the R sdk, as the Python SDK, and the AzureML Studio UI lets you upload a new dataset. Will check in with the R team about this.I would recommend uploading it as a new dataset with a new name, and then replacing the dataset in your experiment with this new dataset. Sorry this seem's round about, but I think is the easier option.Unless you want to upload the new version using the AzureML Studio, in which case go to +NEW, Dataset, select your file and select the checkbox that says this is an existing dataset. The filename should be the same. \n",
      "Python\n",
      " \n",
      "I have done some lab, and found out that your json is encoded into utf-8. For your case, it's simple to get the real content back. See the code below:Python 3.xPython 2.xThis piece of code may not work for some other cases, because 'latin-1' does not cover all 0-255 characters. Hence, I am still looking for a better encoding for this kind of things.\n",
      "Python\n",
      " \n",
      "Transform each record  into a binary vector  so that -th component of  is set to  if  has -th characteristic, and  otherwise.Now run  algorithm on this set of vectors under the  or , whichever you think is more appropriate; also make sure there's a notion of distance between clusters defined in terms of the underlying distance (see ).Then decide where to cut the resulting  based on common sense. Where you cut the dendrogram will affect the number of clusters.One downside of hierarchical clustering is that it's rather slow. It takes  time in general, so it would take quite a while on a large data set. For single- and complete-linkages you can bring the time down to .Hierarchical clustering is very easy to implement in languages such as Python. You can also use the implementation from the  library.2>Here's a code snippet to get you started. I assume  is the set of records transformed into binary vectors (i.e. each list in  corresponds to a record from your data set).The result is as you'd expect: cut at 0.5 to get two clusters, one of the first three vectors (which have ones at beginning, zeros at the end) and the other of the last three vectors (which have ones at the end, zeros at the beginning). Here's the image:Hierarchical clustering starts with each vector being its own cluster. In each successive steps it merges the closest clusters. It repeats this until there is a single cluster left.The dendrogram essentially encodes the whole clustering process. At the beginning each vector is its own cluster. Then  and  merge into  and  and  merge into }. Next,  and  merge into , and  and  merge into . Finally,  and  merge into .From the dendrogram you can usually see at which point it makes the most sense to cut---this will define your clusters.I encourage you to experiment with various distances (e.g. Hamming distance, Jaccard distance) and linkages (e.g. single linkage, complete linkage), and various representations (e.g. binary vectors).\n",
      "Python\n",
      " \n",
      "If you are an owner in the workspace, you can open your dataset in Python inside of a Jupyter Notebook. By the visualize should be an open in notebook button. Then just execute the code that is provided for you, and it should print your dataset. You can then also select specific columns to visualize as well.\n",
      "Python\n",
      " \n",
      "Python is telling you that the data is not in the right format, in particular x must be a 1D array, in your case it is a 2D-ish panda array.You can transform your data in a numpy array and squeeze it to fix your problem.\n",
      "Python\n",
      " \n",
      "I had a very same problem 2 years ago. Unfortunately the only solution is to do this by yourself. Implementing \"predict\" is pretty straight forward, it is a one-liner in Python. Unfortunately .intercept_ is actually a copy of intercept used internally (the libsvm one). Quite confusing thing is that for LinearSVC from the very same library it is not true, and you can actually alternate the bias (however, without access to kernels, obviously).Obviously you do not have to go as deep as computing kernels values yourself. You still have access to \"decision_function\", which in the end, has a bias inside. Simply remove the old bias from decision function, add new one, and take the sign. This will be (up to the sign of bias):\n",
      "Python\n",
      " \n",
      "If you like to use Python, PCA implementation of scikit is available. You may find the usage below;If you are okay with other languages (since you said; \"or a paper or source code for another language\"), here is an example with Apache Mahout (written in Java).\n",
      "Python\n",
      " \n",
      "Linear SVM is a special case of general soft margin kernel SVM in which the model can be expressed as a single weight vector w and a bias b, so that classification is done by y = w'*x+b (this is the decision value; you can threshold it with zero, as you did, or choose another value to tradeoff precision/recall). In the general case, the model is described by its support vectors and their weights, so its size depends on the number of SVs that were found, and may be quite large.Some software libraries, like libsvm, do not have specialized code to deal with linear SVM classification, so the model representation and classification function are inefficient in terms of memory and running time. You can, however, easily convert the model representation to the above linear classifier, by calculated the weighted sum of all the SVs, using the stored SV weight. You can then write your own linear classification function, using whatever means you want to make it run fast (e.g. vectorized code)\n",
      "Linear\n",
      " \n",
      "Linear Regression is SGD based and requires tweaking the step size, see  for more details.In your example, if you set the step size to 0.1 you get better results (MSE = 0.5).For another example on a more realistic dataset, see \n",
      "Linear\n",
      " \n",
      "\"Linear\" in linear regression refers to the fact that we use a line to fit our data. Period.\n",
      "Linear\n",
      " \n",
      "You don't say why you want Linear SVM, but if you can consider another model that often gives superior results then check out the hpelm python package. It can read an HDF5 file directly. You can find it here  It trains on segmented data, that can even be pre-loaded (called async) to speed up reading from slow hard disks.\n",
      "Linear\n",
      " \n",
      "First off there are two issues here, one you need to scale your outputs between 0 and 255. You can do this with some transformation afterwards. By taking the max and min value, then transposing between 0 and 255.On the other hand this network will likely not learn what you'd like it to, your hidden layers are using Linear Layers. This is not very useful, as the weights themselves form a linear transformation. You'll essentially end up with a linear function. I would recommend using a SigmoidLayer for your hidden layers, this of course squashes the values between 0 and 1. You can correct this in the output layer by multiplying by 255. Either via a fixed layer or just transforming the values afterwards.\n",
      "Linear\n",
      " \n",
      "Great question.Logistic Regression also assumes the following:That there isn't (or there is little) multicollinearity (high correlation) among the independent variables.Even though LR doesn't require the dependent and independent variables to be linearly related, it does however require that the independent variables to be linearly related to the log odds. The log odds function is simply .\n",
      "Regression\n",
      " \n",
      "Currently, we don't support disabling CORS on API side but you can either use the above option or you can use the API management service to disable CORS. The links below should help you with thisHere are the links:  guide, also this  on setting headers, and  on policies.API Management service allow CORS by enabling it in the API configuration page\n",
      "CORS\n",
      " \n",
      "I started to improve the solution by transforming the  into a smarter, dichotomous way of finding the maximumThen I realized, after 2 hours of work, that getting all the accuracies were far more cheaper than just finding the maximum !! (Yes it is totally counter-intuitive).I wrote a lot of comments here below to explain my code. Feel free to delete all these to make the code more readable.The all process is just a single loop, and the algorithm is just trivial.In fact, the stupidly simple function is 10 times faster than the solution proposed before me (commpute the accuracies for ) and 30 times faster than my previous smart-ass-dychotomous-algorithm...You can then easily compute ANY KPI you want, for example :If you want to test it :Enjoy ;)\n",
      "-intuitive).I\n",
      " \n",
      "I'm not 100% sure, but I think scikit-learn.naive_bayes requires a purely numeric feature vector instead of a mixture of text and numbers. It looks like it crashes when trying to \"divide\" a unicode string by a long integer.I can't be much help with finding numeric representations for text, but  might be a good start.\n",
      "scikit-learn.naive_bayes\n",
      " \n",
      "1>Previous answers do not specify how to handle the multi-label case so here is such a version implementing three types of multi-label f1 score in tensorflow: micro, macro and weighted (as per scikit-learn)Update (06/06/18): I wrote a  about how to compute the streaming multilabel f1 score in case it helps anyone (it's a longer process, don't want to overload this answer)3>outputs:\n",
      "scikit-learn)Update\n",
      " \n",
      "You didn't show which kind of model you use to me, but I assume that you initialized your model as . In a  model you can only stack one layer after another - so adding a &quot;short-cut&quot; connection is not possible.For this reason authors of Keras added option of building &quot;graph&quot; models. In this case you can build a graph (DAG) of your computations. It's a more complicated than designing a stack of layers, but still quite easy.Check the documentation  to look for more details.\n",
      "quot;short-cut&quot\n",
      " \n",
      "take a simple linear classification problem-y={0 if 5x-3>=0 else 1}here y is class, x is feature, 5,3 are parameters.\n",
      "problem-y={0\n",
      " \n",
      "Here you'll find Ruby code (not python)\n",
      "Ruby\n",
      " \n",
      "There could be many problems when it comes why you don't get the desired results, most often it's either:Features are not strong enoughNot enough training dataWrong classifierCode bugs in NLTK classifiersFor the first 3 reasons, there's no way to verify/resolve unless you post a link to your dataset and we take a look at how to fix it. As for the last reason, there shouldn't be one for the basic  and  classifier. So the question to ask is:How many training data instances (i.e. rows) do you have?Why didn't you normalize your labels (i.e. chinese|Chinese -> chinese) after you've read the dataset before extracting the features?What other features to consider?Have you considered using NaiveBayes instead of PositiveNaiveBayes?\n",
      "training dataWrong\n",
      " \n",
      "That's a tough questions. Generally this kinds of machine predict based on Data Mining result form previous data. These data can be so huge that human won't be able to find any patter on that data. In that case machine is far batter than human as they will implement its algorithm and  predict intelligently. But if you ask that do the machines know what is human's choice as human himself is unpredictable :) Yes there are so many researcher working on data mining, autonomous agent or human-agent teamwork. We can see some of by searching on these topics fat Google school-er. Best of luck  \n",
      "Data\n",
      " \n",
      "Why don't you use the  layer to compensate for the imbalance in your training set?The Infogain loss is defined using a weight matrix  (in your case 2-by-2) The meaning of its entries areSo, you can set the entries of  to reflect the difference between errors in predicting 0 or 1.You can find how to define matrix  for caffe in .Regarding sample weights, you may find  interesting: it shows how to modify the SoftmaxWithLoss layer to take into account sample weights.<hr />Recently, a modification to cross-entropy loss was proposed by .The idea behind focal-loss is to assign different weight for each example based on the relative difficulty of predicting this example (rather based on class size etc.). From the brief time I got to experiment with this loss, it feels superior to  with class-size weights.\n",
      "training set?The\n",
      " \n",
      "You can just configure that by using the modules under Data Format Conversions. Have a look  and . Documentation is in progress, unluckily. \n",
      "Data\n",
      " \n",
      "You can read  about Data sources in Power BI.You can connect your Power Bi to: Azure Blob, Azure SQL server, Azure SQL Data Warehouse, Azure Table Storage, or Azure HDInsight.All these PBI sources are also available as outputs to the Writer module in Azure ML. so you can use them to write your results from Azure ML and later read them as input for Power BI\n",
      "Data\n",
      " \n",
      "Q: How should we deal with unreliable data in data scienceA: Use feature engineering to fix unreliable data (make some transformations on unreliable data to make it reliable) or drop them out completely - bad features could significantly decrease the quality of the modelQ: Is there any way to figure out these misstatements and then report the top 10% rich people with better accuracy using Machine Learning algorithms?A: ML algorithms are not magic sticks, they can't figure out anything unless you tell them what you are looking for. Can you describe what means 'unreliable'? If yes, you can, as I mentioned, use feature engineering or write a code which will fix the data. Otherwise no ML algorithm will be able to help you, without the description of what exactly you want to achieveQ: Is there any idea or application in Machine Learning which tries to improve the quality of collected data?A: I don't think so just because the question itself is too open-ended. What means 'the quality of the data'?Generally, here are couple of things for you to consider:1) Spend some time on googling feature engineering guides. They cover how to prepare your data for you ML algorithms, refine it, fix it. Good data with good features dramatically increase the results.2) You don't need to use all of features from original data. Some of features of original dataset are meaningless and you don't need to use them. Try to run gradient boosting machine or random forest classifier from scikit-learn on your dataset to perform classification (or regression, if you do regression). These algorithms also evaluate importance of each feature of original dataset. Part of your features will have extremely low importance for classification, so you may wish to drop them out completely or try to combine unimportant features together somehow to produce something more important.\n",
      "data scienceA\n",
      " \n",
      "This research field is called \"Data Matching\" or \"Record Linkage\". There is a  He also goes deep down into machine learning models and how to improve them from the basic approach like simple string distances (as other answers already suggested).To give you a head start, you can try to compute character n-grams of your titles.For n = 3 and Hugo Boss, you would get Now you can compute the  between two sets of these ngrams. Here, for example between  and :If you don't want to go down the route of implementing all of these things yourself, use . It's also very fast and scales well to billions of documents. \n",
      "Data\n",
      " \n",
      "2>Don't expect to find an algorithm that exactly does what you need.Customize algorithms as adequate for your problem. That is the very story of the Data Science buzz, the need to experiment and customize instead of hoping for a turnkey solution.You have avery specific idea of what you need. You will have to put this idea into code and plug it into some algorithm. For example, consider complete linkage clustering with maximum norm. It probably is what you explained above, but I don't think it will be useful.\n",
      "Data\n",
      " \n",
      "If there are not so many blobs, you could just add multiple readers with each map to one of your input blobs. Then use modules under \"Data Transformation\" -> \"Manipulation\" to do things like \"Add Rows\" or \"Join\".\n",
      "Data\n",
      " \n",
      "Just to clarify the Training/Validation/Test data sets:The training set is used to perform the initial training of the model, initializing the weights of the neural network.The validation set is used after the neural network has been trained. It is used for tuning the network's hyperparameters, and comparing how changes to them affect the predictive accuracy of the model. Whereas the training set can be thought of as being used to build the neural network's gate weights, the validation set allows fine tuning of the parameters or architecture of the neural network model. It's useful as it allows repeatable comparison of these different parameters/architectures against the same data and networks weights, to observe how parameter/architecture changes affect the predictive power of the network.Then the test set is used only to test the predictive accuracy of the trained neural network on previously unseen data, after training and parameter/architecture selection with the training and validation data sets.\n",
      "Training\n",
      " \n",
      "A good recap can be found , section 1 on Data Augmentation: so namely flips, random crops and color jittering and also lighting noise: proposed fancy PCA when training the famous Alex-Net in 2012. Fancy PCA alters the intensities of the RGB channels in training images.Alternatively you can also have a look at the Kaggle Galaxy Zoo challenge: the winners wrote a . It covers the same kind of techniques:rotation,translation,zoom,flips,color perturbation.As stated they also do it \"in realtime, i.e. during training\".For example here is a practical   by Facebook (for  training).\n",
      "Data\n",
      " \n",
      "When you say \"normalize\" labels, it is not clear what you mean (i.e. whether you mean this in a statistical sense or something else). Can you please provide an example? On Making labels uniform in data analysisIf you are trying to neaten labels for use with the  function, you could try the  function to shorten them, or the  function to align them better. The  function works well for rounding labels on plot axes. For instance, the base function  for drawing histograms calls on Sturges or other algorithms and then uses  to choose nice bin sizes.The  function will standardize values by subtracting their mean and dividing by the standard deviation, which in some circles is referred to as normalization. On the reasons for scaling in regression (in response to comment by questor). Suppose you regress Y on covariates X1, X2, ... The reasons for scaling covariates Xk depend on the context. It can enable comparison of the coefficients (effect sizes) of each covariate. It can help ensure numerical accuracy (these days not usually an issue unless covariates on hugely different scales and/or data is big). For a readable intro see . For a mathematically intense discussion see .In particular, in Bayesian regression, rescaling is advisable to ensure convergence of MCMC estimation; e.g. see . \n",
      "data analysisIf\n",
      " \n",
      "You will see Precision/Recall results reported in academic papers as a aggregate, rather than 10,000 different P/R results. In that respect it gives the reader a very general sense of RS performance. Typically you will see Precision/Recall represented as a curve (as seen here: ). You tend to see that at Recall = 1, Precision is low, and where Precision = 1, recall is low. You can easily create one of these curves in Excel or Google Sheets from your 10,000 results. As mentioned in the comments F-measure is a way to combine P/R to generate a mean value, although you need to be aware of the limitations of the F measure before you go \"boasting\" about it. It is not uncommon to justify some sort of weighting for either precision or recall depending on your application domain, so just be aware that the basic F-measure is balanced (both precision and recall are treated as equally important). Receiver Operator Characteristic () is also commonly used along side P/R curves, and f-measure in recommender system evaluation. If you are looking for extra credit then I would recommend using multiple methods to evaluate RS performance such as P/R curve, F measure, AUC, and ROC. \n",
      "Precision\n",
      " \n",
      "In this context, cross-entropy is one particular form of a genetic algorithm. Its a much more specific thing than saying \"Genetic Algorithms\" as that covers a huge number of different algorithms. Put simply:Genetic Algorithms is a family of algorithms / one type of approach to optimizationCross-entropy is a specific genetic algorithm. \n",
      "Genetic\n",
      " \n",
      "Spiking neural networks are closer models to real neurons in brain. They have the ability of plasticity to change their characteristics over time. Therefore, they tend to mimic the synaptic plasticity in real neurons in brain and change their activations, architecture and outputs of neurons over time. As a small-scale example of these models you can take a look at the neural networks models whose architectures are learned  (Genetic Algorithms for instance). However, due to lack of enough computational power people have not been able to deploy these models in large-scale applications. There is a very small research going on for these models too.Sources    \n",
      "Genetic\n",
      " \n",
      "What you have here is a multi-class classification problem that can be solved with Genetic Programming and related techniques.I suppose that data are those from the well-known Iris data set: If you need a quick start, you can use the source code of my method: Multi Expression Programming (which is based on Genetic Programming) which can be downloaded from here: There is a C++ source name mep_multi_class.cpp in the src folder which can \"solve\" iris dataset. Just call the read_training_data function with iris.txt file (which can also be downloaded from dataset folder from github).Or, if you are not familiar with C++, you can try directly MEPX software which has a simple user-interface: . A project with iris dataset can also be downloaded from github.\n",
      "Genetic\n",
      " \n",
      "Answering my own question below.The last parameter among the three values is not the amount of step size to take. It is the number of steps to perform. So it was incorrect to say above that I want to perform 0.1 steps. It should have been 100 steps so that Weka can infer that it needs to take 0.1 step size internally.Using [1 10 100] helped and it ran fine.  The answer I found is here - \n",
      "Weka\n",
      " \n",
      "You can try  which is based on Weka and is expected to handle multilabel classification problems. \n",
      "Weka\n",
      " \n",
      "I can recommend Weka, where you have , which is an out-of-the-box neural network classifier. But this is probably not suitable for your problem, so you can use the basic class , which is used by the MultilayerPerceptron to build it's neural network.If you manage to separate all the needed code in and pack it into your own custom classes, you will end with a very lightweight solution - not counting the JVM needed to run it.In which programming language or environment do you plan to implement this? If you already use Java, then the JVM will not be an overhead, since you already use it.\n",
      "Weka\n",
      " \n",
      "There are a few different clustering modes in Weka:Use training set (default): After clustering, Weka classifies the training instances into clusters it developed and computes the percentage of instances falling in each cluster. For example, X% in cluster 0 and Y% in cluster 1, etc.Supplied test set: It is possible with Weka to evaluate clusterings on separate test data if the cluster representation is probabilistic like EM algorithm.Clustering evaluation using classes: In this mode Weka first ignores the class attribute and generates the clustering. During testing, it assigns class labels to the clusters on the basis of the majority value of the class attribute within each cluster. Finally, it computes the classification error and also shows the corresponding confusion matrix.\n",
      "Weka\n",
      " \n",
      "I'm not sure about the authentication. But I'll answer anyway.Weka is popular and I have used it before, however I've never used it for such a large number of classes to be classified, but that doesn't mean it can't be done!Weka: It has a GUI and a really easy-to-use Java library, if you're hard-coding.  I don't know what you're looking for as you haven't really said, but this is what I suggest!Good luck!\n",
      "Weka\n",
      " \n",
      "2>The result that Weka gives you are simply a measurement of the number of correctly and incorrectly classified records, i.e.Correctly classified records = TP + TNIncorrectly classified records = FP + FNThis is not the same as measuring precision. Precision is a measurement of how many of the correctly classified instances that are relevant, i.e.Precision = TP / (TP + FP)It is a highly specific metric that is best observed in relation to other metrics such as recall and accuracy. It may not be the best way to measure general performance of your model.For more info about Precision and Recall see this: Still, your numbers don't make much sense. However, not much more can be said without knowing more specifics.\n",
      "Weka\n",
      " \n",
      "This error message tells us that the TextDirectoryLoader class is not present on your current search path. Probably it was not installed with your version of Weka.According to their , this class is only available for Weka versions greater than 3.5.3. Check your installation directory again and look for this class. Else use TextDirectoryToArffHow to change the classpathYou have to edit (sudo gedit or sudo vim) the .bashrc file located at your home directory (~) and add this line:Where your [new_path] is something like /home/users/username/downloads/weka/weka.jarEDIT: Noticed that you should add the weka.jar to the classpath, and not the weka installation directory.\n",
      "Weka\n",
      " \n",
      "Such a scenario is termed as one-class classification. Take a look here to learn on how to use a framework such as Weka in Java to accomplish the same. \n",
      "Weka\n",
      " \n",
      "Use stratified sampling (e.g. train on a 50%/50% sample) or class weights/class priors. It helps greatly if you tell us which specific classifier? Weka seems to .Is the penalty for Type I errors = penalty for Type II errors?This is a special case of the receiver operating curve (ROC). If the penalties are not equal, experiment with the cutoff value and the AUC.You probably also want to read the sister site  for statistics.\n",
      "Weka\n",
      " \n",
      "Your question is how to select class index in Weka. First off, Let's see what class index is in .My suggestion is to use function \n",
      "Weka\n",
      " \n",
      "I assume you use Weka, in this case this not the 'right' way to do it. First create an ARFF file for training data. Then with same data structure (the structure you define in ARFF file which informs how the structured), create a test ARFF file. In the training part, Weka will ask you to supply the training file, then you will have another option to supply test data, basically supply to test file. Simply this is the convention. You can put all your data in one file an you should be able to split it as well, but you might need to use Weka source code and might not be able to do it via explorer of Weka.\n",
      "Weka\n",
      " \n",
      "I would recommend transcribing your data into  format and then use this with Weka.  is a program with many machine learning algorithms you can experiment with, it also has a very simple user interface so is good for beginners! Once you have found an algorithm that works well you can save your trained model and use this to predict new instances!\n",
      "Weka\n",
      " \n",
      "Weka needs one \"flattened\" table, i.e., -file. This process is also called denormalization. Theres a weka package (Denormalize) which contains a filter to perform this operation. There is an example how transactional data can be flattened here: Before using the filter you would have to merge your two files together. If you have -files or something similar you could achieve this by means of Excel, see for example:\n",
      "Weka\n",
      " \n",
      "Colors are a representation of the distribution of  in each column of the histogram. Apparently, it behave like this because of the number of values in this attribute, which is a nominal attribute (I see it above the histogram) with 305 distinct values: there would be too many distinct colors, and each color would represent only one instance. Weka fails to represent all those colors (I suppose it doesn't have so many colors saved).By the way, as Anony-Mousse said in his comment, ignore the colors:  should be a string attribute, not a nominal one, and if you convert it to string attribute, colors will disappear, because watching its distribution doesn't make sense.\n",
      "Weka\n",
      " \n",
      "Given the requirements you mentioned, you can use Weka's Filtered Classifier option during training and testing. I am not re-iterating what I have recorded as a video cast  and . But the basic idea is not to use the StringToWord vector as a direct filter rather to use it as a filtering option in the FilteredClassifier option. The model you generate will be just once. And then you can apply the model directly on your unlabelled data without retraining them or without applying StringToWord vector again on the unlabelled data. FilteredClassifier will take care of these concerns for you.\n",
      "Weka\n",
      " \n",
      "What you're describing is feature or attribute selection. Weka does this from the \"Select attributes\" tab. You can find articles and videos about the topic online. I have found videos from the University of Waikato helpful. Here is one on .You may also want to learn about Principal Component Analysis.\n",
      "Weka\n",
      " \n",
      "If you are confident to run Weka from your own Java code you can create an array of your chosen classifiers and files and loop through them doing your tests. The code would be something like this (for an 80/20 split between training and testing):\n",
      "Weka\n",
      " \n",
      "You can run Weka from you command line. You will first need to add Weka.jar to you classpath. Then you can start reading files, running filters, classifiers, etc. through the command line. You can easily get started from . As an example, you can run randomForest using the following command:Where:You can find the detailed specification .If you want to use split-percentage rather than cross-validation, you simply replace with Alternatively, you can specify a separate test set by -T:If you run the commands as stated, the results will be printed in the command line. If you want to save them in a separate file you can follow what is suggested \n",
      "Weka\n",
      " \n",
      "You can use the Weka Experimenter GUI for this. A  big dialog box will appear next.To get run experiments with multiple algorithms, you must click on the \"New\" Button there, and then... oh well, ... this  will get  you started.\n",
      "Weka\n",
      " \n",
      "The error message you are getting says it cannot find any value after the  option.  The reason for this is that Weka is detecting that the string is immediately ending with a double quote after the  query parameter.  The root cause of this is a rogue quote mark which is appearing before the  term which belongs to the  query parameter:Change the string to the following and all should be fine:\n",
      "Weka\n",
      " \n",
      "Your training set is not appropriate for training a model for Weka however these information can be used in feature extraction.Your Test set can be converted into an arff file. From every message extract these basics features like 1. Any form of the word 'Happy' is present or not2. Any form of the word 'Sad' is present or not3. Any form of the word 'Angry' is present or not4. TF-IDFetc.then for some messages (say 70%) you should assign one class {Happy, Sad, Angry} manually and for remaining 30% you can test through your model.More about arff file is given here:\n",
      "Weka\n",
      " \n",
      "You did some classification. Weka by default always takes the last column/attribute from your training dataset and tries to predict its value from all the other attributes. (Unless you tell it to use a different one).Here, we can't tell if this is meaningful in your case. Probably not. (You don't show us any data). Thus, the Naive-Bayes classification you performed would only be useful if the last column already contained a sentiment classifier with values positive,neutral,negative, created by some unsupervised learning method in a previous preprocessing step. Weka's classification algorithms do not infer this for you. Now the result of what you did has nothing to do with sentiment analysis. I can't do this for you either.By the way, You only have 11 instances. Why don't you classify them yourself?\n",
      "Weka\n",
      " \n",
      "Weka calls the function  for each test set instance.\n",
      "Weka\n",
      " \n",
      "First of all you should always evaluate your algorithms with  For that you split your data into training and validation sets, train your classifiers with the first group and use the latter to get an approximate error of your classifier.That said, usually you'll end up testing different classifiers and algorithms. There is no way to tell for sure which method is going to work how well. Of course if you have more insight on the meaning behind your data, it will make things easier. You might split classification into different stages or modify your algorithm in some ways to suit the problem better. Basically everything is allowed and if for instance your values are scattered over the range of 0-100, but most are either in the range 0-10 or 90-100, you might want to check if it makes sense to make a first decision (like a decision stump) where you check if   or  and then use different classifiers for the two groups. It is simply not possible to give a general advice to an arbitrary problem. The \"best\" choice really depends on the very specific nature of your data.Same goes for the Kernels for some of the Classifiers. If you know anything about the nature of the data, you might be able to create a custom kernel function tailored for your problem. The algorithms you mentioned and others like SVM or adaboost differ in a lot of things. Some classifiers are cheap to train but cost more to evaluate (kNN) while others are expensive to train, but very cheap to evaluate. Like I said, you'll probably end up testing a lot of things. Just make sure you use cross validation to get an estimate of your classifier's performance. It's not enough to only look at the classification error during training. \n",
      "Kernels\n",
      " \n",
      "I'm not an expert on KDEs, so take all of this with a grain of salt, but a very similar (but much faster!) implementation of your code would be:If I'm not mistaken, the density estimate should integrate to 1, that is we would expect  to get at least close to 1. In the implementation above I'm getting there, give or take 5%, but then again we don't know that 0.1 is the optimal bandwith (using  instead I'm getting there to within 0.1%), and the uniform Kernel is known to only be about 93% \"efficient\".In any case, there's a very good Kernel Density package in Julia available , so you probably should just do  instead of trying to code your own Epanechnikov kernel :)\n",
      "Kernel\n",
      " \n",
      "This largely depends on your regression algorithm. Good features for Kernel based regression algorithms might be pretty bad for linear classifiers.()You seem to aim at the \"filter approach\". What works well in many regression settings is the Pearson Correlation. This is also available in ML-Lib.However, you should consider to not add the K top-correlated features, butAvoid selecting pairs of highly-correlated feature. So you have to build the correlation matrix between all pairs of features.Select the top-feature, build a regression model, measure the error of the model, measure the correlation between the error and the remaining features. This will greedily select the best featuresOnce you have selected you features you should consider doing a sensitivity analysis. This is, build a regression model for all features and, for all feature sets where one feature has been removed. If removing does not have a significant impact you can remove it.\n",
      "Kernel\n",
      " \n",
      "An isotropic Kernel is a kernel which depends only on the distance of the kernel arguments, is any suitable norm, usually the L2-norm.Intuitively this means that the direction of the deviation is of no importance. For instance, in two dimensions a change in variable  is equally important to a change in variable  -- which is of course often a too strong assumption. Therefore the predictor variables are often scaled appropriately.How you can use it in regression? Like any other kernel, but it's often simpler as the number of parameters is often quite small. The isotropic Gaussian for example has only one parameter.\n",
      "Kernel\n",
      " \n",
      "Kernel PCA starts by calculating the kernel matrix  with the entries is a  matrix in your case. Next, one diagonalizes the matrix to obtain the decomposition(In practice one would perform a dual PCA as that leads to smaller dimension, but that is not important here as you already have obtained the reduction).Reducing the dataset now corresponds to retaining only the three largest entries in the diagonal matrix of eigenvectors  and neglecting all others (-- your kernel is well chosen if this approximation is justified).Effectively, this means that in the projection matrix  only the first  rows are retained. Call the reduced matrix  which has dimension . Now \"reducing\" the number of features correspons to working with the matrixwhich as well has dimension . These rows are used as input to train the neural net.Let's come to your question: How to treat a new feature vector ?First, calculate a vector  with elements .Second, map that vector onto the KPCA subspace by calculating  -- which is a vector of dimension .Finally, feed the vector  into your Neural Network and use the prediction.\n",
      "Kernel\n",
      " \n",
      "There are some classic algorithms here:Hierarchical Agglomerative ClusteringDBSCANthat you should read and understand.\n",
      "Hierarchical\n",
      " \n",
      "If you know how to train and predict texts (or sentences in your case) using nltk's naive bayes classifier and words as features, than you can easily extend this approach in order to classify texts by pos-tags. This is because the classifier don't care about whether your feature-strings are words or tags. So you can simply replace the words of your sentences by pos-tags using for example nltk's standard pos tagger:['IN', 'PRP', 'VBP', 'JJ', 'IN', 'NNS', 'RB']As from now you can proceed with the \"contains-a-word\" approach.\n",
      "pos tagger:['IN\n",
      " \n",
      "As a general approach, you can use the  between POS vectors as a measure of their similarity. Alternative approach would be using the  between the two vectors. There are plenty of other distance functions between vectors. But it really depends on what you want to do and what does your data look like. You should answer questions like does the position matter? How much similarity would you give to these vectors? ('noun', 'verb') and ('verb', 'noun')? Is the distance between ('adverb') and ('adjective') less than distance between ('adverb') and ('noun')? and so on.\n",
      "POS\n",
      " \n",
      "First of all, learning like in any subject takes time so don't rush it or you will confuse yourself. The output syntax you see is a tree which take on the form of a series of lists and embedded lists. It may remind you of the syntax of a popular LISP such as  or .The tags to the left of the words / lists are what is known as POS () Tags, that represent the grammatical category the word falls into, essentially a word-category disambiguation. POS tagging is still one of the very difficult research areas of Natural Language Processing as a subject with F1-Scores in their high 90%'s. Your tree snippet built out (with the list below) looks as follows:POS Tagging is a great linguistic feature for tasks such as semantic parsing or named entity recognition. Some good resources to learn from include:List of Part-of-Speech Tags (Penn Treebank corpus) CC ~ Coordinating conjunctionCD ~ Cardinal numberDT ~ DeterminerEX ~ Existential thereFW ~ Foreign wordIN ~ Preposition or subordinating conjunctionJJ ~ AdjectiveJJR ~ Adjective, comparativeJJS ~ Adjective, superlativeLS ~ List item markerMD ~ ModalNN ~ Noun, singular or massNNS ~ Noun, pluralNNP ~ Proper noun, singularNNPS ~ Proper noun, pluralPDT ~ PredeterminerPOS ~ Possessive endingPRP ~ Personal pronounPRP$ ~ Possessive pronounRB ~ AdverbRBR ~ Adverb, comparativeRBS ~ Adverb, superlativeRP ~ ParticleSYM ~ SymbolTO ~ toUH ~ InterjectionVB ~ Verb, base formVBD ~ Verb, past tenseVBG ~ Verb, gerund or present participleVBN ~ Verb, past participleVBP ~ Verb, non-3rd person singular presentVBZ ~ Verb, 3rd person singular presentWDT ~ Wh-determinerWP ~ Wh-pronounWP$ ~ Possessive wh-pronounWRB ~ Wh-adverb\n",
      "POS\n",
      " \n",
      "If I understand it correctly, you are trying to treat sentences as a set of POS tags.In your example, the sentence \"My name is XYZ\" would be represented as a set of (PRP$, NN, VBZ, NNP).That would mean, every sentence is actually a binary vector of length 37 (because there are  + the CLASS outcome feature for the whole sentence)This can be encoded for OpenNLP Maxent as follows:or simply:(For working code-snippet see my answer here: )Some more sample data would be:\"By 1978, Radio City had lost its glamour, and the owners of Rockefeller Center decided to demolish the aging hall.\"\"In time he was entirely forgotten, many of his buildings were demolished, others insensitively altered.\"\"As soon as she moved out, the mobile home was demolished, the suit said.\"...This would yield samples:However, I don't expect that such a classification yields good results. It would be better to make use of other structural features of a sentence, such as the parse tree or dependency tree that can be obtained using e.g. .Edited on 28.3.2016:You can also use the whole sentence as a training sample. However, be aware that:- two sentences might contain same words but have different meaning- there is a pretty high chance of overfitting- you should use short sentences- you need a huge training setAccording to your example, I would encode the training samples as follows:Notice that the outcome variable comes as the first element on each line.Here is a fully working minimal example using .And some dummy training data (stored as ):This yields the following output:\n",
      "POS\n",
      " \n",
      "You need to use the Feature Hashing module to convert the text into word features. This, however, might not be enough as words are not good features for your problem. You may want to do some processing of the text and create more useful features (perhaps detecting the presence of zip codes, positions of numbers, etc...)Edit: Using the raw text column as one feature will not get you anywhere. You dont want your model to learn the addresses the way they are written. Instead, you need to learn patterns in the text that provide evidence for address vs. non-address instances.When you use feature hashing, the text column will be transformed to multiple word (or n-gram) columns, where the values represent counts of those words in each text input. The problem here is overfitting. For example, these two addresses have no words in common:100 broadway st, GA and 200 main rd, NY but its clear they have similar structure. One way to create useful features is to replace the words with tags: #NUM #TXT, #STATE and use feature hashing (bi-grams) to create features such as #NUM #TXT and , #STATE. As you can see, these bi-grams count as evidence in both addresses and suggest some kind of similarity between them (compared to other non-address instances). Of course this is an oversimplification of the problem but I hope you see why you cant use the raw text or plain feature hashing.You can still use the Azure ML modules for feature hashing, training, and scoring in addition to an Execute R module to do the text processing before training.Edit: Example of feature hashing usage:  \n",
      "Feature\n",
      " \n",
      "Start here (especially para Feature Selection Tutorials and Recipes):And there (lists the number of available methods for further googling):Also good article with more general discussion on the issue:Also the simplest method is to try to fit a RandomForest or Gradient Boosting Machine on your dataset. These algorithms automatically evaluate the importance of each feature during the fitting, after the classifier or regressor is fit you could access (in scikit-learn) its  property - \n",
      "Feature\n",
      " \n",
      "for dataset visit for fitness function, firstly describe your problem: classification or regression? if classification, how many class? in general for classification problems maximizing (true detection/(false detection + 1)) is a good fitness functionalso i think for feature extraction(selection) tasks, genetic programming led to better answers.\n",
      "feature extraction(selection\n",
      " \n",
      "You can doThat way you do not have to worry about how to do the multiplications.What you obtain after  or  are what is usually called the \"loadings\" for each sample, meaning how much of each component you need to describe it best using a linear combination of the  (the principal axes in feature space).The projection you are aiming at is back in the original signal space. This means that you need to go back into signal space using the components and the loadings.So there are three steps to disambiguate here. Here you have, step by step, what you can do using the PCA object and how it is actually calculated: estimates the components (using an SVD on the centered Xtrain): calculates the loadings as you describe obtains the projection onto components in signal space you are interested inYou can now evaluate the projection loss\n",
      "feature space).The\n",
      " \n",
      "Thank you very much for your answer, I didn't know about the pipeline feature beforeFor the case of L2 normalization turns out you can do it manually.Here is one example for a small array:Or do it manually with the weight of the squareroot of the squaresum:So turning them \"back\" to the raw formate is simple by multiplying with \"w\".\n",
      "feature beforeFor\n",
      " \n",
      "Both methods work with a population that is improved over many generations. The key difference is how the population is represented.Genetic Algorithms (GAs) work on individuals of the population, e.g. through mutation. You can enumerate the ancestors of each individual.The Cross-Entropy Method (CEM) represents the population as a probability distribution. Individuals are drawn from this distribution. The distribution parameters are re-estimated from the best e.g. 2% and the other 98% are discarded.Technically, &quot;the best 2%&quot; is also a probability distribution. You could draw a very large sample from it, but it's expensive. So you try to approximate &quot;the 2% distribution&quot; with your simple distribution. The  measures the difference between two distributions, which you want to minimize. Often this is simpler than it sounds: if your distribution is a Gaussian you can just estimate a new mean and (co-)variance from the best 2%.Practical considereations:The CEM requires you to come up with a probability distribution over individuals. Most GAs also require such a distribution only to generate the initial population, in addition to parameters like mutation strength.The CEM is simple to implement and has few parameters. It is a great baseline algorithm, and occasionally it beats methods that are much more sophisticated. However CMA-ES is a better baseline for continuous problems with just a few hundred parameters, because of its strong track record.Estimating parameters from just 2% of the population requires a very large population size. Throwing away 98% of the information is wasteful. On the other hand, it prevents the CEM from stepping &quot;sideways&quot; and getting distracted by sub-optimal solutions.GAs can be much more fancy and exist in many problem-specific variations. The CEM can be adapted to a problem by choosing a clever distribution. This works great for some discrete problems. Generally I'd say using a GA is one step up from the CEM, both in complexity (harder to get it working right) and in terms of potential performance (more opportunities to adapt its operators to the problem).References: (Mannor et al, 2003)(Szita and Lrincz, 2006) (Botev et al., 2013)\n",
      "Algorithms\n",
      " \n",
      "If you disable the colors, how many clusters do you see?I'd say there is only one big cluster in this data set, at least with this preprocessing/visualization.The three clusters you get from spectral clustering are meaningless. It's essentially doing quantization, but it did not discover structure. It minimized squared deviations by chunking the data into three similarly sized chunks. But if you would run it again, it would probably produce similar looking, but different chunks: the result is largely random, sorry.Do not expect classes to agree with clusters. As you can see in this data set, it may have three labels, but only one big messy \"cluster\".Its easy to produce this effect:produces this:Notice how similar this is to your result? Yet, this is a single gaussian, there are no clusters here. Spectral clustering produced 3 clusters here as requested, but they are totally meaningless.Always check your results if they are meaningful. It's really easy to get something that looks like a nice partitioning, but that is a random convex partitioning. Just because it produced clusters doesn't mean the clusters are there! - Algorithms such as k-means and spectral clustering are prone to Can you see the cluster in this face? ;-) Is it real?Here is a rather successful projection of a text data set using MDS. It shows a number of clusters stretching out into different directions of the data space. K-means and variants will not work well on this data set. Gaussian Mixture Modeling would work, but only on the projected data.\n",
      "Algorithms\n",
      " \n",
      "I'm not an expert in this field but I think you can look at some of the heuristic search algorithms like Genetic Algorithm (GA).Also, every games are different for instance the super mario vs chess game. I'm not sure what game you are after but GA has been successfully implemented to play Super Mario smartly. I think for a start you can develop an AI to play the game first, then meantime collect some data for analytics so that it can play better the next time? Again, I dont know what kind of game you are after so it is  difficult for me to contribute ideas to help you.\n",
      "Algorithm\n",
      " \n",
      "The number of filters is the number of neurons, since each neuron performs a different convolution on the input to the layer (more precisely, the neurons' input weights form convolution kernels).A feature map is the result of applying a filter (thus, you have as many feature maps as filters), and its size is a result of window/kernel size of your filter and stride.The following image was the best I could find to explain the concept at high level:Note that 2 different convolutional filters are applied to the input image, resulting in 2 different feature maps (the output of the filters). Each pixel of each feature map is an output of the convolutional layer.For instance, if you have 28x28 input images and a convolutional layer with 20 7x7 filters and stride 1, you will get 20 22x22 feature maps at the output of this layer. Note that this is presented to the next layer as a volume with width = height = 22 and depth = num_channels = 20. You could use the same representation to train your CNN on RGB images such as the ones from the CIFAR10 dataset, which would be 32x32x3 volumes (convolution is applied only to the 2 spatial dimensions).EDIT: There seems to be some confusion going on in the comments that I'd like to clarify. First, there are no neurons. Neurons are just a metaphor in neural networks. That said, \"how many neurons are there in a convolutional layer\" cannot be answered objectively, but relative to your view of the computations performed by the layer. In my view, a filter is a single neuron that sweeps through the image, providing different activations for each position. An entire feature map is produced by a single neuron/filter at multiple positions in my view. The commentors seem to have another view that is as valid as mine. They see each filter as a set of weights for a convolution operation, and one neuron for each attended position in the image, all sharing the same set of weights defined by the filter. Note that both views are functionally (and even fundamentally) the same, as they use the same parameters, computations, and produce the same results. Therefore, this is a non-issue.\n",
      "convolution kernels).A\n",
      " \n",
      "I suggest you to try out the NLTK library for python. You could get it here, also here is some basic manual for it:It has tons of algorithms for extraction of meaning from text. Also it has some of modules which allow you to:1) Extract entities2) Extract dates3) Establish relationship between extracted entities and dates.I suggest you to pay attention to timex.py module in NLTK library:It is mainly built to tokenize dates and times in text.And here is guide to extracting entity relationship:So I beleive you could extract interesting entities from your text (like the event you mentioned), you could extract dates as another set of entities, and using NLTK you could establish relationship between these extracted entities. As there result you should get what you need - what happened when.\n",
      "NLTK\n",
      " \n",
      "Here's a white-box answer:Using your orginal code, it outputs:Looking at line 17: It seems that the  requires an object that has an attribute  and intuitively it should be a  if the NLTK code is pythonic.Looking at , there isn't any clear explanation of what the  parameter should contain:).Checking the docstring, we see this example:Now we find the  function that converts the sentences into features and returnsBasically this is the thing that has the ability to call . Looking at the dict comprehension it seems like  is a little redundant unless it's for human readability. So you could have just used .Also, the replacement of space with underscore might also be redundant unless there's use for it later on.Lastly, the slicing and limited iteration could have been replaces with the ngram function that can extract character ngrams, e.g. Your feature extraction function could have been simplified as such:[out]:In fact, NLTK classifiers are so versatile that you can use tuples of characters as features so you don't need to patch the ngram up when extracting the features, i.e.:[out]:\n",
      "NLTK\n",
      " \n",
      "In short:Note:As of NLTK version 3.1, default  function is no longer the . It is now the perceptron tagger from , see Still it's better but not perfect:At some point, if someone wants  solutions, see In long:Try using other tagger (see ) , e.g.:HunPosStanford POSSennaUsing default MaxEnt POS tagger from NLTK, i.e. :Using Stanford POS tagger:Using HunPOS (NOTE: the default encoding is ISO-8859-1 not UTF8):Using Senna (Make sure you've the latest version of NLTK, there were some changes made to the API):Or try building a better POS tagger:Ngram Tagger: Affix/Regex Tagger:  Build Your Own Brill (Read the code it's a pretty fun tagger, ), see Perceptron Tagger: LDA Tagger: Complains about  accuracy on stackoverflow include:Issues about NLTK HunPos include: Issues with NLTK and Stanford POS tagger include:\n",
      "NLTK\n",
      " \n",
      "If you want to keep using the NLTK warper, you can simply do the following before training the classifier:\n",
      "NLTK\n",
      " \n",
      "I solved this by getting rid of the NLTK to scikit-learn adapter and by importing an NLTK module to help me convert my data structure to something feedable to the scikit-learn OneVsRestClassifier.HAPPY BEANS\n",
      "NLTK\n",
      " \n",
      "There are plenty of ways to go about solving this problem, however the solution I think would probably have the best chance of success would be to train your own haar cascade.You will need alot of images containing birds and a helluva lot of images without birds in. If you already have access to this dataset you need to analyze, then you already got alot of images (some of which you can train with)A haar cascade essentially searches the image for patterns it has been trained to recognize (such as a face, eyes, nose etc which comes with OpenCV) is a link to a tutorial/tools for making training your own cascade as painless as possible.Follow the instructions and at the end you should have an XML file that will be your Cascade for birds. is a tutorial on how to use your new cascade. I suggest you read up on how cascade classifiers work in general as it will help you understand what kind of images to train on, and what tweaking you can do to your images to ensure the best possible chance of detection.Another option you could pursue would be to just use OpenCV for preprocessing and then send your image to a neural network for classification.This could involve removing as much \"background noise\" information as possible from the image such as removing the colour bright green (unless you have bright green birds of course), smooth and scale down the image (take some weight off the network) and then plug the rest of the info to your network. I have no idea how successful this might be as I have never used a network for detection, only for face recognition, but it did a pretty damn good job at that, so give it a shot.Good luck and I hope this helps.\n",
      "OpenCV\n",
      " \n",
      "I don't know of any programs that can do specifically this, but I think it is an easy problem and you can code it yourself in no time.As you are in the computer vision field you must be used to OpenCV. You can use it to extract the frames from a video and to select the box and head center.Here are some links that can help you out:\n",
      "OpenCV\n",
      " \n",
      "The idea is right. But remember to convert your OpenCV Mats to float or double type before scaling.Something like:Update #1: Converting and scaling can also simply be done in one line, i.e. \n",
      "OpenCV\n",
      " \n",
      "Not sure about smaller libraries but I'm certain that OpenCV has HOG and Java support. Here is the relevant documentation: \n",
      "OpenCV\n",
      " \n",
      "You can find a pre-trained image classifier module in the OpenCV Library which creates a  image classification model for 'frontal faces' using the OpenCV Library. This is all available in Azure ML right now. But the documentation says more will be available in the future. Documentation is here - if it helps you tweak this module for your use:\n",
      "OpenCV\n",
      " \n",
      "The error was due to a bug in OpenCV 3.0.0 which has been fixed in OpenCV 3.1.0.\n",
      "OpenCV\n",
      " \n",
      "It's very hard to judge what the problem might be without additional information. OpenCV: As long as you're using a stable version, I doubt the problem is in the openCV library.The Training Method: Especially given that the data is MNIST, I doubt you would have to do any fine tuning to get a reasonable result. Just leaving parameters as default for opencv should be fine. You're methodology also seems sound.My guess is in the data, since that's the only variable that has changed and resulted in the failure. could you try and iterate over the training dataand see if there's anything wrong there? (number of samples, dimensions, etc.)\n",
      "openCV\n",
      " \n",
      "To know the maximum price for a given distance X.Take subset of your training data for which the vote=TrueBuild label point with label being \"Price\" and feature being \"Distance\"Train a linear regression model on the set of labeled points to predict \"Price\" given the \"Distance\"\n",
      "distance X.Take\n",
      " \n",
      "So DBN's are pretty complicated and it took me a few months to really wrap my head around them. Here's a quick overview though- A neural network works by having some kind of features and putting them through a layer of \"all or nothing activations\". These activations have weights and this is what the NN is attempting to \"learn\". NNs kind of died in the 80-90's because the systems couldn't find these weights properly. This was until the awesome  - who thoughy to pretrain the network with a restricted boltzman machine to get the weights in the right ball park.It depends on your goal, but if your goal is to learn how they work, I would start with Hinton's original paper and rewrite it to have functions instead of the static 3 layer network thats in the paper. This will give you a good intuition of whats going on in terms of the weights being learned and the activations. Now to answer your second question- there's a bit of debate- but in my experience the most key factor is coming up with the architecture of the system these variables are as follows: number of layers number of visible nodes  number of hidden nodes per layerOther variables you can control are what I would classify as optimization variables. These are:the activation function- tanh, sigmoid, reluthe learning rates for the variables learnedI'm going to warn you though, don't expect stellar results-- and be prepared to have a system that takes a long time to train. A second route you could go is to try some other systems out there like Caffe and that might give you more usable results. Anyways, good luck  :) ps, with such small data you might consider using SVMs instead. \n",
      "DBN\n",
      " \n",
      "You can  for C++.\n",
      "C++\n",
      " \n",
      "I may be wrong here as I know C++ and not Python, but  it should just be along the lines of:To save:To re-use later:\n",
      "C++\n",
      " \n",
      "I think you have forgotten to scale the input image during classification time, as can be seen in line 11 of the train_test.prototxt file. You should probably multiply by that factor somewhere in your C++ code, or alternatively use a Caffe layer to scale the input (look into ELTWISE or POWER layers for this).EDIT:After a conversation in the comments, it turned out that the image mean was mistakenly being subtracted in the classification.cpp file whereas it was not being subtracted in the original training/testing pipeline.\n",
      "C++\n",
      " \n",
      "C++  variable is double-precision floating-point number. The IEEE 754 standard specifies a binary64 as having:Sign bit: 1 bitExponent width: 11 bitsSignificand precision: 53 bits (52 explicitly stored)This gives 1517 significant decimal digits precision. If a decimal string with at most 15 significant digits is converted to IEEE 754 double precision representation and then converted back to a string with the same number of significant digits, then the final string should match the original. Check  for more information.\n",
      "C++\n",
      " \n",
      " - Open-source C++ library, with interface to Python and Java.Sounds like it has everything you need.\n",
      "source C++\n",
      " \n",
      "After build the c++ version, copy the release dll and lib files in ../windows/x64/Release/..(if you build x64 version) to ../wrapper/ then run python setup.py install \n",
      "c++\n",
      " \n",
      "The files are read/written using  and  in C++ using what seems to be a special format for tensor data.The files contain the values of the saved tensors. The simplest way to inspect those values would be to restore the tensors from the checkpoint file and inspect the tensors directly. \n",
      "C++\n",
      " \n",
      "A custom pooling layer would probably be implemented in C++. To see what you'd need to do, let's see where the implementation of  lives:The Python wrapper function ( itself) is automatically generated, in . This is ultimately imported into , so that it appears under  when you .In C++, there is an , and a .The .The gradient is defined as a separate op&mdash;&mdash;in the same places.There's a fair amount of work to do to add a new op (see  for a more complete guide), but hopefully these pointers can help!\n",
      "C++\n",
      " \n",
      "if you're open for a C++ library (not really C so) : But you'll need to developp yourself the reco, it's done by using a base of samples which are labelled as the noise they do. It's a long project, do you are familiar with signal processing as Fourier,DTW, etc ? There are standard process to do this.\n",
      "C++\n",
      " \n",
      "You've chosen a slightly confusing example for a couple of reasons. First, when you say , you aren't adding a new training point, you're just doing a new prediction, so that doesn't require any changes at all. Second, KNN algorithms aren't really \"trained\" -- KNN is an  algorithm, which means that \"training\" amounts to storing the training data in a suitable structure. As a result, this question has two different answers. I'll try to answer the KNN question first.K Nearest NeighborsFor KNN, adding new training data amounts to appending new data points to the structure. However, it appears that  doesn't provide any such functionality. (That's reasonable enough -- since KNN explicitly stores every training point, you can't just keep giving it new training points indefinitely.) If you aren't using many training points, a simple list might be good enough for your needs! In that case, you could skip  altogether, and just append new data points to your list. To make a prediction, do a linear search, saving the  nearest neighbors, and then make a prediction based on a simple \"majority vote\" -- if out of five neighbors, three or more are red, then return red, and so on. But keep in mind that every training point you add will slow the algorithm.If you need to use many training points, you'll want to use a more efficient structure for nearest neighbor search, like a . There's a  K-D Tree implementation that ought to work. The  method allows you to find the  nearest neighbors. It will be more efficient than a list, but it will still get slower as you add more training data.Online LearningA more general answer to your question is that you are (unbeknownst to yourself) trying to do something called . Online learning algorithms allow you to use individual training points as they arrive, and discard them once they've been used. For this to make sense, you need to be storing not the training points themselves (as in KNN) but a set of parameters, which you optimize.This means that some algorithms are better suited to this than others.  provides just a few algorithms . These all have a  method that will allow you to pass training data in batches. The  with  or  loss is probably a good starting point. \n",
      "KNN\n",
      " \n",
      "You can plot it bythen check out if the blue dots are somehow separated from the blue dots.KNN can be easily implemented. You need some labeled data set.When classifying a new data point, find the K closest points in the labeled data set and check from which class the majority is. Assing this label to new data point. Shouldn't be that hard to implement.\n",
      "KNN\n",
      " \n",
      "I assume that you want to use KNN in your decision process. To able to use KNN, you need to calculate a distance between two vectors. You can use the norm to calculate distance. Fortunately, MATLAB is doing this for us if you have Statistics and Machine Learning Toolbox.Let X be a vector and the each row of it is your 1x100 BOW vectors(transpose of them). and y be a vector that assign the class of each BOW vector. For instance, if you want to classify the images whether they includes bicycle or not, your y must contain binary(if bicycle is presented in image : 1 or Otherwise: 0) information about each histogram.Actually, I don't know whether it will work or not with BOW, because I always use SVM with it. So, good luck and please inform us if it worked or not. \n",
      "KNN\n",
      " \n",
      "Sounds like you want to use features which are themselves multi-dimensional. I'm not sure this works. Consider the increase in complexity that would occur for a distance-based metric like KNN; multi-dimensional features would require distance metrics and would get a lot more involved.I'd first try just flattening the arrays, so that each exemplar is an n-dimensional vector, rather than an array. So if  in your current approach is given by:you can instead go with:You could also consider creating new features that represent the information carried by the multi-dimensional features. You could use the mean, the max, the min, etc. Here's where your domain-specific knowledge kicks in.\n",
      "KNN\n",
      " \n",
      "The error message is telling you that some of your data have small std and constant, which will cause some problems when using correlation distance. The correlation distance in matlab will subtract the mean of the data first. Thus for a constant data vector, subtracting mean will result in a zero vector, and  the correlation of a constant vector with any other data vector is not defined.My suggestion to fix this problem is as follows:identify these data points based on std, and remove these data with small std before using knn clusfier;normalize your data may also help;try other distance metric.Hope this is helpful.\n",
      "knn\n",
      " \n",
      "Nothing to do with neural nets or your code,but this picture of KNN-nearest digits shows that some MNIST digitsare simply hard to recognize:\n",
      "KNN\n",
      " \n",
      "KNN works in a way that it finds the instances similar to it. As it calculates the  between two points. Now by normalization you are changing the scale of features which changed your accuracy.Look at  research. Go to figures you will find different scaling techniques giving different accuracies.\n",
      "KNN\n",
      " \n",
      "It is very likely that your dataset has sorting or groupings that you are not aware of. Usually you separate your model in training, test, and validation. At first glance in knn that is not explicitly required, because the algorithm in purely online. Let's see how it works,A1. A data set is given.A2. A candidate point is givenA3. The candidate point is classified with a majority voting of the k nearest neighbor classes.However that is the case when the data set encompasses all the required knowledge, i.e. It is the ground truth.In case that the dataset is not as such that we randomize and separate in training and validation, then we classify against train and check against validation to see if the training was successful. This is an iterative process of randomization and test until we get a train set that nicely evaluates on the validation set. Once this process is finished the test set is used to evaluate the generalization ability of the process.\n",
      "knn\n",
      " \n",
      "KNN family class constructors have a parameter called , you can switch between different distance metrics you want to use in nearest neighbour model.A list of available distance metrics can be found If you want to use cosine metric for ranking and classification problem, you can use norm 2 Euclidean distance on normalized feature vector, that gives you same ranking/classification (predictions that made by argmax or argmin operations) results.\n",
      "KNN\n",
      " \n",
      "Looks like you are something fundamentally wrong. Ideally KNN graph looks like above one. Here are few point you can use.Calculate distance in you code.Use below code for prediction in python1>1>\n",
      "KNN\n",
      " \n",
      "I think Naive Bayesian classification and KNN can be used.Naive Bayesian classification was proved can Filter spambut you need a Thesaurus before this.\n",
      "KNN\n",
      " \n",
      "This assignment helps you understand the steps in KNN. KNN is based on distances. Find the K nearest neighbors and then maybe vote for a classification problem. Your training data can be considered as (x1,x2, y) : age and profit are features (x1, x2) while BUY or NOT BUY is the label/output y. To apply Knn you need to calculate the distance, which is based on features. Since the two features share different units ( year, USD), you should convert them into non-unit features which is called normalization, part 4.1 in your handout. After that, the feature vector will look like (-0.4,-0.8). The number should be between -1 and 0 if the suggested formula in part 4.1 is used. Then use the normalized feature to calculate the distances (Euclidean in the handout) between every training data point and your interested company ( normalized as well).  This is required in 4.2. Last step should be to pick K nearest neighbor and decide BUY or NOT BUY judging from the outputs of those neighbors. ( a simple voting maybe?)\n",
      "KNN\n",
      " \n",
      "Logistic regression is applied when your dataset can be fit as a linear graph.Since your dataset contains multiple features,you can go ahead with KNN,Decision trees,Naive Bayes. KNN is simple but is computationally exhaustive.Decision Trees[CART] is a better choice as the algorithm will understand the data unlike KNN. If you are familiar with the concepts of SVM, you can try it,but it requires in depth understanding.\n",
      "KNN\n",
      " \n",
      "If you don't want to do a full database backup to disk and restore then you might try a Synchronize. It will incrementally copy a database from one SSAS instance to another without landing on disk as a backup file first:\n",
      "SSAS\n",
      " \n",
      "Just an excerpt from the Azure ML Book (one may find it useful):Host a web application, such as an ASP.NET webpage, and invoke the Azure Machine Learning web service server-side to conform to the current Azure Machine Learning CORS restrictions.Host your own web service that does provide CORS support and can in turn invoke the Azure Machine Learning web service on behalf of a wide variety of web and mobile clients via modern protocols and data formats like REST and JSON.\n",
      "Azure\n",
      " \n",
      "The Azure ML retraining API is designed to handle the workflow you describe:Hope this helps,Roope - Microsoft Azure ML Team\n",
      "Azure\n",
      " \n",
      "You can do it thru the column selector. In the sample below, the blood donation data (a sample dataset in Azure ML) has 25% of people who donated (class 1).\n",
      "Azure\n",
      " \n",
      "According to the Azure ML  the free tier is standalone, requiring a Live ID.  The Standard tier is associated with your Azure subscription, so you use your org IDs.\n",
      "Azure\n",
      " \n",
      "Azure public IP address are published and refreshed at regular intervals it can be found here: You can use these to specify the restricted IP range for access\n",
      "Azure\n",
      " \n",
      "I believe Azure ML already does this.  When you run it the second time, if nothing upstream from that tick has changed it should just load the results from the previous run.  It may take a few seconds for Azure ML to recognize that it is cached and reload it.\n",
      "Azure\n",
      " \n",
      "For Azure Machine Learning there is Mahout.For Power BI you might want to take a look at this post:For the rest, not sure, but there are open source solutions that can be googled.I am also curious about your motives for looking for a list like this.\n",
      "Azure\n",
      " \n",
      "You will need to first create a new job in the Azure management portal (), where you can configure the URL and the HTTP method to POST, and specify the body. However, the initial configuration screens don't let you add any headers, so once you have created the job, go in and edit it to add the following headers:Content-Type: application/jsonAccept: application/jsonAuthorization: Bearer This will work, but am wondering if this actually serves your purpose. If you're calling the synchronous (request response) endpoint of the AzureML service, you need to specify the inputs in the request payload, which is statically configured with the Azure Scheduler job. So you will effectively be repeating the same call over and over again. You may also want to explore  if your needs are served by calling the asynchronous (batch) endpoint of the AzureML service.\n",
      "Azure\n",
      " \n",
      "I found the problem. I was facing this error because the R module in the Azure ML was was taking variable as the other data type and not producing any outputs results which i was checking through for loop which is why i was getting this error.\n",
      "Azure\n",
      " \n",
      "I was wondering if you were trying to call Batch execution service. You may want to go over this tutorial - I think for Batch execution, you would need to either upload your data into Azure Blob for batch scoring or publish experiment as a web service without input port. \n",
      "Azure\n",
      " \n",
      "You would use Azure Data Factory instead of the scheduler. This would allow you to schedule the BES call into the future while identifying where the result file will end up. There are lots of examples online on how to do that.\n",
      "Azure\n",
      " \n",
      "I think Azure Data factory is the best way to schedule the BES calls.you can schedule your activities using data factory and pipeline.following links are useful:\n",
      "Azure\n",
      " \n",
      "Are you looking for a frequency distribution of string? A simple R script can do it pretty quick. So in Azure ML, you can drag/drop execute R module and use script that is similar to the following script\n",
      "Azure\n",
      " \n",
      "Azure ML supports \"execute-R\" module which can be easily used to accomplish this in R - few examples belowx&lt;-as.Date(\"12/3/2009\", \"%m/%d/%Y\")[1] \"December\"[1] \"Thursday\"[1] \"Q4\"\n",
      "Azure\n",
      " \n",
      "Have you tried saving your data in Azure SQL Database directly using a Data Writer:\n",
      "Azure\n",
      " \n",
      "At this time Azure Machine Learning only accepts the comma  seperated, American style CSV. You will need to convert to a comma separated CSV\n",
      "time Azure\n",
      " \n",
      "I've had similar problems with an Azure ML experiment published as a web service. Most of the times it was running ok, while sometimes it returned with a timeout error. The problem is that the experiment itself has a 90 seconds running time limit. So, most probably your experiment has a running time over this limit and returns with a timeout error. hth\n",
      "Azure\n",
      " \n",
      "I have worked internally to request a confirmation of your concern - but you expected feature is not available today in Azure ML Studio (The reason behind is Azure ML has to be able to read the input data as a tabular format, rows and columns). Such being the case, I would like to suggest you to submit a new feature request via below option:On Azure ML Studio -> the upper right corner, there is a smiley face, please click that and send the feedback.Should you have any further concerns, please feel free to let me know.\n",
      "Azure\n",
      " \n",
      "The Azure machine learning team has  which could help even if you aren't using AzureML.  From that article:Now, regarding your comment about \"fyi: the baseline prediction of always 0 is pretty high at 92.8%\": there are anomaly detection algorithms - meaning that the classification is highly unbalanced, with one classification being an \"anomaly\" that occurs very rarely, like credit card fraud detection (true fraud is hopefully a very small percentage of your total dataset).  In Azure Machine Learning, we use one-class support vector machine (SVM) and PCA-based anomaly detection algorithms.  Hope that helps!\n",
      "Azure\n",
      " \n",
      "This  contains interesting sources of data that you can use with Azure ML.  From the post:  Kaggle -   UCI Machine Learning Repository - Specifically, you could check out the  on data.gov.  \n",
      "Azure\n",
      " \n",
      "Managed to solve it:The manually uploaded dataset was formatted in Azure with \"String features\" only. (Because there where some NA's studio ML formats them this way).The R script however, formats the NA's differently and thus the columns as well.I'm not entirely sure what caused the different results because the data was character-wise identical. Only the NA's where formatted differently, as where the columns.The following solved my problem (at the end of the Rscript in studio ML):\n",
      "Azure\n",
      " \n",
      "based on our understanding of your question, here may has a miss conception that Azure ML parameters are not Global Parameters, as a matter of fact they are just parameter substitution tied to a particular module. So in affect there are no global parameters that are accessible throughout the experiment you have mentioned. Such being the case, we think the experiment below accomplishes what you are asking for:Please add an Enter Data module to the experiment and add Data in csv format. Then for the Data click the parameter to create a web service parameter. Add in the CSV data which will be substituted from data passed by the client application. I.e.Please add an Execute Python module and hook up the Enter Data output to the Execute Python input1. Add the python code to take the dataframe1 and add it to a python list. Once you have it in a list you can use it anywhere in your python code.Python code snippetdef azureml_main(dataframe1 = None, dataframe2 = None):Once you publish your experiment, you can add in new values in the Data: , section below with the new values that you was substituted for the Enter Data values in the experiment.  Please feel free to let us know if this makes sense.\n",
      "Azure\n",
      " \n",
      "Currently, Azure Machine Learning will bring that data onto ML servers.  \n",
      "Azure\n",
      " \n",
      "Read about . This is answer for your question. And here is the link to Azure ML tutorial \n",
      "Azure\n",
      " \n",
      "The accepted answer will not work in the latest Azure Storage SDK. MS has rewritten the SDK completely. It's kind of annoying if you are using the old version and update it. The below code should work in the new version.\n",
      "Azure\n",
      " \n",
      "Per my experience, The issue was caused by the function  load the dataset from the CSV file at your local path. On Azure ML, the dataset should be loaded from the Azure Storage thru programming in Python Storage SDK or using  module in MS Azure ML Studio.So I think you need to upload the existing data csv file into Azure Storage or an Azure ML experiment firstly (Please refer to ).Then you can access the uploaded dataset with Python (Please refer to ) and modify your code to make it works on Azure ML.For importing data into Azure ML, you can also refer to .For the error , there is a troubleshooting blog  to explain and resolve it for Azure ML. \n",
      "Azure\n",
      " \n",
      "After a while, an answer came back from the support team, so I am going to post the relevant part as an answer here for anyone who lands here with the same problem. \"This is a known issue. The container (a sandbox technology known as \"drawbridge\" running on top of Azure PaaS VM) executing the Execute R module doesn't support outbound HTTPS traffic. Please try to switch to HTTP and that should work.\"As well as that a solution is on the way :\"We are actively looking at how to fix this bug. \"Here is the original  as a reference.hth\n",
      "Azure\n",
      " \n",
      "As of today openGL graphics are not supported on Azure Machine Learning.  The rgl library is based on openGL and therefor not supported.  In theory, and this is only in theory, if a package were built on webGL instead, it \"should\" render in the graphics window as long as you are viewing from a modern browser.  That said, I do not see any webgl based r packages, only opengl to webgl conversion packages, which outputs a file.\n",
      "Azure\n",
      " \n",
      "Azure ML studio is for experimenting to find a proper solution to the problem set you have. You can upload data to sample, split and train your algorithms to obtain trained models. Once you feel comfortable with the results, you can turn that training experiment to a Predictive Experiment. From there on, your experiment will not be training but be predicting results based on user input.To do so, you can publish the experiment as a web service, once youve published the web service, under the web services tab you can find your web service and run samples with it. Theres a manual input box dialog ( entry boxes here depend on the features you were using in your data samples), some documentation and REST API info for single query and BATCH query processing with the web service. Under batch you can even find sample code to connect to the published webservice.From here on from any platform that can talk REST API, you can call the published webservice and get the results.Find below the article about converting from training to predictive experimentsHope this helps!\n",
      "Azure\n",
      " \n",
      "Typically you would use the Azure ML matchobox recommender, there is a very nice movie recommender built with Azure ML...However if you want to get down that line you will have to go much further than the movie recommender sample .You start here any way: \n",
      "Azure\n",
      " \n",
      "You could have a look at the Azure Data Factory (ADF), that will help you build data pipelines in the cloud. You can use ADF to read the data from the API (refresh your data), batch-wise-score it in Azure Machine Learning, and push it directly to your Azure SQL making PowerBI always seeing the latest data which will be scored. Take a look at the following blog where they take data through this kind of pipeline. You just have to change that the data doesn't come from Stream Analytics but from your API. \n",
      "Azure\n",
      " \n",
      "You can  an experiment (machine learning functions you hooked together in Azure ML Studio) as an API. When you call that API in your custom code you give it your data and all the computation runs in the cloud in Azure ML. \n",
      "Azure\n",
      " \n",
      "I am reasonably new to Azure machine learning but I do not believe it is possible to use their API without using the MS studio at all. For example you will need the API key to call the API and authenticate with it and the only way I am aware of that you can obtain this key is via the ML studio (after you have published a trained experiment).\n",
      "Azure\n",
      " \n",
      "This question was resolved thanks to some people on the Azure ML forum so I'm going to post an answer for anyone landing here in search for some answers...The short answer is no, this is not possible. The more detailed version is:\"From within the R script you cannot identify the internal AzureML IP addresses or the unique web service instances. When you make an external network call from the R script to an outside URL, that URL will see one of the AzureML public virtual IP's as the source IP. These are IP's of the load balancers, and not of the machines that are physically running the web service. AzureML dynamically allocates the instances of R engine in the backend, handles failures, and uses multiple nodes for running the web service for high availability. The exact layout of these for a given web service is not programmatically discoverable.\"Here is also the  to the original discussion. \n",
      "Azure\n",
      " \n",
      "Azure Machine Learning is part of the Cortana analytics suiteYou will find more info with the link belowAll the best\n",
      "Azure\n",
      " \n",
      "The Azure Machine Learning API requires the payload to be zipped, which is why the package insists on the zip utility being installed. (This is an unfortunate situation, and hopefully we can find a way in future to include a zip with the package.)It is unlikely that you will ever encounter this situation on Linux, since most (all?) Linux distributions includes a zip utility.Thus, on Windows, you have to do the following procedure once:Install a zip utility (RTools has one and this works)Ensure the zip is on your pathRestart R  this is important, otherwise R will not recognize the changed pathUpon completion, the litmus test is if R can see your zip.  To do this, try:You should get a result similar to this:In other words, R should recognize the installation path.On previous occasions when people told me this didnt work, it was always because they thought they had a zip in the path, but it turned out they didnt.One last comment: installing 7zip may not work. The reason is that 7zip contains a utility called 7zip, but R will only look for a utility called zip.\n",
      "Azure\n",
      " \n",
      "Yes, you can definitely do this with Azure Machine Learning.  It sounds like you have a clustering problem (you are trying to group together similar questions).  There is a \"Clustering: Find similar companies\" sample that does a similar thing at .  You can read the description on that page and click the \"Open in Studio\" button in the right-hand sidebar to actually open the workspace in Azure Machine Learning Studio.  In that sample, they are finding similar companies based on the text from the company's Wikipedia article (for example: Microsoft and Apple are similar companies because the word \"computer\" appears a lot in both articles).  Your problem is very similar except you would use the text in your questions to find similar questions and cluster them into groups accordingly.  In k-means clustering, \"k\" is the number of clusters that you want to form, so this number will probably be pretty big for your specific problem.  If you have 500 questions, perhaps start with 250 centroids?  But mess around with this number and see what works.  For performance reasons, you might want to start with a small dataset for testing and then run all of your data through the model after it seems to be grouping well.  Also, the documentation for K-means clustering is .  \n",
      "Azure\n",
      " \n",
      "Yes you can use Azure Machine Learning studio and could use the method Jennifer proposed. However, I would assume it is much better to run a R script against a database containing all current questions in your experiment and return a similarity metric for each comparison. Have a look at the following paper for some examples (from simple/basic to more advanced) how you could do this:A simple way to start would just be to implement a simple \"bags of words\" comparison. This will yield a distance matrix that you could use for clustering or use to give back similar questions. The following R code would so such a thing, in essence you build a large string with as first sentence the new question and then follow it with all known questions. This method will, obviously, not really take into consideration the meaning of the questions and would just trigger on equal word usage. \n",
      "Azure\n",
      " \n",
      "This was an Azure problem. I quote the Microsoft guy, To view the complete discussion, click \n",
      "Azure\n",
      " \n",
      "According to , the connection string should be:Note the difference in the format: different params for user, password and database vs. all in one in the first string.Also related, see this Azure page: . It states using , with no mention of .\n",
      "Azure\n",
      " \n",
      "I have read the source code in the python package azureml, and found out that they are using a simple request post when uploading a dataset, which has a limited content length 4194304 bytes.I tried to modify the code inside \"http.py\" within the python package azureml. I posted the request with a chunked data, and I got the following error:The REST API in azureml package does not support chunked transfer encoding. Hence, I took a look at how the Azure ML studio implements this, and I found out this:It post a request with content-length=0 to , which will return an id in the response body.Break the .csv file into chunks less than 4194304 bytes, and post them to If you really want this functionality, you can implement it with python and the above REST API.If you think it's too complicated, report the issue to . The azureml python package is still under development, so your suggestion would be very helpful for them.\n",
      "Azure\n",
      " \n",
      "Right now, Azure Data Lake Store is not a supported source, as you note.  That said, Azure Data Lake Analytics can also be used to write data out to Azure Blob Store, and so you can use that as an approach to process the data in U-SQL and then stage it for Azure Machine Learning to process it from Blob store.  When Azure ML supports Data Lake store, then you can switch that over. \n",
      "Azure\n",
      " \n",
      "Based on my understanding, I think you want to dynamically get the selected columns data via request the Azure ML webservice with some parameters on the client.You can refer to the offical document  and the blog  to know how to set and use the web service parameters to implement your needs via add the selected column names as array into the json parameter .Meanwhile, there is a client sample on GitHub . Althought it was writen in Java, I think it is easy to understand, then you can rewrite in Python with  package.\n",
      "Azure\n",
      " \n",
      "No. Currently we do not feature exporting weights from the models including with Azure Machine Learning. If you have a method for extracting weights from Python models, you may be able to work this out using the execute Python Script module.The primary purpose of Azure Machine Learning is to make deployable and scalable web services from the machine learning modules. Though the authoring experience for creating ML models is great, it is not intended to be a place to create and export models, but instead a place to create and operationalize your models. UPDATE New features may make this answer outdated. \n",
      "Azure\n",
      " \n",
      "I'm assuming you've created the webservice from the experiment and asking about the consumption of the webservice. You can consume the webservice from anything that can do an API call to the endpoint. I don't know the exact architecture of your solution but take a look at this as it might suit your scenario. Stream analytics on Azure has a new feature called Functions(just a heads-up, its still in preview) that can automate the usage of deployed ML services from your account.Since you are trying to gather info from IoT devices, you might use  or  to get the data and process it using Stream Analytics and during the process you can use the Webservice as Function in SA to achieve on-the-go ML results.Usage is relatively simple if you are familiar with Stream Analytics or SQL queries in general.This  shows the step by step implementation and the usage is below;Hope this helps!Mert\n",
      "Azure\n",
      " \n",
      "You can use \"import images\" module in Azure ML Studio that can read images from Azure blob storage -  is the sample experiment \n",
      "Azure\n",
      " \n",
      "The skip-gram architecture has word embeddings as its output (and its input). Depending on its precise implementation, the network may therefore produce two embeddings per word (one embedding for the word as an input word, and one embedding for the word as an output word; this is the case in the basic skip-gram architecture with the traditional softmax function), or one embedding per word (this is the case in a setup with the hierarchical softmax as an approximation to the full softmax, for example). You can find more information about these architectures in the original word2vec papers, such as  by Mikolov et al.\n",
      "word2vec\n",
      " \n",
      "I agree with you that word2vec is very impressive tool, but this model is trained by predicting the next word in some article or news. All in all, I think that using word2vec on image does not make sense.You can use  to do some image measure. e.g \n",
      "word2vec\n",
      " \n",
      "What is preventing you from saving the word2vec (the C program) output in text format and then read the file with a Java piece of code and load the vectors in a hashmap keyed by the word string?Some code snippets:If you want to get the nearest neighbours for a given word, you can keep a list of N nearest pre-computed neighbours associated with each WordVec object. \n",
      "word2vec\n",
      " \n",
      "Dl4j author here. Our word2vec implementation is targeted  for people who need to have custom pipelines. I don't blame you for going the simple route here.Our word2vec implementation is meant for when you want to do something with them not for messing around. The c word2vec format is pretty straight forward.Here is parsing logic in java if you'd like:Hope that helps a bit\n",
      "word2vec\n",
      " \n",
      " explains it very clearly.First, you should create word2vec model - either by training it on text, e.g. or by loading pre-trained model (you can find them , for example).Then iterate over all your words and check for their vectors in the model:Having that, just write word and vector formatted as you want.\n",
      "word2vec\n",
      " \n",
      "I think you mean texts of a certain language, but the named entities in that text may contain different names (e.g. from other languages)? The first thing that comes to my mind is some semi-supervised learning techniques that the model is being updated periodically to reflect new vocabulary. For example, you may want to use word2vec model to train the incoming data, and compare the word vector of possible NEs with existing NEs. Their cosine distance should be close.\n",
      "word2vec\n",
      " \n",
      "This is my first answer at SO, so here it goes..Your understanding in both parts seem to be correct, according to this paper :The paper explains word2vec in detail and at the same time, keeps it very simple - it's worth a read for a thorough understanding of the neural net architecture used in word2vec.The structure of Skip Gram does use a single neural net, with input as one-hot encoded target-word and expected-output as one-hot encoded context words. After the neural-net is trained on the text-corpus, the input weight matrix W   is used as the input-vector representations of words in the corpus and the output weight matrix W' which is shared across all the C outputs (output-vectors in the terminology of the question, but avoiding that to prevent confusion with output-vector representations used next..), becomes the output-vector representations of words. Usually the output-vector representations are ignored, and the input-vector representations, W are used as the word embeddings. To get into the dimensionality of the matrices, if we assume a vocabulary size of V, size of hidden layer as N, we will have W as (V,N) matrix, with each row representing the input vector of the indexed word in the vocabulary. W' will be a (N,V) matrix, with each column representing the output vector of the indexed word. In this way we get N-dimensional vectors for words.As you mentioned, each of the outputs(avoiding using the term output vector) is of size V and are the result of a softmax function, with each node in the output giving the probability of the word occurring as a context word for the given target word, resulting in the outputs not being one-hot encoded.But the expected outputs are indeed one-hot encoded, i.e in training phase, the error is computed by subtracting the one-hot encoded vector of the actual word occurring at that context position, from the neural-net output and then the weights are updated using gradient descent.Referring to the example you mentioned, with C=1 and with a vocabulary of  ['quick', 'fox', 'jumped', 'lazy', 'dog'] If the output from the skip-gram is [0.2 0.6 0.01 0.1 0.09], where the correct target word is 'fox' then error is calculated as -[0 1 0 0 0] - [0.2 0.6 0.01 0.1 0.09] = [-0.2 0.4 -0.01 -0.1 -0.09]and the weight matrices are updated to minimize this error.Hope this helps !\n",
      "word2vec\n",
      " \n",
      "I wouldn't do that. The quality of data is always an important fact. I would pre-process/filter data first. On the other hand, you can ingest all data and leave unclear words out to treat them later or leave it as not valid data. You can launch a batch process to clean data first so I don't think automatization is a problem. You can even ingest it/filter in real time (streaming) from the crawler and then start training your word2vec as soon as data is filtered.Sorry if my answer is too vague. Maybe if you tell us how you are approaching it or we can see some non-quality register the answer can be more accurate.Maybe this link can give you some clues: \n",
      "word2vec\n",
      " \n",
      "word2vec embeds words in a vector space whose dimension is user-defined. For computation and performance reasons, this dimension is often rather small (ranging between 50-1000).In fact, this  by Levy and Goldberg shows that word2vec efficiently computes a factorization of a PMI matrix, which is similar to the one you have in mind. Therefore, each coordinate in a word embedding can be interpreted as quantifying some unknown linear relation to multiple (if not all) context-words, not just one.\n",
      "word2vec\n",
      " \n",
      "Clustering is one of the biggest areas of machine learning (proportionally you could compare it to, say, \"integration\" in mathematics, or \"sorting\" in programming), and there are literally hundreds of different algorithms, focused on different problem settings and requirements. Some of them require to specify the number of clusters, some don't. Some can work with just the pairwise similarity, some require some explicit representation of the items being clustered, etc.I suggest you to start with two of the classical clustering algorithms: - here, you specify the number of clusters (\"k\") in advance, however the objects being clustered have to be points in a vector space (there are ways to reduce the problem of document clustering to a vector space problem - search for \"term vector representation\"). Since you're dealing with cosine similarity, looks like you already have a vector space, so you can use K-means. (in particular, \"single-linkage agglomerative clustering\" ) - here, you only need the pairwise similarities: you build a tree by repeatedly finding the two most similar documents and joining them into the same cluster, until you have the desired number of clusters.\n",
      "K\n",
      " \n",
      "This is a link for K-Means clustering in OpenCV for Python.Shouldn't be too hard to convert this to c++ code once you understand the logic\n",
      "K\n",
      " \n",
      "K-Means clustering is an iterative algorithm that makes multiple passes over the data. In each pass, points are assigned to cluster centroids and then after all points have been assigned, the cluster centroids are recomputed to be the mean of the assigned points. You can't \"stream\" data to the algorithm in the traditional sense as you'll need to come back to it during the subsequent iterations.Regarding the OpenIMAJ  implementation: yes this can handle \"big data\" in the sense that it doesn't mind where it gets the data from - the  instance that it takes as input can read data from disk if necessary. The only requirement is that you can hold all the centroids in memory during the runtime of the algorithm. The implementation is multi-threaded, so all cpu cores can be used during computation. There is example code here: .The OpenIMAJ  methods can be used to save the resultant cluster centroids in the  object.One of the biggest costs in K-Means is the computation of distances between each data point and each cluster centroid in order to find the closest. The cost of this is related to the dimensionality of the data and the number of centroids. If you've got a large number of centroids and high dimensional data, then using an approximate K-Means implementation can have big speed benefits at the cost of a slight loss in accuracy (see  for example -- this uses an ensemble of KD-Trees to speed neighbour computations). Regarding integration with Hadoop, it is possible to implement K-Means as a series of Map-Reduce tasks (each pair corresponds to an iteration of the algorithm). See this paper for a discussion:  . If you want to go down this route, OpenIMAJ has a very rough implementation here, which you could build off: . As mentioned in the linked paper, Apache Mahout also contains an implementation: . One problem with both of these implementations is that they required quite a lot of data to be transferred between the mappers and reducer (each mapper emits the current data point and its assigned cluster id). The extent of this could mean that it could be faster to use a non-Hadoop implementation of the algorithm, but this would depend on what processing resources you have available and the nature of the dataset. The problem of data-transfer between the map and reduce could probably also be reduced with a clever Hadoop  and computes weighted centroids from subsets of the data and then passes these to the (modified) reducer to compute the actual centroids.\n",
      "K\n",
      " \n",
      "I think there's some confusion here about the space in which you're doing your K-means clustering. In the code snippet that you included in your question, you're training up a  model using the vectorized face images as data points. K-means clusters live in the same space as the data you give it, so (as you noticed) your cluster centroids will also be vectorized face images. Importantly, these face images have dimension 9216, not dimension 2 (i.e., x-y coordinates)!To get 2-dimensional (x, y) coordinates as your K-means centroids, you need to run the algorithm using 2-dimensional input data. Just off the top of my head, it seems like you could apply a darkness threshold to your face images and assemble a clustering dataset of only the dark pixel locations. Then after you run K-means on this dataset, the centroids will hopefully be close to the pixel locations in your face images where there are the most dark pixels. These locations (assuming the face images in your training data were already somewhat registered) should be somewhat close to the eyes and mouth corners that you're hoping for.This can be really confusing so I'll try to add an example. Let's say just for an example you have \"face images\" that are 3 pixels wide by 4 pixels tall. After thresholding the pixels in one of your images it might look like:If you use this \"image\" directly in K-means, you're really running your K-means algorithm in a 12-dimensional space, and the image above would be vectorized as:Then your K-means cluster centroids will also live in this same 12-dimensional space.What I'm trying to suggest is that you could extract the (x, y) coordinates of the 1s in each image, and use those as the data for your K-means algorithm. So for the example image above, you'd get the following data points:In this example, we've extracted 3 2-dimensional points from this one \"image\"; with more images you'd get more 2-dimensional points. After you run K-means with these 2-dimensional data points, you'll get cluster centroids that can also be interpreted as pixel locations in the original images. You could plot those centroid locations on top of your images and see where they correspond in the images.\n",
      "K\n",
      " \n",
      "The most direct way to handle data of that form while still using k-means it to use a kernelized version of k-means. 2 implemtations of it exist in the JSAT library (see here )As Nicholas said, another option is to create a new feature space on which you run k-means. However this takes some prior knowledge of what kind of data you will be clustering. After that, you really just need to move to a different algorithm. k-means is a simple algorithm that makes simple assumptions about the world, and when those assumptions are too strongly violated (non linearly separable clusters being one of those assumptions) then you just have to accept that and pick a more appropriate algorithm. \n",
      "k\n",
      " \n",
      "Only the relative values matter. Say you have the following reward function...Now say we add a constant C to all rewards...We can prove that adding a constant C will add another constant K to the value of all states and thus does not affect the relative values of any state...Where...The values remain consistent throughout, so only the intervals between rewards matter, not their signs.It's important to note, however, that this rule does not apply for all episodic tasks. Generally, the rule only applies if the length of the episodes are fixed. For tasks where the length of each episode is determined by actions (think board games), adding a positive constant may result in a longer learning interval.\n",
      "K\n",
      " \n",
      "Firstly, canny detector is usually performed on grayscale images. If you are using some library like opencv, your rgb image is most likely converted under the hood.I assume that when you say \"edge strength between two colors\" you think of it like half of an image is one color and the other half is the other color. If you only want the strength of an edge in such an image, you don't need the direction of the edge. You don't need the image either. The strength of the edge is just the difference of two intensities which is nice because you can use k-means if your distance metric is transitive.If you have the rgb colors, just convert them to grayscale and group them up with k-means.Another thing is, if you use canny from a library it may be applying some smoothing before calculating the strengths which may not be desired for you.\n",
      "k\n",
      " \n",
      "K-means clustering uses the value of each observation, not the relationships between them. So, if you want to cluster the data in the form you have it you could use some other clustering method that takes pairwise relationships as an input.  can be one example. Another method that can be implemented in a way that accepts pairwise proximity matrix is If you could understand the law by which edge strengths depends on the color values, you could possibly obtain the absolute positions of the color in some coordinate space and cluster by that with k-means. For example, if you'd measure the strengths as euclidian distance in RGB - you could simply do k-means by RGB values of the colors. But looking at your data, seems like your edge strengths is not proportional to RGB euclidian distance.\n",
      "k\n",
      " \n",
      "You are confusingmulticlass scenario - where there are more than 2 classes in general but each object is assigned exactly onemultilabel scenario - where there are multiple labels assigned to each objectSVM cannot do either of the above in its basic formulation/implementation. Although both these problems can be easily decomposed.First one is often approached using one vs all or one vs one, both implemented in , where you have python binding to libsvm.Your scenario looks rather like multilabel, in such a case basic svm can be only used by splitting your problem to K independent ones, simply create K distinct training sets, each answering the question \"Does given file have label i?\" and train K distinct SVMs, each simply gives you one bit of your answer (we assume that labeling procedures are independent, which is a simplification, but other approach would require structural SVM approach, like the one available in svmstruct).You cannot create a single libsvm training file for multilabel classification. Documentation you cite is refering to multiclass, which is not your case, and simply requires using K different label names, not replicating rows.\n",
      "K\n",
      " \n",
      "As it was already noted, the answer heavily depends on an algorithm being used.If you're using distance-based algorithms with (usually default) Euclidean distance (for example, k-Means or k-NN), it'll rely more on features with bigger range just because a \"typical difference\" of values of that feature is bigger.Non-distance based models can be affected, too. Though one might think that linear models do not get into this category since scaling (and translating, if needed) is a linear transformation, so if it makes results better, then the model should learn it, right? Turns out, the answer is no. The reason is that no one uses vanilla linear models, they're always used with with some sort of a regularization which penalizes too big weights. This can prevent your linear model from learning scaling from data.There are models that are independent of the feature scale. For example, tree-based algorithms (decision trees and random forests) are not affected. A node of a tree partitions your data into 2 sets by comparing a feature (which splits dataset best) to a threshold value. There's no regularization for the threshold (because one should keep height of the tree small), so it's not affected by different scales.That being said, it's usually advised to standardize (subtract mean and divide by standard deviation) your data.\n",
      "k\n",
      " \n",
      "If I were you, I would try to try some dimensionality reduction ideas first and then do a multi-class classification. Using simple clustering or feature extraction algorithms, you should be able to create some groups of songs (10-100 groups). If you treat these groups as classes, I think you will be able to learn the features pretty well to be able to recommend your preferred songs in a given environment. Not to mention the problem becomes very tractable after that.However, if you are only after the one \"perfect\" song, K-nearest neighbours is probably your best bet.\n",
      "K\n",
      " \n",
      "The simplest way out of this is to add a column-specific prefix to all entries. Example of a parsed row:There are many other ways around this, including splitting each row into a set of k-mers and applying the Jaccard-based MinHash algorithm to compare these sets, but there is no need in such a thing in your case. \n",
      "k\n",
      " \n",
      "To scale your data, you can use . If  is your , it's as simple as  (if not, then  would also be scaled, which you likely don't want).Normalizing with  is a modeling decision. If you have reason to believe  is a better feature than , then go for it. Better yet, try both.If you're trying to predict  from same-day ,  and , then it's not really a time-series problem and k-Fold cross-validation is certainly sensible. If you're trying to predict a future  from today's , , ,  and maybe the respective lagged variables, then I don't see why you can't cross-validate either; just be careful to set it up such that you train on history and cross-validate against the future.You might get better answers on these modeling decisions on , since StackOverFlow is more programming-focused.\n",
      "k\n",
      " \n",
      "There is a strict link between linear regression and logistic regression.With linear regression you're looking for the ki parameters:With logistic regression you've the same aim but the equation is:Where  is the :So:and you need to fit K to your data.Assuming a binary classification problem, the output  is the estimated probability that the example  is a positive match in the classification task:When the probability is greater than 0.5 then we can predict \"a match\".The probability is greater than 0.5 when:and this is true when:The hyperplane:is the decision boundary.In summary:logistic regression is a generalized linear model using the same basic formula of linear regression but it is  for the probability of a categorical outcome.This is a very abridged version. You can find a simple explanation in  (third week of Machine Learning by Andrew Ng).You can also take a look at  for some notes on the lessons.\n",
      "K\n",
      " \n",
      "To answer the first question:If you have t observations, s states, and each state has e emission probabilities, then the trellis will have  nodes, and to evaluate each node will cost e operations, so the overall complexity of a naive implementation will be .Viterbi decoding can be used to decode sequences of bits.  If the observation depends on the previous k binary bits, then the number of different sequences of k bits is 2^k.  This represents the number s of states you would need to do a stream decoding (each state represents one configuration of previous bits).  However, this is unlikely to be relevant to you.The paper you link to describes an approach which reduces the number of nodes which need to be expanded.  This will not improve the worst case complexity, but may well give significant improvements in typical use depending on the nature of your specific problem.\n",
      "k\n",
      " \n",
      "It depends, but most would classify k-means as unsupervised. This is about k-means normally , so ideally I believe spark is following the same-\n",
      "k\n",
      " \n",
      "You can use  to tune your model parameters with a stratified K-Fold cross-validation split. Here is an example code.\n",
      "K\n",
      " \n",
      "Transform each one of your  strings into an -dimensional vector using a vector space model like  or . Then concatenate these vectors to one vector with  components. Optionally normalize each component to e.g. 0-1. You should then be able to run any regression (or classification) algorithm on the result, e.g. logistic regression.You might also try a clustering approach, where you cluster all the words in your vocabulary into  clusters, e.g. with k-means on the word vectors or using . You could then represent each word in your input array with a one hot vector (i.e.  zeros and a single one at the index of the cluster of that word). Then concatenate them again and run regression on the result.\n",
      "k\n",
      " \n",
      "k-d-tree and similar indexes are for dense and continuous data.Your data most likely is sparse.A good index for finding the nearest neighbors on sparse data is inverted lists. Essentially the same way search engines like Google work.\n",
      "k\n",
      " \n",
      "There is no one \"right way\" to split your data unfortunately, people use different values which are chosen based on different heuristics, gut feeling and personal experience/preference. A good starting point is the  (80-20). Sometimes using a simple split isn't an option as you might simply have too much data - in that case you might need to sample your data or use smaller testing sets if your algorithm is computationally complex. An important part is to randomly choose your data. The trade-off is pretty simple: less testing data = the performance of your algorithm will have bigger variance. Less training data = parameter estimates will have bigger variance.For me personally more important than the size of the split is that you obviously shouldn't always perform your tests only once on the same test split as it might be biased (you might be lucky or unlucky with your split). That's why you should do tests for several configurations (for instance you run your tests X times each time choosing a different 20% for testing). Here you can have problems with the so called model variance - different splits will result in different values. That's why you should run tests several times and average out the results.With the above method you might find it troublesome to test all the possible splits. A well established method of splitting data is the so called  and as you can read in the wiki article there are several types of it (both exhaustive and non-exhaustive). Pay extra attention to the k-fold cross validation.\n",
      "k\n",
      " \n",
      "OK, before we actually get into the details, let's give a brief overview on what K-means clustering is first.   works such that for some data that you have, you want to group them into k groups.  You initially choose k random points in your data, and these will have labels from .  These are what we call the centroids. Then, you determine how close the rest of the data are to each of these points.  You then group those points so that whichever points are closest to any of these k points, you assign those points to belong to that particular group ().  After, for all of the points for each group, you update the centroids, which actually is defined as the representative point for each group.  For each group, you compute the average of all of the points in each of the k groups.  These become the new centroids for the next iteration.  In the next iteration,  you determine how close each point in your data is to each of the centroids.  You keep iterating and repeating this behaviour until the centroids don't move anymore, or they move very little.How you use the  function in MATLAB is that assuming you have a data matrix ( in your case), it is arranged such that each row is a sample and each column is a feature / dimension of a sample.  For example, we could have  or  arrays of Cartesian coordinates, either in 2D or 3D.  In colour images, we could have  arrays where each column is a colour component in an image - red, green or blue.How you invoke  in MATLAB is the following way: is the data matrix we talked about,  is the total number of clusters / groups you would like to see and the outputs  and  are respectively an index and centroid matrix.   is a  array where  is the total number of samples that you have put into the function.  Each value in  tells you which centroid the sample / row in  best matched with a particular centroid.  You can also override the distance measure used to measure the distance between points. By default, this is the Euclidean distance, but you used the cosine distance in your invocation. has  rows where each row is a centroid.  Therefore, for the case of Cartesian coordinates, this would be a  or  array.  Therefore, you would interpret  as telling which group / centroid that the point is closest to when computing k-means.  As such, if we got a value of  for a point, this means that the point best matched with the first centroid, which is the first row of .  Similarly,  if we got a value of  for a point, this means that the point best matched with the third centroid, which is the third row of . Now to answer your questions:We just talked about  and  so this should be clear.The final centres are stored in .  Each row gives you a centroid / centre that is representative of a group.It sounds like you want to find the closest point to each cluster in the data, besides the actual centroid itself.  That's easy to do if you use  which performs K-Nearest Neighbour search by giving a set of points and it outputs the  closest points within your data that are close to a query point.  As such, you supply the clusters as the input and your data as the output, then use  and skip the first point.  The first point will have a distance of 0 as this will be equal to the centroid itself and the second point will give you the closest point that is closest to the cluster.You can do that by the following, assuming you already ran :You run , then toss out the closest point as it would essentially have a distance of 0.  The second column is what you're after, which gives you the closest point to the cluster excluding the actual centroid.   will give you which points in your data matrix  that was closest to each centroid.  To get the actual points, do this:Hope this helps!\n",
      "k\n",
      " \n",
      "I would first try a grid search over the parameter space but while also using a k-fold cross-validation on training set (and keeping the test set to the side of course). Then pick the set of parameters than generalize the best from the k-fold cross validation. I suggest using  with  (it's already the  for GridSearchCV when passing a classifier as estimator).Hypothetically an SVM with rbf can perfectly fit any training set as VC dimension is infinite. So if tuning the parameters doesn't help reduce overfitting then you may want to try a similar parameter tuning strategy for a simpler hypothesis such as a linear SVM or another classifier you think may be appropriate for your domain.Regularization as you mentioned is definitely a good idea if its available.The prediction of the same label makes me think that label imbalance may be an issue and for this case you could use different class weights. So in the case of an SVM each class gets its own C penalty weight. Some estimators in sklearn accept fit params that allow you to set a sample weights to set the amount of penalty for individual training samples.Now if you think the features may be an issue I would use feature selection by looking at F-values provided by f_classif and could be use with something like SelectKBest. Another option would be recursive feature elimination with cross validation. Feature selection can be wrapped into a grid search as well if you use sklearns Pipeline API.\n",
      "k\n",
      " \n",
      "2>If you are in a database context, and you send a query to the databasethen the database would include the query point, if it is stored in the database. In particular, you could easily remove it, if it was not desired to be included. From a database point of view, queries should include the query point, if it is in the database, and not, if it is not stored in the database.Even more, if you do density estimation, then every data point should contribute to the density, shouldn't it? Why would one point be special? What about other points with the exact same coordinates? What if you estimate density at a point that is not in the database? You would see an abrupt density increase if you move a tiny bit away from your query point!If you try to define the k-nearest-neighbors as a query to a database D, and do not require the query point x to be part of the database, then it follows naturally that the result should include the query point if it is part of D.2>On the other hand, it is counterintuitive that the 1-nearest-neighbor usually is the query point. Usually, when you are looking for \"the nearest neighbor\", you do mean \"the nearest other object\", unfortunately.Even if this would formally translate to \"nearest object to the coordinates of my query point in my database without my query point\".2>Unfortunately, this is not used consistently in literature.Some articles/authors/applications doand some do notinclude the query point. I can name plenty of examples from literature for both cases.Even a single article sometimes includes the query point in one figure, but not in the other!There will never be a solution that behaves according to the expectations of everybody, because people do have different ideas of what is \"correct\", unfortunately.2>You will have to decide on what you want the behaviour to be, and double check everything if it behaves the way you expect it to be. Document your decisions and observations.Please check yourself if the implementation of the k-distance graph in ELKI includes the query point. We may even (have) changed the behavior of this class for version 0.7 or 0.8; so it may be different for me than it is for you. Really, really look at the source of the exact version that you are using.If the k-distance graph does not include the query point, you would need to use the 3-distance for . If it does include the query point, then 4-distance agrees with . I'm pretty sure that DBSCAN does count the query point for above reasons (database point of view, density estimation point of view). Thus for DBSCAN, minPts=1 is nonsense (every point is a core point), and minPts=2 is single-linkage clustering (any epsilon-neighbors are merged). Only at minPts > 2 you start getting real DBSCAN results.GDBSCAN suggests to use  instead of 4; I usually start with , then try . There are several reasons to choose a larger :Higher dimensionality usually requires larger  (but for textual data, dimensionality is meaningless - at most choose by the intrinsic dimensionality)Noise: the noisier your data, the higher you need to go with Duplicates: if you have lots of duplicates, you again need to increase But don't overshoot. Index efficiency drops substantially with a large query radiuses. You want to choose  as small as you can, while still getting an interesting result. Also do use multiple values, to get different views.Remember that clustering is explorative data mining. It is meant to require you to experiment with parameters, and study the result, repeat. Because there is no correct clustering result. The quality of a clustering result is whether you can get a new insight on your data. A clustering that only reproduces a known result has in fact failed.\n",
      "k\n",
      " \n",
      "The K-Means clusterer expects a 2D array, each row a data point, which can also be one-dimensional. In your case you have to reshape the pandas column to a matrix having  rows and 1 column. See below an example that works:\n",
      "K\n",
      " \n",
      "Your approach is good. I would start with an unsupervised learning algorithm such as 'k-Nearest Neighbors classifier'. If your team doesn't know the first thing about ML, I recommend you to read this tutorial  . It uses python and a great library called scikit-learn. From there you could do Andrew's NG course () although it does not cover any recommendation systems. I usually go with a Pearson Correlation algorithm () and that suffices me for my problems. The problem with this approach is that it is linear. I have read that the Orange data mining tool provides many correlation measures. Using it you could find which one is best for your data. I would advice against using your own algorithm. There is an older question which provides further information on the matter: \n",
      "k\n",
      " \n",
      "The easiest thing to do is what was already proposed - clustering. More specifically, you extract a single feature vector from set X and then apply K-means clustering to the whole X &amp; Y set. ps: Be careful not to confuse k-means with kNN (k-nearest neighbors). You are able to apply only unsupervised learning methods.\n",
      "K\n",
      " \n",
      "The K-means complexity sounds reasonable for your data (only 4 components). The tricky part is the initialization and the choice of number of clusters. You can try different random initialization but this can be time consuming. An alternative is to sub-sample your data and run a more expensive clustering algorithm like Affinity Propagation. Then use the solution as init for k-means and run it with all your data.\n",
      "K\n",
      " \n",
      "For a billion feature vectors I'd be dubious of using K-means on its own.  I'm sure you could do it, but it would take a long time and would thusly be difficult to debug.  I recommend using  first then applying K-means to reduce the complexity and computations. These sub-clusters could then be reduced further with a Map Reduce implementation to solve even faster.\n",
      "K\n",
      " \n",
      "2>K-Fold CV works by randomly partitioning your data into  (fairly) equal partitions. If your data were evenly balanced across classes like , randomly sampling with (or without replacement) will give you approximately eqal sample sizes of  and . However, if your data is more like where one class over represents the data, k-fold cv without weighted sampling would give you erroneous results.If you use ordinary k-fold CV without adjusting sampling weights from uniform sampling, then you'd obtain something likewhere there are clearly splits without useful representation of both classes.The point of k-fold CV is to train/test a model across all subsets of data, while at each trial leaving out 1 subset and training on k-1 subsets.In this scenario, you'd want to use split by strata. In the above data set, there are 27  and 5 . If you'd like to compute k=5 CV, it wouldn't be reasonable to split the strata of  into 5 subsets. A better solution is to split it into k &lt; 5 subsets, such as 2. The strata of  can remain with k=5 splits since it's much larger. Then while training, you'd have a simple product of  from the data set. Here is some code to illustrateThis method can accomplish splitting the data into partitions where all partitions are eventually left out for testing. It should be noted that not all statistical learning methods allow for weighting, so adjusting methods like CV is essential to account for sampling proportions.Reference: James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An introduction to statistical learning: With applications in R.\n",
      "K\n",
      " \n",
      "Spectral clustering does not compute any centroids. In a more practical context, if you really need a kind of 'centroids' derived by the spectral clustering algorithm you can always compute the average (mean) of the points belonging at the same cluster, after the end of the clustering process. These would be an approximation of the centroids defined in the context of the typical k-means algorithm. The same principle applies also in other clustering algorithms that do not produce centroids (e.g. hierarchical).\n",
      "k\n",
      " \n",
      "The clusters produced by the K-mean algorithms separate your input space into K regions. When you have new data, you can tell which region it belongs to, and thus classify it.The centroids are just a property of these clusters.You can have a look at the scikit-learn  if you are unsure, and at the  to make sure you choose the right algorithm.\n",
      "K\n",
      " \n",
      "This is sort of a circular question: \"understand\" requires knowing something about the features outside of the k-means process.  All that k-means does is to identify k groups of physical proximity.  It says \"there are clumps of stuff in these 'k' places, and here's how the all the points choose the nearest.\"What this means in terms of the features is up to the data scientist, rather than any deeper meaning that k-means can ascribe.  The variance of each group may tell you a little about how tightly those points are clustered.  Do remember that k-means also chooses starting points at random; an unfortunate choice can easily give a sub-optimal description of the space.A centroid is basically the \"mean\" of the cluster.  If you can ascribe some deeper understanding from the distribution of centroids, great -- but that depends on the data and features, rather than any significant meaning devolving from k-means.Is that the level of answer you need?\n",
      "k\n",
      " \n",
      "The centroids are in fact the features learnt. Since k-means is a method of vector quantization we look up which observation belongs to which cluster and therefore is best described by the feature vector (centroid). By having one observation e.g. separated into 10 patches before, the observation might consist of 10 feature vectors max.Example: Method: K-means with k=10Dataset: 20 observations divided into 2 patches each = 40 data vectorsWe now perform K-means on this patched dataset and get the nearest centroid per patch. We could then create a vector for each of the 20 observations with the length 10 (=k) and if patch 1 belongs to centroid 5 and patch 2 belongs to centroid 9 the vector could look like: 0 - 0 - 0 - 0 - 1 - 0 - 0 - 0 - 1 - 0.This means that this observation consists of the centroids/ features 5 and 9. You could also measure use the distance between patch and centroid instead of this hard assignment.\n",
      "k\n",
      " \n",
      "Not sure about iris classification, but I've done written digit recognition from photos. I would recommend tuning up the contrast and saturation, then use a k-nearest neighbour algorithm to classify your images. Depending on your training set, you can get as high as 90% accuracy.I think you are on the right track. Do image preprocessing to make classification easier, then train an algorithm of your choice. You would want to treat each image as one input vector though, instead of classifying each pixel!\n",
      "k\n",
      " \n",
      "From an algorithmic standpoint, this likely means that there are too many neighbors equidistant to the target point, such that the algorithm cannot choose only k of them.  This often happens when you have a discrete distance function.Another problem that arises sometimes is when the classification is discrete classes, and the algorithm cannot handle ties properly.  However, reducing K to 1, which you've done, guarantees that you don't have that problem.\n",
      "K\n",
      " \n",
      "When you have many features and want to use some of them then you can apply feature selection (i.e. mRMR). So, it means that you have applied a dimensionality reduction.However, clustering is the assignment of a set of observations into subsets (called clusters) so that observations in the same cluster are similar in somesense. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis used in many fields (check ). When you want to group (cluster) different data points according to their features you can apply clustering (i.e. k-means) with/without using dimensionality reduction.\n",
      "k\n",
      " \n",
      "It is a general misconception that normalization will never reduce classification accuracy. It very well can.HOW ?Relative values in a row are also very important. They do determine as a matter of fact, the placement of points in the feature space. When you carry out normalization, it can severely offset that relative placement. This is felt, especially in k-NN classification, because it directly operates with respect to distance between points. Compared to that, its effect is not felt so strongly in SVM, because in that case, the optimization process can still be able to find a reasonably accurate hyperplane.You should also note that here, you normalize using avg(X). Thus, consider two points in adjacent columns of a particular row. If the first point falls well below average, and the second point falls well above average of their respective columns, while in an unnormalized sense, they are very close numerical values, distance calculation can differ hugely.Never expect normalization to do wonders.\n",
      "k\n",
      " \n",
      "Every time you add a dimension you make life more difficult, so moving from 1 to 3 dimensions is not likely to make live easier.I would sort the data into order and then think about ways to move along it from right to left. For example, move pointers so that they have a window of N items between them and find the position where the difference between the value at the left end of the window and the value at the right end of the window is smallest.If you really want to use k-means like clustering you can use dynamic programming on an array of sorted values to get an exact answer. Work from left to right and at each point compute, for i=1..k, the lowest cost way to divide up the values to that point into i clusters. You can work out the best answer at position N for i clusters by considering the cost, for each m, of making m..N a single cluster and looking at the answers for m-1 to work out the best cost of dividing the previous values up into i-1 clusters.\n",
      "k\n",
      " \n",
      "There is no one good answer for such question. What is interesting? For clustering, unfortunately, there is no such thing as an interesting or even well posed problem. Clustering as such has no well defineid evaluation, consequently each method is equally good/bad, as long as it has well defined internal objective. So k-means will always be good one to minimize inter-cluster euclidean distance and will struggle with sparse data, non-convex, imbalanced clusters. DBScan will always be the best in greedy density based sense and will strugle with diverse density clusters. GMM will be always great fitting on gaussian mixtures, and will strugle with clusters which are not gaussians (for example lines, squares etc.).From the question one could deduce that you are at the very begining of work with clustering and so need \"just anything more complex than uniform\", so I suggest you take a look at datasets generators, in particular accesible in scikit-learn (python)  or in clusterSim (R)  or clusterGeneration (R) \n",
      "k\n",
      " \n",
      "This may be similar to your question. A quote from this link: \"Your input file may have multiple features like age, location etc.  R could help you in applying K-Means clustering on multiple features. Apache Mahout implementation overwrite features instead of applying multiple features. And when you apply clustering on these multiple features, clusters would be formed based on all features instead of one. However, I am not sure about the use-case, So I am just discussing technical feasibility here. You may need to apply based on your use-case.\"Hope this helps.\n",
      "K\n",
      " \n",
      " is the primary method to access  and  index of   (or , in this case  only). It is quite well explained . In this specific case, from the  : divides all the samples in k groups of samples, called folds  (if k = n, this is equivalent to the Leave One Out strategy), of equal  sizes (if possible). The prediction function is learned using k - 1  folds, and the fold left out is used for test. Example of 2-fold  cross-validation on a dataset with 4 samples:In other words,  picks the  positions, these are used in the  loop over  and passed to  so that is selects the appropriate  (and all ) from the   containing the training set.\n",
      "k\n",
      " \n",
      "The most efficient approach would be to focus on one-class classifiers, then you just need to add one new model to the ensemble. Just to compare:Let us assume that we have K classes and you get 1 new plus P new points from it, your whole dataset consists of N points (for simpliciy - equaly distributed among classes) and your training algorithm complexity is  and if your classifier supports incremental learning then its complexity if OVO (one vs one) - in order to get the exact results you need to train K new classifiers, each with about N/K datapoints thus leading to , there is no place to use incremental trainingOVA (one vs all) - in order to get the exact results you retrain all classifiers, if done in batch fassion you need , worse than the above. However if you can train in incremental fashion you just need  which might be better (depending on the classifier).One-class ensemble - it might seem a bit weird, but for example Naive Bayes can be seen as such approach, you have generative model which models each class conditional distribution, thus your model for each class is actually independent on the remaining ones. Thus the complexity is This list is obviously not exhaustive but should give you general idea in what to analyze.\n",
      "K\n",
      " \n",
      "In general classification speed should not be a problem. Some exceptions are algorithms which have a time complexity depending on the number of samples you have for training. One example is k-Nearest-Neighbors which has no training time, but for classification it needs to check all points (if implemented in a naive way). Other examples are all classifiers which work with kernels since they compute the kernel between the current sample and all training samples.Many classifiers work with a scalar product of the features and a learned coefficient vector. These should be fast enough in almost all cases. Examples are: Logistic regression, linear SVM, perceptrons and many more. See @lejlot's answer for a nice list.If these are still too slow you might try to reduce the dimension of your feature space first and then try again (this also speeds up training time).Btw, this question might not be suited for StackOverflow as it is quite broad and recommendation instead of problem oriented. Maybe try  next time.\n",
      "k\n",
      " \n",
      "Effective way to get top K values is using of some K-select algorithm.For example, fast and reliable one is  (based on the same partition procedure as QuickSort), which has average complexity O(N). with regard to maximum seems pure mathematical conception, doesn't it?So I doubt that sum of infinite powers is applicable in real life (programming).\n",
      "K\n",
      " \n",
      "During the supervised training you usually assume that during training you get complete representation of the future types of objects. Typically - these are all labeled instances. In your case - there are also \"noise\" instances, thus there are basically two main approaches:As multi-class neural network has K output neurons, each representing the probability of being a member of a particular class, you can simply condition on these distribution to say that the new object does not belong to any of them. One particular approach is to check if  (where p(y|x) is the activation of output neuron) for some threshold . You can either set this value by hand, or through analysis of you \"noise\" instances (with you do have some for training). Simply pass them through your network and compare what value of T gives you best recognition rateAdd another one-class classifier (anomaly detector) before your network - so you will end up with the sequence of two classifiers, first is able to recognize if it is a noise or element of any of your classes (notice, that this can be trained without access to noise samples, see one-class classification or anomaly detection techniques.You could also add another output to the network to represent noise, but this probably will not work well, as you will force your network to generate consistent internal representation for both noise vs data and inter-class decisions.\n",
      "K\n",
      " \n",
      "The algorithm being used is K-Means. K-Means is an unsupervised clustering algorithm. Articles are clustered by topic. Some articles contain multiple topics, many of which are the same between article. Those shared topics are then branches emerging from the initial topic. SKLearn is a great library for Python that does clustering very well. R is also great for clustering. Hope this helps!\n",
      "K\n",
      " \n",
      "In order to be fully aware of what is happening in your model, you must first take some time to study the logistic regression algorithm (eg. from lecture notes or ). As with other supervised techniques, logistic regression has hyper-parameters and parameters. Hyper-parameters basically specify how your algorithm runs, which you must provide at initialisation (ie. before it sees any data). For example, you could have prior information about the distribution of classes, which then would be a hyper-parameter.  Parameters are \"learnt\" from your data.Once you understand the algorithm, the interesting question will be what the parameters of your model are (recall that these are retrieved from the data). By visiting the , you find in the attributes section, that this classifier has 3 parameters, which you can access by their field names.If you are not interested in such details, but only want to assess the accuracy of your classifier, a useful technique is cross-validation. You split your labeled data into k equal sized subsets, and train your classifier using k-1 of them. Then you evaluate the trained classifier on the remaining 1 subset and calculate the accuracy (ie. what proportion of the data could be predicted properly). This method has its drawbacks, but proves to be very useful in general.\n",
      "k\n",
      " \n",
      "First of all, you can calculate the distance matrix for your points and then use any clustering algorithm: K-Means, DBSCAN, and so on.You can also try  - the fast spatial clustering algorithm.\n",
      "K\n",
      " \n",
      "I can think of three options.  I am unaware of any well developed methodology to do this specifically with K-means clustering.Look at the confusion matrices between the two approaches.Compare the  between the clusters, and between items in clusters to their nearest other clusters.Look at the Vornoi cells and see how far your points are from the boundaries of the cells.The problem with 3, is the distance metrics get skewed, 3D distance vs. 73D distances are not commensurate, so I'm not a fan of that approach.  I'd recommend reading some books on K-means if you are adamant of that path, rank speculation is fun, but standing on the shoulders of giants is better.\n",
      "K\n",
      " \n",
      "I have got the same issue and posted a , which luckily got resolved.In my case it's because the data are put in order, and I'm using K-fold cross-validation without shuffling when doing the test-train split. This means that the model is only trained on a chunk of adjacent samples with certain pattern.An extreme example would be, if you have 50 rows of sample all of class A, followed by 50 rows of sample all of class B, and you manually do a train-test split right in the middle. The model is now trained with all samples of class A, but tested with all samples of class B, hence the test accuracy will be 0.In scikit, the  do the shuffling by default, while the  doesn't. So you should do one of the following according to your context:Shuffle the data firstUse train_test_split with shuffle=True (again, this is the default)Use KFold and remember to set shuffle=True\n",
      "K\n",
      " \n",
      "There are two ways to perform the K-Nearest Neighbour in Matlab. The first one is by using  as you did. However, this function will return the predicted labels and you cannot use  with this. The cross-validation is performed on a model, not on its results. In Matlab, the model is described by an object.   only works with objects (classifier objects, be it K-NN, SVM and so on...). In order to create the so-called nearest-neighbor classification object you need to use the  function. Given the Training Set and the Training Labels as input (in this order), such function will return your object, which you can give as input in .There's only one thing left though: how do I predict the labels for my validation set? In order to do this, you need to use the  function. Given the model (kNN object) and the Validation Set as input (again, in this order), such function will return (as in ) the predicted labels vector.\n",
      "K\n",
      " \n",
      "Q-Learning is a TD (temporal difference) learning method. I think you are trying to refer to TD(0) vs Q-learning. I would say it depends on your actions being deterministic or not. Even if you have the transition function, it can be expensive to decide which action to take in TD(0) as you need to calculate the expected value for each of the actions in each step. In Q-learning that would be summarized in the Q-value.\n",
      "Q-Learning\n",
      " \n",
      "In the for loop of classify you are trying to find the training example that is closest to a test point. You need to switch that with a code that finds K of the training points that is the closest to the test data. Then you should call getClassCode for each of those K points and find the majority(i.e. the most frequent) of the class codes among them. classify will then return the major class code you found.You may break the ties (i.e. having 2+ most frequent class codes assigned to equal number of training data) in any way that suits your need.I am really inexperienced in Java, but just by looking around the language reference, I came up with the implementation below.All you need to do is adding K as a parameter. Keep in mind, however, that the code above does not handle ties in a particular way.\n",
      "K\n",
      " \n",
      "With the K vs Weights plot you've just answered your own question. That is indeed very smart.  The optimal K value for your dataset is where the elbow is (circa 350).What does it mean? It basically means that taking into account another neighbour does not give a better modelling of the data.You can object then that choosing 350 or 400 will lead to the same results since the weights are equal. Correct. However it is always recommended to choose for the smallest value because the model you're training will have a minor complexity (fewer number of neighbours to take into account) with respect to the same results (i.e. weights).Such bruteforcing techniques are commonly used for many algorithms in machine learning:in K-NN to find the optimal number of neighboursin K-Means to find the optimal number of clustersin SVMs to find optimal tuning parametersand so on and so forth...I've been doing the very same experiment as you did, but with another dataset and I obtained the following plot:and as you can see for this dataset the optimal K is circa 50.\n",
      "K\n",
      " \n",
      "With k-means, the chances are that you already have a big heap of garbage. Because it is an incredibly crude heuristic, and unless you were extremely careful at designing your features (at which point you would already know how to check the quality of a cluster assignment) the result is barely better than choosing a few centroids at random. In particular with k-means, which is very sensitive to the scale of your features. The results are very unreliable if you have features of different types and scale (e.g. height, shoe size, body mass, BMI: k-means on such variables is statistical nonsense).Do not dump your data into a clustering algorithm and expect to get something useful. Clustering follows the GIGO principle: garbage-in-garbage-out. Instead, you need to proceed as follows:identify what is a good cluster in your domain. This is very data and problem dependant.choose a clustering algorithm with a very simialar objective.find a data transformation, distance function or modification of the clustering algorithm to align with your objectivecarefully double-check the result for trivial, unwanted, biased and random solutions.For example, if you blindly threw customer data into  clustering algorithm, the chances are it will decide the best answer to be 2 clusters, corresponding to the attributes \"gender=m\" and \"gender=f\"simply because this is the most extreme factor in your data. But because this is a know attribute, this result is entirely useless.\n",
      "k\n",
      " \n",
      "There are minor differences in multiple logistic regression models and a softmax output.  Essentially you can map an input of size d to a single output k times, or map an input of size d to k outputs a single time. However, multiple logistic regression models are confusing, and perform poorer in practice. This is because most libraries (TensorFlow, Caffe, Theano) are implemented in low level compiled languages and are highly optimized. Since managing the multiple logistic regression models is likely handled at a higher level, it should be avoided. \n",
      "k\n",
      " \n",
      "Edited my answer as I miss-read your question.  Yes, that is correct. Normally you only run k-means once. The maximum iterations is the maximum number of iterations you will allow for the k-means centroid update loop to occur. Spark's implementation does supports what have described with runs, ie. how many times do you want to run the algorithm. Its usually not necessary. Especially since optimizing the k-means metric does not necessarily optimize what your actual goal is. You should not ask such kinds of questions, these things are always problem and data dependent. You should work to better understand the tools you are using and what they mean and how to iterate with them. This will help you avoid putting yourself in such a situation that you want to ask that kind of question, or if it is warranted - what other context is needed (just the number of datums is certainly not enough context for any meaningful discussion). \n",
      "k\n",
      " \n",
      "My go to for this would either be a deep-learning framework using convolutional layers for pixel classification, or K-means/ K-Nearest Neighbour algorithm. This does depend on your data, however. From your post I am assuming that your data isn't labelled? meaning you are unable to feed in the 'truth' to the algorithm for classification. you could perhaps use a CNN (convolutional neural network) for pixel classification (image segmentation) which should identify the location of a person. given this, perhaps you could run a 'local' CNN i a region close to the face identified to classify the region the body is located in as a certain pose. This would probably be my first take on the problem but would depend on the exact structure of your data, and the structure of your labels (if you have any).I have to say it does sound like a fun project!\n",
      "K\n",
      " \n",
      "I think you've answered your own question: if you know it looks like , then use .Support Vector Machines are likely to work well or even best in a lot of problems. Since your data is obviously not generated by a linear process, I suggest you don't use any linear estimator. Other than that, a lot of estimators can work well. Pick one and see if you're happy with it.\n",
      "Vector Machines\n",
      " \n",
      "3>Classification is about determining a (categorial) class (or label) for an element in a datasetPrediction is about predicting a missing/unknown element(continuous value) of a dataset3>In classification, data is grouped into categories based on a training dataset.In prediction, a classification/regression model is built to predict the outcome(continuous value)3>In a hospital, the grouping of patients based on their medical record or treatment outcome is considered classification, whereas, if you use a classification model to predict the treatment outcome for a new patient, it is considered a prediction.\n",
      "Classification\n",
      " \n",
      "Classification is the prediction of a categorial variable within a predefined vocabulary based on training examples.The prediction of numerical (continuous) variables is called regression.In summary, classification is one kind of prediction, but there are others. Hence, prediction is a more general problem.\n",
      "Classification\n",
      " \n",
      "Classification is the process of identifying the category or class label of the new observation to which it belongs.Predication is the process of identifying the missing or unavailable numerical data for a new observation. That is the key difference between classification and prediction. The predication does not concern about the class label like in classification.\n",
      "Classification\n",
      " \n",
      "Restating, the problem is that the five binary models you've trained are not mutually exhaustive.  There are several possibilities.First of all, do you have a 100% clean classification for each of the five sentiments, or are there some acknowledged classification errors?You need a set that is mutually exclusive and exhaustive.  Your approach suggests, but hardly guarantees, this result.  You might consider an integrated solution that does make this guarantee.  Multi-class SVM is one such, but may not apply well to your situation.If the classes are not 100% accurate, you can easily have all five rejecting a particular observation.  This suggests that your classification algorithms need tuning, or that the data themselves are not as amenable to classification as you would like.You might also check that you've cleaned that data appropriately; a few errors can seriously move the class boundaries.What I suspect is happening is a small-boundary effect: each class, when compared against the combination of the other four, \"pulls in\" its boundaries, leaving unclaimed territory between the final sets.Do you have a way to check the classification parameters after training?  If so, can you visualize the five boundaries selected?  If you do find pathological gaps, are there training parameters you can tune, such as giving a larger epsilon to the training groups?I hope this helps.\n",
      "classification errors?You\n",
      " \n",
      "Classification examples:- Predicting whether a share of a company is good to buy or no given that the previous history of the company, along with the buyer's review on it saying yes or no for buying the share. (Discrete answer: Buy - Yes/No)Regression example:-Predicting the best price at which one should buy the share of a company given that the previous history of the company, along with the buyer's review of the price at which they bought the share in the past. (Continuous answer:- Price range)\n",
      "Classification examples:-\n",
      " \n",
      "An unsupervised clustering technique fails on scanned documents since it fails to grasp the underlying structure and ends up giving non nonsensical clusters. So the approach is fundamentally flawed. However Classification using deep convolutional neural networks, with sufficient data and carefully chosen distinct classes, can outperform OCR techniques if the documents have a distinct structure. \n",
      "Classification\n",
      " \n",
      "If you have a large number of categories, Classification algorithm does not work well. Instead, there is a better approach of doing this. You apply regression algorithm on data and then train offset on those output. It would give you better results. A sample code can be found . \n",
      "Classification\n",
      " \n",
      "There are a couple of things that make this code run slowly. is essentially just syntactic sugar for a  loop over the rows of a column. There's also an explicit  loop over a NumPy array in your function (the  part). Looping in this (non-vectorised) way is best avoided whenever possible as it impacts performance heavily. is looking up  across an entire column for each row of , then returning a sub-DataFrame and then creating a new NumPy array. Repeatedly looking for values in this way is expensive ( complexity each lookup). Creating new arrays is going to be expensive since memory has to be allocated and the data copied across each time.If I understand what you're trying to do, you could try the following approach to get your new column.First use  to group the rows of  by the values in 'session'.  is used to join up the strings for each value:where  is the column from  which contains the values you want to join together into a string.  means that only one pass through the DataFrame is needed and is pretty well-optimised in Pandas. The use of  to join the strings is unavoidable here, but only one pass through the grouped values is needed. is now a Series of strings indexed by the unique values in the 'session' column. This is useful because lookups to Pandas indexes are  in complexity.To match each string in  to the approach value in  you can use . Unlike ,  is fully vectorised and should be very fast:\n",
      "NumPy\n",
      " \n",
      "Not sure if this is your error at all but I got this when I was using regular arrays when I should have been using Numpy.\n",
      "Numpy\n",
      " \n",
      "Matplotlib is a very good library for that task. You can plot histograms, scatter plots and lot of other things. You just have to know what you want and then you can probably do it with that. I use that for similar tasks a lot.[UPDATE]As I said, you can do that with matplotlib. Here is an example from their gallery: It's not so pretty as with the answer in the comment of @lejlot, but still correct.\n",
      "Matplotlib\n",
      " \n",
      "Goal was to achieve similar results using Numpy and Tensorflow. The only change from original answer is  parameter for  api.Initial approach :  - This however does not provide intended results when dimensions are N.Modified approach:  - Always sum on the last dimension. This provides similar results as tensorflow's softmax function.\n",
      "Numpy\n",
      " \n",
      "It's possible to do what you've started in Numpy, but I think it's too low-level for this sort of stuff. I'd suggest you install . Then, you can do the following\n",
      "Numpy\n",
      " \n",
      "If you read the error message you can see that passing single dimensional arrays will soon not be supported. Instead you have to ensure that your single sample looks like a list of samples, in which there is just one. When dealing with NumPy arrays (which is recommended), you can use  however as you're using lists then the following will do: \n",
      "NumPy\n",
      " \n",
      "I'm not sure your conjecture is true. Because of the nature of complete-linkage clustering, each time you agglomerate two clusters, you are doing so because the two elements that are farthest apart between those two clusters, are still mutually closer to each other than the farthest elements to any other cluster.What you're trying to prove is that However, after merging two clusters A and B due to complete-linkage clustering, there could still exist an element in cluster C that is nearer to an element in Cluster AB than any other element in cluster AB because complete-linkage is only concerned about maximal distances.2>3>The observations, A, B, C, D, and E are arranged in a straight line.Observation A is 1 unit away from observation BObservation B is 3 units away from observation CObservation C is 2.5 units away from observation DObservation D is 2 units away from observation ELet's perform hierarchical clustering:First A and B are merged because distance is 1:New picture:Cluster AB is 4 units away from observation C (because A is 4 units from C due to complete-linkage clustering), which is 2.5 units from D, which is 2 units from ENext, D and E are merged because distance is 2New pictureCluster AB is 4 units from observation C (as before), which is 4.5 units from Cluster DE because C is 4.5 units from E due to complete-linkage clustering.Next, C would need to be merged into AB because its distance is 4 whereas it's 4.5 to DE:Cluster ABC is 8.5 units from DE because A is 8.5 units from E.BUT, at this point we've disproven your conjecture. Element C is 3 units from B and 4 units from A (refer to original diagram). However, Element C is only 2.5 units from element D, which is inside another cluster.\n",
      "Cluster\n",
      " \n",
      "Are you sure you want cluster analysis?To find similar records you don't need cluster analysis. Simply find similar records with any distance measure such as Jaccard similarity or Hamming distance (both of which are for binary data). Or cosine distance, so that you can use e.g. Lucene to find similar records fast.To find common patterns, the use of frequent itemset mining may yield much more meaningful results, because these can work on a subset of attributes only. For example, in a supermarket, the columns Noodles, Tomato, Basil, Cheese may constitute a frequent pattern.Most clustering algorithms attempt to divide the data into k groups. While this at first appears a good idea (get k target groups) it rarely matches what real data contains. For example customers: why would every customer belong to exactly one audience? What if the audiences are e.g. car lovers, gun lovers, football lovers, soccer moms - are you sure you don't want to allow overlap of these groups?Furhermore, a problem with cluster analysis is that it's incredibly easy to use badly. It does not \"fail hard\" - you always get a result, and you might not realize that it's a bad result...\n",
      "cluster analysis?To\n",
      " \n",
      "That's a tough questions. Generally this kinds of machine predict based on Data Mining result form previous data. These data can be so huge that human won't be able to find any patter on that data. In that case machine is far batter than human as they will implement its algorithm and  predict intelligently. But if you ask that do the machines know what is human's choice as human himself is unpredictable :) Yes there are so many researcher working on data mining, autonomous agent or human-agent teamwork. We can see some of by searching on these topics fat Google school-er. Best of luck  \n",
      "Data\n",
      " \n",
      "You can just configure that by using the modules under Data Format Conversions. Have a look  and . Documentation is in progress, unluckily. \n",
      "Data\n",
      " \n",
      "You can read  about Data sources in Power BI.You can connect your Power Bi to: Azure Blob, Azure SQL server, Azure SQL Data Warehouse, Azure Table Storage, or Azure HDInsight.All these PBI sources are also available as outputs to the Writer module in Azure ML. so you can use them to write your results from Azure ML and later read them as input for Power BI\n",
      "Data\n",
      " \n",
      "Q: How should we deal with unreliable data in data scienceA: Use feature engineering to fix unreliable data (make some transformations on unreliable data to make it reliable) or drop them out completely - bad features could significantly decrease the quality of the modelQ: Is there any way to figure out these misstatements and then report the top 10% rich people with better accuracy using Machine Learning algorithms?A: ML algorithms are not magic sticks, they can't figure out anything unless you tell them what you are looking for. Can you describe what means 'unreliable'? If yes, you can, as I mentioned, use feature engineering or write a code which will fix the data. Otherwise no ML algorithm will be able to help you, without the description of what exactly you want to achieveQ: Is there any idea or application in Machine Learning which tries to improve the quality of collected data?A: I don't think so just because the question itself is too open-ended. What means 'the quality of the data'?Generally, here are couple of things for you to consider:1) Spend some time on googling feature engineering guides. They cover how to prepare your data for you ML algorithms, refine it, fix it. Good data with good features dramatically increase the results.2) You don't need to use all of features from original data. Some of features of original dataset are meaningless and you don't need to use them. Try to run gradient boosting machine or random forest classifier from scikit-learn on your dataset to perform classification (or regression, if you do regression). These algorithms also evaluate importance of each feature of original dataset. Part of your features will have extremely low importance for classification, so you may wish to drop them out completely or try to combine unimportant features together somehow to produce something more important.\n",
      "data scienceA\n",
      " \n",
      "This research field is called \"Data Matching\" or \"Record Linkage\". There is a  He also goes deep down into machine learning models and how to improve them from the basic approach like simple string distances (as other answers already suggested).To give you a head start, you can try to compute character n-grams of your titles.For n = 3 and Hugo Boss, you would get Now you can compute the  between two sets of these ngrams. Here, for example between  and :If you don't want to go down the route of implementing all of these things yourself, use . It's also very fast and scales well to billions of documents. \n",
      "Data\n",
      " \n",
      "2>Don't expect to find an algorithm that exactly does what you need.Customize algorithms as adequate for your problem. That is the very story of the Data Science buzz, the need to experiment and customize instead of hoping for a turnkey solution.You have avery specific idea of what you need. You will have to put this idea into code and plug it into some algorithm. For example, consider complete linkage clustering with maximum norm. It probably is what you explained above, but I don't think it will be useful.\n",
      "Data\n",
      " \n",
      "If there are not so many blobs, you could just add multiple readers with each map to one of your input blobs. Then use modules under \"Data Transformation\" -> \"Manipulation\" to do things like \"Add Rows\" or \"Join\".\n",
      "Data\n",
      " \n",
      "A good recap can be found , section 1 on Data Augmentation: so namely flips, random crops and color jittering and also lighting noise: proposed fancy PCA when training the famous Alex-Net in 2012. Fancy PCA alters the intensities of the RGB channels in training images.Alternatively you can also have a look at the Kaggle Galaxy Zoo challenge: the winners wrote a . It covers the same kind of techniques:rotation,translation,zoom,flips,color perturbation.As stated they also do it \"in realtime, i.e. during training\".For example here is a practical   by Facebook (for  training).\n",
      "Data\n",
      " \n",
      "When you say \"normalize\" labels, it is not clear what you mean (i.e. whether you mean this in a statistical sense or something else). Can you please provide an example? On Making labels uniform in data analysisIf you are trying to neaten labels for use with the  function, you could try the  function to shorten them, or the  function to align them better. The  function works well for rounding labels on plot axes. For instance, the base function  for drawing histograms calls on Sturges or other algorithms and then uses  to choose nice bin sizes.The  function will standardize values by subtracting their mean and dividing by the standard deviation, which in some circles is referred to as normalization. On the reasons for scaling in regression (in response to comment by questor). Suppose you regress Y on covariates X1, X2, ... The reasons for scaling covariates Xk depend on the context. It can enable comparison of the coefficients (effect sizes) of each covariate. It can help ensure numerical accuracy (these days not usually an issue unless covariates on hugely different scales and/or data is big). For a readable intro see . For a mathematically intense discussion see .In particular, in Bayesian regression, rescaling is advisable to ensure convergence of MCMC estimation; e.g. see . \n",
      "data analysisIf\n",
      " \n",
      "This task is usually known as Entity Linking. If you are working on popular entities, e.g. those that have an article in Wikipedia, then you may have a look at  or  that address this issue.If you'd like to implement your own linker, than you may have a look at . In most cases, a named entity linker detects mentions (person names in your case), then a disambiguation step is required, which computes probabilities for available references (and NIL as a mention may not have a reference available), for any specific mention in text, and by using contextual clues (e.g. words of sentence, paragraph or whole article containing the mention).\n",
      "Entity\n",
      " \n",
      "This problem can be tackled by breaking it down to smaller subtasks. A possible pipeline approach may be:The first stage is aspect term extraction which will identify aspect terms in the raw text. This too can be broken down to two  subtasks. Firstly your system will need to label tokens in text that are aspect terms. Let's call the these labelled tokens aspect term mentions. This is called Named Entity Recognition (NER). Next, if you have a pre-defined set of aspect term classes, the systems will need to link the aspect term mentions found in the previous task to those classes. This is called Entity Linking. It's worth noting that from the example that you give  the labelled dataset is not yet suitable for the above tasks as the labels are not anchored in text. You may be able to create a suitable dataset by guessing which tokens in text do your given labels correspond to. This is similar to the Distant Supervision work.The next task is aspect term sentiment classification. Convolutional Neural Networks have been used for sentence and document sentiment classification but they can probably be adapted for your purposes if at the input you provide a marker for which tokens are being classified. This is called a position embedding in this work: \n",
      "Entity\n",
      " \n",
      "A  is a type of Neural Network architecture where the connections are \"fed forward\", i.e. do not form cycles (like in recurrent nets).The term \"Feed forward\" is also used when you input something at the input layer and it travels from input to hidden and from hidden to output layer. The values are \"fed forward\".Both of these uses of the phrase \"feed forward\" are in a context that has nothing to do with training per se.Backpropagation is a training algorithm consisting of 2 steps: 1) Feed forward the values 2) calculate the error and propagate it back to the earlier layers. So to be precise, forward-propagation is part of the backpropagation algorithm but comes before back-propagating.\n",
      "Neural\n",
      " \n",
      "Neural Networks can have different architectures. The connections between their neurons decide direction of flow of information. Depending on network connections, they are categorised as - Feed-Forward and Recurrent (back-propagating).Feed Forward Neural NetworksIn these types of neural networks information flows in only one direction i.e. from input layer to output layer. When the weights are once decided, they are not usually changed. One either explicitly decides weights or uses functions like Radial Basis Function to decide weights. The nodes here do their job without being aware whether results produced are accurate or not(i.e. they don't re-adjust according to result produced). There is no communication back from the layers ahead.Recurrent Neural Networks (Back-Propagating)Information passes from input layer to output layer to produce result. Error in result is then communicated back to previous layers now. Nodes get to know how much they contributed in the answer being wrong. Weights are re-adjusted. Neural network is improved. It learns. There is bi-directional flow of information. This basically has both algorithms implemented, feed-forward and back-propagation.\n",
      "Neural\n",
      " \n",
      "As the name suggest, stochastic gradient descent updates the parameter vector of the Neural Network (NN) with the STOCHASTIC gradient of the loss function.A stochastic gradient is an \"\" of the true gradient. In other words, a stochastic gradient is a random variable whose expected value is equal to the \"true\" gradient.In NN, the gradient is the gradient of the loss function. Fortunately, the loss function is linear, and it is thus the average of the loss function computed at each sample. When we take one sample and we compute the gradient of the loss function there, the value we get can be seen as an estimate of the true gradient. Now, if your samples are ordered in a certain way, the sequence of loss functions computed on them are not \"unbiased\".To recap, to apply stochastic gradient descent, the samples must be taken such that they are independently thrown from a probability distribution, such that the gradient of the loss function computed on each of them \"seems\" an unbiased estimate of the true loss function.\n",
      "Neural\n",
      " \n",
      "It turned out that the problem was a normalization problem. In fact, I trained the Neural Network on a reference signal that was preprocessed and an output signal obtained with an unnormalized signal. This causes problems in case of nonlinear systems. Now I generate training outputs using a normalized inputs and everything works fine.If anyone need more details, feel free to contact meregardsAndrea\n",
      "Neural\n",
      " \n",
      "You use the  command from the Neural Networks toolbox.  Basically, you would submit a  matrix where  is the total number of features and  is the total number of samples.   is associated with the total number of input layer neurons you have.  Therefore, for each input, you provide a column into this matrix. As such, once you train your network and if you follow the example, your neural network should be stored in .  Simply create your input data, and \"simulate\" the neural network with your desired inputs.As such, you would do this, given that your input data is stored in :However, if you wanted to see how your neural network performs with the same input, try using the variable  as that is the where the input data is stored in that dataset mentioned in the post:Alternatively,  is an object that can be callable.  You can achieve the same thing as  by just using the object  directly:If you want to do a small test, try loading up the housing price dataset:This returns a data matrix of 13 features (rows) and there are 506 samples.  This is stored in the matrix .  The vector  is the output layer values, or the target values after running through the neural network.  You can train a neural network for this dataset using a network of 1 hidden layer with 10 neurons, then we can see how well the neural network did with the same input data:You can then display the results side by side and see how they compareThe top row is the predicted values and the bottom row is the true values:You can see that the majority of the samples match and are relatively close.  Some outputs however are far off.... so it's really about tuning the neural network at this point, but don't tune too much or you will overfit the data.Check the documentation for more details on : \n",
      "Neural\n",
      " \n",
      "I've never seen the Neural Network Toolbox being used that way.  Try using just the  method rather than just :\n",
      "Neural\n",
      " \n",
      "First off, you're not training your neural network properly.  You'd have to use both  and  as input samples into your neural network and the output will have to be a two neuron output where  as the output would denote that the circles class is what the classification should be and  is what the crosses classification would be. In addition, each column is an input sample while each row is a feature.  Therefore, you have to transpose both of these and make a larger input matrix.  You'll also need to make your output labels in accordance with what we just talked about:Now train your network:Now, if you want to figure out which class each point belongs to, you simply take the largest neuron output and whichever one gave you the largest, that's the class it belongs to.Now, to answer your actual question, there's no direct way to show the \"lines\" of separation with Neural Networks if you use the MATLAB toolbox.  However, you can show regions of separation and maybe throw in some transparency so that you can overlay this on top of the figure.To do this, define a 2D grid of coordinates that span your two classes but with a finer grain... say... 0.01.  Run this through the neural network, see what the maximum output neuron is, then mark this accordingly on your figure.Something like this comes to mind:The first two lines of code generate a bunch of test  points and ensures that they're in a 2 row input matrix as that is what the network inputs require.  I use  for generating these points.  Next, we use  to simulate or put in inputs into the neural network.  Once we do this, we will have two output neuron neural network responses per input point where we take a look at which output neuron gave us the largest response.  If the first output gave us the largest response, we consider the input as belonging to the first class.  If not, then it's the second class.  This is facilitated by using  and looking at each column independently - one column per input sample and seeing which location gave us the maximum.Once we do this, we create a new figure and plot the points that belonged to class 1, which is the circles, in yellow and the second class, which is the crosses, in green.  I throw in some transparency to make sure we can see the regions with the points.   After, I plot the points as normal using your code.With the above code, I get this figure:As you can see, your model has some classification inaccuracies.  Specifically, there are three crosses that would be misclassified as circles.  You'll have to play around with number of neurons in the hidden layer as well as perhaps using a different activation function but this certainly is enough to get you started.Good luck!\n",
      "Neural\n",
      " \n",
      "You can save the variables in the network usingTo restore the network for reuse later or in another script, use:Important points: must be same between first and later runs (coherent structure).  needs the path of the folder of the saved files, not an individual file path. \n",
      "network usingTo\n",
      " \n",
      "I don't believe there would be any optimal answer, but would really come down to how well the Neural Network is performing on the training data.I've usually split the data 60/20/20, but this could change depending on the quality and complexity of the data.You could run a few tests with different combinations and see how that affects your validation / evaluation performance.Hope this helps!\n",
      "Neural\n",
      " \n",
      "1) Yes. This is well received strategy to counter imbalanced data.  But this strategy is good in Neural Nets only if you using SGD.Another  easy way to balance the training data is using weighted examples. Just amplify the per-instance loss by a larger weight/smaller when seeing imbalanced examples. If you use online gradient descent, it can be as simple as using a larger/smaller learning rate when seeing imbalanced examples.Not sure about 2.\n",
      "Neural\n",
      " \n",
      "First, you should understand the use of Neural Network (NN) before going ahead with this problem. NN tries to find out complex relationship between input and output. (here your input is five cards and output is predicted class). Here in this question, relationship between input and output can easily be formulated. i.e. you can easily choose some set of rules which declares a final winner. Still like any other problem, this problem can also be dealt with NN. First you need to prepare your data.There are total 52 type of inputs possible. So, take 52 column in dataset. Now in these 52 column you can fill three type of categorical data. Either it belongs to 'A' or 'B' or no body. 'C' and output can be the winner . Now you can train it using NN.\n",
      "Neural\n",
      " \n",
      "As they said, there is no \"magic\" rule to calculate the number of hidden layers and nodes of Neural Network, but there are some tips or recomendations that can helps you to find the best ones.The number of hidden nodes is based on a relationship between:Number of input and output nodesAmount of training data availableComplexity of the function that is trying to be learnedThe training algorithmTo minimize the error and have a trained network that generalizes well, you need to pick an optimal number of hidden layers, as well as nodes in each hidden layer. Too few nodes will lead to high error for your system as the predictive factors might be too complex for a small number of nodes to captureToo many nodes will overfit to your training data and not generalize wellYou could find some general advices on this page:If your data is linearly separable then you don't need any hidden layers at all. Otherwise there is a consensus on the performance difference from adding additional hidden layers: the situations in which performance improves with a second (or third, etc.) hidden layer are very small. Therefore,  one hidden layer is sufficient for the large majority of problems.There are some empirically-derived rules-of-thumb, of these, the most commonly relied on is 'the optimal size of the hidden layer is usually between the size of the input and size of the output layers'. In sum, for most problems, one could probably get decent performance by setting the hidden layer configuration using just two rules: The number of hidden layers equals oneThe number of neurons in that layer is the mean of the neurons in the input and output layers.\n",
      "Neural\n",
      " \n",
      "A random forest fits a number of decision tree classifiers on various sub-samples of the dataset. Every time you call the classifier, sub-samples are randomly generated and thus different results. In order to control this thing you need to set a parameter called .Note that  is the seed used by the random number generator. You can use any integer to set this parameter. Whenever you change the  value the results are likely to change. But as long as you use the same value for  you will get the same results.The  parameter is used in various other classifiers as well. For example in Neural Networks we use  in order to fix initial weight vectors for every run of the classifier. This helps in tuning other hyper-parameters like learning rate, weight decay etc. If we don't set the , we are not sure whether the performance change is due to the change in hyper-parameters or due to change in initial weight vectors. Once we tune the hyper-parameters we can change the  to further improve the performance of the model.\n",
      "Neural\n",
      " \n",
      "Short answer: No NNs cannot help.Long answer: Maybe they can if you really, REALLY want them to and have tons of time and skill.The Problem is that Neural Networks are used to handle numbers and not words.Most types of Neural Networks rely on the capability to decide if two values are close to each other. This is still not easy with strings in a language context.So if you don't want to spend the next few years researching Neural Networks I would look for a different approach ;)\n",
      "Neural\n",
      " \n",
      "This is to be expected in my experience, there are a lot of calculations involved in Neural Nets. I personally have one written in Python (2 hidden layers), detailed including momentum term, I have about 38,000 patterns of 56 inputs and 3 outputs.  Splitting them into 8,000 chunks took about 10 minutes to run and just under a week to learn to my satisfaction.The whole set of 38,000 had a larger hidden nodes to store all the patterns and that took over 6 hrs to go through one cycle and over 3 months to learn.  Neural Networks is a very powerful tool but it comes at a price in my experience, others may have better implementation but all the comparisons of classification algorithms I have seen, have always mentioned the time to learn as being significant.\n",
      "Neural\n",
      " \n",
      "Your intuition is right - randomization can help surface results that get a lower than deserved score due to uncertainty in the estimation. Empirically, Google search ads seemed to have sometimes been randomized, and e.g.  is hinting at it (see Section 6).This problem describes an instance of a class of problems called Explore/Exploit algorithms, or Multi-Armed Bandit problems; see e.g. . There is a large body of mathematical theory and algorithmic approaches. A general idea is to not always order by expected, \"best\" utility, but by an optimistic estimate that takes the degree of uncertainty into account. A readable, motivating blog post can be found .\n",
      "Google\n",
      " \n",
      "Since it is still a research task, I suggest several links to scientific papers (links and the following summary are mostly taken from 'related work' section of  - unfortunately, in Russian, so I edited Google translation a little).So, take a look at these works (marked by year): , , , , .In summary: you should find or create tagged corpora and use supervised machine learning with the following features: text features: n-grams over words and characters,stylistic features: parts of speech, slang, the average sentence length, punctuation, acronyms, emoticons, etc.social network features: the number of friends a user, the number of posts displayed on the page of the user, the total number of posts, the average number of comments for a post of the user.\n",
      "Google\n",
      " \n",
      "I would try to split this problem into 3 smaller problems:checking whether image shows shirt with long or short sleevschecking pattern (stipped, plain, something else?)determining color of shirtChecking whether image shows shirt with long or short sleevsThis one is in my opinion the easiest. You mentioned that you have category name, but basing on google graphics it seems that it may not be obvious whether Shirt or TShirt has long or short sleevs.My solution is quite simple:Find face on imageUse grabcut algorithm to extract face mask from imageMask face (so after this step only face is left - everythin else is black). Note that this step is not necessary - i've mentioned it only, because it's shown on final image.Convert image to HSV color spaceUsing face mask calculate histogram for H and S color channels of FACE ONLY (without rest of the image)Calculate back projection of hsv image using histogram from previous step. Thanks for that you will get only regions which color (in HSV) is similar to color of face - so you will get only regions which contains skin.  Threshold the result (there is always some noise :) )The final result of this algorithm is black and white image which shows skin regions. Using this image you can calculate number of pixels with skin and check whether skin is only on face or maybe somewhere else. You can try to find contours as well - generally both solututions will give chance to check if hands  are visible. Yes - shirt has short sleevs, no - long sleevs.  Here are the results (from top-left corner - original image, face mask (result of grabcut algorithm), masked face, hsv image, result of calculating back projection, result of using threshold on previous one):As you can see, unfortunetely it fails for image 3, because face is in very similar color to shirt pattern (and generally face color is quite close to white - something is wrong with this guy, he should spend more time outside ;) ).  Source is quite simple, but if you don't understand something feel free to ask:Checking pattern (stipped, plain, something else?) and determining color of shirtHere i would try to extract (mask) the shirt from the image and than operate only on it. To achieve it i would try to use similar approach as in previous part - grabcut algorithm. Initializing it might be harder this time. Quite easy (but probably not perfect) solution which comes to my mind is:set rect around almost whole area (leave just few pixels on each side)initialize mask to  value in the middle of the image - just draw some circle in the middle using  \"color\"set mask to  in the corners of the image set mask to  in face rectangle (the one founded using Haar cascade in step \"Checking whether image shows shirt with long or short sleevs\")Alternatively you can initialize whole mask as  or  and use watershed algorithm to find the big white area (which is backgrund). Once you have this area - use it as background.Most likely using those 2 solutions together will give you the best results.  You can try much easier solution as well. It looks like all images has got SHIRT not background, skin or anything else sligtly over the center of it. Just like here:so you can just analyze only this part of shirt. You can try to localize this sure shirt part of image using Haar cascade as well - just find face and than move the founded rectangle down. Once you have masked shirt you can calculate its parameters. 2 things which i would try are:convert it to HSV color space and calculate histograms (2 separates - not one as we did in previous step) for Hue and for Saturations channels. Comparing those histograms for 2 shirts should give you chance to find shirt with similar colors. For comparing histogram i would use some (normalized) correlation coefficients.use Fourier transform to see what frequencies are the most common in this shirt. For plain shirts it should be much smaller frequencies than for stripped.  I know that those solutons aren't perfect, but hope it helps. If you will have any problems or questions - feel free to ask.//edit:I've done some simple pattern comparision using Fourier transform. Results are... not very good, not very bad - better than nothing, but definitely not perfect ;) I would say it's good point to start.Package with code and images (yours + some from google) is . Code:Some lines are commented - you can try to use them, maybe you will achieve better results.Basic algorithm is quite simple - for each image:cut shirt sample from image (just move down rectangle with face by 2* it height)convert this rect to gray colorspace and resize to (256, 256)calculate fft of this samplecalculate magnite spectrum of fft transformnormalize it (from 0 to 255)threshold it (clear all values &lt;200) - this will remove noise etc.Now we can calculate normalized cross corelation of this image between all shirt samples. High result -> similar samples. Final results:Image with all the shirt samples, magnitude spectrums (before and after threshold) is here:  The images names are (in the same order as samples on this big image): As you can see, thresholded images are quite similar for samples with same pattern. I think that this solution could work better if you just find a better way to compare those images (thresholded magnitude spectrums).edit2:Just a simple idea - after you crop shirt samples from lot of shirts, you can try to train some classifier and than recognize patterns them using this classifier. Look for tutorials about training Haar or LBP(local binary pattern) cascades. \n",
      "google\n",
      " \n",
      "As far as Google is concerned Caffe won't help you with \"Extreme Learning Machines\".No, that is not possible. You will have to implement new layers and algorithms in C++. Afterwards you can deal with them via Python.For a primer on Caffe, check out .\n",
      "Google\n",
      " \n",
      "Google has an API for this in Google Play Services.  Check out I wouldn't suggest coding it on your own, its not easy (I had a version about a year before Google did, it was buggy and battery draining).\n",
      "Google\n",
      " \n",
      "I've previously had success with using word embeddings to capture paragraph similarity. Word embeddings, such as those produced by Google's , model words in a high-dimensional vector space. As such, they make it possible to compute the semantic similarity between two words, for example as a cosine between their individual vectors. You can download these embeddings directly from the word2vec site, or from related project sites such as . In order to model similarity between paragraphs, one simple solution is to compute paragraph embeddings by taking a weighted sum of the embeddings of all the words in that paragraph. Because some words are more informative than others, you can weight the word embeddings by the  of the word, for example. You can then compute the similarity between two paragraphs as the cosine between their embeddings. \n",
      "Google\n",
      " \n",
      "Those are my methods:Download the crops from Google news and merge them into your data, then train them!Divide your data set into 2 equal size data set, then train both of them. Now you have 3 models, so you can use blending method to predict.I hope these may help you!\n",
      "Google\n",
      " \n",
      "There's no straightforward way to merge the end-results of separate training sessions.Even for the exact same data, slight randomization from initial seeding or thread scheduling jitter will result in diverse end states, making vectors only fully comparable within the same session.This is because every session finds a useful configuration of vectors... but there are many equally useful configurations, rather than a single best. For example, whatever final state you reach has many rotations/reflections that can be exactly as good on the training prediction task, or perform exactly as well on some other task (like analogies-solving). But most of these possible alternatives will not have coordinates that can be mixed-and-matched for useful comparisons against each other. Preloading your model with data from prior training runs might improve the results after more training with new data, but I'm not aware of any rigorous testing of this possibility. The effect likely depends on your specific goals, your parameter choices, and how much the new and old data are similar, or representative of the eventual data against which the vectors will be used. For example, if the Google News corpus is unlike your own training data, or the text you'll be using the word-vectors to understand, using it as a starting point might just slow or bias your training. On the other hand, if you train on your new data long enough, eventually any influence of the preloaded values could be diluted to nothingness. (If you really wanted a 'blended' result, you might have to simultaneously train on the new data with an interleaved goal for nudging the vectors back towards the prior-dataset values.)Ways to combine the results from independent sessions might make a good research project. Maybe the method used in the word2vec language-translation projects  learning a projection between vocabulary spaces  could also 'translate' between the different coordinates of different runs. Maybe locking some vectors in place, or training on the dual goals of 'predict the new text' and 'stay close to the old vectors' would give meaningfully improved combined results. \n",
      "Google\n",
      " \n",
      "Google and Bing don't strictly use direct image to reject them from safe search.  It depends on number of factor like name of image, website that has this image etc. However if you want to look for a algorithm to detect nude images. Look at this problem as what will you do to detect any kind of images. If are interested in doing research look at this . and if you are developing a web application look at nude.js \n",
      "Google\n",
      " \n",
      "That speech recognition library is using Google's speech recognition engine so there is no particular provision for training at the user end. Your sound data goes to Google (in digest form). If you get a dedicated API (as that documentation page suggests) it is possible Google will be building a user-specific profile on your voice and will gain statistical quality over time based on this, but that is not something that would be stored or written at your end.Any further questions or unaddressed elements of your question, please let me know.\n",
      "Google\n",
      " \n",
      "Sounds like you are effectively trying to do  on named entities, where the entities are distinct dishes.  You can check out projects like  and .However, from your example it's a little unclear why a pulled pork sandwich should be considered the same dish as pulled pork, so you may need a way to come up with your own training set (e.g. culled from google) that tags entities as distinct within your desired tolerance.\n",
      "google\n",
      " \n",
      "Material recognition is an active research area in computer vision. Google \"material recognition computer vision\" for a list of recent papers.\n",
      "Google\n",
      " \n",
      "It an evolving area . I would suggest you to read some papers. You can use google or Google Scholar for that.Use Few approaches are :Using Bayesian Framework SVMNeural Networks\n",
      "google\n",
      " \n",
      "What Google does is they have each thread/node train only a subset of the neurons. Then, it's fairly easy to combine them back because each neuron was written only once.You have to combine them regularly, though, so that they do not drift apart too much.The unit that they use to partition the net is a \"column\" of neurons.\n",
      "Google\n",
      " \n",
      "Are the new comments very different from the old comments in terms of vocabulary? Because words is almost everything the classifiers for this task look at. You always can try using your old training data and apply the classifier to the new domain. You would have to label a few examples from your new domain in order to measure performance (or better, let others do the labeling in order to get more reliable results).If this doesn't work well, you could try  or look for some datasets more similar to your new domain, using Google or looking at  spam/ham corpora.Finally, there may be some regularity or pattern in your new setting, e.g. downvotes for a comment, which may indicate spam/ham. In such cases, you could compile training data yourself. This would them be called distant supervision (you can search for papers using this keyword).\n",
      "Google\n",
      " \n",
      "Google just released their tensorflow framework. Its perfect to start with and offers even for high skilled NN-architectures a lot of feautes. I highly recommend it for everyone.\n",
      "Google\n",
      " \n",
      "Google TensorFlow is one big library with a Python interface for the creation of neural networks. See  for more information and examples about reinforcement learning.\n",
      "Google\n",
      " \n",
      "Your model can have any number of hidden layers with any number of hidden units. However, in case you want to do multi-dimensional regression, the number of output units of your neural network model must have the same dimensionality of your desired output vector y. For instance, suppose your examples have three features as shown in the picture below (borrow from a Google search). You may have any number of hidden layers (here one) and in the last layer you have a vector of size two, corresponding to the number of dimensions your target y has.\n",
      "Google\n",
      " \n",
      "Sorry for the excavation, but this question is quite popular, and now it has a different answer.Google officially announced the addition of Windows (7, 10, and Server 2016) support for TensorFlow:The Python module can be installed using pip with a single command:And if you need GPU support:Another useful information are included in release notes:UPD: As @m02ph3u5 right mentioned in the comments TF for windows supports only Python 3.5.x \n",
      "Google\n",
      " \n",
      "(Just in case anybody's stumbled across this on Google). The issue tends to arise when the second  produces instances which are subtly different from the first. To ensure that this isn't the problem, use , so becomes\n",
      "Google\n",
      " \n",
      "No i don't think google prediction api will works for image recognition.because prediction api knows only numeric and string.for image recognition Google Vision Api is the best , i think it cant able to recognize humans or persons but it is recognize place like eiffel tower and all.even it can able to read the image written strings.\n",
      "google\n",
      " \n",
      "If you are looking for all adjectives that could describe a noun your best starting place might be the Google NGram dataset. You can try the viewer  which shows that 'horned', 'barn', 'screech' are all common adjectives for owls.Alternatively, if you are trying to tag specific sentences to find adjectives related to a noun you should try one of the part of speech taggers.\n",
      "Google\n",
      " \n",
      "In recent years, the models developed to solve various machine learning problems have become far more complex, with a very large number of layers.  For example, Google's Inception-v3 model has (I think) 42 layers. Traditional neural networks used to typically use only a handful of hidden layers.  The term \"Deep\" used in the context of \"Deep Learning\" and \"Deep Convolutional Neural Nets\" is a nod to the substantial # of layers involved.\n",
      "Google\n",
      " \n",
      "Recent advances had alleviate the effects of vanishing gradients in deep neural networks. Among contributing advances include:Usage of GPU for training deep neural networksUsage of better activation functions. (At this point rectified linear units (ReLU) seems to work the best.)With these advances, deep neural networks can be trained even without layerwise pretraining. Source:\n",
      "GPU\n",
      " \n",
      "Did the code originally come with GPU support, or did you add it yourself?You should replaced the depreceated layers, i.e. replace:withCheck the documentation for the layers.Edit: Use , I fixed it for GPU support.\n",
      "GPU\n",
      " \n",
      "The \"units\" are just floating point values.All computations happening there are vector multiplications, and thus can be parallelized well using matrix multiplications and GPU hardware.The general computation looks like this:except that you don't want to do this in Java code yourself. You want to do this on a GPU in a parallelized way, because this will be 100x faster.\n",
      "GPU\n",
      " \n",
      "I don't know if this format really provides better representation, but I can speculate why it can be more efficient.First, as they state at format description, \"Having data of the same precision consecutive enables hardware vectorization.\"; consider also : \"Vector processing techniques have since been added to almost all modern CPU designs\".Second, their format allows you to mix sparse and non-sparse features, but since all sparse features are placed consequently, it is possible to easily take them as a  and optimize methods for learning like  conjugate gradient.What do you mean by ML algorithm tuning? The learning algorithm doesn't know and doesn't need to know anything about file format of the dataset; and you can't increase or decrease accuracy if you know file format. In theory, you can speed up the concrete  optimization algorithm (like Gradient descent) if you can rely on some properties of data (and, I guess, Ismion PaperBoat does it), but I don't think that you can tune it by yourself.\n",
      "CPU designs\"\n",
      " \n",
      "For the matrix/vector side there's  which is a plug-able library with an implementation at  that is being actively developed, and other high perf libs exist. Usage from the readme:A 'mentor' to the project mentioned  that GPU was a target, but no mention of it in the docs.What kind of specific functionality do you need as your question is a bit broad? Have you tried anything?\n",
      "GPU\n",
      " \n",
      "I agree with lejlot. The batchsize is not the problem in your current model building, given the very small data size. Once you move on to larger data that can't fit in memory, then try different batch sizes (say, some powers of 2, i.e. 32, 128, 512,...). The choice of batch size depends on:your hardware capacity and model architecture. Given enough memory and the capacity of the bus carrying data from memory to CPU/GPU, the larger batch sizes result in faster learning. However, the debate is whether the quality remains.Algorithm and its implementation. For example, Keras python package (which is based on either Theano and TensorFlow implementation of neural network algorithms) :You will have a better intuition after having tried different batch sizes. If your hardware and time allows, have the machine pick the right batch for you (loop through different batch sizes as part of the grid search.Here are some good answers: , .\n",
      "GPU\n",
      " \n",
      "The difference lies in the speed. cnn is computationally expensive, so a GPU implementation is at least 10 times faster than CPU. caffe and theano provide seamless  integration of calling either CPU or GPU, which may not be easy for you to implement without much GPU programming experience.Other factors may exist including a unified interface for multiplayer, stochastic gradient descent, and etc. but I think speed issue is most crucial among all these factors.\n",
      "GPU\n",
      " \n",
      "In  you haveThis means it will try to include  unless  flag is defined.Since your machine doesn't have Nvidia GPU, you have to define  flag during compilation with So your full compilation command should look like\n",
      "Nvidia\n",
      " \n",
      "So after reading your responses, and some blog posts, my conclusion is:Don't try to make a laptop your main workstation for deep learning ! It is just too expensive : You'll just spend thousands of $ for a laptop configuration that could cost hundreds for a desktop. And the less expensive is not worth it.I think I'm going to buy a 13\" laptop and start building a strong desktop, then I'd make an ssh access to it.These two links are great for having a good understanding on GPU choice.Thank you all !\n",
      "GPU\n",
      " \n",
      "Machine learning operations are usually reduced to matrix-matrix multiplications. Currently, matrix-matrix multiplications are very efficient on GPUs than CPUs because GPUs have much more threads than CPUs. Furthermore, NVIDIA have supported for CUDA toolkit for quite number of years. The platform is now matured. Many DL libraries (e.g., Caffe, Theano, Torch, TensorFlow) are exploiting the CUDA supports for BLAS (basic linear algebra subroutines) and DNN (deep neural networks) libraries. Deep learning libraries developers will not have to devote significant times to optimize matrix-matrix multiplications. Furthermore, it seems that optimizing CPUs code for the same level of GPUs in certain operations (matrix-matrix operations) is pretty hard (what we called ninja optimizations). For better appreciations of what others have experienced, please see discussions at .\n",
      "NVIDIA\n",
      " \n",
      "The only limit is your machine's memory: When caffe loads the model it allocates memory for all the parameters and all the intermediate data blobs. The more images you process concurrently, the larger the memory you need to allocate in advance.The easiest (and crudest) way of determining this number is simply trail-and-error, try setting it to 200 and see if you get an \"out of memory\" error when loading the model.Note that the number of images you can process at the same time depends also on whether you are using GPU or CPU: usually GPU memory is smaller than CPU memory and thus allows you to process fewer images.\n",
      "GPU\n",
      " \n",
      "If you are feeling adventurous and need some computational power using GPU. I'd recommend . I also started with PyBrain, but eventually I moved on to more updated libraries like Keras and Theano. Keras is very easy to learn and it is able to reproduce some state of the are results with very few lines of codes. There's a very active community behind Keras developing latest features whereas PyBrain is not under active development now.\n",
      "GPU\n",
      " \n",
      "Well there is usually a trade-off between performance and precision. You may have to compensate one in favor or the other. Although I personally do not believe a difference of 0.0000001 is a big deal in most applications.  If you seek higher precision you'd better go with , but note that  operations are extremely slow on the GPUs, especially NVIDIA 9xx series GPUs.I may note that the mentioned issue seems to depend on your hardware settings too cause I do not encounter such problem on my machine.You may also use  to see if the difference is tangible.\n",
      "NVIDIA\n",
      " \n",
      "Assuming you know the number of elements to be stored on a GPU you can easily compute the amount of memory required to store those elements.A simple example:Assuming the over-head constant is 0 will print:If you are using an NVIDIA graphics card and have installed CUDA on your machine then you can easily get the total amount of free memory on your GPU  using the following line of code:Then the output is in the following format (for my machine here):By monitoring the amount of free memory and calculating the size of your model/data you can better use the GPU memory. However, be aware of the  issue as it may cause  unexpectedly.\n",
      "NVIDIA\n",
      " \n",
      "Neural networks learn by gradient descent an error function in the weight space which is parametrized by the training examples. This means the variables are the weights of the neural network. The function is \"generic\" and becomes specific when you use training examples. The \"correct\" way would be to use all training examples to make the specific function. This is called \"batch gradient descent\" and is usually not done for two reasons:It might not fit in your RAM (usually GPU, as for neural networks you get a huge boost when you use the GPU).It is actually not necessary to use all examples.In machine learning problems, you usually have several thousands of training examples. But the error surface might look similar when you only look at a few (e.g. 64, 128 or 256) examples.Think of it as a photo: To get an idea of what the photo is about, you usually don't need a 2500x1800px resolution. A 256x256px image will give you a good idea what the photo is about. However, you miss details.So imagine gradient descent to be a walk on the error surface: You start on one point and you want to find the lowest point. To do so, you walk down. Then you check your height again, check in which direction it goes down and make a \"step\" (of which the size is determined by the learning rate and a couple of other factors) in that direction. When you have mini-batch training instead of batch-training, you walk down on a different error surface. In the low-resolution error surface. It might actually go up in the \"real\" error surface. But overall, you will go in the right direction. And you can make single steps much faster!Now, what happens when you make the resolution lower (the batch size smaller)?Right, your image of what the error surface looks like gets less accurate. How much this affects you depends on factors like:Your hardware/implementationDataset: How complex is the error surface and how good it is approximated by only a small portion?Learning: How exactly are you learning (momentum? newbob? rprop?)\n",
      "GPU\n",
      " \n",
      "I just happen to be having the same non-deterministic results problem of my tensorflow code.   I also tried setting the seed for random generators but does not help.Later I found this discussion :()   . It states that the cause is likely due to the modern GPU framework, so it seems like there's no good fix until CUDA updates it. \n",
      "GPU\n",
      " \n",
      "the linessuggest that your input data is not in the correct shape. For an input of 100 batchs of 96x96 grey-scale image the shape should be: 100 1 96 96.Try to change this. (my guess is that for shape: N C H W, where N number of batches, c channels, h height, w weight)\n",
      "N\n",
      " \n",
      "Another way is to have R or python code that replicates the status for each image and then use add-columns. I think R/Python code to just replicate the status for each image may be easier and faster than outer join.\n",
      "R\n",
      " \n",
      "One more idea: some data transformation. Let have N big enough that always bigger than n. We make a net with 2*N inputs. First N inputs are for data. If n less then N, then rest inputs set to 0. Last N inputs are intended for specifying which numbers are useful. Thus 1 is data, 0 is not data. As follows in Matlab notation: if v is an input, and it is a vector of length 2*N, then we put into v(1:n) our original data. After that, we put to v(n+1:N) zeros. Then put to v(N+1:N+n) ones, and then put V(N+n+1:2*N) zeros. It is just an idea, which I have not checked. If you are interested in the application of neural networks, take a look at the example of how we have chosen an appropriate machine learning algorithm to .\n",
      "N\n",
      " \n",
      "After some more reading I understand that scikit-learn implements a regularized logistic regression model, whereas glm in R is not regularized. Statsmodels' GLM implementation (python) is unregularized and gives identical results as in R.The R package LiblineaR is similar to scikit-learn's logistic regression (when using 'liblinear' solver).\n",
      "R\n",
      " \n",
      "We have generated random samples (with some structure predetermined in them) and verified our implementations by comparing them to R and/or SAS. Once the results (say the beta coefficients in logistic) were (manually) vetted to be close enough to R/SAS they were hard-coded into unit tests for the regression regression testing (pun intended) with very small tolerances. In cases there were no reference implementations, we just approved the results based on whether the learning method was able to reveal the structure and the parameters that have been put into the random sample during its generation.\n",
      "R\n",
      " \n",
      "You can use  to tell R that you want to analyse the dependency between a class variable and all other variables in the data frame. For example for the  dataset:If you want to analyse the interaction with respect to only a subset of the variables, you can use explicit names:\n",
      "R\n",
      " \n",
      "You can use  to find row indices that are not included in your training set:One consequence of sampling with replacement is that the number of examples eligible for inclusion in the test set will vary according to the number of repeated examples in the training set:If you want to ensure that the size of the test set is consistent, you could resample with replacement from the set of indices that aren't in the training set:If you don't actually care about sampling with replacement then a simpler option would be to generate a random permutation of all the indices, then take the first N as training examples and the rest as test examples:Or you could just shuffle the rows of your array in place using .I should also point out that scikit-learn has  for partitioning data into training and test sets for the purposes of cross-validation.\n",
      "N\n",
      " \n",
      "The difference between the mean and the sum is just the multiplication by 1/N. The problem with using the sum is that the batch size (N) will influence your gradients. The learning rate indicates how much in the direction of the gradient you want to adjust your parameters. If your gradient is larger for larger batch sizes (N), it means that you will need to adjust the learning rate as you increase the batch size (N).In practice, in order to keep these two (learning rate and batch size) independent, it is common to use the mean instead of the sum. This makes the gradient magnitude independent of N.If you are not using a batch, then N=1 and the mean is the same as the sum.\n",
      "N\n",
      " \n",
      "It's not clear what you are asking for.If it's how to find more literature concerning this is a Nearest Neighbor problem or, perhaps more correctly, a Nearest Set problem.You haven't explained how you use this metric. It will be most positive if the nearest neighbors are of color==match; most negative if they are of opposite color; and zero if equidistant from both.Is this what you want?If so, this is correct. The only question would be if it falls off rapidly enough so that its magnitude is less than some value unless \"pretty close\" to some set, say by using 1/(1-distance^2).Edit:Actually, 1/(1+distance) might be better or you may find yourself dividing by zero.\n",
      "Nearest\n",
      " \n",
      "First, you have to understand the concepts here. RBM can be seen as a powerful clustering algorithm and clustering algorithms are unsupervised, i.e., they don't need labels. Perhaps, the best way to use RBM in your problem is, first to train an RBM (which only needs data without labels) and then use the RBM weights to initialize a Neural network. To get a logistic regression in the output, you have to add an output layer with logistic reg. cost function to this neural net and train this neural network. This setting may result in performance improvement.  \n",
      "RBM\n",
      " \n",
      "RBM and LDA are not directly comparable, as RBM doesn't perform classification on its own. Though you are using it as a feature engineering step with logistic regression at the end, LDA is itself a classifier - so the comparison isn't very meaningful. The BernoulliRBM in scikit learn only handles binary inputs. The iris dataset has no sensible binarization, so you aren't going to get any meaningful outputs.\n",
      "RBM\n",
      " \n",
      "RBM's do not do prediction tasks. They are generative models. You can use the transform method to get a hidden state transformation of the input, or your gan use the gibbs method to sample from the network. No, this is not available in scikit-learn. It sounds like your assignment might be meant for you to implement a simpler problem from scratch rather than use another library, as hill climbing isn't normally used for training a neural network. And they probably don't want you to do hill climbing for an RBM neural network. You should probably consult your professor for more direction on what you really should be doing. \n",
      "RBM\n",
      " \n",
      "In short, an RBM is simply a Markov random field on a bipartite graph. So therefore you can have any probability distribution to describe the relationship between nodes. In terms of code, you don't really need to copy things explicitly. The role that the probability function selected will play is in the contrastive divergence algorithm. You should only have to change the way that the samples are selected. The parts of the code that need to be changed are copied below.\n",
      "RBM\n",
      " \n",
      "3>Classification is about determining a (categorial) class (or label) for an element in a datasetPrediction is about predicting a missing/unknown element(continuous value) of a dataset3>In classification, data is grouped into categories based on a training dataset.In prediction, a classification/regression model is built to predict the outcome(continuous value)3>In a hospital, the grouping of patients based on their medical record or treatment outcome is considered classification, whereas, if you use a classification model to predict the treatment outcome for a new patient, it is considered a prediction.\n",
      "Classification\n",
      " \n",
      "Classification is the prediction of a categorial variable within a predefined vocabulary based on training examples.The prediction of numerical (continuous) variables is called regression.In summary, classification is one kind of prediction, but there are others. Hence, prediction is a more general problem.\n",
      "Classification\n",
      " \n",
      "Classification is the process of identifying the category or class label of the new observation to which it belongs.Predication is the process of identifying the missing or unavailable numerical data for a new observation. That is the key difference between classification and prediction. The predication does not concern about the class label like in classification.\n",
      "Classification\n",
      " \n",
      "Restating, the problem is that the five binary models you've trained are not mutually exhaustive.  There are several possibilities.First of all, do you have a 100% clean classification for each of the five sentiments, or are there some acknowledged classification errors?You need a set that is mutually exclusive and exhaustive.  Your approach suggests, but hardly guarantees, this result.  You might consider an integrated solution that does make this guarantee.  Multi-class SVM is one such, but may not apply well to your situation.If the classes are not 100% accurate, you can easily have all five rejecting a particular observation.  This suggests that your classification algorithms need tuning, or that the data themselves are not as amenable to classification as you would like.You might also check that you've cleaned that data appropriately; a few errors can seriously move the class boundaries.What I suspect is happening is a small-boundary effect: each class, when compared against the combination of the other four, \"pulls in\" its boundaries, leaving unclaimed territory between the final sets.Do you have a way to check the classification parameters after training?  If so, can you visualize the five boundaries selected?  If you do find pathological gaps, are there training parameters you can tune, such as giving a larger epsilon to the training groups?I hope this helps.\n",
      "classification errors?You\n",
      " \n",
      "Classification examples:- Predicting whether a share of a company is good to buy or no given that the previous history of the company, along with the buyer's review on it saying yes or no for buying the share. (Discrete answer: Buy - Yes/No)Regression example:-Predicting the best price at which one should buy the share of a company given that the previous history of the company, along with the buyer's review of the price at which they bought the share in the past. (Continuous answer:- Price range)\n",
      "Classification examples:-\n",
      " \n",
      "An unsupervised clustering technique fails on scanned documents since it fails to grasp the underlying structure and ends up giving non nonsensical clusters. So the approach is fundamentally flawed. However Classification using deep convolutional neural networks, with sufficient data and carefully chosen distinct classes, can outperform OCR techniques if the documents have a distinct structure. \n",
      "Classification\n",
      " \n",
      "If you have a large number of categories, Classification algorithm does not work well. Instead, there is a better approach of doing this. You apply regression algorithm on data and then train offset on those output. It would give you better results. A sample code can be found . \n",
      "Classification\n",
      " \n",
      "A  is a collection of 's. No matter how big your training set, a decision tree simply returns: a decision. One class has probability 1, the other classes have probability 0.The RandomForest simply votes among the results.  returns the number of votes for each class (each tree in the forest makes its own decision and chooses exactly one class), divided by the number of trees in the forest. Hence, your precision is exactly . Want more \"precision\"? Add more estimators. If you want to see variation at the 5th digit, you will need  estimators, which is excessive. You normally don't want more than 100 estimators, and often not that many.\n",
      "probability 0.The\n",
      " \n",
      "Just to make it a bit more clear. The general form of most loss functions isthus the whole problem with  is to find a balance between sum of losses over training samples and regularizer value. Now, if loss is bounded (like in the case of logistic regression), then without proper normalization L2 regularizer (||theta||^2) may grow to infinity, thus you will need very high C to make it irrelevant and thus equal in solution to L1 (max_j |theta_j|). Similarly if you have loss which grows very fast, such as Lp loss for p>=2, then regularizer might be very small thus you will need very small  to make it do anything.  \n",
      "normalization L2\n",
      " \n",
      "If you have jTessBoxEditor, then you have Tesseract bin files. Go to the tesseract-ocr subfolder of jTessBoxEditor and run the following command :It should generate the file D:\\testocr\\TestImage.box.Then in jTessBoxEditor, go to Box Editor tab and open your image. The box file is automatically loaded, you can check if everything is ok and correct possible mistakes.\n",
      "file D:\\testocr\\TestImage.box\n",
      " \n",
      "The only issue here is:How to initialize caffe net from text file weights?I assume you have a  describing the net's architecture (layer types, connectivity, filter sizes etc.). The only issue remaining is how to set the internal weights of  to pre-defined values saved as text files.You can get access to  internals, see  tutorial on how this can be done in python.Once you are able to set the weights according to your text file, you can  the new weights into a binary  file to be used from now on. You do not have to train the net if you already have trained weights, and you can use it for generating predictions (\"test\").\n",
      "file weights?I\n",
      " \n",
      "Yes, deeper neural network can have less parameters. It doesn't matter if they are CNNs. You might be confused, because in the graphical representation one tends to focus on the neurons. However, what gets learned are the weights, which are on the edges between neurons.Besides the link to \"Deep Residual Learning for image recognition\" (please upvote Midos answer for that), I would like to give a toy example of a multilayer perceptron (MLP).2>The first MLP has an input layer of 784 neurons, two hidden layers of 2000 neurons each and an output layer of 10 neurons (short: 784:2000:2000:10). This results in a network with  neurons. Now consider a network with the architecture 784:2000:50:2000:10. This has  neurons.This means adding another layer, even without reducing any of the layer, reduced the size of the network to 32% of the previous size!\n",
      "Learning\n",
      " \n",
      "Generally, the more data you have, the better. You will get diminishing returns at some point. It is often a good idea to see if your training set size is a problem by plotting the cross validation performance while varying the size of the training set. In scikit-learn has an example of this type of \".\"Scikit-learn Learning Curve ExampleYou may consider bringing in outside sample posts to increase the size of your training set.As you grow your training set, you may want to try reducing the bias of your classifier. This could be done by adding n-gram features, or switching to a logistic regression or SVM model.\n",
      "Learning\n",
      " \n",
      "1>Pretty much every design choice in machine learning signifies some sort of inductive bias.  is an amazing  read, which I will be referring to throughout this answer.2>Concretely speaking, the very composition of layers  in deep learning provides a  type of relational inductive bias: hierarchical processing. The type of layer imposes further relational inductive biases:More generally, non-relational inductive biases used in deep learning include:activation non-linearities,weight decay,dropout,batch and layer normalization,data augmentation,training curricula,optimization algorithms,anything that imposes constraints on the learning trajectory.2>In a Bayesian model, inductive biases are typically expressed through the choice and parameterization of the prior distribution. Adding a Tikhonov regularization penalty to your loss function implies assuming that simpler hypotheses are more likely.1>The stronger the inductive bias, the better the sample efficiency--this can be understood in terms of the bias-variance tradeoff. Many modern deep learning methods follow an end-to-end design philosophy which emphasizes minimal a priori representational and computational assumptions, which explains why they tend to be so data-intensive. On the other hand, there is a lot of research into baking stronger relational inductive biases into deep learning architectures, e.g. with graph networks.2>In philosophy, inductive reasoning refers to generalization from specific observations to a conclusion. This is a counterpoint to deductive reasoning, which refers to specialization from general ideas to a conclusion.\n",
      "learning trajectory.2\n",
      " \n",
      "The standard backpropagation algorithm (gradient descent) gets serious issues when the number of layers becomes large. The probability of local minima in the error function increases with every layer. Not only local minima in a mathematical sense cause problems, sometimes there are just flat regions in the error function (modifying one or more weights does not significantly change the error) where gradient descent does not work.On the other hand, networks with many layers can solve more difficult problems, as every layer of cells can also provide a layer of abstraction.Deep Learning addresses exactly this problem. The basic idea is to perform an unsupervised learning procedure on every single layer in addition to using gradient descent for the network as a whole. The goal of the unsupervised learning is to make each single layer extract characteristic features out of its input that can be used by subsequent layers.Although the term \"Deep Learning\" is currently being used much too widely, it is more than just a marketing hype.Edit: A few years ago, many people, including myself, believed that unsupervised pre-training was the main enabler of deep learning. Since then, other techniques became popular that produce even better results in many cases. As mentioned in the comment by @Safak Okzan (below his own answer), these include:Residual NetworksBatch normalizationRectified linear units\n",
      "Learning\n",
      " \n",
      "Broadly speaking you can divide this process into 2 phases:Determining location of text. It's at the intersection of ml and Computer Vision, because before text recognition part you need to find where this text is located. It's not an easy task, you can find lines, boxes, etc, look at  lib for example, it may be useful for CV-related tasks. If all of your documents have same precise form (location of fields relative to scanned list itself) and you can scan them perfectly, without distortions (rotations, offsets) you can try to search text in static areas, where fields are.When you have found the text, you have to break contents of each field to words, then words to characters, and then you can feed your recognizer (ML part) with these characters and get labels of each character itself. And it's almost impossible(nowadays) for handwritten text, thus it's hard to recognize handwritten text in general case. Even if fields contain only printed text i recommend you to avoid this step, and use special lib for OCR, like \n",
      "Computer\n",
      " \n",
      "Try the bag-of-features approach. See  using the Computer Vision System Toolbox.\n",
      "Computer\n",
      " \n",
      "You can use the  function to compute various shape features, like area, perimeter, eccentricity, etc. In particular, the eccentricity will tell you how close the shape is to a circle.For texture features, try  and  functions in the Computer Vision System Toolbox.\n",
      "Computer\n",
      " \n",
      "I'll try to give you some intuition over the problem...Initially, updates were made in what you (correctly) call (Batch) Gradient Descent. This assures that each update in the weights is done in the \"right\" direction (Fig. ): the one that minimizes the cost function.With the growth of datasets size, and complexier computations in each step, Stochastic Gradient Descent came to be preferred in these cases. Here, updates to the weights are done as each sample is processed and, as such, subsequent calculations already use \"improved\" weights. Nonetheless, this very reason leads to it incurring in some misdirection in minimizing the error function (Fig. ).As such, in many situations it is preferred to use Mini-batch Gradient Descent, combining the best of both worlds: each update to the weights is done using a small batch of the data. This way, the direction of the updates is somewhat rectified in comparison with the stochastic updates, but is updated much more regularly than in the case of the (original) Gradient Descent.[UPDATE] As requested, I present below the pseudocode for batch gradient descent in binary classification:(In the case of multi-class labeling, error represents an array of the error for each label.)This code is run for a given number of iterations, or while the error is above a threshold. For stochastic gradient descent, the call to neural_network.backpropagate_and_update() is called inside the for cycle, with the sample error as argument.\n",
      "Gradient\n",
      " \n",
      "Gradient Clipping basically helps in case of exploding or vanishing gradients.Say your loss is too high which will result in exponential gradients to flow through the network which may result in Nan values . To overcome this we clip gradients within a specific range (-1 to 1 or any range as per condition) .where grads _and_vars are the pairs of gradients (which you calculate via tf.compute_gradients) and their variables they will be applied to.After clipping we simply apply its value using an optimizer.\n",
      "Gradient Clipping\n",
      " \n",
      "I'm not sure if I fully understand what your question is exactly. Bag of words works well for some purposes, but in a lot of cases it throws away a lot of potentially useful information (which could be taken from word order, for example).And assuming that you get a grammatical sentence as input, why not use your sentence as document and still use LDA? The position of a word in your sentence can still be verymeaningful.There are plenty of classification methods available. Which one is best depens largely on your purpose. If you're neew to this area, this may be interesting to have a look at: \n",
      "LDA\n",
      " \n",
      "a) The method you describe sounds fine, but everything will depend on the implementation of labeled LDA that you're using. One of the best implementations I know is the . It is not actively developed anymore, but it worked great when I used it. b) You can look for topical content on , which has a structured ontology of topics/entities, and links to Wikipedia articles on those topics/entities. \n",
      "LDA\n",
      " \n",
      "There are two things you can do: 1) If your documents really contain texts that are exclusively about music or science, it is strange that the LDA topics give such a mixed result. Trying to improve the model may be worthwhile. You may consider dropping stopwords, ignoring low-frequency words, and so on. 2) However, the method that you're really looking for is so-called labeled LDA. With Labeled LDA, you train your model on documents that have already been labeled with the target topics, rather than having the model infer the most appropriate topics itself. As far as I know, labeled LDA has not been implemented in gensim, but you'll find it in the , among other places.\n",
      "LDA\n",
      " \n",
      "LDA algorithms don't take tf-idf weights in input, but bag of words, however you could first filter words from your corpus based on their tf-idf score, and then feed the new texts to your LDA program. \n",
      "LDA\n",
      " \n",
      "After you trained your LDA model with some data , you may want to project some other data, . in this case what you should do is:Note that 'fit' is used for fitting the model, not fitting the data.So  is used in order to build the representation (projection in this case), and  is used for predicting the label of each sample. (this is used for ALL classes that inherits from  in sklearn. You can read the  for farther options and properties.Also, sklearn's API allow you to do  instead of . Use this version when you are not interested in model itself after this point in the code. A few comments:Since PCA is an , LDA is a better approach for doing this \"visual\" classification you are currently doing.Moreover, If you are interested in classification, you may consider using different type of classifiers, not necessarily LDA, although it is a great approach for visualization.   \n",
      "LDA\n",
      " \n",
      "A very simple LDA implementation using gensin. You can find more informations here: I hope it can help you[(0, u'0.066*animal + 0.065*, + 0.047*product + 0.028*philosophy'), (1, u'0.085*. + 0.047*product + 0.028*dietary + 0.028*veg')]\n",
      "LDA\n",
      " \n",
      "Why are you removing frequent words? Leave them in. LDA doesn't always work well when given a very large number of features. A lot of the published results restrict LDA to the top 20k most frequent English words (sans stop words). I'm guessing thats a lot of your problems right now. There can be other issues as well, are you running the algorithm to convergence? is 10 topics too small to get reasonable topics?  You've given very little info. Go to the original Online LDA papers and try and replicate their results first to confirm you are using the library correctly, then adjust to new corpa once you've got the hang of it. \n",
      "LDA\n",
      " \n",
      "You should rethink your approach, since you are mixing probabilistic methods (LDA) with Linear Algebra (dimensional reduction). When you feel more comfortable with Linear Algebra, consider Non Negative Matrix Factorisation.Also note that your topics already constitute the reduced dimensions, there is no need to jump back to the extracted top words in the topics.\n",
      "LDA\n",
      " \n",
      "There is a  field in your fitted object containing all the relevant information In particular, if you fit a linear LDA there will be  field which is the linear operator used for projection. However, one should bear in mind that value of coefficients of linear models are not feature importances. There is much more in that to consider. Weight can be big because your feature have small values or because there is a highly biased distribution of the values. If you need feature selection technique - use feature selection methods (like L1 regularized models) otherwise you might easily get wrong conclusions from your data.\n",
      "LDA\n",
      " \n",
      "Your suggestion of considering this as a multiple problem + final normalization, has some sense, but it's known to be problematic in many cases (see, e.g., the problem of ).What you're describing here is , and there are many many known techniques for doing so. You didn't specify which language/tool/library you're using, or if you're planning on rolling your own (which only makes sense for didactic purposes). I'd suggest starting with  which is very simple to understand and implement, and - despite its strong assumptions - is known to often work well in practice (see ).Irrespective of the underlying algorithm you use for soft binary classification (e.g., LDA or not), It is not very difficult to transform aggregate input into labeled input. Consider for example the instanceIf your classifier supports instance weights, feed it 4 instances, labeled 1, 2, ..., with weights given by p_1, p_2, ..., respectively.If it does not support instance weights, simply simulate what the law of large numbers says would happen: generate some large n instance from this input; for each such new input, choose a label randomly proportionally to its probability.\n",
      "LDA\n",
      " \n",
      "First of all you should make yourself familiar with the  concept.The basic idea ist to map each word in a sentence on the number of occurrences, e.g., for the sentences ,  would get mapped ontoThis allows you to use one of the standard approaches. Also worthwhile would be having a look at  it is made to reweigh words in a  representation, with their importance to distinguish the documents (or sentences in your case)Secondly, you should look at LDA which was made specifically for clustering words to concepts. Nevertheless, it is made of a view concepts. Most promising to me sounds like a combination of these approaches. Generate, , reweigh the  using , run  and augment the reweighed  with the  concepts and then use a standard clustering algorithm.\n",
      "LDA\n",
      " \n",
      "Simply put, as with any algorithm your complexity increases with larger input numbers. Dividing the problem into smaller sub-problems and subsequently combining these may prove faster (called divide and conquer algorithms).Now, with these type of Machine Learning algorithms there's an additional need for abstraction in the features. You neither wanna input every single pixel at once (having only local information), nor want to represent the whole image by a single number/symbol (having only global information). A number of approaches combine these sorts of data into hierarchical representations (mostly called Deep Learning).If you bring together these two concepts, it should be clear(er) that processing small image patches first gives you a bigger amount of local information which you can then combine to infer into global information at a later stage. So \"because it's easier\" is not the full reasoning behind it. It also makes everything perform better/more accurate. I hope this answers your question without being too vague (a thorough answer would become too long). For a more detailed introduction on RBMs have a look at e.g. chapter 7 on this \n",
      "Machine\n",
      " \n",
      "Yes, it's possible with supervised learning. You pick yourself a model which you \"train\" with the data you already have. The model/algorithm then \"generalizes\" to previously unseen data from the known data. What you specify as a group would be called class or \"label\" which needs to be predicted based on 2 input features (code/description). Whether you input these features directly or preprocess them into more abstract features which suits the algorithm better, depends on which algorithm you choose. If you have no experience with Machine Learning, you might start with learning some basics while testing already implemented algorithms in tools such as RapidMiner, Weka or Orange.\n",
      "Machine\n",
      " \n",
      "If you read the book called \"Machine Learning with Spark\", the author wrote,  Categorical featuresCategorical features cannot be used as input in their raw form, as they are notnumbers; instead, they are members of a set of possible values that the variable can take. In the example mentioned earlier, user occupation is a categorical variable that can take the value of student, programmer, and so on.:To transform categorical variables into a numerical representation, we can use acommon approach known as 1-of-k encoding. An approach such as 1-of-k encodingis required to represent nominal variables in a way that makes sense for machinelearning tasks. Ordinal variables might be used in their raw form but are oftenencoded in the same way as nominal variables.:I had exactly the same thought.I think that if there is a meaningful(well-designed) transformation function that maps categorical(nominal) to real values, I may also use learning algorithms that only takes numerical vectors.Actually I've done some projects where I had to do that way and there was no issue raised concerning the performance of learning system.To someone who took a vote against my question,please cancel your evaluation.\n",
      "Machine\n",
      " \n",
      "Majority of machine learning algorithms work with numbers, so you can to transform your categorical values and string into numbers.Popular python machine-learning library scikit-learn has the . With 'yes/no' everything is easy - just put 0/1 instead of it.Among many other important things it explains the process of  using their .When you work with text, you also have to transform your data in a suitable way. One of the common feature extraction strategy for text is a  score, and I wrote a .\n",
      "scikit-learn\n",
      " \n",
      "Machine Learning Studio services that you create needs to receive requests from a device that has SSL capabilities to perform HTTPS requests. AFAIK, Arduino doesn't support SSL capabilities.One usual scenario is to attach the Arduino to a third device like Raspberry Pi 2 etc to use it as a gateway and do the call from the Pi itself.Here's a sample  from Microsoft Open Technologies team that utilizes Arduino Uno, Raspberry pi and Azure stuff.Hope this helps!\n",
      "Machine\n",
      " \n",
      "Congrats for completing the Machine Learning course by Andrew Ng, longtime back I have also done this awesome course.Anyway, so I will answer your question one by one, although there are few questions that are interrelated.Q-1) Where can I find the Real world Machine Learning use case examples?Here are a few links where you can find tutorials of machine learning with real-world use cases:Machine Learning example Scikit learn: Machine Learning tutorials H20: Sagemaker Machine Learning &amp; Deep Learning example: Q-2) What tools or framework are used in Industry/Production for MachineLearning projects?There are a wide variety of tools or framework that are used in Industry level like:Machine Learning:R (mostly used in academia nowadays)Python(Sci-kit learn)GraphLabApache MahoutSpark MLlibH20Deep Learning:Tensorflow and KerasMxnetPytorchDeepLearning4jAnalytics Zoo (mainly for deep learning with big data using spark)While R,Scikit learn,GraphLab works great on single machine and most popular choices among data scientist or machine learning practitioners, but Mahout, H20 and recently Spark (MLlib) has gained a lot of popularity in this era of Big Data, where you want to do machine learning on large dataset that will not fit on single machine.Also, there are some other tools like Weka, Rapid Miner for GUI based workflow of machine learning work.Choice of this tool or framework really depends on the factors like project requirement, team members knowledge about the tool/language, also ease of development and scalability of deployment.Q-3) How Machine Learning models are used or deploy in production?In production, you have to first build a model, validate &amp; evaluate that model, then the model is most finally deployed as web/rest service to be used by other applications/services. Deploying a machine learning model depends on a lot of factors such as-Is the model trained offline? Or are you deploying an online learning model?How often you will retrain your model?How would you test your newer version of the model? - A/B testing or Bandit variation.Along with other generic things - latency, throughput, data input/output format etc. There are some cloud-based machine learning service provider like Azure ML() BigML() etc, where you can upload your dataset,do some data processing, train|validate|evaluate your machine learning model and then finally deploy it as web service in the cloud.Also all major cloud platform (aws, google cloud, azure) nowadays provide you with a machine learning platform, where you can build your own model, evaluate them and then finally deploy it in the cloud. It gives you the flexibility to build the model with almost all major machine learning or deep learning frameworks, and as per your requirement gives you the flexibility to deploy ( what type of server/containers, number of inference/prediction server, etc).Amazon SageMaker:[+] Google Cloud Machine Learning (ML) Engine: [+] Q-4) How to become Data Scientist? Or What should I do next?It's a million dollar question and lots of google serach on this question..haha.. I will try to give you a short and concise answer based on my knowledge. First of all Data Science is much broader field of study,that comprise of the following common steps:Business understanding or Questioning PhaseData gathering or acquiringData processing and preparationModel BuildingValidation and EvaluationAlong with this, you also need to do Model Retraining to depending upon the change of data variability, or you can deploy online learning model(which will adapt itself based on the data that it is seeing).But basic ingredients for anyone to become data scientist/machine learning practitioner is to have the curiosity about data (i.e to understand the data &amp; find valuable knowledge out of it). Neither there is a shortcut to becoming a data scientist nor there is any course that will make you become data scientist overnight. There is no predefined role/scope what a data science person should be knowing or doing on a day to day basis in a company. Different industry or company has its own job requirement/description for a data scientist depending on their business problem.A good versatile data scientist must have the following skills in order to sustain confidently across  various industry and succeed in his/her career:Good knowledge in Statistics (including a little bit of Bayesian)- essential during EDA phase.Maths (especially linear algebra,matrix,vector, multivariate calculus): Good practical knowledge of machine learning algorithms: Some deep learning &amp; reinforcement learning knowledge. Stanford Deep Learning course:  and youtube video of this course by Andrej Karpathy-  . Also DeepLearning Coursera:   and Reinforcement Learning course- Berkley University: Large dataset analysis through big data tools like Spark and SQL. Machine Learning with Big Data: Curious mind to explore data and learn new things (to stay up to date with the latest innovation in this domain). And some Business domain knowledge- good to have (Optional)The best way is to play with data or do some real-world projects.Lots of real-world datasets available publicly, you can pick a dataset of your choice of interest. Also, you can test your skill and expertise by participating in Machine Learning and Data Science competition at Kaggle.To gain some knowledge about data science, how it works along with some hand on exercise, you can try course online like:\n",
      "Machine\n",
      " \n",
      "tf-idf vectors are only one kind of vectorizer. You're free to try many others. The scenario above is possible, but you'll find that machine learning is much more about real datasets than theoretical guarantees.In practice tf-idf works quite well, but it's not radically different from say a count vectorizer. There are also nearly a dozen different ways of specifically weighting both your tf, and idf components to add robustness to certain word distributions such as those you've proposed above.In Machine Learning you shouldn't think of \"good\" or \"bad\" ideas, you should run an experiment and determine what the effect on the model performance is.\n",
      "Machine\n",
      " \n",
      "First: Congrats on taking the course on Machine Learning on Coursera! :) will compute the hypothesis for all x(i) at the same time, saving each h_theta(x(i)) as a row of . So there is no need to reference a single row.Same is true for .\n",
      "Machine\n",
      " \n",
      " is excellent, but I thought I'd add a bit of intuition as to why random forest fails in this case.You have to keep in mind that Machine Learning isn't some magic way to impart intelligence to computers; it's simply a way of fitting a particular model to your data and using that model to make generalizations. As the old adage goes, \"all models are wrong, but some are useful\". You've hit on a case where the model is wrong as usual, but also happens to be useless!The output space: Random forests at their core are basically a clever and generalizable way of mapping inputs to outputs. Your output space has  possible unique outputs, and only  of these are valid (i.e. outputs with one of each number). The only way for a random forest to know whether an output is valid is if it has seen it: so in order to work correctly, your training set will have to include examples with all of those 120 outputs.The input space: when a random forest encounters an input it has seen before, it will map that directly to an output that it has seen before. But what if it encounters an input it has not seen? For example, what if you ask for the answer to  and this is not in the training set? In terms of Euclidean distance (a useful way to think about how things are grouped) the closest result will probably be something like , which might map to , which is wrong. Thus we see that in order for random forests to work correctly, your training set will have to have all possible inputs. Some quick combinatorics show that your training set will have to be of size at least , with no duplicates.The forest itself: even if you have a complete input space, the random forest does not consist of a simple dictionary mapping of inputs to outputs. Depending on the parameters of the model, the mapping is typically from groups of nearby results to a single answer, so that, for example,  will all map to . This sort of generalization is useful in most cases, but is not useful for your particular problem. The only way for the random forest to work in your case would be to push the  and  parameters to their extreme values, so that the forest is essentially a one-to-one mapping of inputs to their correct outputs: in other words your classifier would be just an extremely complicated way of building a dictionary.To summarize: Machine Learning is just a model applied to data, which is useful in certain cases. In your case, the model is not all that useful: in order for Random Forests to work on your problem, you'd need to over-fit a comprehensive set of inputs and outputs. At that point, you might as well just construct a dictionary and call it a day.\n",
      "Machine\n",
      " \n",
      "You have a multi-class classification problem with 1728 samples. The features are in 6 groups:what you need to do for features is to create features like this:at the end you'll have features. The output classes are:You need to try several classification algorithms to see which one works better. For evaluation you can use cross-validation or you can put away say 728 or the samples and evaluate on that.For classification models you iterate over 10 different classification models available in Machine Learning libraries and check which one is better. I suggest using scikit-learn for simplicity.You can find a simple iterator over several classifiers in .Remember that you need to tune some parameters for each model and you shouldn't tune them on the test set. So it is better to divide your samples into 1000 (training set), 350 (development set), 378 (test set). Use the development set to tune your parameters and to choose the best performing model and then use the test set to evaluate that model over unseen data.\n",
      "Machine\n",
      " \n",
      "From my understanding of Machine Learning theory, Gaussian Mixture Model(GMM) and K-Means differ in the fundamental setting that K-Means is a Hard Clustering Algorithm, while GMM is a Soft Clustering Algorithm. K-Means will assign every point to a cluster whereas GMM will give you a probability distribution as to what is the probability of the point to belong in each of the 5 clusters. Furthermore, this also depends on the kind of parameters are you using for GMM. It could be possible for GMM to produce clusters somewhat similar to K-Means if you use a constant variance.Now, I am not sure about this because you need to provide more information on how you are picking Hard Clusters from GMM and how are you calculating the cluster centers. If you are just making a hard assignment from GMM based on the cluster which has the maximum probability, then it could be possible that they get assigned to the same clusters. In my opinion this will be possible only if the data points are easily separable and your GMM is assuming constant variance. As far as the cluster centers go, it depends on the way you are calculating them. If you are using the mean vectors obtained from GMM, then it is very very unlikely that K-Means and GMM will give you same cluster centers. On the other hand if you are first generating Hard clusters like mentioned above and then calculating the centers yourself, then it could be possible that they have the same centers when the hard clustering for all your points is the same in both K-Means and GMM.I think you should provide more information about the way you are doing this, so that the community members can better help you. Also you should also identify your use case well and decide whether you need Hard or Soft Clustering. Choose GMM only if you desire soft clustering and/or you have a prior belief that your data points have been generated from Gaussian Distributions for each cluster.\n",
      "Machine\n",
      " \n",
      "As a beginner in Machine Learning you should start with the bookThere are some prerequisites other than programming are Linear Algebra, Probability theory, etc. i.e. you should have a strong background in Mathematics. Although the book I suggested covers the common mathematical frameworks needed for understanding Machine Learning in its introductory chapters.Moreover, you should practice implementing different learning algorithms (start from smaller ones) to grab the concepts well. As  said,\n",
      "Machine\n",
      " \n",
      "The short answer is yes, kind of. Read on.Definition of Machine LearningTo understand what Machine Learning is let's first define the term . The often quoted definition by Tom M. Mitchell (1) is as follows:Meaning?This sounds quite formal, however it just says computers learn from experience that they are presented with in terms of data. The data to enable learning exists relative to a specific task and consists of several parameters:T, a task to accomplish, e.g. predict housing price predictionsE, some value of experience, e.g. prices observedP, some value of performance, e.g. how many prices are predicted Example: Housing pricesOnce a program has learnt from these inputs, it can take a new, previously unseen experience and from that predict, in our example, the specific housing price. The housing price might be strongly correlated to say location, age and size of house or apartment, and the luxury of its interiors. What is the result of a learning algorithm?In its simplest form then a machine learning algorithm for housing prices might implement a  analysis. It takes as input a body of data that relates real, observed prices to the four features location, age, size, luxury. The process of learning produces a regression model that in essence assigns a weight to each feature, of the formThat is, the weights  are learned from the input data,  is the predicted price. The learning is considered successful once the formula is able to successfully predict housing prices based on a list of features alone. Usually a prediction is considered successful if it falls within a certain bound (%-range) of the real price. Note that the definition of successful very much depends on the kind of task that the program must learn, however the result needs to be substantially better than a pure random guess (that is, the ratio of correct results needs to be statistically significant).Is there more to it?Yes, a lot. Some pointers can be found in this . If you are keen to get into the subject, professor  is quite famous, although there are many more courses if you look for it. Pick the one that best suits your interests.References(1): Mitchell, T. (1997). Machine Learning, McGraw Hill. ISBN 0-07-042807-7, p.2. as referenced by \n",
      "Machine\n",
      " \n",
      "For statistical machine learning lejlot has given a very good answer. However, decision tree learning are a bit different. It uses a greedy search towards a good structure which might result in weird outcomes here. Historically this is because they come from a time when Machine Learning was more of an art (or engineering, but that sounds less sexy)..Before continuing we need two ensure two properties (the first has already been explained by lejlot)This duplications are not due to an error in the data collection processYour data is still i.i.d. in case you want to do LOO or CV, train, test, validation split. This one looks extremely important to me. If seen many example where this spoiled all properties of the statistical evaluation. Especially, train/test splits might be applied for pruning etc. If so and your examples are not independent you need to make sure that they end up in the same fold For numerical data I would immediately become very suspicious that either of these is violated. Given these assumption -- and in addition that you use C4.5 -- we can analyse the impact on the learned model. If we assume, that your classes are separable by a decision tree and you do not put up any constrains it doesn't make any difference. In this case you can actually safely remove them, however, keeping them doesn't make a big difference either on the classifications. However, the structure of the tree can be largely influenced by these added examples. Without calculating the actual values for / index I assume that the first split will be on . And this is the most important result of these extra example: the greediness of C4.5 will drive the structure towards something which captures these examples very well. You could call that structure overfitting for a certain class.  This can be a problem if this difference is rather marginal compared to other features. SVMs would react here much nicer (not a surprise: statistical ML).In total I would actually keep them and remove them. During inducing the structure of the tree i would ignore them and only calculate the structure on the remaining examples. However, for calculating the classifications in the leaves and for pruning I would use them. The previous paragraph contains the most important recommendation: I would apply pruning on the tree in such a domain. \n",
      "Machine\n",
      " \n",
      "Q-Learning is a TD (temporal difference) learning method. I think you are trying to refer to TD(0) vs Q-learning. I would say it depends on your actions being deterministic or not. Even if you have the transition function, it can be expensive to decide which action to take in TD(0) as you need to calculate the expected value for each of the actions in each step. In Q-learning that would be summarized in the Q-value.\n",
      "Q-Learning\n",
      " \n",
      "What you typically do is to use an extensive dataset, and then split that dataset randomly. For example, if you have 100 000 rows of data to train your model with, you could give a random 80% of that data to train the model with, and use the remaining 20 000 rows to validate it. This is a common pattern in Machine Learning. In this approach, you can now work with your model to improve the scores you get. You do NOT want to create false testdata on your model. \n",
      "Machine\n",
      " \n",
      "What makes you think your features are good enough to predict if something is popular? Thing about all the information you've thrown away (the content of the actual document), the information your missing (who is the source of the publication, what does their network look like) questions that aren't obvious from your data (popular with whom? Different groups of people may like / dislike different things). You need to Learn more about Machine Learning in general.Learn about feature engineering, look for some previous work people have done with text based data (sentiment analysis, topic modeling, spam filtering).Learn to build and test hypothesis with your data. While in this case, its obvious you are probably making a lot of errors -- but thats mostly because this is novice work. These questions in general will always be problem and data dependent, so its often not helpful to provide generic information and ask for advice. Once you learn some more ML, you need to think about what your models mean - and what their performance means. From that you can build a hypothesis about your performance, and try and test it. Based on the results, you change something about your model or data and repeat. \n",
      "Machine\n",
      " \n",
      "Have you cross validated your models' performance at all? Have you done a grid search for the hyper parameters? Your output could potentially be easy explained as  poor execution of Machine Learning. If you are using the RBF kernel, and its width is too small, then the only factor that will contribute to a classification decision is the bias term. The bias term, by its nature, is the same for all inputs. Thus, you would get all the same outputs exactly (or almost) for all unseen test data (unless it was very very close to a training datum). I can't say for sure this is what happened without knowing more details about your data and what you actually did. But this seems a likely scenario. \n",
      "Machine\n",
      " \n",
      "Both methods work with a population that is improved over many generations. The key difference is how the population is represented.Genetic Algorithms (GAs) work on individuals of the population, e.g. through mutation. You can enumerate the ancestors of each individual.The Cross-Entropy Method (CEM) represents the population as a probability distribution. Individuals are drawn from this distribution. The distribution parameters are re-estimated from the best e.g. 2% and the other 98% are discarded.Technically, &quot;the best 2%&quot; is also a probability distribution. You could draw a very large sample from it, but it's expensive. So you try to approximate &quot;the 2% distribution&quot; with your simple distribution. The  measures the difference between two distributions, which you want to minimize. Often this is simpler than it sounds: if your distribution is a Gaussian you can just estimate a new mean and (co-)variance from the best 2%.Practical considereations:The CEM requires you to come up with a probability distribution over individuals. Most GAs also require such a distribution only to generate the initial population, in addition to parameters like mutation strength.The CEM is simple to implement and has few parameters. It is a great baseline algorithm, and occasionally it beats methods that are much more sophisticated. However CMA-ES is a better baseline for continuous problems with just a few hundred parameters, because of its strong track record.Estimating parameters from just 2% of the population requires a very large population size. Throwing away 98% of the information is wasteful. On the other hand, it prevents the CEM from stepping &quot;sideways&quot; and getting distracted by sub-optimal solutions.GAs can be much more fancy and exist in many problem-specific variations. The CEM can be adapted to a problem by choosing a clever distribution. This works great for some discrete problems. Generally I'd say using a GA is one step up from the CEM, both in complexity (harder to get it working right) and in terms of potential performance (more opportunities to adapt its operators to the problem).References: (Mannor et al, 2003)(Szita and Lrincz, 2006) (Botev et al., 2013)\n",
      "Algorithms\n",
      " \n",
      "If you disable the colors, how many clusters do you see?I'd say there is only one big cluster in this data set, at least with this preprocessing/visualization.The three clusters you get from spectral clustering are meaningless. It's essentially doing quantization, but it did not discover structure. It minimized squared deviations by chunking the data into three similarly sized chunks. But if you would run it again, it would probably produce similar looking, but different chunks: the result is largely random, sorry.Do not expect classes to agree with clusters. As you can see in this data set, it may have three labels, but only one big messy \"cluster\".Its easy to produce this effect:produces this:Notice how similar this is to your result? Yet, this is a single gaussian, there are no clusters here. Spectral clustering produced 3 clusters here as requested, but they are totally meaningless.Always check your results if they are meaningful. It's really easy to get something that looks like a nice partitioning, but that is a random convex partitioning. Just because it produced clusters doesn't mean the clusters are there! - Algorithms such as k-means and spectral clustering are prone to Can you see the cluster in this face? ;-) Is it real?Here is a rather successful projection of a text data set using MDS. It shows a number of clusters stretching out into different directions of the data space. K-means and variants will not work well on this data set. Gaussian Mixture Modeling would work, but only on the projected data.\n",
      "Algorithms\n",
      " \n",
      "I'm not an expert in this field but I think you can look at some of the heuristic search algorithms like Genetic Algorithm (GA).Also, every games are different for instance the super mario vs chess game. I'm not sure what game you are after but GA has been successfully implemented to play Super Mario smartly. I think for a start you can develop an AI to play the game first, then meantime collect some data for analytics so that it can play better the next time? Again, I dont know what kind of game you are after so it is  difficult for me to contribute ideas to help you.\n",
      "Algorithm\n",
      " \n",
      "Not sure you can do what you suggest because you have no score to boost without CF.You are indeed using a hybrid, much the same as the . To do purely content-based recommendations you have to implement two methodsPersonalized recommendations: here you have to look at the content of items the user preferred and find items that have similar content. This can be done by using something like the Mahout spark-rowsimilarity job to create a model of item: list-of-similar-items then indexing the results with a search engine and using the user's preferred item ids as the query. This is being added to the Universal Recommender.\"People who liked this also liked these\": these are items similar to one being viewed, for example, and are the same for all users. They are not personalized and so are useful even for anonymous users with no history. This can be done with the same indexed ids as above but using the items similar to the one being viewed as the query. One might think to use only the similar items themselves but by using them as a query you can put the categorical boost in the search engine query and have boosted items returned. This already works in the Universal Recommender but the similar items are not in the model yet.That said mixing content with collaborative-filtering will almost surely give better results since CF works better when the data is available. The only time to rely on content-based recommendations is when your catalog is of one-off items, which never get enough CF interactions or you have rich content, which has a short lifetime like breaking news. BTW anyone who wants to help add the pure content-based part to the Universal Recommender can contact the new maintainers of it at \n",
      "Mahout\n",
      " \n",
      "I get more than one digit in my results, are you sure it is not due to your dataset ? (for example using a very small dataset would yield to simple decision trees and so to 'simple' probabilities). Otherwise it may only be the display that shows one digit, but try to print .I am not sure to understand what you mean by \"the probabilities aren't affected by the size of my data\". If your concern is that you don't want to predict, eg, too many spams, what is usually done is to use a threshold  such that you predict 1 if . This way you can use the threshold to balance your predictions, for example to limit the global probabilty of spams. And if you want to globally analyse your model, we usually compute the Area under the curve (AUC) of the Receiver operating characteristic (ROC) curve (see wikipedia article ). Basically the ROC curve is a description of your predictions depending on the threshold .Hope it helps!\n",
      "AUC\n",
      " \n",
      "AUC is not always area under the curve of a ROC curve. Area Under the Curve is an (abstract) area under some curve, so it is a more general thing than AUROC. With imbalanced classes, it may be better to find AUC for a precision-recall curve.See sklearn source for :As you can see, this first gets a roc curve, and then calls  to get the area.I guess your problem is the  call. For a normal  the outputs are always the same:If you change the above for this, you'll sometimes get different outputs:\n",
      "AUC\n",
      " \n",
      "Its probably a better idea to cross validate your models with different test samples through cross validation to avoid biases. Also check your models against different evaluation metrics depending upon your application type. For instance use recall, accuracy and AUC for each model if its a classification problem.Evaluation results can be pretty deceptive and require extensive validation. \n",
      "AUC\n",
      " \n",
      "Your results seem to indicate the classifier is prediction 0 in almost all cases.Below is an example where the data is 90% in class 0 and the classifier always predicts 0. It looks very similar to your results.Also, for measuring AUC you should be predicting probabilities using  instead of predicting labels.\n",
      "AUC\n",
      " \n",
      "One must understand crucial difference between AUC ROC and \"point-wise\" metrics like accuracy/precision etc. ROC is a function of a threshold. Given a model (classifier) that outputs the probability of belonging to each class, we predict the class that has the highest probability (support). However, sometimes we can get better scores by changing this rule and requiring one support to be 2 times bigger than the other to actually classify as a given class. This is often true for imbalanced datasets. This way you are actually modifying the learned prior of classes to better fit your data. ROC looks at \"what would happen if I change this threshold to all possible values\" and then AUC ROC computes the integral of such a curve. Consequently:high AUC ROC vs low f1 or other \"point\" metric, means that your classifier currently does a bad job, however you can find the threshold for which its score is actually pretty decentlow AUC ROC and low f1 or other \"point\" metric, means that your classifier currently does a bad job, and even fitting a threshold will not change ithigh AUC ROC and high f1 or other \"point\" metric, means that your classifier currently does a decent job, and for many other values of threshold it would do the samelow AUC ROC vs high f1 or other \"point\" metric, means that your classifier currently does a decent job, however for many other values of threshold - it is pretty bad\n",
      "AUC\n",
      " \n",
      "You are heading into the right direction. The confusion matrix definetly is the right start for computing the accuracy of your classifier. It seems to me that you are aiming at reciever operating characteristics.The AUC (area under the curve) is a measurement of your classifiers performance. More information and explanation can be found here:This is my implementation, which you are welcome to improve/comment:\n",
      "AUC\n",
      " \n",
      "You almost have it right. The Likelihood of a model () for the observed data () is the probability of observing , given :For Maximum Likelihood Estimation (MLE), you choose the value of  that provides the greatest value of . This does not necessarily mean that the observed value of  is the most probable for the MLE estimate of . It just means that there is no other value of  that would provide a higher probability for the observed value of .In other words, if  is the MLE estimate of , and if  is any other possible value of , then . However, there still could be another possible value of the data () different than the observed data () such that . The probability of  for the MLE estimate of  is not necessarily 1 (and probably never is except for trivial cases). This is expected since  can take multiple values that have non-zero probabilities.\n",
      "Likelihood\n",
      " \n",
      "A  is a collection of 's. No matter how big your training set, a decision tree simply returns: a decision. One class has probability 1, the other classes have probability 0.The RandomForest simply votes among the results.  returns the number of votes for each class (each tree in the forest makes its own decision and chooses exactly one class), divided by the number of trees in the forest. Hence, your precision is exactly . Want more \"precision\"? Add more estimators. If you want to see variation at the 5th digit, you will need  estimators, which is excessive. You normally don't want more than 100 estimators, and often not that many.\n",
      "probability 0.The\n",
      " \n",
      "Feature selection is of crucial importance,it gives information of the relevance of features for your problem.Good theoretical explanation is given in Pattern Recognition by Sergios Theodoridis and Konstantinos Koutroumbas book.I found this simple code exampleResult You can read more [ examples.\n",
      "Pattern\n",
      " \n",
      "Confusion Matrix tells us about the distribution of our predicted values across all the actual outcomes.Accuracy_scores, Recall(sensitivity), Precision, Specificity and other similar metrics are subsets of Confusion Matrix.F1 scores are the harmonic means of precision and recall.Support columns in Classification_report tell us about the actual counts of each class in test data.Well, rest is explained above beautifully.Thank you.\n",
      "Confusion Matrix\n",
      " \n",
      "if you have more than one classes in your classifier, you might want to use pandas-ml at that part. Confusion Matrix of pandas-ml give more detailed information. \n",
      "Confusion Matrix\n",
      " \n",
      "I'll try to give you some intuition over the problem...Initially, updates were made in what you (correctly) call (Batch) Gradient Descent. This assures that each update in the weights is done in the \"right\" direction (Fig. ): the one that minimizes the cost function.With the growth of datasets size, and complexier computations in each step, Stochastic Gradient Descent came to be preferred in these cases. Here, updates to the weights are done as each sample is processed and, as such, subsequent calculations already use \"improved\" weights. Nonetheless, this very reason leads to it incurring in some misdirection in minimizing the error function (Fig. ).As such, in many situations it is preferred to use Mini-batch Gradient Descent, combining the best of both worlds: each update to the weights is done using a small batch of the data. This way, the direction of the updates is somewhat rectified in comparison with the stochastic updates, but is updated much more regularly than in the case of the (original) Gradient Descent.[UPDATE] As requested, I present below the pseudocode for batch gradient descent in binary classification:(In the case of multi-class labeling, error represents an array of the error for each label.)This code is run for a given number of iterations, or while the error is above a threshold. For stochastic gradient descent, the call to neural_network.backpropagate_and_update() is called inside the for cycle, with the sample error as argument.\n",
      "Gradient\n",
      " \n",
      "There are different approaches. For example Matlab uses 'random' or 'nearest' as documented .\n",
      "example Matlab\n",
      " \n",
      "Try the followingAfter this run MATLAB from terminal. Your code should run. What I have found is that the symbol: _ZN7CvBoostC1EPKc is there in filepath/cps/thirdparty/opencv/lib/libml.so.4Not in the file filepath/cps/thirdparty/OpenCV-2.0.0/lib/libml.so.4 and somehow setting the environment variable from the MATLAB is not working. Exporting LD_LIBRARY_PATH before running MATLAB is necessary.\n",
      "MATLAB\n",
      " \n",
      "I would like to suggest  () as an approach to this problem. On how cross-correlation and euclidian distance are related, refer for instance to . It is not invariant to scaling in time or amplitude and may be sensitive to noise, but the question does not imply any such distortions.An advantage of cross-correlation is its efficient implementation in the transform domain. Unfortunately I have only an old Matlab version without  at hands, so I can not time it. But considerwhich should recover . \n",
      "Matlab\n",
      " \n",
      "First, assuming that you're using a newish  of Matlab, the  documentation  indicates that the  method is deprecated and that  should be used. See  for an example.Second, the documentation for  indicates that if the  option is larger than 1, the  start method will be used to produce the initial parameters. This may be the cause (or at least one of the causes) of your observed variability.Finally, you can also try using  before calling  to set the seed of the global random number stream (assuming the function doesn't use it's own stream internally). Alternatively, you can try specifying an  parameter via :I can't test this fully myself  see the  class documentation for further details.\n",
      "Matlab\n",
      " \n",
      "you must download and make libsvm, open the zip file and select your langauges like Matlab etc. then make it! it would give you two files, now you are using Matlab SVM not libsvm.good luck\n",
      "Matlab\n",
      " \n",
      "Your output layer seems to have a linear activation function. Therefor your output vectors components have values that are not restricted to be between 0 and 1. For classification you should use a softmax activation function:()The use of softmax results in vector components which have values between 0 and 1 and which sum to 1 for each vector. So basically you get a probability distribution over your classes. The  has an image showing the effects (left input, right after softmax):There's more information about it in the .From what I could find, the following code change might work in Matlab:\n",
      "Matlab\n",
      " \n",
      "There quite a few number of maths software that will compute derivatives and integral calculus for you. Some of the popular software include MATLAB, Maple, Mathematica, etc. These software will help you learn quite easily.As for you making a machine learn calculus ...You can read up on the following on wikipedia or other books,Newton's Method - Solve the roots of a function numericallyMonte Carlo Integration - uses RNG to compute numeric integrationRunge Kutta Method - Solves ODE's iterativelyThere are many more. These are just the ones I was taught in undergraduate school. They are also fairly simple to understand, depending on your level of academia. But in general, people have been try to numerically compute solutions to models since Newton. Computers have just made everything a lot easier.\n",
      "MATLAB\n",
      " \n",
      "You should look for the file  it has 1000 line each line provides a description of a different class.For more information on how to get this file (and some others you might need) you can read .If you want all the labels to be ready-for-use in Matlab, you can read the txt file into a cell array (a cell per class):\n",
      "Matlab\n",
      " \n",
      "Try the  app in MATLAB.\n",
      "MATLAB\n",
      " \n",
      " in MATLAB / Octave returns a binary /  matrix.   requires that the input be a  or  matrix.Therefore, simply cast  to  and continue:\n",
      "MATLAB\n",
      " \n",
      "Look at  in Matlab.You could just save the classifier variable in a file:And then load it before executing predict:\n",
      "Matlab\n",
      " \n",
      "You were definitely on the right path. While description in the documentation of  (as you posted in the question) is very short, you should have a look at the  site in the MATLAB documentation.In the non-separable case (often called Soft-Margin SVM), one allows misclassifications, at the cost of a penalty factor . The mathematical formulation of the SVM then becomes:with the slack variables  which cause a penalty term which is weighted by .Making  large increases the weight of misclassifications, which leads to a stricter separation.This factor  is called box constraint. The reason for this name is, that in the formulation of the dual optimization problem, the Langrange multipliers are bounded to be within the range .  thus poses a box constraint on the Lagrange multipliers.tl;dr your guess was right, it is the  in the soft margin SVM.\n",
      "MATLAB\n",
      " \n",
      "Before I answer the three parts, I'll just explain the syntax that is used in MATLAB's explanation of k-means ().  is your data matrix (it's represented as  in the link). There are  rows (in this case, 2000), which represent the number of observations/data points that you have. There are also  columns (in this case, 1000), which represent the number of \"features\" that each data points has. For example, if your data consisted of 2D points, then  would equal 2. is the number of clusters that you want to group the data into. Based on the dimensions of  that you gave,  must be 8.Now I will answer the three parts:The  matrix has dimensions . Each row represents a centroid. Centroid locations DO NOT have to be (x, y) at all. The dimensions of the centroid locations are equal to . In other words, if you have 2D points, you could graph the centroids as (x, y). If you have 3D points, you could graph the centroids as (x, y, z). Since each data point in  has 1000 features, your centroids therefore have 1000 dimensions.This is sort of difficult to explain without knowing what your data is exactly. Centroids are certainly not just values, and they may not necessarily be locations. If your data  were coordinate points, you could certainly represent the centroids as locations. However, we can view it more generally. If you had a cluster centroid  and the data points  that are grouped with that centroid, the centroid would represent the data point that is most similar to those in its cluster. Hopefully, that makes sense, and I can give a clearer explanation if necessary.The k-means method actually gives us a good way to accomplish this. The function actually has 4 possible outputs, but I will focus on the 4th, which I will call : has dimensions . For a data point , the row  in the  matrix gives the distance from that point to every centroid. Therefore, for each centroid, you simply need to find the data point closest to this, and return that corresponding data point. I can supply the short code for this if you need it.Also, just a tip. You should probably use kmeans++ method of initializing the centroids. It's faster and generally better. You can call it using this:2>Here is the code necessary for part 3:Each row  of  is the vector that is closest to centroid .\n",
      "MATLAB\n",
      " \n",
      "Here is the link to the actual paper, MATLAB code is provided here Co-ordinate histogram (mentioned in your post) is just a sub-region in the image in which you compute the histogram. These slides explain it visually, . You have multiple histograms here, one for each different region in the image. The probability (or the number of items would depend on the sift points in that sub-region).I think you need to define your pyramid kernel as mentioned in the slides.A Convolutional Neural Network may be better suited for your task if you have enough training samples. You can probably have a look at Torch or Caffe.\n",
      "MATLAB\n",
      " \n",
      "Try using the  in MATLAB. It gives you an easy way to experiment with different color spaces.\n",
      "MATLAB\n",
      " \n",
      "Yes that's certainly possible.  By looking at the example, the training dataset is a set of images and you are finding a common vocabulary of 500 \"words\" / features that describes all of them with adequacy.  By using , what you are doing is you are determining what fraction of each word exists to describe the input image .  Specifically, if you look at the code in that example section, they plot a bar graph where the horizontal axis represents the word index and the vertical axis represents what fraction each word / feature in the vocabulary is used to represent that image.Specifically, this is the bar graph that gets produced (taking from the link):Therefore, similar images should be described with similar features / words and so you could certainly use this as input into your neural network.However, before you train your neural network, as you suspected, you must represent every image you wish to train with this feature vector.  If you intend to use MATLAB's neural network toolbox, you must make sure that each column is an input sample and each row is a feature.   would actually return a  vector where  is the total number of features.  However, if you want to do this more smartly, simply create an  of all of the images you want to transform: , then use one call to  to create this desired feature matrix:The result will be a  matrix where  is the total number of input images you have and  is the total number of features.  To respect the neural networks toolbox, you must transpose this matrix because each column needs to be an input sample, not each row.\n",
      "MATLAB\n",
      " \n",
      "There are various methods of implementing domain adaptation or transfer learning.  that you can find a lot of research about transfer learning applied on different problems (most of which has the source code available). Me, personally used the Matlab code in , it was easy for me to understand and use it for my research.Hope that helps!\n",
      "Matlab\n",
      " \n",
      "Matlab tries to not waste too much resources computing it.But you still can do what you want, just use:\n",
      "Matlab\n",
      " \n",
      "If you look at the  you'll find that there is no property , which is what's giving you your error.You need to calculate the weights  yourself, since MATLAB is solving the dual form of the SVM. More details can be found here. Take a look at  if you want to learn more. You can use the following formula:You can calculate the  vector using a combination of the alpha's, which are returned in your model, and your data.\n",
      "MATLAB\n",
      " \n",
      "You are correct.   requires that the input training examples is a  matrix where  is the total number of samples and  is the total number of features.  What you have to do in your case is reshape your array so that  and  are .  The 784 is due to .  Specifically, you must unroll each slice of your 3D matrix so that it fits into a single vector.  Similarly the class labels must be , so you just need to transpose  and .To achieve the desired reshaping, you use  like so:Now the reshaping of the training and testing examples is a bit odd.  How MATLAB works when reshaping is that it performs this on a column major basis.  This means that when you reshape, it takes columns at a time to produce your results.  As such, because your matrix is , each slice of your 3D matrix is .  Therefore, to facilitate the column major ordering, you take each 2D slice and fit it to a single column.  You would thus have 60000 columns corresponding to 60000 training examples.  The last thing you need to do now is transpose this result to get what is required for .Now that this is done, you can train your model.\n",
      "MATLAB\n",
      " \n",
      "I would have put this in comments if I had enough points, but there is an implementation in Matlab, written by a few members of the team that wrote the original glmnet package:\n",
      "Matlab\n",
      " \n",
      "I haven't seen the python code but i think the following would be useful:sigmoid function to tanh function: Not necessary, many publications use sigmoid for non linear regressions. To be frank, the choice is to be made from the type of data you have. I use sigmoid for many non linear models and it had worked for me. least-square error: You can perform this by inbuilt Matlab function regress instead of confusing it with so many parameters. Normalisations: You can do a min-max normalisation(refer Data Mining by Charu Agarwal), my implementation in Matlab is as follows: Hope it helps. Let me know if you have further questions. \n",
      "Matlab\n",
      " \n",
      "Basically you need to project the data along the direction of the classifier, plot a histogram for each class, and then rotate the histogram so its x axis is parallel to the classifier. Some trial-and-error with scaling the histogram is needed in order to get a nice result. Here's an example of how to do it in Matlab, for the naive classifier (difference of class' means). For the Fisher classifier it is of course similar, you just use a different classifier . I changed the parameters from your code so the plot is more similar to the one you gave.\n",
      "Matlab\n",
      " \n",
      "I believe I've found the problem. This was a combination of the dataset itself (this problem didn't occur with all data sets) and the way in which I scaled the data. My original scaling method, which processed results between 0 and 1, was not helping the situation, and caused the poor results seen:I've found another scaling method, provided by the sklearn preprocessing package: This scaling method is not between 0 and 1, and I have further investigation to determine why it has helped so much, but results are now coming back with an accuracy of 96 to 100%. Very on-par with the MATLAB results, which I figure is using a similar (or same) preprocessing scaling method. As I said above, this isn't the case with all datasets. Using the built in sklearn iris or digit datasets seemed to produce good results without scaling.\n",
      "MATLAB\n",
      " \n",
      "In MATLAB, and so I presume in Octave as well,  is not the same as . This is your source of error.\n",
      "MATLAB\n",
      " \n",
      "There are different ways of extracting clusters from a dendrogram. You are not required to do a single cut (although matlab may only offer this choice). Selecting regions like you did is also reasonable, and so is cutting the dendrogram at multiple heights. But not every tools has all the capabilities.Notice that c3 was split into two, half of which is not well separated from c2.\n",
      "matlab\n",
      " \n",
      "I'm not a Matlab user, but your code makes me think you have a standard shallow autoencoder. You can't really approximate a nonlinearity using a single autoencoder, because it won't be much more optimal than a purely linear PCA reconstruction (I can provide a more elaborate mathematical reasoning if you need it, though this is not math.stackexchange). You need to build a deep network to approximate your nonlinearity with several layers of linear transformations. Then, autoencoder is a bad model to choose (hardly anyone uses them in practice today), when you have denoising autoencoders, that tend to learn more important representations by trying to reconstruct a prior from its noisy version. Try building a deep denoising autoencoder.  introduces the concept of denoising autoencoders. That course has a video about deep denoising autoencoders as well.\n",
      "Matlab\n",
      " \n",
      "It's a lot simpler to just use MATLAB's  function than to do it manually using . Since you are just asking how to get the test \"score\" from cross-validation, as opposed to using it to choose an optimal parameter like for example the number of hidden nodes, your code will be as simple as this:All that remains is to write that function  which takes in input and output training and test sets (all provided to it by the  function so you don't need to worry about splitting your data yourself), trains a neural net on the training set, tests it on the test set and then output a score using your preferred metric. So something like this:I don't have the neural network toolbox so I am unable to test this I'm afraid. But it should demonstrate the principle. \n",
      "MATLAB\n",
      " \n",
      "Short answer: No Matlab does not support it (at least not that i'm aware of). Therefore you need to create a whole new model every time you get new input data. Depending on the size of the task this might still be the best choice.Workaround: You can implement it yourself, by creating a loss function which updates every time. Take a look at this paper if you decide to go this way (it about many kinds of loss function but you are interested in the logistic one):Or you could go Bayesan and update your priors any time a new point comes in. \n",
      "Matlab\n",
      " \n",
      "First of all show all data, because we can't reproduce your mistake.I suggest, you take this error because of this: or  is a vector and then we can't simply multiply it or minus it with .But the answer for you:just use build-in Matlab functions  and . This let you use it in this way:avoiding using secondary variables like .I checked your code and found mistake: is an 100x2 array. And using  on it returns 1x2 array! You can read about this in help:So your next action: returns 1x2 array and matlab can't make this: .Answer is easy:use  twice like this:  \n",
      "Matlab\n",
      " \n",
      "Not a mathematical answer but Matlab provides the pdf evaluations using the 'pdf' method.y = pdf(obj,X)where obj is the gmdistribution object.\n",
      "Matlab\n",
      " \n",
      "It depends on the optimization algorithm you use. The backpropagation by itself calculates only the gradient, which is used by the next iteration of the algorithm. In the simplest case you can use a self-developed gradient descent and check the behavior of your cost function. If the cost function decreases less than some threshold , you might break the optimization loop for the current instance. You can also limit the maximum number of iterations. It is worth  using some advanced optimizers such  in Matlab, which will stop by themselves when reached an optimum. You may find  post about different termination conditions of gradient descent very useful.I think, learning only using one single instance is not really efficient. The cost function can behave jerky. You may consider the batch learning method, where you learn using small batches of new instances. It should provide a better learning rate.In order to illustrate how network's accuracy depends on the iteration number and on the batch size, I experimented a bit with a neural network used to recognize hand written digits. I had 4000 examples in the training set  and 1000 examples in the validation set. Then I started the learning algorithm with different parameters and measured the resulted accuracy. You can see the result here:Of course this plot describes only my particular case, but you can get some intuition on what to expect and on how to validate network parameters.\n",
      "Matlab\n",
      " \n",
      "Try using MatLab's  to break up the edge into sections\n",
      "MatLab\n",
      " \n",
      "In MATLAB, use instead of As per the :\n",
      "MATLAB\n",
      " \n",
      "I guess what you want to do is similar to cross validation, in Matlab you can use the function  that allow you to split your dataset.I added the example shown on the page for splitting your data into 10 bins (called 10-fold cross validation).You should do this operation after doing the pre-processing of your data (normalisation / standardisation).\n",
      "Matlab\n",
      " \n",
      "In Matlab you can create N-dimensional matrices, so you can arrange your 3D data in a N*M*3 matrix (you might want to look for the  function to help you out).There are several functions that allow you to plot in 3D, one of these is  which is perfect for K-Means clustering. I don't really understand which lines you do want to plot: K-Means is about clusters and centroids (i.e. points).  If a 4th variable is involved, you can as well create a 4D matrix. Although I reckon plotting a 4D graph isn't going to be easy. A first approach might be using several colours for your scatter points with different colours for different temperatures (or temperatures range). In this case the 5th input argument for  will be helpful.Help for  .Help for  .\n",
      "Matlab\n",
      " \n",
      "No, MATLAB has a  which is not LIBSVM, but does the same thing.Note that it is not a free package which comes with MATLAB. You can check whether you have it installed by calling , which on my PC results in:where you can see I have the Statistics and machine learning toolbox installed.\n",
      "MATLAB\n",
      " \n",
      "MATLAB could come with  a statistics and machine-learning toolbox if you pay for it, or choose it when using Student Suite.The main difference with the toolbox and LIBSVM it's that LIBSVM supports multi-class classification, which is a great advantage, especially when you have 4 or more clases.If you need to classify between 3 or less clases, you could use the toolbox and 1vs1 aproach, instead of installing LIBSVM and overwriting the toolbox.UPDATEAs is stated in the comments 2015 version of MATLAB supports multi-class classification. So the most important difference is that LIBSVM is free.\n",
      "MATLAB\n",
      " \n",
      "As an alternative to , you can also use  that, in the recent versions of Matlab I've used, permit direct indexing of the referenced field:The sub-expression  resolves to the data in the field  of the  struct and  then indexes that data.\n",
      "Matlab\n",
      " \n",
      "There are several issues in your code:if your filenames are  (e.g. they start with a lowercase \"o\") than you must change your first line of code to . Matlab is case-sensitive. If your file names start with a lowercase \"o\" then Matlab will never find files whose name start with \"Output\" (capital \"O\"). This will maybe solve the error in . The same file name problem will affect the , so also in  you must have the lowercase \"o\".the RMSE formula is incorrect: you do not square both items and then subtract them, you must do the other way round; that is, subtraction first and then raise to the power. The correct formula is the variables ,  and  are scalars, so it is pointless to do the  operator, which is used to unwrap variables as column vectors is declared as a 24x3 matrix but then with the line  you make  a 3x1 vector.in the final for loop there are a couple of issues: the first argument in  (e.g. ) must be a string, not a variable (unless such variable contains the file name as a string). And the second argument must be replaced with . and  are both column vectors, so it is pointless to do the  operator. See above.Since your main concern is regarding the saving-to-disk problem, I'll concentrate on that. My suggestion is to build  as a matrix and then save such matrix outside the for-loop, once all of your files have been scanned and processed. As your code is now, it is not clear whether you want  to be a matrix and save to disk all at once or you want  to be a vector and save to disk in a file-by-file fashion.  the first thing to do then is to move  all the way up, before the for-loop starts. If you also want to append the file ID, then  should actually be a 24x4 matrix.change  to . In this manner you write  on the k-th row and 1st column , then you write  on the k-th row and 2nd column and same story goes for . As  goes, this will fill the matrix . If you also want to append the file ID, keep in mind that  is a string and you cannot concatenate strings and numbers into the same matrix. The immediate option is to convert  from string to numeric thanks to  but keep in mind that such conversion will remove the leading zeros (Matlab does not allow leading zeros) so the file ID  will be , the file  will be  and so on. So you might want to add  and this will also fill the 4th column in .remove the inner for-loop, the one that contains . Instead of writing a single line for every single file, we'll write the whole  matrix in a batch mode.after the main for-loop, put  as follows:  and if everything went ok,  should contain your  matrix as a 24x3 matrix. You might as well change  with the file name you prefer.\n",
      "Matlab\n",
      " \n",
      "One of the best local search algorithms for low number of dimensions (up to about 10 or so) is the . By the way, it is used as the default optimizer in MATLAB's  function. I personally used this method for finding parameters of some textbook 2nd or 3rd degree dynamic system (though very simple one).Other option are the already mentioned evolutionary strategies. Currently the best one is the Covariance Matrix Adaption ES, or . There are variations to this algorithm, e.g. BI-POP CMA-ES etc. that are probably better than the vanilla version.You just have to try what works best for you.\n",
      "MATLAB\n",
      " \n",
      "The main benefit of GLM over logistic regression is overfitting avoidance. GLM usually try to extract linearity between input variables and then avoid overfitting of your model. Overfitting means very good performance on training data and poor performance on test data. \n",
      "GLM\n",
      " \n",
      "On a similar error, I forced the GLM predictions to have the same class as the dependent variable.For example, a GLM will predict a &quot;numeric&quot; class. But with the target variable being a &quot;factor&quot; class, I ran into an error.erroneous code:Result:corrected code:Result:\n",
      "GLM\n",
      " \n",
      "Majority of machine learning algorithms work with numbers, so you can to transform your categorical values and string into numbers.Popular python machine-learning library scikit-learn has the . With 'yes/no' everything is easy - just put 0/1 instead of it.Among many other important things it explains the process of  using their .When you work with text, you also have to transform your data in a suitable way. One of the common feature extraction strategy for text is a  score, and I wrote a .\n",
      "scikit-learn\n",
      " \n",
      "Q-Learning is a TD (temporal difference) learning method. I think you are trying to refer to TD(0) vs Q-learning. I would say it depends on your actions being deterministic or not. Even if you have the transition function, it can be expensive to decide which action to take in TD(0) as you need to calculate the expected value for each of the actions in each step. In Q-learning that would be summarized in the Q-value.\n",
      "Q-Learning\n",
      " \n",
      "Using Text Classification...To be able to treat this as a classification problem, you need a training set, which is a set of AngelList entries that are labeled with correct LinkedIn categories. This can be done manually, or you can hire some  to do the job for you.Since you have ~150 categories, I'd imagine you need at least 20-30* AngelList entries for each of them. So your training set will be  {input: angellist_description, result: linkedin_id}After that, you need to dig through text classification techniques to try and optimize the accuracy/precision of your results. The book \"Taming Text\" has a full chapter on text classification. And a good tool to implement a text-based classifier would be Apache Solr or Apache Lucene.* 20-30 is a quick personal estimate and not based on a scientific method. You can look up some methods online for a good estimation method.\n",
      "Text\n",
      " \n",
      "Yes and No.There is no directly implemented convnet library bundled into OpenCV, however Caffe (one of the leading convolutional neural network packages) interacts with it rather well.If you install Caffe, one of its requisites is OpenCV, and you can then use OpenCV through Caffe's C or Python API's. See .If you install OpenCV (a recent enough version) you can use the new opencv_dnn module to interact with Caffe. See .\n",
      "Caffe\n",
      " \n",
      "It is a common practice to decrease the learning rate (lr) as the optimization/learning process progresses. However, it is not clear how exactly the learning rate should be decreased as a function of the iteration number.If you use  as an interface to Caffe, you will be able to visually see how the different choices affect the learning rate.fixed: the learning rate is kept fixed throughout the learning process.inv: the learning rate is decaying as ~step: the learning rate is piecewise constant, dropping every X iterations multistep: piecewise constant at arbitrary intervalsYou can see exactly how the learning rate is computed in the function  (solvers/sgd_solver.cpp line ~30).Recently, I came across an interesting and unconventional approach to learning-rate tuning: . In his report, Leslie suggests to use  that alternates between decreasing and increasing the learning rate. His work also suggests how to implement this policy in Caffe.\n",
      "Caffe\n",
      " \n",
      "Recently, input transformation (scaling/cropping etc.) was separated from the IMAGE_DATA layer into a separate object: data transformer. This change affected the protobuffer syntax and the syntax of the IMAGE_DATA layer.It appears as if your  is in the old format and Caffe converts it for you to the new format. You can do this conversion manually yourself using  (for prototxt files) and  (for binaryproto files).  \n",
      "Caffe\n",
      " \n",
      "To train a network which predicts values of several variables at the same time you just need to setup your network to have multiple output neurons and feed it with the training data just the same way you do know but with multiple target values at the same time. I haven't used pylearn ever - I prefer Caffe, nolearn(lasagne) or pybrain, each of these libraries are able to easily handle such cases.Example of pybrain implementation (code was used in kaggle's BikeShare challenge):\n",
      "Caffe\n",
      " \n",
      "Caffe is a tool for training and evaluating deep neural networks. It is quite a versatile tool allowing for both deep convolutional nets as well as other architectures.Of course it can be used to process pre-computed image features. \n",
      "Caffe\n",
      " \n",
      "Caffe is trained using : that is, at each iteration it computes the (stochastic) gradient of the parameters w.r.t the training data and makes a move (=change the parameters) in the direction of the gradient.Now, if you write the equations of the gradient w.r.t. training data you'll notice that in order to compute the gradient exactly you need to evaluate all your training data at each iteration: this is prohibitively time consuming, especially when the training data gets bigger and bigger.In order to overcome this, SGD approximates the exact gradient, in a stochastic manner, by sampling only a small portion of the training data at each iteration. This small portion is the batch.Thus, the larger the batch size the more accurate the gradient estimate at each iteration.TL;DR: batch size affect the accuracy of the estimated gradient at each iteration, changing the batch size therefore affect the \"path\" the optimization takes and may change the results of the training process.Update:In ICLR 2018 conference an interesting work was presented:Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, Quoc V. Le .This work basically relates the effect of changing batch size and learning rate.\n",
      "Caffe\n",
      " \n",
      "It uses the chain rule to propagate the gradient backwards through the local response normalization layer. It is somewhat similar to a nonlinearity layer in this sense (which also doesn't have trainable parameters on its own, but does affect gradients going backwards).From the code in Caffe that you linked to I see that they take the error in each neuron as a parameter, and compute the error for the previous layer by doing following:First, on the forward pass they cache a so-called scale, that is computed (in terms of AlexNet paper, see the formula from section 3.3) as:Here and below  is sum indexed by  and goes from  to (note that in the paper they do not normalize by , so I assume this is something that Caffe does differently than AlexNet). Forward pass is then computed as .To backward propagate the error, let's say that the error coming from the next layer is , and the error that we need to compute is . Then  is computed as:Since you are planning to implement it manually, I will also share two tricks that Caffe uses in their code that makes the implementation simpler:When you compute the addends for the sum, allocate an array of size , and pad it with  zeros on each end. This way you can compute the sum from  to , without caring about going below zero and beyond .You don't need to recompute the  on each iteration, instead compute the the addends in advance ( for the front pass,  for the backward pass), then compute the  for , and then for each consecutive  just add  and subtract , it will give you the value of the sum for the new value of  in constant time.\n",
      "Caffe\n",
      " \n",
      "Caffe's documentation is somewhat scant on details. What I was finally told is this counterintuitive solution:In your solver.prototxt, take the lines for  and and simply omit them. If you'd like to prevent the test at the beginning, you would add a line as :\n",
      "Caffe\n",
      " \n",
      "Caffe offers a  layer that can save to hdf5 binary file the outputs of certain layers.You can see the  for more information.\n",
      "Caffe\n",
      " \n",
      "@Saman - I would suggest you to use the original version of Caffe which is designed for Linux . As they have mentioned in their website, the Windows version is unofficial. Hope it helps. \n",
      "Caffe\n",
      " \n",
      "The above code expects images from a given path, and the labels of each image as .npy file.Credits: Note: I had seen 's  to a question, which claims that lmdb doesnot support float-type data. But, it does work for me with the latest version of Caffe and LMDB and using this code snippet. As his answer is way too old, its highly likely that older versions may not have supported float-type data.\n",
      "Caffe\n",
      " \n",
      "The output shape of your network depends on the input : if you define  than your net processes a single example each time, thus it only reads a single . If you change  to 2, caffe will read two samples and consequently the shape of  will become 2.One exception to this \"shape rule\" is the loss output: the loss defines a scalar function with respect to which gradients are computed. Thus, the loss output will always be a scalar regardless of the input shape.Regarding the data type of : Caffe stores all variables in \"Blobs\" of type .\n",
      "Caffe\n",
      " \n",
      "At least no support for multi cups by default.And I'll try Caffe, it's more popular and powerful.\n",
      "Caffe\n",
      " \n",
      "Caffe, like many other deep-learning frameworks, trains its models using stochastic gradient decent (SGD), implemented as gradient back propagation. That is, for a mini-batch of training examples, caffe feed the batch through the net (\"forward pass\") to compute the loss w.r.t the net's parameters. Then, it propagates the loss gradient back (\"backward pass\") to update all the parameters according to the estimated gradient.By \"bookkeeping\" the tutorial means, you do not need to worry about estimating the gradients and updating the parameters. Once you are using existing layers (e.g., , ,  etc.) you only need to define the graph structure (the net's architecture) and supply the training data, and caffe will take care of the rest of the training process: It will forward/backward each mini batch, compute the loss, estimate the gradients and update the parameters - all for you.Pretty awesome, don't you think? ;)\n",
      "Caffe\n",
      " \n",
      "As was pointed out in the comments, Caffe expects that the labels are integers between 0 and . In your case, as you set number of labels to 5, Caffe will create five output neurons in the last layer. When you ask it to predict class 6 or 8, you are asking it to maximize the output of a non-existent neuron, which Caffe obviously cannot do.Now, when you relabel your data, and set  to 5, you do the correct thing, and it, therefore, works. When you set  to 10, the network will still work, because now it has 10 output neurons, which is enough to be able to classify five classes. It will learn that classes from 5 to 9 are never present, and therefore should never be predicted, and it will just adjust the weights in a way that will always result in very small values returned by those output neurons. It is important to note, however, that neural networks are naturally random, so it might still occasionally return a class that was never presented to it, so I would expect a NN with  larger than the actual number of classes to perform worse than the one with the correct .\n",
      "Caffe\n",
      " \n",
      "You can change the  in the solver parameters.Caffe accumulates gradients over  x  instances in each stochastic gradient descent step.So increasing  can also get more stable gradient when you cannot use large batch_size due to the limited memory.\n",
      "Caffe\n",
      " \n",
      "Linear SVM is a special case of general soft margin kernel SVM in which the model can be expressed as a single weight vector w and a bias b, so that classification is done by y = w'*x+b (this is the decision value; you can threshold it with zero, as you did, or choose another value to tradeoff precision/recall). In the general case, the model is described by its support vectors and their weights, so its size depends on the number of SVs that were found, and may be quite large.Some software libraries, like libsvm, do not have specialized code to deal with linear SVM classification, so the model representation and classification function are inefficient in terms of memory and running time. You can, however, easily convert the model representation to the above linear classifier, by calculated the weighted sum of all the SVs, using the stored SV weight. You can then write your own linear classification function, using whatever means you want to make it run fast (e.g. vectorized code)\n",
      "Linear\n",
      " \n",
      "Linear Regression is SGD based and requires tweaking the step size, see  for more details.In your example, if you set the step size to 0.1 you get better results (MSE = 0.5).For another example on a more realistic dataset, see \n",
      "Linear\n",
      " \n",
      "\"Linear\" in linear regression refers to the fact that we use a line to fit our data. Period.\n",
      "Linear\n",
      " \n",
      "You don't say why you want Linear SVM, but if you can consider another model that often gives superior results then check out the hpelm python package. It can read an HDF5 file directly. You can find it here  It trains on segmented data, that can even be pre-loaded (called async) to speed up reading from slow hard disks.\n",
      "Linear\n",
      " \n",
      "First off there are two issues here, one you need to scale your outputs between 0 and 255. You can do this with some transformation afterwards. By taking the max and min value, then transposing between 0 and 255.On the other hand this network will likely not learn what you'd like it to, your hidden layers are using Linear Layers. This is not very useful, as the weights themselves form a linear transformation. You'll essentially end up with a linear function. I would recommend using a SigmoidLayer for your hidden layers, this of course squashes the values between 0 and 1. You can correct this in the output layer by multiplying by 255. Either via a fixed layer or just transforming the values afterwards.\n",
      "Linear\n",
      " \n",
      "I have do some research and i find this MIn the original viola-Jones paper here, section 3.1 Learning Discussion (para 4, to be precise) you will find out the procedure to find optimal threshold.I'll sum up the method quickly below.Optimal threshold for each feature is sample-weight dependent and therefore calculated in very iteration of adaboost. The best weak classifier's threshold is saved as mentioned in the pseudo code.In every round, for each weak classifier, you have to arrange the N training samples according to the feature value. Putting a threshold will separate this sequence in 2 parts. Both parts will have either positive or negative samples in majority along with a few samples of other type.T+ : total sum of positive sample weightsT- : total sum of negative sample weightsS+ : sum of positive sample weights below the thresholdS- : sum of negative sample weights below the thresholdError for this particular threshold is -e = MIN((S+) + (T-) - (S-), (S-) + (T+) - (S+))You calculate this error for all N possible ways of separating the samples.The minimum error will give you the range of threshold values. Questions:1. I haven't understand this '' The minimum error will give you the range of threshold values. ''2. if i find the threshold how i can detrmine the parity of the weak classifier using min error\n",
      "viola-Jones\n",
      " \n",
      "That's a tough questions. Generally this kinds of machine predict based on Data Mining result form previous data. These data can be so huge that human won't be able to find any patter on that data. In that case machine is far batter than human as they will implement its algorithm and  predict intelligently. But if you ask that do the machines know what is human's choice as human himself is unpredictable :) Yes there are so many researcher working on data mining, autonomous agent or human-agent teamwork. We can see some of by searching on these topics fat Google school-er. Best of luck  \n",
      "Data\n",
      " \n",
      "You can just configure that by using the modules under Data Format Conversions. Have a look  and . Documentation is in progress, unluckily. \n",
      "Data\n",
      " \n",
      "You can read  about Data sources in Power BI.You can connect your Power Bi to: Azure Blob, Azure SQL server, Azure SQL Data Warehouse, Azure Table Storage, or Azure HDInsight.All these PBI sources are also available as outputs to the Writer module in Azure ML. so you can use them to write your results from Azure ML and later read them as input for Power BI\n",
      "Data\n",
      " \n",
      "Q: How should we deal with unreliable data in data scienceA: Use feature engineering to fix unreliable data (make some transformations on unreliable data to make it reliable) or drop them out completely - bad features could significantly decrease the quality of the modelQ: Is there any way to figure out these misstatements and then report the top 10% rich people with better accuracy using Machine Learning algorithms?A: ML algorithms are not magic sticks, they can't figure out anything unless you tell them what you are looking for. Can you describe what means 'unreliable'? If yes, you can, as I mentioned, use feature engineering or write a code which will fix the data. Otherwise no ML algorithm will be able to help you, without the description of what exactly you want to achieveQ: Is there any idea or application in Machine Learning which tries to improve the quality of collected data?A: I don't think so just because the question itself is too open-ended. What means 'the quality of the data'?Generally, here are couple of things for you to consider:1) Spend some time on googling feature engineering guides. They cover how to prepare your data for you ML algorithms, refine it, fix it. Good data with good features dramatically increase the results.2) You don't need to use all of features from original data. Some of features of original dataset are meaningless and you don't need to use them. Try to run gradient boosting machine or random forest classifier from scikit-learn on your dataset to perform classification (or regression, if you do regression). These algorithms also evaluate importance of each feature of original dataset. Part of your features will have extremely low importance for classification, so you may wish to drop them out completely or try to combine unimportant features together somehow to produce something more important.\n",
      "data scienceA\n",
      " \n",
      "This research field is called \"Data Matching\" or \"Record Linkage\". There is a  He also goes deep down into machine learning models and how to improve them from the basic approach like simple string distances (as other answers already suggested).To give you a head start, you can try to compute character n-grams of your titles.For n = 3 and Hugo Boss, you would get Now you can compute the  between two sets of these ngrams. Here, for example between  and :If you don't want to go down the route of implementing all of these things yourself, use . It's also very fast and scales well to billions of documents. \n",
      "Data\n",
      " \n",
      "2>Don't expect to find an algorithm that exactly does what you need.Customize algorithms as adequate for your problem. That is the very story of the Data Science buzz, the need to experiment and customize instead of hoping for a turnkey solution.You have avery specific idea of what you need. You will have to put this idea into code and plug it into some algorithm. For example, consider complete linkage clustering with maximum norm. It probably is what you explained above, but I don't think it will be useful.\n",
      "Data\n",
      " \n",
      "If there are not so many blobs, you could just add multiple readers with each map to one of your input blobs. Then use modules under \"Data Transformation\" -> \"Manipulation\" to do things like \"Add Rows\" or \"Join\".\n",
      "Data\n",
      " \n",
      "A good recap can be found , section 1 on Data Augmentation: so namely flips, random crops and color jittering and also lighting noise: proposed fancy PCA when training the famous Alex-Net in 2012. Fancy PCA alters the intensities of the RGB channels in training images.Alternatively you can also have a look at the Kaggle Galaxy Zoo challenge: the winners wrote a . It covers the same kind of techniques:rotation,translation,zoom,flips,color perturbation.As stated they also do it \"in realtime, i.e. during training\".For example here is a practical   by Facebook (for  training).\n",
      "Data\n",
      " \n",
      "When you say \"normalize\" labels, it is not clear what you mean (i.e. whether you mean this in a statistical sense or something else). Can you please provide an example? On Making labels uniform in data analysisIf you are trying to neaten labels for use with the  function, you could try the  function to shorten them, or the  function to align them better. The  function works well for rounding labels on plot axes. For instance, the base function  for drawing histograms calls on Sturges or other algorithms and then uses  to choose nice bin sizes.The  function will standardize values by subtracting their mean and dividing by the standard deviation, which in some circles is referred to as normalization. On the reasons for scaling in regression (in response to comment by questor). Suppose you regress Y on covariates X1, X2, ... The reasons for scaling covariates Xk depend on the context. It can enable comparison of the coefficients (effect sizes) of each covariate. It can help ensure numerical accuracy (these days not usually an issue unless covariates on hugely different scales and/or data is big). For a readable intro see . For a mathematically intense discussion see .In particular, in Bayesian regression, rescaling is advisable to ensure convergence of MCMC estimation; e.g. see . \n",
      "data analysisIf\n",
      " \n",
      "There are a couple of things that make this code run slowly. is essentially just syntactic sugar for a  loop over the rows of a column. There's also an explicit  loop over a NumPy array in your function (the  part). Looping in this (non-vectorised) way is best avoided whenever possible as it impacts performance heavily. is looking up  across an entire column for each row of , then returning a sub-DataFrame and then creating a new NumPy array. Repeatedly looking for values in this way is expensive ( complexity each lookup). Creating new arrays is going to be expensive since memory has to be allocated and the data copied across each time.If I understand what you're trying to do, you could try the following approach to get your new column.First use  to group the rows of  by the values in 'session'.  is used to join up the strings for each value:where  is the column from  which contains the values you want to join together into a string.  means that only one pass through the DataFrame is needed and is pretty well-optimised in Pandas. The use of  to join the strings is unavoidable here, but only one pass through the grouped values is needed. is now a Series of strings indexed by the unique values in the 'session' column. This is useful because lookups to Pandas indexes are  in complexity.To match each string in  to the approach value in  you can use . Unlike ,  is fully vectorised and should be very fast:\n",
      "NumPy\n",
      " \n",
      "Not sure if this is your error at all but I got this when I was using regular arrays when I should have been using Numpy.\n",
      "Numpy\n",
      " \n",
      "Matplotlib is a very good library for that task. You can plot histograms, scatter plots and lot of other things. You just have to know what you want and then you can probably do it with that. I use that for similar tasks a lot.[UPDATE]As I said, you can do that with matplotlib. Here is an example from their gallery: It's not so pretty as with the answer in the comment of @lejlot, but still correct.\n",
      "Matplotlib\n",
      " \n",
      "Goal was to achieve similar results using Numpy and Tensorflow. The only change from original answer is  parameter for  api.Initial approach :  - This however does not provide intended results when dimensions are N.Modified approach:  - Always sum on the last dimension. This provides similar results as tensorflow's softmax function.\n",
      "Numpy\n",
      " \n",
      "It's possible to do what you've started in Numpy, but I think it's too low-level for this sort of stuff. I'd suggest you install . Then, you can do the following\n",
      "Numpy\n",
      " \n",
      "If you read the error message you can see that passing single dimensional arrays will soon not be supported. Instead you have to ensure that your single sample looks like a list of samples, in which there is just one. When dealing with NumPy arrays (which is recommended), you can use  however as you're using lists then the following will do: \n",
      "NumPy\n",
      " \n",
      "Supervised learning is when you have labeled training data. In other words, you have a well-defined target to optimize your method for.Typical (supervised) learning tasks are classification and regression: learning to predict categorial (classification), numerical (regression) values or ranks (learning to rank).Unsupservised learning is an odd term. Because most of the time, the methods aren't \"learning\" anything. Because what would they learn from? You don't have training data?There are plenty of unsupervised methods that don't fit the \"learning\" paradigm well. This includes dimensionality reduction methods such as PCA (which by far predates any \"machine learning\" - PCA was proposed in 1901, long before the computer!). Many of these are just data-driven statistics (as opposed to parameterized statistics). This includes most cluster analysis methods, outlier detection, ... for understanding these, it's better to step out of the \"learning\" mindset. Many people have trouble understanding these approaches, because they always think in the \"minimize objective function f\" mindset common in learning.Consider for example DBSCAN. One of the most popular clustering algorithms. It does not fit the learning paradigm well. It can nicely be interpreted as a graph-theoretic construct: (density-) connected components. But it doesn't optimize any objective function. It computes the transitive closure of a relation; but there is no function maximized or minimized.Similarly APRIORI finds frequent itemsets; combinations of items that occur more than minsupp times, where minsupp is a user parameter. It's an extremely simple definition; but the search space can be painfully large when you have large data. The brute-force approach just doesn't finish in acceptable time. So APRIORI uses a clever search strategy to avoid unnecessary hard disk accesses, computations, and memory. But there is no \"worse\" or \"better\" result as in learning. Either the result is correct (complete) or not - nothing to optimize on the result (only on the algorithm runtime).Calling these methods \"unsupervised learning\" is squeezing them into a mindset that they don't belong into. They are not \"learning\" anything. Neither optimizes a function, or uses labels, or uses any kind of feedback. They just SELECT a certain set of objects from the database: APRIORI selects columns that frequently have a 1 at the same time; DBSCAN select connected components in a density graph. Either the result is correct, or not.Some (but by far not all) unsupervised methods can be formalized as an optimization problem. At which point they become similar to popular supervised learning approaches. For example k-means is a minimization problem. PCA is a minimization problem, too - closely related to linear regression, actually. But it is the other way around. Many machine learning tasks are transformed into an optimization problem; and can be solved with general purpose statistical tools, which just happen to be highly popular in machine learning (e.g. linear programming). All the \"learning\" part is then wrapped into the way the data is transformed prior to feeding it into the optimizer. And in some cases, like for PCA, a non-iterative way to compute the optimum solution was found (in 1901). So in these cases, you don't need the usual optimization hammer at all.\n",
      "PCA\n",
      " \n",
      "PCA extracts the most important information from the data set and compresses the size of the data set by keeping only the important information - principal components. The first principal component is constructed in such a way that it has the largest possible variance. The second component is computed under the constraint of being orthogonal to the first component and to have the largest possible variance.In your case the data is a set of images. Let's say you have 1000 images and you compute the first five principal components (5 images, constructed by the PCA algorithm). You may represent any image as 900 data points (30x30 pixels) or by the combination of 5 images with the corresponding miltiplication coefficients. The goal of the PCA algorithm is to construct these 5 images (principal componenets) in such a way that images in your data set are represented most accurately with the combination of the given number of principal components.UPDATE:Consider the image below (from the amazing book by ). The image shows how points in 2 dimensions (red dots) are represented in 1 dimension (green crosses) by projecting them to the vector (purple line). The vector is the first principal component. The purpose of PCA is to build these vectors to minimize the reconstruction error. In your case these vectors can be represented as images.You may refer to  article for more details on using PCA for handwritten digit recognition.\n",
      "PCA\n",
      " \n",
      "First a bit of nomenclature: PCA finds the eigenvectors of the data covariance matrix, and then transforms the data into the basis of eigenvectors. The result of this operation has two parts: The transformed data, and the eigenvectors used for transformation. Usually it is the first that is called principal components (PCs). There is no established name for the second, but I like the term principal modes (PMs). Your question is about the interpretation of the PMs.The interpretation of PMs is generally hard, if not impossible. Of course they have a simple technical interpretation, PCA looks for directions in the data space along which a maximum amount of variation occurs, and the PMs are those directions. For the first component this maximization is free, and captures the main direction along which variation occurs. Later components are constrained to be orthogonal to the ones before, which often leads to more and more complex, high-frequency patterns which are harder and harder to interpret.In the case of your data set, the situation may be different, because it probably has some kind of cluster structure in a very high-dimensional space, where the clusters correspond to the digits 09. It has  that in such a case there is a weak correspondence between PCA and k-means clustering, such that the first PMs tend to recover the space spanned by the cluster centroids. In this case the first PMs will be mixtures of cluster centroid patterns, possibly even approximately coinciding with these patterns. I think this is what explains your observation.More information in response to the OP's comments:The Wikipedia snippet cited above refers to Ding and He (2004), K-means Clustering via Principal Component Analysis . They write in the abstract: \"Here we prove that principal components are the continuous solutions to the discrete cluster membership indicators for K-means clustering.\" To my understanding this means that the \"component load\", the value of the principal component for a given data point is or at least is related to an indicator of whether that data point belongs to a cluster. They continue, \"Equivalently, we show that the subspace spanned by the cluster centroids are given by spectral expansion of the data covariance matrix truncated at K  1 terms.\" This means that the subspace of the data space (in your case 900-dimensional) that is spanned by the first K  1 principal modes (eigenvectors) is or is close to the space spanned by the (differences between) the cluster centroids (the mean image for each digit). If this is the case, most of the between-cluster variance is captured by those first principal components.Think of it this way: The PCA allows to reduce the 900 dimensions of the data to about 10 dimensions, by reconstructing all 30x30 images from a set of 10 \"typical\" 30x30 images. That means, each image can be approximately encoded by 10 numbers instead of 900 numbers. In order for this to be possible, the \"typical\" images have to be similar to how a \"0\", a \"1\", etc. looks like on average. In the simplest case, the 10 images could just be the average \"0\", average \"1\" etc. itself. That is not normally the case, but it can be approximately the case. Does this help? That \"0\" corresponds to the strongest PC is just coincidence I think.\n",
      "PCA\n",
      " \n",
      "It is unclear from your description what the 5 sets (what sound like labels) correspond to, but I will assume that you are essentially asking about feature learning: you would like to know which features to choose to best predict what set a given sequence is from. Determining this from scratch is an open problem in machine learning and there are many possible approaches depending on the particulars of your situation.You can select a set of features (just by making logical guesses) and calculate them for all sequences, then perform  on all the vectors you have generated. PCA will give you the linear combination of features that accounts for the most variability in your data which is useful in designing meaningful features.\n",
      "PCA\n",
      " \n",
      "Yes. In fact, there are infinitely many ways to reduce the dimension of the features. It's by no means clear, however, how they perform in practice.  A feature reduction usually is done via a principal component analysis (PCA) which involves a singular value decomposition. It finds the directions with highest variance -- that is, those direction in which \"something is going on\".In your case, a PCA might find the black line as one of the two principal components:The projection of your data onto this one-dimensional subspace than yields the reduced form of your data.Already with the eye one can see that on this line the three feature sets can be separated -- I coloured the three ranges accordingly. For your example, it is even possible to completely separate the data sets. A new data point then would be classified according to the range in which its projection onto the black line lies (or, more generally, the projection onto the principal component subspace) lies.Formally, one could obtain a division with further methods that use the PCA-reduced data as input, such as for example clustering methods or a K-nearest neighbour model.So, yes, in case of your example it could  be possible to make such a strong reduction from 2D to 1D, and, at the same time, even obtain a reasonable model.\n",
      "PCA\n",
      " \n",
      "Typical methods for identifying important features include PCA and ICA.  However, even more valuable than these methods is having an understanding of the underlying system your data is representing. \n",
      "PCA\n",
      " \n",
      "As other answers explain it,  does not need to be doing anything (except from returning the transformer object). It is there so that all transformers have the same interface and work nicely with stuff like pipelines.Of course some transformers need a  method (think tf-idf, PCA...) that actually does things.The  method needs to return the transformed data. is a convenience method that chains the fit and transform operations. You can get it for free (!) by deriving your custom transformer class from  and implementing  and .Hope this clarifies it a bit.\n",
      "PCA\n",
      " \n",
      "First of all 40 inputs is a very small space and it should not be reduced. Large number of inputs is 100,000, not 40. Also, 600x40 is not a big dataset, nor the one \"increasing the CPU time dramaticaly\", if it learns slowly than check your code because it appears to be the problem, not your data.Furthermore, feature selection is not a good way to go, you should use it only when gathering features is actually expensive. In any other scenario you are looking for dimensionality reduction, such as PCA, LDA etc. although as said before - your data should not be reduced, rather - you should consider getting more of it (new samples/new features).\n",
      "PCA\n",
      " \n",
      "1) I don't think that clustering eigenvectors (columns of PCA result) of covariance matrix makes any sense. All eigenvectors pairwise orthogonal and equally far one from another in sense of Euclidian distance. You can pick any eigenvectors and compute distance between them, distance will be sqrt(2) between any pair. But clustering rows of PCA result can provide something useful.\n",
      "PCA\n",
      " \n",
      "So you have an n x 200 feature matrix, where n is your number of samples, and 200 features per sample, and each feature is temporally related to all others? Or you have individual feature matrices, one for each time point, and you want to run PCA on each of these individual feature matrices to find a single eigenvector for that time point, and then concatenate those together?PCA seems more useful in the second case.While this is doable, this is maybe not the best way to go about it because you lose temporal sensitivity by collapsing together features from different times. Even if each feature in your final feature matrix represents a different time, most classifiers cannot learn about the fact that feature 2 follows feature 1 etc. So you lose the natural temporal ordering by doing this.If you care about the the temporal relationship between these features you may want to take a look at recurrent neural networks, which allow you feed information from t-1 into a node, at the same time as feeding in your current t features. So in a sense they learn about the relationship between t-1 and t features which will help you preserve temporal ordering. See this for an explanation: If you don't care about time and just want to group everything together, then yes PCA will help reduce your feature count. Ultimately it depends what type of information you think is more relevant to your problem. \n",
      "PCA\n",
      " \n",
      "You can read about caret in detail here: 1) Not exactly, you are creating PCA preprocessing model, which is stored in preProc now, it will combine all 57 different predictors into 2 predictors with some weights (every new feature is different linear combination of original features), keeping as much as possible of variance.2) And now you are applying transformation computed on previous step to your features, trainPC now contains only 2 features.3) Yes, at this point you are fitting \"glm\" Generalized Linear Model, which itself is maybe classification or regression task (not transformation of features as in previous step, but you still can use predict as in previous step to predict values).\n",
      "PCA\n",
      " \n",
      "PCA needs to compute a correlation matrix, which would be 100,000x100,000. If the data is stored in doubles, then that's 80 GB. I would be willing to bet your Macbook does not have 80 GB RAM.The PCA transformation matrix is likely to be nearly the same for a reasonably sized random subset.\n",
      "PCA\n",
      " \n",
      "Try to divide your data or load it by batches into script, and fit your PCA with  with it's partial_fit method on every batch.\n",
      "PCA\n",
      " \n",
      "Typically, dimensionality reduction tasks (such as PCA and FA) are performed in order to decide which features are the most significant.For example, in the case of PCA which is the most popular and easily employed dimensionality reduction task, significance is defined by largest variation of values.By performing PCA, you \"wash\" out variables that are insignificant yet can cause overfitting. I suggestyou familiarize yourself with topics such as PCA, FA and SVD. \n",
      "PCA\n",
      " \n",
      "Don't confuse PCA with dimensionality reduction.PCA is a rotation transformation that aligns the data with the axes in such a way that the first dimension has maximum variance, the second maximum variance among the remainder, etc. Rotations preserve pairwise distances.When you use PCA for dimensionality reduction, you discard dimensions of your rotated data that have the least variance. High variance is achieved when points are spread far from the mean. Low-variance dimensions are those, in which the values are mostly the same, so their absence is presumed to have the least effect on pairwise distances.\n",
      "PCA\n",
      " \n",
      "The PCA or any dimensionality reduction technique should be applied with the data that will be analyzed. If we wana evaluate the performance of the subset corresponding to less channels (eg 1:4), any dimensionality reduction technique should be applied in this data (PCA(data([1:4),:,:). Hence, Option 2 is the correct option. \n",
      "PCA\n",
      " \n",
      "PCA is a dimension reduction algorithm, as such it tries to reduce the number of features to principal components (PC) that each represents some linear combination of the total features. All of this is done in order to reduce the dimensions of the feature space, i.e. transform the large feature space to one that is more manageable but still retains most if not all of the information. Now for your problem, you are trying to explain the variance across your 400 observations using 153600 features, however, we don't need that much information 399 PC's will explain 100% of the variance across your sample (I will be very surprised if that is not the case). The reason for that is basicly overfitting, your algorithm finds noise that explain every observation in your sample. So what the rayryeng was telling you is correct, if you want to reduce your feature space to 10,000 PC's you will need 100,000 observations for the PC's to mean anything (that is a rule of thumb but a rather stable one).And the reason that matlab was giving you 399 PC's because it was able to correctly extract 399 linear combinations that explained some #% of the variance across your sample. If on the other hand what you are after are the most relevant features than you are not looking for dimensional reduction flows, but rather feature elimination processes. These will keep only the most relevant feature while nulling the irrelevant ones.  So just to make clear, if your feature space is rubbish and there isn't any information there just noise, the variance explained will be irrelevant and will indeed be less than 100% for example see the following   Again if you want to reduce your feature space there are ways to that even with a small m, but PCA is not one of them. Good Luck    \n",
      "PCA\n",
      " \n",
      "PCA doesn't give you the vectors in your dataset. From Wikipedia :Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.\n",
      "PCA\n",
      " \n",
      "Say you have a column vector V containing ONE flattened spectrogram. PCA will find a matrix M whose columns are orthogonal vectors (think of them as being at right angles to every other column in M).Multiplying M and T will give you a vector of \"scores\", which can be used to determine how much variance each column of M captures from the original data and each column of M captures progressively less variance in the data.Multiplying matrix M' (the first 2 columns of M) by V will produce a 2x1 vector T' representing the \"dimension-reduced spectrogram\". You could reconstruct an approximation of V by multiplying T' by the inverse of M'.  This would work if you had a matrix of spectrograms, too. Keeping only two principal components would produce an extremely lossy compression of your data.But what if you want to add a new song to your dataset? Unless it is very much like the original song (meaning it introduces little variance to the original data set), there's no reason to think that the vectors of M will describe the new song well. For that matter, even multiplying all the elements of V by a constant would render M useless. PCA is quite data specific. Which is why it's not used in image/audio compression. The good news? You can use a Discrete Cosine transform to compress your training data. Instead of lines, it finds cosines that form a descriptive basis, and doesn't suffer from the data specific limitation. DCT is used in jpeg, mp3 and other compression schemes.\n",
      "PCA\n",
      " \n",
      "See . You'll want to use  to do a PCA on a sparse matrix. For example:If your sparse matrix itself is too big to live in memory, you may be able to use the  method of  for online updates, though it only accepts dense input.\n",
      "PCA\n",
      " \n",
      "There is a fundamental difference between feature reduction (such as PCA) and feature selection (which you describe). The crucial difference is that feature reduction (PCA) maps your data to lower dimensional through some projection of all original dimensions, for example PCA uses linear combination of each. So final data embedding has information from all features. If you perform feature selection you discard information, you completely loose anything that was present there. Furthermore, PCA guarantees you retaining given fraction of the data variance.\n",
      "PCA\n",
      " \n",
      "I'm sorry if this is not exactly the answer, but why are you using PCA at all? You are reducing data from four to two dimensions, which is one-way operation: you won't get back all four parameters from two, and you may also slightly compromise distance estimations (therefore clustering).On the other hand, if you use k-means on raw data, cluster centers will be described by the same property list as original items.\n",
      "PCA\n",
      " \n",
      "There are two ways to do what you ask.You can get the nearest approximation of the centers in the original feature space using PCA's inverse transform:Or, you can recompute the mean in the original space using the original data and the cluster labels:Even though the resulting centers are not in the original dataset, you can treat them as if they are! For example, if you're clustering images, the resulting centers can be viewed as images to get insight into the clustering. Alternatively, you can do a nearest-neighbor search on these results to recover the original data point that most closely approximates the center.Keep in mind, though, that PCA is lossy and KMeans is fast, and so it's probably going to be more useful to run KMeans on the full, unprojected data:In this simple case, all three methods produce very similar results.\n",
      "PCA\n",
      " \n",
      "There is no best technique, period. It applies to all unsupervised learning, where there is no actual aim/criterion different than the one inside the method. This is why for classification you have better and worse methods, but you do not have better/worse clusterers and/or dimensionality reduction. You only have different ones, doing different things, thats all.Each method is best in what it does. PCA is best for linear reduction leading to highest preserved variance, because it is its definition, not because it is better than other doing the same - there are no others doing the same. I deliberately omit problems with non-converging methods, there you can obviously say that some optimization technique (algorithm) is better than the other. But it is important to make a distinction between method (such as PCA) and a particular solver/implementation (such as SVD, randomized PCA, etc.)Listing all dimensionality reduction techniques with their definitions (as this is what they are \"best at\") is beyond the scope of SO, in particular because there are dozens (hundreads) of them and you can easily find them by googling.\n",
      "PCA\n",
      " \n",
      "Just chiming in because I feel like the other answers answered the question of adding steps to a pipeline really well, but didn't really cover how to delete a step from a pipeline.Watch out with my approach though. Slicing lists in this instance is a bit weird. If you want to create a pipeline with just steps PCA/Polynomial you can just slice the list step by indexes and pass it to PipelineWant to just use steps 2/3?Watch out these slices don't always make the most amount of senseWant to just use steps 1/3? I can't seem to do using this approach\n",
      "steps PCA\n",
      " \n",
      "First of all, you can do whatever you want my friend. It's your model and you can use it in the way you want.However, I wouldn't do that. PCA is a way to reduce dimensionality, you lose data to train your model faster. PCA can be seen as if you reduce a sphere of 3 dimensions to a circle of 2 dimensions. In some situations it can be useful. With the output of PCA you can train your model and see what you get. However, have you tried to train your model without PCA? Maybe you don't need it and you are losing information that would improve your model\n",
      "PCA\n",
      " \n",
      "1- Do the vectorization with available verctorizers in sklearn which produces sparse matrices instead of a full matrix with massive zero values.2- Do the PCA only if needed3- For performance play with the parameters of your vectorizer and pca if needed.\n",
      "PCA\n",
      " \n",
      "You can't do PCA on this, since you have more features than your single observation. Get more observations, some 10,000 presumably, and you can do PCA.See  for the more detailed and mathematical explanation as to why this is the case.\n",
      "PCA\n",
      " \n",
      "PCA (Principal Component Analysis) is unsupervised or what is the same, it does not use class-label information. Therefore, discriminative information is not necessarily preserve.Minimizes the projection error.Maximizes the variance of projected points.Example: Reducing the number of features of a face (Face detection).LDA (Linear Discriminant Analysis): A PCA that takes class-labels into consideration, hence, it's supervised.Maximizes distance between classes.Minimizes distance within classes.Example: Separating faces into male and female clusters (Face recognition).With regard to the step by step process, you can easily find an implementation in Google.Regarding the classification:Project input x into PCA subspace U, and calculate its projection aProject a into LDA subspace VFind the class with the closest centerIn simple words, project the input x and then check from which cluster center is closer.\n",
      "PCA\n",
      " \n",
      "I don't know much about your dataset but it would be useful if you look your factor variables and try to decrease your factor levels. Is 194 level in Sub_Brand really necessary? How many of data are assigned to those certain levels?Also, since PCA is not a good approach on categorical data on most cases, you can try one-hot coding as recommended in . \n",
      "PCA\n",
      " \n",
      "Of course, it may be that your intuition that large amounts of data are not valuable may be incorrect: \".\"And if you add a weight / count for how many times a feature-set has identical data, you have increased your memory requirement by m * 32-bits (or whatever), so you might not be coming out ahead unless you have a lot of duplicates or a large feature-set.The suggestion to use PCA makes sense, because by reducing the size of each record, again you're getting m * (however much you've saved / record). I also thing the suggestion to use k-means is a good one, although I would probably use the centroid of the cluster as my exemplar (rather than a representative data point). If you go this route, I think you would definitely want to include a count/weight of how much data there is in that cluster. After all, the fact that data is duplicated is probably highly relevant in many models! \n",
      "PCA\n",
      " \n",
      "The short answer is: you cannot. Your data is 50 dimensional. You cannot plot 50 dimensions. The only thing you can do are some rough approximations, reductions and projections, but none of these can actually represent what is happening inside. In order to plot 2D/3D decision boundary your data has to be 2D/3D (2 or 3 features, which is exactly what is happening in the link provided - they only have 3 features so they can plot all of them). With 50 features you are left with statistical analysis, no actual visual inspection.You can obviously take a look at some slices (select 3 features, or main components of PCA projections). If you are not familiar with underlying linear algebra you can simply use  which does this for you. Simply train svm and plot it forcing \"pca\" visualization, like here: . which givesfor more examples you can refer to project website However this only shows points projetions and their classification - you cannot see the hyperplane, because it is highly dimensional object (in your case 49 dimensional) and in such projection this hyperplane would be ... whole screen. Exactly no pixel would be left \"outside\" (think about it in this terms - if you have 3D space and hyperplane inside, this will be 2D plane.. now if you try to plot it in 1D you will end up with the whole line \"filled\" with your hyperplane, because no matter where you place a line in 3D, projection of the 2D plane on this line will fill it up! The only other possibility is that the line is perpendicular and then projection is a single point; the same applies here - if you try to project 49 dimensional hyperplane onto 3D you will end up with the whole screen \"black\").\n",
      "PCA\n",
      " \n",
      "In Deep Neural Networks the depth refers to how deep the network is but in this context, the depth is used for visual recognition and it translates to the 3rd dimension of an image.In this case you have an image, and the size of this input is 32x32x3 which is . The neural network should be able to learn based on this parameters as depth translates to the different channels of the training images.UPDATE:In  each layer of your CNN it learns regularities about training images.  In the very first layers, the regularities are curves and edges, then when you go deeper along the layers you start learning higher levels of regularities such as colors, shapes, objects etc. This is the basic idea, but there lots of technical details. Before going any further give this a shot : UPDATE 2:Have a look at the first figure in the link you provided. It says 'In this example, the red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be 3 (Red, Green, Blue channels).' It means that a ConvNet neuron transforms the input image by arranging its neurons in three dimeonsion. As an answer to your question, depth corresponds to the different color channels of an image.Moreover, about the filter depth. The tutorial states this.Every filter is small spatially (along width and height), but extends through the full depth of the input volume.Which basically means that a filter is a smaller part of an image that moves around the depth of the image in order to learn the regularities in the image.UPDATE 3:For the real world example I just browsed the original paper and it says this : The first convolutional layer filters the 2242243 input image with 96 kernels of size 11113 with a stride of 4 pixels.In the tutorial it refers the depth as the channel, but in real world you can design whatever dimension you like. After all that is your designThe tutorial aims to give you a glimpse of how ConvNets work in theory, but if I design a ConvNet nobody can stop me proposing one with a different depth.Does this make any sense?\n",
      "Deep\n",
      " \n",
      "Majority of machine learning algorithms work with numbers, so you can to transform your categorical values and string into numbers.Popular python machine-learning library scikit-learn has the . With 'yes/no' everything is easy - just put 0/1 instead of it.Among many other important things it explains the process of  using their .When you work with text, you also have to transform your data in a suitable way. One of the common feature extraction strategy for text is a  score, and I wrote a .\n",
      "scikit-learn\n",
      " \n",
      "Yes, deeper neural network can have less parameters. It doesn't matter if they are CNNs. You might be confused, because in the graphical representation one tends to focus on the neurons. However, what gets learned are the weights, which are on the edges between neurons.Besides the link to \"Deep Residual Learning for image recognition\" (please upvote Midos answer for that), I would like to give a toy example of a multilayer perceptron (MLP).2>The first MLP has an input layer of 784 neurons, two hidden layers of 2000 neurons each and an output layer of 10 neurons (short: 784:2000:2000:10). This results in a network with  neurons. Now consider a network with the architecture 784:2000:50:2000:10. This has  neurons.This means adding another layer, even without reducing any of the layer, reduced the size of the network to 32% of the previous size!\n",
      "Learning\n",
      " \n",
      "Generally, the more data you have, the better. You will get diminishing returns at some point. It is often a good idea to see if your training set size is a problem by plotting the cross validation performance while varying the size of the training set. In scikit-learn has an example of this type of \".\"Scikit-learn Learning Curve ExampleYou may consider bringing in outside sample posts to increase the size of your training set.As you grow your training set, you may want to try reducing the bias of your classifier. This could be done by adding n-gram features, or switching to a logistic regression or SVM model.\n",
      "Learning\n",
      " \n",
      "Q-Learning is a TD (temporal difference) learning method. I think you are trying to refer to TD(0) vs Q-learning. I would say it depends on your actions being deterministic or not. Even if you have the transition function, it can be expensive to decide which action to take in TD(0) as you need to calculate the expected value for each of the actions in each step. In Q-learning that would be summarized in the Q-value.\n",
      "Q-Learning\n",
      " \n",
      "1>Pretty much every design choice in machine learning signifies some sort of inductive bias.  is an amazing  read, which I will be referring to throughout this answer.2>Concretely speaking, the very composition of layers  in deep learning provides a  type of relational inductive bias: hierarchical processing. The type of layer imposes further relational inductive biases:More generally, non-relational inductive biases used in deep learning include:activation non-linearities,weight decay,dropout,batch and layer normalization,data augmentation,training curricula,optimization algorithms,anything that imposes constraints on the learning trajectory.2>In a Bayesian model, inductive biases are typically expressed through the choice and parameterization of the prior distribution. Adding a Tikhonov regularization penalty to your loss function implies assuming that simpler hypotheses are more likely.1>The stronger the inductive bias, the better the sample efficiency--this can be understood in terms of the bias-variance tradeoff. Many modern deep learning methods follow an end-to-end design philosophy which emphasizes minimal a priori representational and computational assumptions, which explains why they tend to be so data-intensive. On the other hand, there is a lot of research into baking stronger relational inductive biases into deep learning architectures, e.g. with graph networks.2>In philosophy, inductive reasoning refers to generalization from specific observations to a conclusion. This is a counterpoint to deductive reasoning, which refers to specialization from general ideas to a conclusion.\n",
      "learning trajectory.2\n",
      " \n",
      "The standard backpropagation algorithm (gradient descent) gets serious issues when the number of layers becomes large. The probability of local minima in the error function increases with every layer. Not only local minima in a mathematical sense cause problems, sometimes there are just flat regions in the error function (modifying one or more weights does not significantly change the error) where gradient descent does not work.On the other hand, networks with many layers can solve more difficult problems, as every layer of cells can also provide a layer of abstraction.Deep Learning addresses exactly this problem. The basic idea is to perform an unsupervised learning procedure on every single layer in addition to using gradient descent for the network as a whole. The goal of the unsupervised learning is to make each single layer extract characteristic features out of its input that can be used by subsequent layers.Although the term \"Deep Learning\" is currently being used much too widely, it is more than just a marketing hype.Edit: A few years ago, many people, including myself, believed that unsupervised pre-training was the main enabler of deep learning. Since then, other techniques became popular that produce even better results in many cases. As mentioned in the comment by @Safak Okzan (below his own answer), these include:Residual NetworksBatch normalizationRectified linear units\n",
      "Learning\n",
      " \n",
      "I'm not an expert on KDEs, so take all of this with a grain of salt, but a very similar (but much faster!) implementation of your code would be:If I'm not mistaken, the density estimate should integrate to 1, that is we would expect  to get at least close to 1. In the implementation above I'm getting there, give or take 5%, but then again we don't know that 0.1 is the optimal bandwith (using  instead I'm getting there to within 0.1%), and the uniform Kernel is known to only be about 93% \"efficient\".In any case, there's a very good Kernel Density package in Julia available , so you probably should just do  instead of trying to code your own Epanechnikov kernel :)\n",
      "Kernel\n",
      " \n",
      "This largely depends on your regression algorithm. Good features for Kernel based regression algorithms might be pretty bad for linear classifiers.()You seem to aim at the \"filter approach\". What works well in many regression settings is the Pearson Correlation. This is also available in ML-Lib.However, you should consider to not add the K top-correlated features, butAvoid selecting pairs of highly-correlated feature. So you have to build the correlation matrix between all pairs of features.Select the top-feature, build a regression model, measure the error of the model, measure the correlation between the error and the remaining features. This will greedily select the best featuresOnce you have selected you features you should consider doing a sensitivity analysis. This is, build a regression model for all features and, for all feature sets where one feature has been removed. If removing does not have a significant impact you can remove it.\n",
      "Kernel\n",
      " \n",
      "An isotropic Kernel is a kernel which depends only on the distance of the kernel arguments, is any suitable norm, usually the L2-norm.Intuitively this means that the direction of the deviation is of no importance. For instance, in two dimensions a change in variable  is equally important to a change in variable  -- which is of course often a too strong assumption. Therefore the predictor variables are often scaled appropriately.How you can use it in regression? Like any other kernel, but it's often simpler as the number of parameters is often quite small. The isotropic Gaussian for example has only one parameter.\n",
      "Kernel\n",
      " \n",
      "Kernel PCA starts by calculating the kernel matrix  with the entries is a  matrix in your case. Next, one diagonalizes the matrix to obtain the decomposition(In practice one would perform a dual PCA as that leads to smaller dimension, but that is not important here as you already have obtained the reduction).Reducing the dataset now corresponds to retaining only the three largest entries in the diagonal matrix of eigenvectors  and neglecting all others (-- your kernel is well chosen if this approximation is justified).Effectively, this means that in the projection matrix  only the first  rows are retained. Call the reduced matrix  which has dimension . Now \"reducing\" the number of features correspons to working with the matrixwhich as well has dimension . These rows are used as input to train the neural net.Let's come to your question: How to treat a new feature vector ?First, calculate a vector  with elements .Second, map that vector onto the KPCA subspace by calculating  -- which is a vector of dimension .Finally, feed the vector  into your Neural Network and use the prediction.\n",
      "Kernel\n",
      " \n",
      "Applying MapReduce Jobs on the data to do data analysis.HBase supports two types of read access: table scans by row key and MapReduce jobs. Table scans enable you to retrieve the exact subset of rows you are looking for, and MapReduce jobs enable you to perform analysis across a greater set of data. \n",
      "MapReduce\n",
      " \n",
      "You can write your own MapReduce programs and call the third party machine learning algorithms in mapper and reducer. There are several open source machine learning libraries are available like Weka, open nlp, ctakes, mallet,uima etc...Another best way is use Spark ML lib or H2O, Oryix on top of Hadoop.Another solution: use PMML and JPMML to integrate the machine learning tools like R, Weka, SAS with Hadoop.You can develop the model using any one of the tools (R, SAS, Weka) and use the model in MapReduce programs with help of JPMML.\n",
      "MapReduce\n",
      " \n",
      "As your model is taking too long to train, I think you should first try and understand how spark actually benefits the model training part. As per ,Many common machine learning algorithms apply a function repeatedly to the same dataset to optimize a parameter (e.g., through gradient descent). While each iteration can be expressed as a MapReduce/Dryad job, each job must reload the data from disk, incurring a significant performance penaltySpark mllib's libraries remove this performance penalty by caching the data in memory during the first iteration. So subsequent iterations are extremely quick compared to the first iteration and hence, there is a significant reduction in model training time. I think, in your case, the executor memory might be insufficient to load a partition of data in memory. Hence contents would be spilled to disk and would need to be fetched from disk again in every iteration, thus killing any performance benefits of spark. To make sure, this is actually the case, you should try and look at the executor logs which would contain some lines like \"Unable to store rdd_x_y in memory\".If this is indeed the case, you'll need to adjust --num-executors, --executor-memory and numPartitions to see which values of these parameters are able to load the entire data into memory. You can try out with a small data set, single executor and a small value of executor memory on your local machine and analyze logs while incrementally increasing executor memory to see at which config the data is totally cached in memory. Once you have the configs for the small data set, you can do the Maths to figure out how many executors with how much memory are required and what should be the number of partitions for the required partition size.I had faced a similar problem and managed to bring down model training time from around 4 hours to 20 minutes by following the above steps.\n",
      "MapReduce\n",
      " \n",
      "Yes, deeper neural network can have less parameters. It doesn't matter if they are CNNs. You might be confused, because in the graphical representation one tends to focus on the neurons. However, what gets learned are the weights, which are on the edges between neurons.Besides the link to \"Deep Residual Learning for image recognition\" (please upvote Midos answer for that), I would like to give a toy example of a multilayer perceptron (MLP).2>The first MLP has an input layer of 784 neurons, two hidden layers of 2000 neurons each and an output layer of 10 neurons (short: 784:2000:2000:10). This results in a network with  neurons. Now consider a network with the architecture 784:2000:50:2000:10. This has  neurons.This means adding another layer, even without reducing any of the layer, reduced the size of the network to 32% of the previous size!\n",
      "Learning\n",
      " \n",
      "Generally, the more data you have, the better. You will get diminishing returns at some point. It is often a good idea to see if your training set size is a problem by plotting the cross validation performance while varying the size of the training set. In scikit-learn has an example of this type of \".\"Scikit-learn Learning Curve ExampleYou may consider bringing in outside sample posts to increase the size of your training set.As you grow your training set, you may want to try reducing the bias of your classifier. This could be done by adding n-gram features, or switching to a logistic regression or SVM model.\n",
      "Learning\n",
      " \n",
      "What you are looking for is a policy that maps state to action.While you can use a neural network to store your policy you need some way to interact with the environment to collect data.What you are describing is a typical Reinforcement Learning problem. I would suggest you have a look at Q-learning. For the size of the state space, you could easily store your policy in a table, but if you want, neural network are also easy to combine with Q-learning (though when using non-linear approximation schemes convergence is not ensured).\n",
      "Reinforcement\n",
      " \n",
      "1>Pretty much every design choice in machine learning signifies some sort of inductive bias.  is an amazing  read, which I will be referring to throughout this answer.2>Concretely speaking, the very composition of layers  in deep learning provides a  type of relational inductive bias: hierarchical processing. The type of layer imposes further relational inductive biases:More generally, non-relational inductive biases used in deep learning include:activation non-linearities,weight decay,dropout,batch and layer normalization,data augmentation,training curricula,optimization algorithms,anything that imposes constraints on the learning trajectory.2>In a Bayesian model, inductive biases are typically expressed through the choice and parameterization of the prior distribution. Adding a Tikhonov regularization penalty to your loss function implies assuming that simpler hypotheses are more likely.1>The stronger the inductive bias, the better the sample efficiency--this can be understood in terms of the bias-variance tradeoff. Many modern deep learning methods follow an end-to-end design philosophy which emphasizes minimal a priori representational and computational assumptions, which explains why they tend to be so data-intensive. On the other hand, there is a lot of research into baking stronger relational inductive biases into deep learning architectures, e.g. with graph networks.2>In philosophy, inductive reasoning refers to generalization from specific observations to a conclusion. This is a counterpoint to deductive reasoning, which refers to specialization from general ideas to a conclusion.\n",
      "learning trajectory.2\n",
      " \n",
      "The standard backpropagation algorithm (gradient descent) gets serious issues when the number of layers becomes large. The probability of local minima in the error function increases with every layer. Not only local minima in a mathematical sense cause problems, sometimes there are just flat regions in the error function (modifying one or more weights does not significantly change the error) where gradient descent does not work.On the other hand, networks with many layers can solve more difficult problems, as every layer of cells can also provide a layer of abstraction.Deep Learning addresses exactly this problem. The basic idea is to perform an unsupervised learning procedure on every single layer in addition to using gradient descent for the network as a whole. The goal of the unsupervised learning is to make each single layer extract characteristic features out of its input that can be used by subsequent layers.Although the term \"Deep Learning\" is currently being used much too widely, it is more than just a marketing hype.Edit: A few years ago, many people, including myself, believed that unsupervised pre-training was the main enabler of deep learning. Since then, other techniques became popular that produce even better results in many cases. As mentioned in the comment by @Safak Okzan (below his own answer), these include:Residual NetworksBatch normalizationRectified linear units\n",
      "Learning\n",
      " \n",
      "There are a lot of possible strategies to do this, the purpose in asking the question seems to be to try to determine which works the best.It seems impossible to get 100% correct, so the goal is really to come up with a strategy that provides the correct answer more often than not, and hopefully beats the current best. On that note, the best at the time of writing, seems to give about 46% more correct answers than wrong ones, if I'm reading the scoring right.A quick glance at the first few training numbers and looks like the English score might be a good indicator of the Mathematics score. The reason being that it seems to often be within 1 of the English score ( seems to yield slightly better results). Thus a simple strategy might be to simply return the English score as the Mathematics score.However, you'd want to go through the data and see if that is actually the case that the Mathematics score is within 1 of the English score more often than not This ends up looking to be significantly better than random, but roughly equal in terms of correct VS wrong answers, so it would need to be improved upon by a fair bit to get close to the current best. So you then might want to look into what the other scores look like when using the English score would be counted as wrong. Alternatively, you could look for a different indicator.\n",
      "Mathematics\n",
      " \n",
      "Segmentation, as in Image Segmentation means creating parts of an image into segments which are conceptually meaningful or simple for further analysis. Usually we want to locate objects and boundaries in the images. Simplest example is removing background from foreground. You can relate it with edge detection and countour detection. There are various methods to do this like clustering using K-Means, compressing image to reduce texture, edge detection or markov fields.Classification is entirely different. In classification, you want to find LABEL of the given data item. The Labels are usually predefined classes or categories - like whether an email is spam or not, or an image contains a human or animal. Decision tree is one of the approaches to do this.There have been experiments on segmenting images with the help of classification algorithms.  (link in reference) tries to create too many segments using cut algorithm and use classification to combine them based on whether it is a good segment based on human intervention or not. Their algorithm didn't perform very well but gave a sign that classification can be used in this task. p.s. Segmentation is more related to clustering than classification.\n",
      "Image\n",
      " \n",
      "Positive image setsYes you should definitely choose images which look more similar to the test images. Negative image setsIt can be any image set however, it is better to include images from the environment where this algorithm is going to be tested without object of interest.GenerallyPlease read my  to some other SO question, it would be useful. Discussion continued in comments, so that might be useful as well.\n",
      "image setsIt\n",
      " \n",
      "Try using Microsoft R Open and the checkpoint function.  \n",
      "Microsoft\n",
      " \n",
      "Note that as of the most recent release the Microsoft Visual Studio instructions no longer seem to apply as this link returns a 404 error:You can read more about the removal of the MSVC build from Tianqi Chen's comment .So here's what I did to finish a 64-bit build on Windows:Download and install MinGW-64:  On the first screen of the install prompt make sure you set the Architecture to x86_64 and the Threads to win32I installed to C:\\mingw64 (to avoid spaces in the file path) so I added this to my PATH environment variable:  C:\\mingw64\\mingw64\\binI also noticed that the make utility that is included in bin\\mingw64 is called mingw32-make so to simplify things I just renamed this to makeOpen a Windows command prompt and type gcc.  You should see something like \"fatal error: no input file\"Next type make.  You should see something like \"No targets specified and no makefile found\"Type git.  If you don't have git, install it and add it to yourPATH.These should be all the tools you need to build the xgboost project.  To get the source code run these lines:cd c:\\git clone --recursive cd xgboostgit submodule initgit submodule updatecp make/mingw64.mk config.mkmake -j4Note that I ran this part from a Cygwin shell.  If you are using the Windows command prompt you should be able to change cp to copy and arrive at the same result.  However, if the build fails on you for any reason I would recommend trying again using cygwin.If the build finishes successfully, you should have a file called xgboost.exe located in the project root.  To install the Python package, do the following:  cd python-packagepython setup.py installNow you should be good to go.  Open up Python, and you can import the package with:To test the installation, I went ahead and ran the basic_walkthrough.py file that was included in the demo/guide-python folder of the project and didn't get any errors.  \n",
      "Microsoft\n",
      " \n",
      "you will need the Microsoft ODBC Driver to use pyodbc. You can download it from here: . Once you download that, try using the following connection string: If this string does no work, try this one:If you still can't use pyodbc let me know. Cheers,Meet\n",
      "Microsoft\n",
      " \n",
      "Microsoft open sourced theirs and it's in C++ which you may be able to get to work with that Objective-C++ compiler. Not sure thoughheres some details:heres the github:heres help on using the C++ in iOS:\n",
      "Microsoft\n",
      " \n",
      "I found it out myself. It is documenten on Microsoft's site .The steps, very short, are:Include all the python you want in a .zipUpload that zip as a datasetDrag the dataset as the third option parameter in the 'execute python'-block (example below)execute said function by importing  (the name of the file, not the zip) and running \n",
      "Microsoft\n",
      " \n",
      "You are facing a typical Signal Processing problem, or more specifically, a source separation problem. First you must determine whether your problem is over determined or underdetermined. This means that if you Observation is more than the number of independent components that makes up the mixture, it is overdetermined and vice-versa. To me, it looks like your problem is overdetermined, which makes it much easier. You can imagine it as a system of equations, say you have 4 observations Y1,Y2,Y3 and Y4 (which are the 4 rows of data you are talking about, and ofc you need all 4 rows). And say you have 4 independent source in your mixture. You can represent this as:and Y3 and Y4 should follow suit. To separate, you have several methods from PCA, ICA, to model based approach such as GMM/HMM/FHMM and Neural Network based methods ANN/DNN etc. The specific method you choose is somewhat dependent on how mixed your signals are, in other words, how sparse is your signal. For example, if your signals are completely frequency independent, then a simple filter would suffice. You choose ICA which returns 4 rows representing the 4 components of your mixture (assuming that you used ICA correctly). You need to provide your ICA result for us to help you further. \n",
      "Signal\n",
      " \n",
      "Elements of Statistical Learning book is useful to understand basic programming and skills. You can also download this books from \n",
      "Statistical\n",
      " \n",
      "The  function (in the Statistics and Machine learning toolbox) performs exactly this. Simply use:to get the desired output.Best,\n",
      "Statistics\n",
      " \n",
      "Usually, n-grams more than 1 is better as it carries more information about the context in general. However, sometimes unigrams are also calculated besides bigram and trigrams and used as fallback for them. This is usefull also, if you want high recall than precision to search unigrams, for instance, you are searching for all possible uses of verb \"make\". Lets use Statistical Machine Translation as an Example:Intuitively, the best scenario is that your model has seen the full sentence (lets say 6-grams) before and knows its translation as a whole. If this is not the case you try to divide it to smaller n-grams, keeping into consideration that the more information you know about the word surroundings, the better the translation. For example, if you want to translate \"Tom Green\" to German, if you have seen the bi-gram you will know it is a person name and should remain as it is but if your model never saw it, you would fall back to unigrams and translate \"Tom\" and \"Green\" separately. Thus \"Green\" will be translated as a color to \"Grn\" and so on. Also, in search knowing more about the surrounding context makes the results more accurate.\n",
      "Statistical\n",
      " \n",
      "Yes, deeper neural network can have less parameters. It doesn't matter if they are CNNs. You might be confused, because in the graphical representation one tends to focus on the neurons. However, what gets learned are the weights, which are on the edges between neurons.Besides the link to \"Deep Residual Learning for image recognition\" (please upvote Midos answer for that), I would like to give a toy example of a multilayer perceptron (MLP).2>The first MLP has an input layer of 784 neurons, two hidden layers of 2000 neurons each and an output layer of 10 neurons (short: 784:2000:2000:10). This results in a network with  neurons. Now consider a network with the architecture 784:2000:50:2000:10. This has  neurons.This means adding another layer, even without reducing any of the layer, reduced the size of the network to 32% of the previous size!\n",
      "Learning\n",
      " \n",
      "Generally, the more data you have, the better. You will get diminishing returns at some point. It is often a good idea to see if your training set size is a problem by plotting the cross validation performance while varying the size of the training set. In scikit-learn has an example of this type of \".\"Scikit-learn Learning Curve ExampleYou may consider bringing in outside sample posts to increase the size of your training set.As you grow your training set, you may want to try reducing the bias of your classifier. This could be done by adding n-gram features, or switching to a logistic regression or SVM model.\n",
      "Learning\n",
      " \n",
      "1>Pretty much every design choice in machine learning signifies some sort of inductive bias.  is an amazing  read, which I will be referring to throughout this answer.2>Concretely speaking, the very composition of layers  in deep learning provides a  type of relational inductive bias: hierarchical processing. The type of layer imposes further relational inductive biases:More generally, non-relational inductive biases used in deep learning include:activation non-linearities,weight decay,dropout,batch and layer normalization,data augmentation,training curricula,optimization algorithms,anything that imposes constraints on the learning trajectory.2>In a Bayesian model, inductive biases are typically expressed through the choice and parameterization of the prior distribution. Adding a Tikhonov regularization penalty to your loss function implies assuming that simpler hypotheses are more likely.1>The stronger the inductive bias, the better the sample efficiency--this can be understood in terms of the bias-variance tradeoff. Many modern deep learning methods follow an end-to-end design philosophy which emphasizes minimal a priori representational and computational assumptions, which explains why they tend to be so data-intensive. On the other hand, there is a lot of research into baking stronger relational inductive biases into deep learning architectures, e.g. with graph networks.2>In philosophy, inductive reasoning refers to generalization from specific observations to a conclusion. This is a counterpoint to deductive reasoning, which refers to specialization from general ideas to a conclusion.\n",
      "learning trajectory.2\n",
      " \n",
      "The standard backpropagation algorithm (gradient descent) gets serious issues when the number of layers becomes large. The probability of local minima in the error function increases with every layer. Not only local minima in a mathematical sense cause problems, sometimes there are just flat regions in the error function (modifying one or more weights does not significantly change the error) where gradient descent does not work.On the other hand, networks with many layers can solve more difficult problems, as every layer of cells can also provide a layer of abstraction.Deep Learning addresses exactly this problem. The basic idea is to perform an unsupervised learning procedure on every single layer in addition to using gradient descent for the network as a whole. The goal of the unsupervised learning is to make each single layer extract characteristic features out of its input that can be used by subsequent layers.Although the term \"Deep Learning\" is currently being used much too widely, it is more than just a marketing hype.Edit: A few years ago, many people, including myself, believed that unsupervised pre-training was the main enabler of deep learning. Since then, other techniques became popular that produce even better results in many cases. As mentioned in the comment by @Safak Okzan (below his own answer), these include:Residual NetworksBatch normalizationRectified linear units\n",
      "Learning\n",
      " \n",
      "It looks like  was recently updated to include Bayesian Networks.  I haven't tried it myself, but the interface looks nice and sklearn-ish.\n",
      "Bayesian\n",
      " \n",
      "Bayesian Probability allows incorporating your prior belief and knowledge by selecting a prior distribution. In that way your actuals data will be merged with the prior beliefs to form your final posterior distribution.So your prior distribution in this case should include information that \n",
      "Bayesian\n",
      " \n",
      "Broadly speaking you can divide this process into 2 phases:Determining location of text. It's at the intersection of ml and Computer Vision, because before text recognition part you need to find where this text is located. It's not an easy task, you can find lines, boxes, etc, look at  lib for example, it may be useful for CV-related tasks. If all of your documents have same precise form (location of fields relative to scanned list itself) and you can scan them perfectly, without distortions (rotations, offsets) you can try to search text in static areas, where fields are.When you have found the text, you have to break contents of each field to words, then words to characters, and then you can feed your recognizer (ML part) with these characters and get labels of each character itself. And it's almost impossible(nowadays) for handwritten text, thus it's hard to recognize handwritten text in general case. Even if fields contain only printed text i recommend you to avoid this step, and use special lib for OCR, like \n",
      "Computer\n",
      " \n",
      "Try the bag-of-features approach. See  using the Computer Vision System Toolbox.\n",
      "Computer\n",
      " \n",
      "You can use the  function to compute various shape features, like area, perimeter, eccentricity, etc. In particular, the eccentricity will tell you how close the shape is to a circle.For texture features, try  and  functions in the Computer Vision System Toolbox.\n",
      "Computer\n",
      " \n",
      "Using Text Classification...To be able to treat this as a classification problem, you need a training set, which is a set of AngelList entries that are labeled with correct LinkedIn categories. This can be done manually, or you can hire some  to do the job for you.Since you have ~150 categories, I'd imagine you need at least 20-30* AngelList entries for each of them. So your training set will be  {input: angellist_description, result: linkedin_id}After that, you need to dig through text classification techniques to try and optimize the accuracy/precision of your results. The book \"Taming Text\" has a full chapter on text classification. And a good tool to implement a text-based classifier would be Apache Solr or Apache Lucene.* 20-30 is a quick personal estimate and not based on a scientific method. You can look up some methods online for a good estimation method.\n",
      "Text\n",
      " \n",
      "A baseline statistical approach would treat this as a classification problem.  Features are bags-of-words processed by a maximum entropy classifier like Mallet .  Maxent (aka logistic regression) is good at handling large feature spaces.  Take the probability associated with each each tag (i.e., the class labels) and choose some decision threshold that gives you a precision/recall tradeoff that works for your project.  Some of the Mallet documentation even mentions topic classification, which is very similar to what you are trying to do.The open questions are how well Mallet handles the size of your data (which isn't that big) and whether this particular tool is a non-starter with the technology stack you mentioned.  You might be able to train offline (dump the reddis database to a text file in Mallet's feature format) and run the Mallet-learned model in Python.  Evaluating a maxent model is simple.  If you want to stay in Python and have this be more automated, there are Python-based maxent implementations in NLTK and probably in scikit-learn.  This approach is not at all state-of-the-art, but it'll work okay and be a decent baseline with which to compare more complicated methods.\n",
      "Mallet\n",
      " \n",
      "There has been many sharedTasks in NLP community on textual similarity/entailment (STS 2015, 2014, 2013, RTE 2010, ...) etc. This is the latest competition:Some of them release the submitted systems or the baseline to use which I assume you can use for your task as well. Have a look at this:\n",
      "NLP\n",
      " \n",
      "The idea I'm suggesting is originated in text-processing, NLP and information retrieval and very widely used in situations where you have sequences of characters/information like Genetic information. Because you have to preserve the sequence, we can use the concepts of n-grams. I'm using bi-grams in the following example, though you can generalize for higher order grams. N-grams help in preserving the sequential pattern in the data. Don't worry - we keep borrowing ideas from other fields of science - even edit distance and dynamic programming were not originally computer science concepts.There are many possible approaches to tackle this problem - each one unique, and there's no right one - atleast there's no sufficient research that proves which one is right. Here's my take on it. So the goal is to create a Bag-Of-Words like vector out of your data strings - and these vectors can be easily fed to any machine learning tool or library for clustering. A quick summary of steps:-Collect bigrams (and unigrams etc)Create a dictionary to get Bag Of Words (Code attached)Create functionality to get vector from a stringLet's get startedSo here These functions would convert the given string to a set of two characters (and single character if there's only one). You can modify them to capture 3-grams or even complete set of all possible uni-, bi- and tri-grams. Feel free to experiment.Now how to convert strings to vectors? We'll define a function that will convert string to vectors taking care of how many times a particular n-gram appears. This is called BAG OF WORDS. Here, these are BAGS OF SCREENS. Following two functions help you with that:Voila! We're done. Now feed your data vectors to any K-Means implementation of your choice. I used SKLearn. You should rather choose to read strings from a fileAnd then I finally see what my clusters look like using km.labels_ property of KMeans class.Here're your clusters. Look at the console (bottom) window - There are ten clusters.Now you can modify feature generation I wrote in the code and see how your modifications performs. Rather than just bigrams, extract all the possible unigrams, bigrams and trigrams and use them to create BOW. There will be significant difference. You can also use length of the string as a feature. You can also experiment with other algorithms including hierarchical clustering. Be sure to send me updated results after your modifications.Enjoy! \n",
      "NLP\n",
      " \n",
      "It depends what do you mean by term. If - as usual - term is just a word, then a probability model will work the same as... simple tf weighting (even without idf!). Why? Beacause empirical estimator of  is just , and as  is constant, then the weight becomes just , which is simple term frequency. So in this sense, scikit does what you need.Ok, so maybe you want to consider context? Then what kind of context? Do you want to analyze independently  and use it as a weighted sum for ? Then why not ? Why not  etc.? Why not to include some reweighting based on unigrams when bigrams are not available? The answer is quite simple, once you go into using language models as a weighting schemes, amount of possible introductions grows exponentialy, and there is no \"typical\" approach, which is worth implementing as a \"standard\" for a library which is not a NLP library.\n",
      "NLP\n",
      " \n",
      "NLP generally does this with real-time interpretation.  Set a match threshold; when a sequence of motions resolves to a unique gesture and meets the threshold, you interpret that as a gesture.This is simple in description.  In practice, there is a lot of feedback, especially if some gestures are subsets of others, or if the matches are not quite as crisp as we'd like.If you want to use HMM, can you seed it after some training with markers for terminal states?\n",
      "NLP\n",
      " \n",
      "2>Sampled, in both cases means you don't calculate it for all of what's possible as an output (e.g.: if there are too many words in a dictionary to take all of them at each derivation, so we take just a few samples and learn on that for NLP problems). 3>This is the cross entropy and receives logits as inputs and yields what can be used as a loss.3>This is a sampled softmax_cross_entropy_with_logits, so it takes just a few samples before using the cross entropy rather than using the full cross entropy: \n",
      "NLP\n",
      " \n",
      "NLP stand for Natural Language Processing. Thus everything that involves processing natural language can be described as NLP. This is not any strict science, this is just a bag term for everything related to this field. Even if you just map your text to vector space and forget about any language and semantics, you are still processing natural language, thus - NLP.\n",
      "NLP\n",
      " \n",
      "I think you can use datumbox api:Common API URL: API doc PDF: As you can see it used REST  API, so i think it will be not hard to used it.EDIT:Also for this API exits PHP client:\n",
      "PHP\n",
      " \n",
      "You can use the the PhP SDK to send events to PredictionIO. Here is a quickstart on how to setup the ecommerce engine with an app. eIn your case, each app would have its own access key and you need to setup an engine for each app if their data don't mix together - i.e. if they have different products and users. -Isabelle \n",
      "PhP SDK\n",
      " \n",
      "Disclaimer: I am the developer of this annotation tool.The  should suit your requirements because:this tool is based solely on HTML, CSS and Javascript (no external javascript libraries), can be used off-line (full application in a single html file of size &lt; 200KB) and requires nothing more than a modern web browser (tested on Firefox, Chrome and Safari)you do not need to upload your images and can run this tool locally in your web browser. This tool is self contained and does not require any installation.\n",
      "HTML\n",
      " \n",
      "You can try aho-corasick finite state machine and augment it with a wildcard. Another option is a suffix tree, i.e. a levensthein distance. You can try my PHP implementation of aho-corasick @ .\n",
      "PHP\n",
      " \n",
      "I don't see this mentioned in your post... Elasticsearch offers a random scoring feature:  would be returned in the same order every time. It would be good to introduce some randomness here, to ensure that all documents in a single score level get a similar amount of exposure.We want every user to see a different random order, but we want the same user to see the same order when clicking on page 2, 3, and so forth. This is what is meant by consistently random.The  function, which outputs a number between 0 and 1, will produce consistently random results when it is provided with the same seed value, such as a users session ID\n",
      "Elasticsearch\n",
      " \n",
      "I'd write this as a comment isntead of an answer but SO won't let me commet yet.This would be \"trivially\" solved by utilizing ElasticSearch's . From docs you can see how it works and which factors are taken into account, which should be useful info even if you end up implementing this in Python.They have also implemented other interesting algorithms such as the .\n",
      "ElasticSearch\n",
      " \n",
      "Umm... surely it should be easy enough to code? The stupidest, yet guaranteed to work, approach will be to iterate over all the documents twice. During the first iteration, create a hashmap of the words and a unique index (a structure like HashMap), and during the second iteration, you do a table lookup and print the word's index to create a numerical representation of the data. If you want a bag of words representation, during the second iteration, you could create a hashmap (HashMap) every time you see a new document, and incremement the counts of each word index, and once you reach the end of a document, you read out the counts, and print them.\n",
      "HashMap\n",
      " \n",
      "Using Text Classification...To be able to treat this as a classification problem, you need a training set, which is a set of AngelList entries that are labeled with correct LinkedIn categories. This can be done manually, or you can hire some  to do the job for you.Since you have ~150 categories, I'd imagine you need at least 20-30* AngelList entries for each of them. So your training set will be  {input: angellist_description, result: linkedin_id}After that, you need to dig through text classification techniques to try and optimize the accuracy/precision of your results. The book \"Taming Text\" has a full chapter on text classification. And a good tool to implement a text-based classifier would be Apache Solr or Apache Lucene.* 20-30 is a quick personal estimate and not based on a scientific method. You can look up some methods online for a good estimation method.\n",
      "Text\n",
      " \n",
      "Great question.Logistic Regression also assumes the following:That there isn't (or there is little) multicollinearity (high correlation) among the independent variables.Even though LR doesn't require the dependent and independent variables to be linearly related, it does however require that the independent variables to be linearly related to the log odds. The log odds function is simply .\n",
      "Regression\n",
      " \n",
      "I started to improve the solution by transforming the  into a smarter, dichotomous way of finding the maximumThen I realized, after 2 hours of work, that getting all the accuracies were far more cheaper than just finding the maximum !! (Yes it is totally counter-intuitive).I wrote a lot of comments here below to explain my code. Feel free to delete all these to make the code more readable.The all process is just a single loop, and the algorithm is just trivial.In fact, the stupidly simple function is 10 times faster than the solution proposed before me (commpute the accuracies for ) and 30 times faster than my previous smart-ass-dychotomous-algorithm...You can then easily compute ANY KPI you want, for example :If you want to test it :Enjoy ;)\n",
      "-intuitive).I\n",
      " \n",
      "Majority of machine learning algorithms work with numbers, so you can to transform your categorical values and string into numbers.Popular python machine-learning library scikit-learn has the . With 'yes/no' everything is easy - just put 0/1 instead of it.Among many other important things it explains the process of  using their .When you work with text, you also have to transform your data in a suitable way. One of the common feature extraction strategy for text is a  score, and I wrote a .\n",
      "scikit-learn\n",
      " \n",
      "I'm not 100% sure, but I think scikit-learn.naive_bayes requires a purely numeric feature vector instead of a mixture of text and numbers. It looks like it crashes when trying to \"divide\" a unicode string by a long integer.I can't be much help with finding numeric representations for text, but  might be a good start.\n",
      "scikit-learn.naive_bayes\n",
      " \n",
      "1>Previous answers do not specify how to handle the multi-label case so here is such a version implementing three types of multi-label f1 score in tensorflow: micro, macro and weighted (as per scikit-learn)Update (06/06/18): I wrote a  about how to compute the streaming multilabel f1 score in case it helps anyone (it's a longer process, don't want to overload this answer)3>outputs:\n",
      "scikit-learn)Update\n",
      " \n",
      "You didn't show which kind of model you use to me, but I assume that you initialized your model as . In a  model you can only stack one layer after another - so adding a &quot;short-cut&quot; connection is not possible.For this reason authors of Keras added option of building &quot;graph&quot; models. In this case you can build a graph (DAG) of your computations. It's a more complicated than designing a stack of layers, but still quite easy.Check the documentation  to look for more details.\n",
      "quot;short-cut&quot\n",
      " \n",
      "Q-Learning is a TD (temporal difference) learning method. I think you are trying to refer to TD(0) vs Q-learning. I would say it depends on your actions being deterministic or not. Even if you have the transition function, it can be expensive to decide which action to take in TD(0) as you need to calculate the expected value for each of the actions in each step. In Q-learning that would be summarized in the Q-value.\n",
      "Q-Learning\n",
      " \n",
      "take a simple linear classification problem-y={0 if 5x-3>=0 else 1}here y is class, x is feature, 5,3 are parameters.\n",
      "problem-y={0\n",
      " \n",
      "See  for further details about these hijinx.  I'm assuming you're working on Linux, with Linux-PAM.  I am assuming pam_test.so is something you've written.  As such, you should be returning PAM_AUTH_ERR when the password is invalid.  You can specify different actions based on the different return codes with an \"advanced\" syntax.For reference, from the (my) man page for pam.d, the \"simple\" actions have the following \"advanced\" syntax:These are of the form retval=action where retval is the PAM_* return code, with the PAM_ removed and converted to lower case (so PAM_SUCCESS becomes success).  The actions bad and die both give failure status, but die exits the stack.  The actions ok and done (and an integer N) indicate a successful status.  The 'done' also stops execution of the stack.  Using an integer N skips that many following modules. The special \"default\" means \"any other return value from the module\".  You can then do:This will stop execution of the auth stack with failure when the password is wrong, stop execution with success when the module passes, and move along the auth stack in any other scenario.As for getting back to your code... that's a whole new can of worms.  You can do it, but I can't think of a way that's not a total hack.  For instance, jump through hoops with the control flow by using the code=N to jump around and call your module with special args that then succeed or fail based on the argument.  For completeness, (and a complete HACK) something like:This has the following properties: If the password is invalid or it satisfies the pam_test.so conditions, there is no further authentication processing and finishes with failure or success, respectively.  If pam_test failed for any other reason, it calls google authenticator.  If that succeeds, it moves on to the next line calling , and otherwise, it goes to .  Only one of the two subsequent pam_test.so calls are made.  After this snippet, the return code is success or failure, depending on the google authenticator status.  This is effectively \"require pam_google_authenticator.so but also call my module with the appropriate flag\".  If you want auth to end with this no matter what, you can use:\n",
      "Linux\n",
      " \n",
      "I had the same problem on Linux and solved installing pydotplus and its dependencies.After installing pydotplus and its dependencies:\n",
      "Linux\n",
      " \n",
      "EDIT: The main problem was that the training data was not shuffled in random order. This is needed when using any online learning (unless the training data is already shuffled or if it is a real time series). It can be done using Unix command .Explanation: In an extreme case, if the training data contains first all negative examples followed by all positive examples, then it is quite probable that the model will learn to classify (almost) everything as positive.Another common reason that might result in low F1-measure (and almost all predictions positive) is imbalanced data (many positive examples, few negative examples). This was not the case of the dataset in Satarupa Guha's question, but I keep my original answer here:The obvious solution is to give a higher (than the default 1) importance weight to the negative examples. The optimal value of the importance weight can be found using a heldout set.The threshold for negative vs. positive prediction should be 0.Note that one of the great advantages of Vowpal Wabbit is that you do not need to convert feature names (words in your case) to integers. You can use the raw (tokenized) text, just make sure to escape pipe \"|\" and colon \":\" (and space and newline). Of course, if you already converted the words to integers, you can use it.\n",
      "Unix\n",
      " \n",
      "I suggest to use the Linux \"date\" command to convert the date to the unix time stamps. Unix time stamp are the number of seconds elapsed since 1 January 1970. So basically a year is 60s*60m*24h*256d seconds. So, if the difference between the time stamps is more than this number then it is longer than a year.It will be something like this:So if you use perl, which is a pretty cool file handling language, you will parse your whole file in a few lines and use eval\"you date command\".\n",
      "Linux\n",
      " \n",
      "Tensorflow works fine for me with VMware &amp; Ubuntu. Please try:\n",
      "VMware\n",
      " \n",
      "Try installing Ubuntu 14.04 in VMware and use the same command.\n",
      "Ubuntu\n",
      " \n",
      "The  is a virtual filesystem for passing data between programs that implementation of traditional shared memory on Linux.So you could not increase it via set up some options on Application Layout.But for example, you can remount  with 8G size in Linux Shell with administrator permission like  as follows.However, it seems that Azure ML studio not support remote access via SSH protocol, so the feasible plan is upgrade the standard tier if using free tier at present.\n",
      "Linux\n",
      " \n",
      "I'm not sure your conjecture is true. Because of the nature of complete-linkage clustering, each time you agglomerate two clusters, you are doing so because the two elements that are farthest apart between those two clusters, are still mutually closer to each other than the farthest elements to any other cluster.What you're trying to prove is that However, after merging two clusters A and B due to complete-linkage clustering, there could still exist an element in cluster C that is nearer to an element in Cluster AB than any other element in cluster AB because complete-linkage is only concerned about maximal distances.2>3>The observations, A, B, C, D, and E are arranged in a straight line.Observation A is 1 unit away from observation BObservation B is 3 units away from observation CObservation C is 2.5 units away from observation DObservation D is 2 units away from observation ELet's perform hierarchical clustering:First A and B are merged because distance is 1:New picture:Cluster AB is 4 units away from observation C (because A is 4 units from C due to complete-linkage clustering), which is 2.5 units from D, which is 2 units from ENext, D and E are merged because distance is 2New pictureCluster AB is 4 units from observation C (as before), which is 4.5 units from Cluster DE because C is 4.5 units from E due to complete-linkage clustering.Next, C would need to be merged into AB because its distance is 4 whereas it's 4.5 to DE:Cluster ABC is 8.5 units from DE because A is 8.5 units from E.BUT, at this point we've disproven your conjecture. Element C is 3 units from B and 4 units from A (refer to original diagram). However, Element C is only 2.5 units from element D, which is inside another cluster.\n",
      "Cluster\n",
      " \n",
      "Are you sure you want cluster analysis?To find similar records you don't need cluster analysis. Simply find similar records with any distance measure such as Jaccard similarity or Hamming distance (both of which are for binary data). Or cosine distance, so that you can use e.g. Lucene to find similar records fast.To find common patterns, the use of frequent itemset mining may yield much more meaningful results, because these can work on a subset of attributes only. For example, in a supermarket, the columns Noodles, Tomato, Basil, Cheese may constitute a frequent pattern.Most clustering algorithms attempt to divide the data into k groups. While this at first appears a good idea (get k target groups) it rarely matches what real data contains. For example customers: why would every customer belong to exactly one audience? What if the audiences are e.g. car lovers, gun lovers, football lovers, soccer moms - are you sure you don't want to allow overlap of these groups?Furhermore, a problem with cluster analysis is that it's incredibly easy to use badly. It does not \"fail hard\" - you always get a result, and you might not realize that it's a bad result...\n",
      "cluster analysis?To\n",
      " \n",
      "For boosting task you need to pick best classifier on each iteration of algorithm. To do so you need to minimize average error of stump on dataset with respect to weights, so you must take into account weights of objects while counting error measure of a classifier. Thus penalty of classifier for incorrect labeling of object with big weight will be bigger than penalty for incorrect labeling of object with small weight.You can see my  of boosting on decision trees on R language, it works well, for decision stumps, just change depth of tree on line 132 to 1, and you can test accuracy with different number of classifiers changing parameter T.If you need deeper understanding: You can learn stumps in same way as tree of depth 1. To learn tree on weighted dataset you need to pick feature and value that separates dataset in best way to 2 parts by chosen feature according to weighted metrics, for example . You can just iterate with for loop over all available features, in nested loop sort picked feature and try all possible separations of dataset into two sets S, according to chosen feature and separator value, then calculate entropy over every set as it is written on wikipedia, but instead of calculating p(x) asyou need to sum all weights of objects with class x in set and divide this number by sum over all weights of objects in this set.where  - all weights of objects in set S, and  - all weights of objects in set S with class x.Then you can calculate information gain, but again, you need to use weighted proportion p(t) instead of variant in wikipedia (proportion of numbers).where  - set of weights of objects from initial (not divided by separator) set.and  - set of weights of objects from set t (you will get 2 set's t by separating S with some separator value)Pick feature and separator value which gives you biggest gain and that's all, you have just learned new stump classifier on weighted data and it is ready for work.I've made some pic to provide example of computation, here i picked only 1 separator, you need to check gain for every possible separator.\n",
      "parameter T.If\n",
      " \n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# Term similarity\n",
    "# second try\n",
    "\n",
    "# loop through relavant words and get their vectors\n",
    "technology = []\n",
    "for tag in tag_set:\n",
    "    term_vector = []    \n",
    "    tag_doc = nlp(str(tag))\n",
    "    tag_vector = tag_doc.vector    \n",
    "    \n",
    "    for i,span in enumerate(wordlist):\n",
    "        if span.doc != wordlist[i-1].doc:\n",
    "            span_vector = span.vector        \n",
    "            # add these vectors to term_vector of this word        \n",
    "            if tag_doc.similarity(span) >= 0.7:\n",
    "            #if cosine_similarity([tag_vector], [span_vector])[0][0] >= 0.7:\n",
    "                #technology.append(span)\n",
    "                print(span.doc)\n",
    "                print(span.text)\n",
    "                print(\" \")\n",
    "            \n",
    "            \n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "doc = nlp(\"In Encog SVM is just a classification Scikit-learn or regression Spark MLib model and can be used mostly interchangably C++ with other model types.  I modified the hello Java World XOR example to use it, you can see the results below.This is a decent intro to them:  This is a more basic intro to modeling in general, I wrote it for neural networks, but it applies to SVM as well:\")\n",
    "\n",
    "technology_pattern1 = [{'OP': '+', 'POS': 'NOUN'},\n",
    "                               {'POS': 'PROPN', 'OP': '+'}]\n",
    "\n",
    "matcher.add(\"Propntest\", [technology_pattern1])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    print(doc[start:end].text)    \n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"In Encog SVM is just a classification Scikit-learn or regression Spark MLib model and can be used mostly interchangably C++ with other model types.  I modified the hello Java World XOR example to use it, you can see the results below.This is a decent intro to them:  This is a more basic intro to modeling in general, I wrote it for neural networks, but it applies to SVM as well:\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "    print(token.pos_)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "postal-diameter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.019616807984967306"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-giant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
