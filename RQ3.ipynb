{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2478025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy \n",
    "from spacy.matcher import Matcher\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example\n",
    "from spacy.tokens import Doc\n",
    "from spacy.util import filter_spans\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import copy\n",
    "import csv\n",
    "\n",
    "# configuration\n",
    "n_tag_posts = 500\n",
    "n_answer_posts = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53651360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv data\n",
    "filepath = os.path.join(os.getcwd(), 'QueryResults_sample.csv')\n",
    "\n",
    "stack_posts = pd.read_csv(filepath, sep = \",\")\n",
    "\n",
    "print(\"loaded csv data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7344f623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervised-learning', 'tic-tac-toe', 'apriori', 'algorithmic-trading', 'sampling', 'projection', 'cascade-classifier', 'signal-processing', 'ruby', 'computer-science', 'cryptography', 'fuzzy-search', 'mallet', 'statistics', 'kernel-density', 'gearman', 'azure-machine-learning-studio', 'r', 'treemodel', 'authentication', 'neural-network', 'tf-idf', 'reinforcement-learning', 'pos-tagger', 'dlib', 'black-box', 'word2vec', 'data-analysis', 'matrix', 'test-data', 'vgg-net', 'cuda', 'vectorization', 'deep-learning', 'pylearn', 'python-3.x', 'liblinear', 'metrics', 'distributed-computing', 'time-series', 'cvx', 'ensemble-learning', 'large-files', 'mahout', 'lstm', 'gnuplot', 'matplotlib', 'probability-density', 'python', 'confusion-matrix', 'precision-recall', 'security', 'api', 'pam', 'tokenize', 'bayesian', 'knn', 'audio', 'sift', 'object-recognition', 'text', 'scikit-learn', 'mysql', 'algorithm', 'kernel', 'cloudera', 'differentiation', 'e-commerce', 'genetic-programming', 'implementation', 'perceptron', 'pybrain', 'scala', 'mapreduce', 'node.js', 'pca', 'decision-tree', 'hadoop', 'linear-algebra', 'chess', 'image-recognition', 'standardized', 'r-caret', 'maven', 'image-processing', 'unsupervised-learning', 'naivebayes', 'viola-jones', 'evolutionary-algorithm', 'twitter', 'data-mining', 'datumbox', 'pmml', 'mixture-model', 'collaborative-filtering', 'reduction', 'mfcc', 'convolution', 'anpr', 'gaussian', 'pattern-matching', 'text-analysis', 'speech-recognition', 'google-analytics-api', 'summarization', 'smo', 'compilation', 'tagged-corpus', 'theano', 'reservoir-sampling', 'nearest-neighbor', 'regex', 'indices', 'mex', 'dirichlet', 'bayesian-networks', 'evaluation', 'apache-spark-mllib', 'glm', 'python-2.7', 'porter-stemmer', 'ranking', 'php', 'elasticsearch', 'search', 'gradient-descent', 'plot', 'data-visualization', 'scientific-computing', 'mp3', 'normalization', 'google-analytics', 'pruning', 'recommendation-engine', 'gate', 'office365', 'missing-data', 'cors', 'haar-classifier', 'apache-spark', 'feature-detection', 'svm', 'opennlp', 'feature-extraction', 'keyword-search', 'data-structures', 'numpy', 'random', 'java', 'stanford-nlp', 'biometrics', 'c++', 'prediction', 'gpu', 'information-retrieval', 'topic-modeling', 'random-forest', 'id3', 'single-sign-on', 'multilabel-classification', 'lda', 'nltk', 'training-data', 'document-classification', 'arff', 'grouping', 'gradient', 'artificial-intelligence', 'entity-framework', 'feature-selection', 'filesize', 'probability', 'logistic-regression', 'cluster-analysis', 'cluster-computing', 'parameter-passing', 'libsvm', 'regression', 'pattern-recognition', 'backpropagation', 'orange', 'computer-vision', 'conv-neural-network', 'euclidean-distance', 'k-means', 'mathematical-optimization', 'csv', 'macos', 'math', 'text-mining', 'matlab', 'measurement', 'dbn', 'arrays', 'classification', 'linear-regression', 'scipy', 'prolog', 'genetic-algorithm', 'opencv', 'face-recognition', 'quadratic-programming', 'c#', 'vector', 'weka', 'hashmap', 'detection', 'text-classification', 'io', 'optimization', 'encog', 'data-manipulation', 'sparse-matrix', 'pom.xml', 'rbm', 'nlp', 'wordnet', 'auc', 'calibration', 'predictionio', 'elki', 'contour', 'cross-validation', 'simplecv', 'ssas', 'microsoft-account', 'gps', 'kaggle', 'object-detection', 'logarithm', 'image', 'amazon-web-services', 'svmlight', 'hierarchical-clustering', 'q-learning', 'oauth', 'azure', 'hessian-matrix', 'caffe', 'pandas', 'linux'}\n"
     ]
    }
   ],
   "source": [
    "# drop all duplicates in posts\n",
    "df = pd.DataFrame(stack_posts[0:n_tag_posts])\n",
    "df = df.drop_duplicates([\"QuestionId\"])\n",
    "df = df.sort_values(by=[\"QuestionId\"])\n",
    "#df.duplicated([\"QuestionId\"])\n",
    "\n",
    "\n",
    "# get all tags of questions\n",
    "tag_set = set()\n",
    "tag_list = []\n",
    "for tags in df[\"Tags\"]:\n",
    "    # clean tags from '>' and '<' occurences\n",
    "    tags = re.sub('><', ' ', tags) \n",
    "    tags = re.sub('<|>', '', tags)\n",
    "    # add single tag of tags and add it to lists and sets\n",
    "    for tag in tags.split():               \n",
    "        tag_list.append(tag)\n",
    "\n",
    "#tag_set = set(tag_list)\n",
    "# filter term 'machine-learning', because sql export filters for this term\n",
    "tag_set = set(filter(lambda a: a != 'machine-learning', tag_list))\n",
    "tag_Counter = Counter(tag_list)\n",
    "\n",
    "print(tag_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8c58058c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "# clean posts and match words\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "technology_pattern1 = [{'POS': 'PROPN', 'OP': '+'},\n",
    "                       {'POS': 'NUM', 'OP': '?'}\n",
    "                      ]\n",
    "\n",
    "technology_pattern2 = [{'OP': '+', 'POS': 'PROPN'},\n",
    "                       {'TEXT': '-', 'OP': '+'},\n",
    "                       {'POS': 'VERB', 'OP': '+'}\n",
    "                      ]\n",
    "\n",
    "technology_pattern3 = [{'OP': '+', 'POS': 'NOUN'},\n",
    "                       {'TEXT': '-', 'OP': '?'},\n",
    "                       {'POS': 'PROPN', 'OP': '+'}\n",
    "                      ]\n",
    "\n",
    "ml_pattern1 = [{'LOWER': 'machine', 'OP': '!'},\n",
    "                       #{'TEXT': '-', 'OP': '!'},\n",
    "                       {'LOWER': 'learning', 'OP': '!'}\n",
    "                      ]\n",
    "\n",
    "\n",
    "word_set = set()\n",
    "regex_pattern = '(<(pre|code|blockquote|a|strike)(.|\\n)*?\\/(pre|code|blockquote|a|strike)>)*?|<(p|b|br|br(.|\\n)*?\\/|sub|sup|em|strong|hr|s|i|ol|ul|li|code)*?>|<\\/(p|b|br|sub|sup|em|strong|s|i|ol|ul|li|div|pre|blockquote|a|code)>|<h(.|\\n)*?>(.|\\n)*?<\\/h(.|\\n)*?>*?|(<(img|div|ol|ul|li)(.|\\n)*?\\/*?>)|\\n'\n",
    "\n",
    "matcher.add(\"match_technology1\", [technology_pattern1])\n",
    "matcher.add(\"match_technology2\", [technology_pattern2])\n",
    "matcher.add(\"match_technology3\", [technology_pattern3])\n",
    "#matcher.add(\"unmatch_ml_pattern\", [ml_pattern1])\n",
    "\n",
    "for text in stack_posts[\"AnswerBody\"][n_tag_posts:n_answer_posts]:\n",
    "    text = re.sub(regex_pattern, '', text, flags=re.I)\n",
    "    text = re.sub('\\(|\\)', ' ', text, flags=re.I)    \n",
    "    doc = nlp(text)    \n",
    "    \n",
    "    matches = matcher(doc)    \n",
    "    match_set = set()\n",
    "    for match_id, start, end in matches:\n",
    "        match_set.add(doc[start:end])\n",
    "    [word_set.add(filtered_span) for filtered_span in filter_spans(match_set)]\n",
    "    \n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "edaaa905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-63-3141c5bde907>:13: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  if tag_doc.similarity(span) >= 0.8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "# Term similarity\n",
    "\n",
    "# loop through relavant words and get their vectors\n",
    "technology_list = []\n",
    "technology_string_list = []\n",
    "technology_counter = Counter()\n",
    "for tag in tag_set:\n",
    "    term_vector = []    \n",
    "    tag_doc = nlp(str(tag))\n",
    "    #tag_vector = tag_doc.vector    \n",
    "    \n",
    "    for i,span in enumerate(word_set):        \n",
    "        if tag_doc.similarity(span) >= 0.8:\n",
    "        # very slow\n",
    "        # if cosine_similarity([tag_vector], [span_vector])[0][0] >= 0.7:\n",
    "            if span.text not in technology_string_list:\n",
    "                technology_list.append(span)\n",
    "                technology_string_list.append(span.text)\n",
    "            technology_counter[span.text] = technology_counter[span.text] + 1\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a0c2429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('technology_list.txt', 'w', newline='') as myfile:\n",
    "    myfile.truncate(0)  \n",
    "    \n",
    "for technology in technology_list:    \n",
    "    with open('technology_list.txt', 'a', newline='') as myfile:        \n",
    "        wr = csv.writer(myfile)\n",
    "        wr.writerow([technology])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8bd9d10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ruby,\n",
       " Statistics,\n",
       " Kernel Density Estimation,\n",
       " R,\n",
       " R.The R,\n",
       " r,\n",
       " Convolutional Neural Networks,\n",
       " Neural Networks,\n",
       " Neural Network,\n",
       " Convolutional Neural Network,\n",
       " modelsArtificial Neural Networks,\n",
       " Recurrent Neural Network,\n",
       " word2vec,\n",
       " Data,\n",
       " analysis Kibana,\n",
       " Analysis ofStructured Data,\n",
       " data scienceA,\n",
       " Singular Matrix,\n",
       " Confusion Matrix,\n",
       " Matrix,\n",
       " test-setModel fittingOverall accuracyBetter,\n",
       " CUDA,\n",
       " Deep Learning,\n",
       " Mahout,\n",
       " Python,\n",
       " Execute Python,\n",
       " 0.My Python,\n",
       " Python API,\n",
       " bit Python,\n",
       " Information Security,\n",
       " System Security,\n",
       " API,\n",
       " URLGetRankedNamedEntities API,\n",
       " Java API,\n",
       " PAM,\n",
       " KNN,\n",
       " knn clusfier;normalize,\n",
       " SIFT,\n",
       " text categorization Largeron,\n",
       " Scikit-learn,\n",
       " findNeighbors - get,\n",
       " SQL,\n",
       " Algorithms,\n",
       " kernel SVC.If,\n",
       " Scala,\n",
       " scala,\n",
       " MapReduce,\n",
       " Node.js,\n",
       " PCA,\n",
       " example PCA,\n",
       " Decision Trees,\n",
       " Decision Tree,\n",
       " Hadoop,\n",
       " Unsupervised Deep Learning,\n",
       " Facebook,\n",
       " Web Searchand Data Mining,\n",
       " Data Mining,\n",
       " Gaussian,\n",
       " Nearest Neighbors,\n",
       " Nearest Neighbour,\n",
       " Regex,\n",
       " GLM,\n",
       " PHP,\n",
       " PhP SDK,\n",
       " ElasticSearch,\n",
       " Elasticsearch,\n",
       " GATE,\n",
       " Apache Spark,\n",
       " SVM,\n",
       " svm,\n",
       " sklearn SVM,\n",
       " thing SVM,\n",
       " Pegasos SVM,\n",
       " example SVM,\n",
       " Matlab SVM,\n",
       " svm Categorical,\n",
       " SciPy,\n",
       " NumPy,\n",
       " Numpy,\n",
       " Java,\n",
       " Java SDK,\n",
       " Java APIs,\n",
       " Java Library,\n",
       " Stanford NLP - Produces,\n",
       " C++,\n",
       " legacy c++,\n",
       " Prediction,\n",
       " GPU,\n",
       " Word2Vec GPU,\n",
       " Nvidia GPU,\n",
       " Random Forest,\n",
       " Random Forests,\n",
       " LDA,\n",
       " NLTK,\n",
       " NLTK classifiersFor,\n",
       " NLTK HunPos,\n",
       " training DATA,\n",
       " Gradient,\n",
       " Projected Gradient,\n",
       " Likelihood,\n",
       " probability 0.The RandomForest,\n",
       " Linear Regression,\n",
       " Orange,\n",
       " Computer Vision,\n",
       " info providedUse Computer Vision,\n",
       " K-means,\n",
       " k-Fold,\n",
       " k-fold,\n",
       " k-means,\n",
       " MATLAB,\n",
       " Matlab,\n",
       " MatLab,\n",
       " matlab,\n",
       " DBN,\n",
       " Improved Classification,\n",
       " Classification,\n",
       " Genetic Algorithms,\n",
       " OpenCV,\n",
       " OpenCV +,\n",
       " face recognition taskIn,\n",
       " Vector,\n",
       " Avni ClusteringParagraph Vector,\n",
       " Support Vector Classifier,\n",
       " Weka,\n",
       " weka GUI,\n",
       " HashMap,\n",
       " RBM,\n",
       " NLP,\n",
       " WordNet,\n",
       " AUC,\n",
       " cross-validation procedureBut,\n",
       " SSAS,\n",
       " kaggle,\n",
       " Hierarchical Agglomerative Clustering,\n",
       " Azure HDInsight,\n",
       " Azure,\n",
       " Azure Blob,\n",
       " Caffe,\n",
       " Pandas,\n",
       " Linux]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "technology_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea54e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
