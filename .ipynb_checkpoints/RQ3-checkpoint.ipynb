{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "255df5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy \n",
    "from spacy.matcher import Matcher\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example\n",
    "from spacy.tokens import Doc\n",
    "from spacy.util import filter_spans\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import copy\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3127a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "n_tag_posts = 1000\n",
    "n_answer_posts = 42636 #max 42636"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7763c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded csv data\n"
     ]
    }
   ],
   "source": [
    "# Load csv data\n",
    "filepath = os.path.join(os.getcwd(), 'QueryResults_sample_42636_14_05_21.csv')\n",
    "\n",
    "stack_posts = pd.read_csv(filepath, sep = \",\")\n",
    "\n",
    "print(\"loaded csv data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f92bac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'object-detection', 'precision', 'topic-modeling', 'sift', 'self-organizing-maps', 'parallel-processing', 'cloudera', 'azure-virtual-machine', 'pybrain', 'web-scraping', 'web-services', 'gbm', 'contour', 'numpy-ndarray', 'missing-data', 'recommendation-engine', 'powerbi', 'node.js', 'definition', 'hierarchical-clustering', 'rfe', 'evolutionary-algorithm', 'json', 'logarithm', 'mahout', 'prediction', 'security', 'tm', 'linear-regression', 'integration', 'opennlp', 'cors', 'ocr', 'cluster-computing', 'speech-recognition', 'pyspark', 'overlapping', 'data-science', 'scikit-image', 'apache-spark-ml', 'gridworld', 'reduction', 'topic-maps', 'validation', 'python-multiprocessing', 'sql-server', 'bigdata', 'r', 'julia', 'mixture-model', 'tokenize', 'precision-recall', 'audio', 'feedparser', 'tesseract', 'nearest-neighbor', 'r-caret', 'interaction', 'amazon-web-services', 'celery', 'scientific-computing', 'analysis', 'data-visualization', 'optics-algorithm', 'apache-spark-mllib', 'vectorization', 'php', 'auc', 'multithreading', 'text', 'categorical-data', 'matrix-multiplication', 'naivebayes', 'azure', 'compression', 'text-classification', 'dbn', 'excel', 'sqlite', 'anpr', 'one-hot-encoding', 'information-retrieval', 'linux', 'emgucv', 'j48', 'data-manipulation', 'microsoft-account', 'nlp', 'webforms', 'pmml', 'chess', 'database', 'feature-extraction', 'vector', 'pam', 'wordnet', 'tic-tac-toe', 'random', 'regex', 'mathematical-optimization', 'apriori', 'spam-prevention', 'mp3', 'netcdf', 'mahout-recommender', 'evaluation', 'overlap', 'arrays', 'notifications', 'principal-components', 'csv', 'bayesian-networks', 'ruby', 'openimaj', 'single-sign-on', 'pandas', 'machine-translation', 'acl', 'neural-network', 'input', 'feature-selection', 'ranking', 'email-spam', 'smo', 'algorithm', 'sparse-matrix', 'matlab-figure', 'correlation', 'sentiment-analysis', 'entity-framework', 'calculus', 'autosuggest', 'search', 'genetic-programming', 'tagged-corpus', 'opencv', 'data-mining', 'oauth', 'scikits', 'backpropagation', 'parameter-passing', 'artificial-intelligence', 'conv-neural-network', 'dataset', 'dbscan', 'unsupervised-learning', 'matrix-storage', 'prolog', 'nltk', 'glm', 'elki', 'word2vec', 'gnuplot', 'r-package', 'mysql', 'image-recognition', 'performance', 'confusion-matrix', 'ensemble-learning', 'text-mining', 'compilation', 'standardized', 'plot', 'expectation-maximization', 'document-classification', 'quadratic-programming', 'normalization', 'indices', 'logistic-regression', 'textblob', 'c4.5', 'keyword-search', 'multinomial', 'joblib', 'stockquotes', 'sarsa', 'pruning', 'predict', 'numpy', 'python-3.x', 'linear-algebra', 'svm', 'gps', 'bayesian', 'perceptron', 'e-commerce', 'scipy', 'svmlight', 'gate', 'classification', 'pom.xml', 'amazon-s3', 'datumbox', 'ssas', 'filesystems', 'kernel-density', 'linguistics', 'lstm', 'probability', 'kaggle', 'algorithmic-trading', 'azure-machine-learning-studio', 'archive', 'viola-jones', 'mex', 'feature-detection', 'rpart', 'mfcc', 'data-representation', 'pylearn', 'gearman', 'mallet', 'resize', 'filesize', 'computer-vision', 'genetic-algorithm', 'large-files', 'kernel', 'math', 'scikit-learn', 'cuda', 'projection', 'haar-classifier', 'cross-validation', 'vgg-net', 'detection', 'arff', 'solr', 'lua', 'libsvm', 'implementation', 'som', 'summarization', 'rbm', 'gensim', 'cluster-analysis', 'vowpalwabbit', 'logging', 'distributed-computing', 'google-analytics', 'encog', 'c++', 'k-means', 'hadoop', 'signal-processing', 'euclidean-distance', 'optimization', 'mapreduce', 'freebase', 'pickle', 'multilabel-classification', 'regression', 'model', 'treemodel', 'matlab', 'tf-idf', 'matplotlib', 'spss-modeler', 'modeling', 'knn', 'matrix', 'caffe', 'html', 'macos', 'quicksort', 'reservoir-sampling', 'gpu', 'black-box', 'gaussian', 'predictionio', 'python', 'training-data', 'orange', 'moses', 'alchemyapi', 'adaboost', 'nupic', 'ubuntu-12.04', 'background-subtraction', 'pattern-matching', 'subsequence', 'authentication', 'c#', 'weka', 'hessian-matrix', 'pos-tagger', 'supervised-learning', 'porter-stemmer', 'statistics', 'sampling', 'dlib', 'probability-density', 'azureservicebus', 'similarity', 'grid-search', 'outliers', 'vision', 'pca', 'maven', 'etl', 'roc', 'apache-spark', 'netflix', 'cascade-classifier', '7zip', 'matconvnet', 'elasticsearch', 'matrix-inverse', 'dictionary', 'javascript', 'gradient-descent', 'function', 'google-analytics-api', 'intrusion-detection', 'differentiation', 'q-learning', 'hashmap', 'grouping', 'api', 'pydot', 'convolution', 'java', 'object-recognition', 'dataframe', 'vb.net', 'accord.net', 'io', 'xml', 'cvx', 'deep-learning', 'theano', 'computer-science', 'collaborative-filtering', 'sorting', 'azure-sql-database', 'data-analysis', 'metrics', 'office365', 'calibration', 'face-recognition', 'liblinear', 'wikipedia', 'cryptography', 'information-theory', 'image', 'named-entity-recognition', 'text-analysis', 'twitter', 'data-structures', 'stanford-nlp', 'time-series', 'dirichlet', 'pattern-recognition', 'biometrics', 'deployment', 'decision-tree', 'id3', 'image-processing', 'python-2.7', 'simplecv', 'non-linear-regression', 'scala', 'fuzzy-search', 'model-fitting', 'lda', 'torch', 'hungarian-algorithm', 'apache-flink', 'gradient', 'join', 'azure-scheduler', 'test-data', 'measurement', 'random-forest', 'snort', 'reinforcement-learning', 'lucene'}\n"
     ]
    }
   ],
   "source": [
    "# drop all duplicates in posts\n",
    "df = pd.DataFrame(stack_posts[0:n_tag_posts])\n",
    "df = df.drop_duplicates([\"QuestionId\"])\n",
    "df = df.sort_values(by=[\"QuestionId\"])\n",
    "#df.duplicated([\"QuestionId\"])\n",
    "\n",
    "\n",
    "# get all tags of questions\n",
    "tag_set = set()\n",
    "tag_list = []\n",
    "for tags in df[\"Tags\"]:\n",
    "    # clean tags from '>' and '<' occurences\n",
    "    tags = re.sub('><', ' ', tags) \n",
    "    tags = re.sub('<|>', '', tags)\n",
    "    # add single tag of tags and add it to lists and sets\n",
    "    for tag in tags.split():               \n",
    "        tag_list.append(tag)\n",
    "\n",
    "#tag_set = set(tag_list)\n",
    "# filter term 'machine-learning', because sql export filters for this term\n",
    "tag_set = set(filter(lambda a: a != 'machine-learning', tag_list))\n",
    "tag_Counter = Counter(tag_list)\n",
    "\n",
    "print(tag_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b488bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "# clean posts and match words\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "technology_pattern1 = [{'POS': 'PROPN', 'OP': '+'},\n",
    "                       {'POS': 'NUM', 'OP': '?'}\n",
    "                      ]\n",
    "\n",
    "technology_pattern2 = [{'OP': '+', 'POS': 'PROPN'},\n",
    "                       {'TEXT': '-', 'OP': '?'},\n",
    "                       {'POS': 'VERB', 'OP': '+'}\n",
    "                      ]\n",
    "\n",
    "technology_pattern3 = [{'OP': '+', 'POS': 'NOUN'},\n",
    "                       {'TEXT': '-', 'OP': '?'},\n",
    "                       {'POS': 'PROPN', 'OP': '+'}\n",
    "                      ]\n",
    "\n",
    "\n",
    "word_set = set()\n",
    "word_list = []\n",
    "regex_pattern = '(<(pre|code|blockquote|a|strike)(.|\\n)*?\\/(pre|code|blockquote|a|strike)>)*?|<(p|b|br|br(.|\\n)*?\\/|sub|sup|em|strong|hr|s|i|ol|ul|li|code)*?>|<\\/(p|b|br|sub|sup|em|strong|s|i|ol|ul|li|div|pre|blockquote|a|code)>|<h(.|\\n)*?>(.|\\n)*?<\\/h(.|\\n)*?>*?|(<(img|div|ol|ul|li)(.|\\n)*?\\/*?>)|\\n'\n",
    "\n",
    "matcher.add(\"match_technology1\", [technology_pattern1])\n",
    "matcher.add(\"match_technology2\", [technology_pattern2])\n",
    "matcher.add(\"match_technology3\", [technology_pattern3])\n",
    "#matcher.add(\"unmatch_ml_pattern\", [ml_pattern1])\n",
    "\n",
    "for text in stack_posts[\"AnswerBody\"][n_tag_posts:n_answer_posts]:\n",
    "    text = re.sub(regex_pattern, '', text, flags=re.I)\n",
    "    text = re.sub('\\(|\\)', ' ', text, flags=re.I)    \n",
    "    doc = nlp(text)    \n",
    "    \n",
    "    matches = matcher(doc)    \n",
    "    match_set = set()\n",
    "    for match_id, start, end in matches:\n",
    "        match_set.add(doc[start:end])\n",
    "    [word_list.append(filtered_span) for filtered_span in filter_spans(match_set)]    \n",
    "    \n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e813e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list\n",
    "with open('word_list.txt', 'w', newline='') as myfile:\n",
    "    myfile.truncate(0)  \n",
    "    \n",
    "for word in word_list:    \n",
    "    with open('word_list.txt', 'a', newline='', encoding=\"utf-8\") as myfile:                \n",
    "        wr = csv.writer(myfile)\n",
    "        wr.writerow([word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5db6ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-4e2305201fa3>:15: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  if tag_doc.similarity(span) >= 0.8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "# Term similarity\n",
    "\n",
    "word_set = set(word_list)\n",
    "\n",
    "# loop through relavant words and get their vectors\n",
    "technology_list = []\n",
    "technology_string_set = set()\n",
    "technology_counter = Counter()\n",
    "for tag in tag_set:\n",
    "    term_vector = []    \n",
    "    tag_doc = nlp(str(tag))\n",
    "    #tag_vector = tag_doc.vector    \n",
    "    \n",
    "    for i,span in enumerate(word_set):        \n",
    "        if tag_doc.similarity(span) >= 0.8:\n",
    "            if span.text not in technology_string_set:\n",
    "                technology_list.append(span.text)                \n",
    "                technology_string_set.add(span.text.lower())\n",
    "            technology_counter[span.text] = technology_counter[span.text] + 1\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fe48b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('technology_list.txt', 'w', newline='') as myfile:\n",
    "    myfile.truncate(0)  \n",
    "    \n",
    "for technology in technology_string_set:    \n",
    "    with open('technology_list.txt', 'a', newline='', encoding=\"utf-8\") as myfile:        \n",
    "        wr = csv.writer(myfile)\n",
    "        wr.writerow([technology])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b092c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "technology_string_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18116b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_tp = \n",
    "len_fp = \n",
    "\n",
    "precision = len_tp/(len_tp + len_fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a2409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
