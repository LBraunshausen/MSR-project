QuestionUserId;QuestionUserReputation;QuestionUserDN;Tags;QuestionId;QuestionScore;title;QuestionBody;AcceptedAnswer;AnswerUserId;AnswerUserReputation;AnswerUserDN;AnswerScore;AnswerId;AnswerBody
3625340;33;user3625340;<image-processing><machine-learning><svm><feature-detection><feature-extraction>;27729199;1;How to find Relevent Features for Comparing Diagrams(Images)?;"<p>Currently we are doing a project on diagram comparison using SVM. Diagrams mainly include flow charts and block diagram.we are stuck on determining relevant features for training the SVM. The problem is that diagram and its components to be compared can be of various scales.  Please help for the same.</p>
";27733517;1056563;45835;StephenBoesch;0;27733517;"<p>In regard solely to the difference in scales:  this seems relatively straightforward. Set up classifier(s) based on the characteristics of the images in terms of the shapes/ pixel densities/ arrangements of artifacts. Those aspects are scale-independent.  You may also wish to introduce rotation and shear invariant capabilities into the svm.</p>
"
3625340;33;user3625340;<image-processing><machine-learning><svm><feature-detection><feature-extraction>;99999999;5;How to find Relevent Features for Comparing Diagrams(Images)?;"<p>Currently we are doing a project on diagram comparison using SVM. Diagrams mainly include flow charts and block diagram.we are stuck on determining relevant features for training the SVM. The problem is that diagram and its components to be compared can be of various scales.  Please help for the same.</p>
";27733517;1056563;45835;StephenBoesch;0;27733517;"<p>In regard solely to the difference in scales:  this seems relatively straightforward. Set up classifier(s) based on the characteristics of the images in terms of the shapes/ pixel densities/ arrangements of artifacts. Those aspects are scale-independent.  You may also wish to introduce rotation and shear invariant capabilities into the svm.</p>
"
4409773;788;Avis;<java><machine-learning><svm><encog>;27729238;1;SVM using Encog in Java for beginners;"<p>I am beginner in SVM. Could someone please help me to understand the concepts of SVM using Encog from the very basics?? It will be helpful with sample Java code.</p>
";27808712;173355;3162;JeffHeaton;1;27808712;"<p>In Encog SVM is just a classification or regression model and can be used mostly interchangably with other model types.  I modified the Hello World XOR example to use it, you can see the results below.</p>

<p>This is a decent intro to them:  <a href=""http://webdoc.nyumc.org/nyumc/files/chibi/user-content/Final.pdf"" rel=""nofollow"">http://webdoc.nyumc.org/nyumc/files/chibi/user-content/Final.pdf</a>
This is a more basic intro to modeling in general, I wrote it for neural networks, but it applies to SVM as well:  <a href=""http://www.heatonresearch.com/content/non-mathematical-introduction-using-neural-networks"" rel=""nofollow"">http://www.heatonresearch.com/content/non-mathematical-introduction-using-neural-networks</a></p>

<pre><code>package org.encog.examples.neural.xor;

import org.encog.Encog;
import org.encog.ml.data.MLData;
import org.encog.ml.data.MLDataPair;
import org.encog.ml.data.MLDataSet;
import org.encog.ml.data.basic.BasicMLDataSet;
import org.encog.ml.svm.SVM;
import org.encog.ml.svm.training.SVMTrain;

public class XORHelloWorld {

    /**
     * The input necessary for XOR.
     */
    public static double XOR_INPUT[][] = { { 0.0, 0.0 }, { 1.0, 0.0 },
            { 0.0, 1.0 }, { 1.0, 1.0 } };

    /**
     * The ideal data necessary for XOR.
     */
    public static double XOR_IDEAL[][] = { { 0.0 }, { 1.0 }, { 1.0 }, { 0.0 } };

    /**
     * The main method.
     * @param args No arguments are used.
     */
    public static void main(final String args[]) {

        // create a SVM for classification, change false to true for regression     
        SVM svm = new SVM(2,false);

        // create training data
        MLDataSet trainingSet = new BasicMLDataSet(XOR_INPUT, XOR_IDEAL);

        // train the SVM
        final SVMTrain train = new SVMTrain(svm, trainingSet);
        train.iteration();
        train.finishTraining();

        // test the SVM
        System.out.println(""SVM Results:"");
        for(MLDataPair pair: trainingSet ) {
            final MLData output = svm.compute(pair.getInput());
            System.out.println(pair.getInput().getData(0) + "","" + pair.getInput().getData(1)
                    + "", actual="" + output.getData(0) + "",ideal="" + pair.getIdeal().getData(0));
        }

        Encog.getInstance().shutdown();
    }
}
</code></pre>
"
4408281;715;datavinci;<python-2.7><machine-learning>;27730775;1;Why does not the following code snippet run successfully?;"<p>I was reading Programming Collective Intelligence,chapter on Search Engines where I came across the following piece of code and upon implementation,it gave me error.Please Help.</p>

<pre><code>import urllib2
from BeautifulSoup import *
from urlparse import urljoin

class crawler:
def __init__(self,dbname):
    pass

def __del__(self):
    pass
def dbcommit(self):
    pass

def getentryid(self,table,field,value,createnew=True):
    return None

def addtoindex(self,url,soup):
    print 'Indexing %s' % url

def gettextonly(self,soup):
    return None

def seperatewords(self,text):
    return None

def isindexed(self,url):
    return False

def addlinkref(self,urlFrom,urlTo,linkText):
    pass

def crawl(self,pages,depth=2):
    for i in range(depth):
        newpages=set()
        for page in pages:
            try:
                c=urllib2.urlopen(page)
            except:
                print 'Could not open %s'%page
                continue
            soup=BeautifulSoup(c.read())
            self.addtoindex(page,soup)

            links=soup('a')
            for link in links:
                if('href' in dict(link.attrs)):
                    url=urljoin(page,link['href'])
                    if url.find(""'"")!=-1: continue
                    url=url.split('#')[0]
                    if url[0:4]=='http' and not self.isindexed(url):
                        newpages.add(url)
                    linkText=self.gettextonly(link)
                    self.addlinkref(page,url,linkTest)
                self.dbcommit()
            pages=newpages


def createindextables(self):
    pass
</code></pre>

<p>I got the following error:</p>

<pre><code>&gt;&gt;cwlr.crawl(pagelist)
Indexing http://en.wikipedia.org/wiki/Artificial_neural_network
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
&lt;ipython-input-50-97778b0c0db8&gt; in &lt;module&gt;()
----&gt; 1 cwlr.crawl(pagelist)

C:\Users\Blue\Anaconda\searchengine.py in crawl(self, pages, depth)
     47                                                 url=urljoin(page,link['href'])
     48                                                 if url.find(""'"")!=-1: continue
---&gt; 49                                                 url=url.split('#')[0]
     50                                                 if url[0:4]=='http' and not      self.isindexed(url):
 51                                                         newpages.add(url)

NameError: global name 'linkTest' is not defined
</code></pre>
";27730829;367273;436215;NPE;1;27730829;"<blockquote>
  <p>NameError: global name 'linkTest' is not defined</p>
</blockquote>

<p>You've misspelt <code>linkText</code> as <code>linkTest</code>:</p>

<pre><code>linkText=self.gettextonly(link)
      â†‘
self.addlinkref(page,url,linkTest)
                               â†‘
</code></pre>
"
3512217;119;Shlomi;<machine-learning><svm>;27730870;-1;division of two proper kernels;"<p>Let <img src=""https://i.stack.imgur.com/Z1G34.gif"" alt=""""> be proper kernels. Does it imply that </p>

<p><img src=""https://i.stack.imgur.com/2UgyP.gif"" alt=""enter image description here""></p>

<p>is a proper kernel? prove or give a counter example. It seems wrong but I didn't find how to prove it. Any suggestion? thanks!</p>
";;1060350;70512;Has QUIT--Anony-Mousse;0;27742921;"<p>K2(x,z) can be 0.</p>

<p>Then this value is not well-defined.</p>

<p>Therefore, this is not a proper kernel.</p>
"
4405757;14440;user7;<machine-learning><classification><weka><libsvm><arff>;27732503;0;One class SVM to detect outliers;"<p>My problem is</p>

<blockquote>
  <p>I want to build a <strong>one class SVM classifier</strong> to identify the nouns/aspects from test file.
  The training file has list of nouns. The test has list of words. </p>
</blockquote>

<p>This is what I've done:</p>

<p>I'm using Weka GUI and I've trained a one class SVM(libSVM) to get a model. </p>

<blockquote>
  <p>Now the model classifies those words in test file that the classifier identified as nouns in the generated model. Others are classified as outliers. ( So it is just working like a look up. If it is identified as noun in trained model, then 'yes' else 'no')</p>
</blockquote>

<p>So how to build a proper classifier?. ( I meant the format of input and what it information it should contain?)</p>

<p>Note: </p>

<ul>
<li>I don't give negative examples in training file since it is one class.</li>
<li>My input format is <em>arff</em> </li>
<li>Format of training file is a set of <strong>word,yes</strong></li>
<li>Format of test file is a set of <strong>word,?</strong></li>
</ul>

<p><strong>EDIT</strong>
My test file will have <strong>noun phrases</strong>. So my classifier's job is to get the nouns words from candidates in test file.</p>
";;1060350;70512;Has QUIT--Anony-Mousse;5;27739848;"<p>Your data is not formatted appropriately for this problem.</p>

<p>If you put</p>

<pre><code>word,class
</code></pre>

<p>pairs into a SVM, what you are <em>really</em> putting into the SVM are <em>sparse vectors</em> that consist of a single one, corresponding to your word, i.e.</p>

<pre><code>0,0,0,0,0,...,0,0,1,0,0,0,...,0,0,0,0,yes
</code></pre>

<p><strong>Anything a classifier can do on such data is overfit and memorize. On unknown new words, the result will be useless.</strong></p>

<hr>

<p>If you want your classifier to be able to <em>abstract</em> and <em>generalize</em>, then <strong>you need to carefully extract features from your words</strong>.</p>

<p>Possible features would be n-grams. So the word ""example"" could be represented as</p>

<pre><code>exa:1, xam:1, amp:1, mpl:1, ple:1
</code></pre>

<p>Now your classifier/SVM could learn that having the n-gram ""ple"" is typical for nouns.
Results will likely be better if you add ""beginning-of-word"" and ""end-of-word"" symbol,</p>

<pre><code>^ex:1, exa:1, xam:1, amp:1, mpl:1, ple:1, le$:1
</code></pre>

<p>and maybe also use more than one n-gram length, e.g.</p>

<pre><code>^ex:1, ^exa:1, exa:1, exam: 1, xam:1, xamp:1, amp:1, ampl:1, mpl:1, mple1:1, ple:1, ple$.1, le$:1
</code></pre>

<p>but of course, the more you add the larger your data set and search space grows, which again may lead to overfitting.</p>
"
1448255;192;seekme_94;<r><machine-learning><neural-network>;27732566;0;Extreme learning machines in R for discrete attributes;"<p>I'm trying to run the following example of ELM in R on famous Titanic dataset:</p>

<pre><code>library(elmNN)
dataset &lt;- read.csv(training_data)
formula &lt;- as.factor(Survived) ~ Sex + Age + Pclass + Fare + SibSp + Parch + Embarked
elm_fit &lt;- elmtrain(formula, dataset, nhid=10, actfun=""sig"")
testset$PredSurvived &lt;- predict(elm_fit, testset, type=""class"")
</code></pre>

<p>The training method throws me the following error:</p>

<pre><code>Error in ginv(t(H), tol = sqrt(.Machine$double.eps)) %*% t(T) : requires numeric/complex matrix/vector arguments
</code></pre>

<p>As I understand, this is because the algorithm is expecting numeric data, while my attributes are discrete.</p>

<p>My question is that is there a way to apply this algorithm on my data set?</p>
";;4437789;11;ppp;1;27865953;"<p>translate factors to binary representation using model.matrix. For instance, Species in iris can be generated as model.matrix( ~ Species, data=iris)</p>
"
4410616;61;Rohan Pota;<python-2.7><machine-learning>;27733238;-1;What is the error in the following python code snippet?;"<p>I was reading the the chapter on search engines in programming collective intelligence and came across the following code snippet and tried to implement it in IPython.However, I encountered error.Please Help.</p>

<pre><code>def getmatchrows(self,q):
    fieldlist='w0.urlid'
    tablelist=''
    clauselist=''
    wordids=[]

    words=q.split()
    tablenumber=0

    for word in words:
        wordrow=self.con.execute(""select rowid from wordlist where word='%s'"" % word).fetchone()
        if wordrow!=None:
            wordid=wordrow[0]
            wordids.append(wordid)
            if tablenumber&gt;0:
                tablelist+=','
                clauselist+=' and '
                clauselist+='w%d.urlid=w%d.urlid and '&amp;(tablenumber-1,tablenumber)
            fieldlist+=',w%d.location'%tablenumber
            tablelist+='wordlocation w%d'%tablenumber
            clauselist+='w%d.wordid=%d'%(tablenumber,wordid)
            tablenumber+=1
    fullquery=""select %s from %s where %s""%(fieldlist,tablelist,clauselist)
    cur=self.con.execute(fullquery)
    rows=[row for row in cur]
    return row,wordids
</code></pre>

<p>I got the following error:</p>

<pre><code>e.getmatchrows('the')
---------------------------------------------------------------------------
OperationalError                          Traceback (most recent call last)
&lt;ipython-input-4-65d6f760f6c8&gt; in &lt;module&gt;()
----&gt; 1 e.getmatchrows('the')

C:\Users\Blue\Anaconda\searchengine.py in getmatchrows(self, q)
    134                                 tablenumber+=1
    135                 fullquery=""select %s from %s where %s""%(fieldlist,tablelist,clauselist)
--&gt; 136                 cur=self.con.execute(fullquery)
    137                 rows=[row for row in cur]
    138                 return row,wordids

OperationalError: near ""where"": syntax error
</code></pre>
";;2058922;743;Bee Smears;0;27736501;"<p>You have an SQL error.  Start by properly formatting your SQL.  <a href=""http://zetcode.com/db/mysqlpython/"" rel=""nofollow"">Here is a quick tutorial</a>.  And here is the syntax change I would make first.</p>

<pre><code>fullquery=""SELECT %s FROM %s WHERE %s""%(fieldlist,tablelist,clauselist)
</code></pre>

<p>In other words, consider using uppercase on your SQL keywords and rerun it.</p>
"
495815;1370;Wouter;<apache-spark><machine-learning><prediction><apache-spark-mllib><predictionio>;27734329;18;Incremental training of ALS model;"<p>I'm trying to find out if it is possible to have ""incremental training"" on data using MLlib in Apache Spark.</p>

<p>My platform is Prediction IO, and it's basically a wrapper for Spark (MLlib), HBase, ElasticSearch and some other Restful parts.</p>

<p>In my app data ""events"" are inserted in real-time, but to get updated prediction results I need to ""pio train"" and ""pio deploy"". This takes some time and the server goes offline during the redeploy.</p>

<p>I'm trying to figure out if I can do incremental training during the ""predict"" phase, but cannot find an answer.</p>
";;4674497;602;BartÅ‚omiej Twardowski;0;36767722;"<p>For updating Your model near-online (I write near, because face it, the true online update is impossible) by using fold-in technique, e.g.:
<a href=""http://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2008-Online_Updating_Regularized_Kernel_Matrix_Factorization_Models.pdf"" rel=""nofollow noreferrer"">Online-Updating Regularized Kernel Matrix Factorization Models for Large-Scale Recommender Systems.</a></p>

<p>Ou You can look at code of:</p>

<ul>
<li><a href=""http://www.mymedialite.net"" rel=""nofollow noreferrer"">MyMediaLite</a></li>
<li><a href=""https://github.com/cloudera/oryx"" rel=""nofollow noreferrer"">Oryx</a> - framework build with Lambda Architecture paradigm. And it should have updates with fold-in of new users/items.</li>
</ul>

<p>It's the part of my answer for similar <a href=""https://stackoverflow.com/questions/36723429/how-can-i-handle-new-users-items-in-model-generated-by-spark-als-from-mllib"">question</a> where both problems: near-online training and handling new users/items were mixed.</p>
"
495815;1370;Wouter;<apache-spark><machine-learning><prediction><apache-spark-mllib><predictionio>;27734329;18;Incremental training of ALS model;"<p>I'm trying to find out if it is possible to have ""incremental training"" on data using MLlib in Apache Spark.</p>

<p>My platform is Prediction IO, and it's basically a wrapper for Spark (MLlib), HBase, ElasticSearch and some other Restful parts.</p>

<p>In my app data ""events"" are inserted in real-time, but to get updated prediction results I need to ""pio train"" and ""pio deploy"". This takes some time and the server goes offline during the redeploy.</p>

<p>I'm trying to figure out if I can do incremental training during the ""predict"" phase, but cannot find an answer.</p>
";;2220275;737;Dr VComas;4;36917856;"<p>I imagine you are using spark MLlib's ALS model which is performing matrix factorization. The result of the model are two matrices a user-features matrix and an item-features matrix. </p>

<p>Assuming we are going to receive a stream of data with ratings or transactions for the case of implicit, a real (100%) online update of this model will be to update both matrices for each new rating information coming by triggering a full retrain of the ALS model on the entire data again + the new rating. In this scenario one is limited by the fact that running the entire ALS model is computationally expensive and the incoming stream of data could be frequent, so it would trigger a full retrain too often.</p>

<p>So, knowing this we can look for alternatives, a single rating should not change the matrices much plus we have optimization approaches which are incremental, for example SGD. There is an interesting (still experimental) library written for the case of Explicit Ratings which does incremental updates for each batch of a DStream:</p>

<p><a href=""https://github.com/brkyvz/streaming-matrix-factorization"" rel=""nofollow"">https://github.com/brkyvz/streaming-matrix-factorization</a></p>

<p>The idea of using an incremental approach such as SGD follows the idea of as far as one moves towards the gradient (minimization problem) one guarantees that is moving towards a minimum of the error function. So even if we do an update to the single new rating, only to the user feature matrix for this specific user, and only the item-feature matrix for this specific item rated, and the update is towards the gradient, we guarantee that we move towards the minimum, of course as an approximation, but still towards the minimum. </p>

<p>The other problem comes from spark itself, and the distributed system, ideally the updates should be done sequentially, for each new incoming rating, but spark treats the incoming stream as a batch, which is distributed as an RDD, so the operations done for updating would be done for the entire batch with no guarantee of sequentiality.</p>

<p>In more details if you are using Prediction.IO for example, you could do an off line training which uses the regular train and deploy functions built in, but if you want to have the online updates you will have to access both matrices for each batch of the stream, and run updates using SGD, then ask for the new model to be deployed, this functionality of course is not in Prediction.IO you would have to build it on your own. </p>

<p>Interesting notes for SGD updates:   </p>

<p><a href=""http://stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf"" rel=""nofollow"">http://stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf</a>   </p>
"
3423045;978;Deepak;<c++><opencv><machine-learning><svm>;27738986;3;Find confidence of prediction in SVM;"<p>I am doing English digit classification using SVM classifier in opencv. 
I am able to predict the classes using <code>predict()</code> function. 
But I want get confidence of prediction between 0-1. Can somebody provide a method to do it using opencv</p>

<pre><code> //svm parameters used
 m_params.svm_type    = CvSVM::C_SVC;
 m_params.kernel_type = CvSVM::RBF;
 m_params.term_crit   = cvTermCriteria(CV_TERMCRIT_ITER, 500, 1e-8);

 //for training
 svmob.train_auto(m_features, m_labels, cv::Mat(), cv::Mat(), m_params, 10);

 //for prediction
 predicted = svmob.predict(testData);
</code></pre>
";27739386;1190430;5186;Artem Sobolev;10;27739386;"<p>SVM during training tries to find a separating hyperplane such that trainset examples lay on different sides. There could be many such hyperplanes (or none), so to select the ""best"" we look for the one for which total distance from all classes are maximized. Indeed, the further away from the hyperplane point is located â€” the more confident we are in the decision. So what we are interested in is distance to the hyperplane.</p>

<p>As per OpenCV <a href=""http://docs.opencv.org/modules/ml/doc/support_vector_machines.html#cvsvm-predict"">documentation</a>, <code>CvSVM::predict</code> has a default second arguments which specifies what to return. By default, it returns classification label, but you can pass in <code>true</code> and it'll return the distance.</p>

<p>The distance itself is pretty ok, but if you want to have a confidence value in a range (0, 1), you can apply <a href=""http://en.wikipedia.org/wiki/Sigmoid_function"">sigmoidal</a> function to the result. One of such functions if logistic function.</p>

<pre><code>decision = svmob.predict(testData, true);
confidence = 1.0 / (1.0 + exp(-decision));
</code></pre>
"
4410616;61;Rohan Pota;<python-2.7><machine-learning>;27744741;1;How to resolve the following error?;"<p>I was reading the the chapter on search engines in programming collective intelligence and came across the following code snippet and tried to implement it in IPython. However, I encountered error:</p>

<pre><code>def getmatchrows(self,q):
    fieldlist='w0.urlid'
    tablelist=''
    clauselist=''
    wordids=[]

    words=q.split()
    tablenumber=0

    for word in words:
        wordrow=self.con.execute(""select rowid from wordlist where word='%s'"" % word).fetchone()
        if wordrow!=None:
            wordid=wordrow[0]
            wordids.append(wordid)
            if tablenumber&gt;0:
                tablelist+=','
                clauselist+=' and '
                clauselist+='w%d.urlid=w%d.urlid and '%(tablenumber-1,tablenumber)
            fieldlist+=',w%d.location'%tablenumber
            tablelist+='wordlocation w%d'%tablenumber
            clauselist+='w%d.wordid=%d'%(tablenumber,wordid)
            tablenumber+=1
    fullquery=""SELECT %s FROM %s WHERE %s""%(fieldlist,tablelist,clauselist)
    cur=self.con.execute(fullquery)
    rows=[row for row in cur]
    return row,wordids
</code></pre>

<p>I got the following error:</p>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-25-e5fade95f22f&gt; in &lt;module&gt;()
----&gt; 1 e.getmatchrows('data analysis')

C:\Users\Blue\Anaconda\searchengine.pyc in getmatchrows(self, q)
    128                                         tablelist+=','
    129                                         clauselist+=' and '
--&gt; 130                                         clauselist+='w%d.urlid=w%d.urlid and '%tablenumber-   1,tablenumber
131                                 fieldlist+=',w%d.location'%tablenumber
132                                 tablelist+='wordlocation w%d'%tablenumber

TypeError: unsupported operand type(s) for &amp;: 'str' and 'tuple'
</code></pre>
";27744808;4408281;715;datavinci;0;27744808;"<p>Put parantheses at line 130:</p>

<pre><code>tablenumber-1,tablenumber&gt;&gt;(tablenumber1,tablenumber)
</code></pre>
"
1243926;6046;Sergey Orshanskiy;<machine-learning><cross-validation>;27745033;0;Evaluating models on the entire training set with no cross-validation;"<p>We have a dataset with 10,000 manually labeled instances, and a classifier that was trained on all of this data.
The classifier was then evaluated on ALL of this data to obtain a 95% success rate.</p>
<p>What exactly is wrong with this approach? Is it just that the statistic 95% is not very informative in this setup? Can there still be some value in this 95% number? While I understand that, theoretically, it is not a good idea, I don't have enough experience in this area to be sure by myself. Also note that I have neither built nor evaluated the classifier in question.</p>
<p>Common sense aside, could someone give me a very solid, authoritative reference, saying that this setup is somehow wrong?</p>
<p>For example, <a href=""http://www.saedsayad.com/model_evaluation.htm"" rel=""nofollow noreferrer"">this page</a> does say</p>
<blockquote>
<p>Evaluating model performance with the data used for training is not acceptable in data mining because it can easily generate overoptimistic and overfitted models.</p>
</blockquote>
<p>However, this is hardly an authoritative reference. In fact, this quote is plainly wrong, as the evaluation has nothing to do with generating overfitted models. It could generate overoptimistic data scientists who would choose the wrong model, but a particular evaluation strategy does not have anything to do with overfitting models per se.</p>
";;1361822;16143;bogatron;1;27745227;"<p>The problem is the possibility of <a href=""http://en.wikipedia.org/wiki/Overfitting"" rel=""nofollow"">overfitting</a>. That does not mean that there is no value in the accuracy you reported for that entire data set, as it can be considered an estimate of the upper bound for the performance of the classifier on new data.</p>

<p>It is subjective to say who constitutes a ""very solid, authoritative reference""; however <em>Machine Learning</em> by Tom Mitchell (ISBN 978-0070428072) is a widely read and oft-cited text that discusses the problem of overfitting in general and specifically with regard to decision trees and artificial neural networks. In addition to discussion of overfitting, the text also discusses various approaches to the <em>training and validation set</em> approach (e.g., cross-validation).</p>
"
3455587;27;user3455587;<python><machine-learning><scikit-learn><random-forest>;27752121;0;Need Assistance In Random Forest Programming In Python;"<p>I am right now trying to make a simple program on random forest. Taking two sequences to train and predict and plot the final random forest curve.</p>
<p>But I am unable to do it as I cant understand which kind of sequence I should take and how to plot the random forest result on graph as we used to do in R language.</p>
<p>I have tried this as far as now -</p>
<pre><code>import numpy as np

from pylab import *

test=np.random.rand(1000,10)

print (test)

train=np.random.rand(1000,5)

print (train)


from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=100,n_jobs=10)

rfc.fit(test, train)
</code></pre>
<p>Kindly see the code and it would be a great help if you could correct the code and also show me how to plot for random forest result.</p>
<p>I am expecting your kind reply as soon as possible.</p>
<hr />
<p>In R language, I did this -</p>
<h1>simulate the data</h1>
<pre><code>train=rnorm(1,1000,.2)

predict=rnorm(1100,1200,.5)

df=data.frame(train, predict)
</code></pre>
<h1>run the randomForest implementation</h1>
<pre><code>library(randomForest)

rf1 &lt;- randomForest(predict~., data=df, mtry=2, ntree=500, importance=TRUE)

importance(rf1,type=1)
</code></pre>
<h1>run the party implementation</h1>
<pre><code>library(party)

cf1 &lt;- cforest(predict~.,data=df,control=cforest_unbiased(mtry=2,ntree=50))

varimp(cf1)

varimp(cf1,conditional=TRUE)
</code></pre>
<h1>plots</h1>
<pre><code>plot (rf1, log = &quot;y&quot;)
</code></pre>
<hr />
";;163740;35373;ogrisel;0;27758066;"<p>What is the expected meaning of the <code>train</code> and <code>test</code> variable?</p>

<p>THe <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit"" rel=""nofollow"">documentation</a> of <code>RandomForestClassifier.fit</code> tells that for a classifier you need to pass class labels for the second argument (named <code>y</code> in the documentation). This can be either integer values (on integer per possible class) or a list of string labels.</p>

<p>Also <code>fit</code> is expected to be called with training data only (training set input features and training set labels) so passing a variable named <code>test</code> is really confusion.</p>

<p>Please start by following one of the tutorials of scikit-learn to understand how to train a classifier with that library:</p>

<ul>
<li><a href=""http://scikit-learn.org/stable/documentation.html"" rel=""nofollow"">http://scikit-learn.org/stable/documentation.html</a></li>
</ul>

<p>then read the documentation of the random forests in particular:</p>

<ul>
<li><a href=""http://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees"" rel=""nofollow"">http://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees</a></li>
</ul>

<p>If you want to compute the variable importances, read this section in particular:</p>

<ul>
<li><a href=""http://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation"" rel=""nofollow"">http://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation</a></li>
</ul>
"
4140027;3985;tumbleweed;<python><pandas><machine-learning><nlp><scikit-learn>;27753168;0;Wrong prediction with SVC classifier in scikit-learn?;"<p>I generated my own corpus, so I split into a training text file like this:</p>

<pre><code>POS|This film was awesome, highly recommended
NEG|I did not like this film
NEU|I went to the movies
POS|this film is very interesting, i liked a lot
NEG|the film was very boring i did not like it
NEU|the cinema is big
NEU|the cinema was dark
</code></pre>

<p>And for testing I have another text review, which is unlabeled:</p>

<pre><code>I did not like this film
</code></pre>

<p>Then I do the following: </p>

<pre><code>import pandas as pd
from sklearn.feature_extraction.text import HashingVectorizer

trainingdata = pd.read_csv('/Users/user/Desktop/training.txt',
                 header=None, sep='|', names=['labels', 'movies_reviews'])


vect = HashingVectorizer(analyzer='word', ngram_range=(2,2), lowercase=True, n_features=7)
X = vect.fit_transform(trainingdata['movies_reviews'])
y = trainingdata['labels']
TestText= pd.read_csv('/Users/user/Desktop/testing.txt',
                     header=None, names=['test_opinions'])
test = vect.transform(TestText['test_opinions'])
from sklearn.svm import SVC
svm = SVC()
svm.fit(X, y)

prediction = svm.predict(test)
print prediction
</code></pre>

<p>And the prediction is:</p>

<pre><code>['NEU']
</code></pre>

<p>Then something that comes to my mind is why this prediction is wrong?. Does this is a code problem or a feature or a classification algorithm problem?, I tried to play with this and when I remove the last review from the training text file I realize that always is predicting the last element of that file. Any idea of how to fix this problem?.</p>
";;419338;13702;mbatchkarov;1;27753656;"<p>SVMs are notoriously sensitive to parameter settings. You will need to do a grid search to find the right values. I tried training two kinds of Naive Bayes on your dataset and I got perfect accuracy on the training set:</p>

<pre><code>from sklearn.naive_bayes import *
from sklearn.feature_extraction.text import *

# first option- Gaussian NB
vect = HashingVectorizer(analyzer='word', ngram_range=(2,2), lowercase=True)
X = vect.fit_transform(trainingdata['movies_reviews'])
y = trainingdata['labels']
nb = GaussianNB().fit(X.A,y) # input needs to be dense
nb.predict(X.A) == y

# second option- MultinomialNB (input needs to be positive, use CountingVect instead)
vect = CountVectorizer(analyzer='word', ngram_range=(2,2), lowercase=True)
X = vect.fit_transform(trainingdata['movies_reviews'])
y = trainingdata['labels']
nb = MultinomialNB().fit(X,y)
nb.predict(X.A) == y
</code></pre>

<p>In both cases the output is</p>

<pre><code>Out[33]: 
0    True
1    True
2    True
3    True
4    True
5    True
6    True
Name: labels, dtype: bool
</code></pre>
"
713087;2949;codious;<python><macos><machine-learning><nlp><stanford-nlp>;27758540;0;Stanford pos tagger not displaying the output elements in Python (MAC);"<pre><code>#-*- coding:Utf-8 -*-
import sys
reload(sys)
sys.setdefaultencoding('utf8')
import os
java_path = ""/usr/libexec/java_home"" # replace this
os.environ['JAVAHOME'] = java_path

from nltk.tag.stanford import POSTagger

french_postagger = POSTagger(""stanford-postagger-full-2014-10-26/models/french.tagger"", ""stanford-postagger-full-2014-10-26/stanford-postagger.jar"", encoding=""utf-8"")
english_postagger = POSTagger(""stanford-postagger-full-2014-10-26/models/english-bidirectional-distsim.tagger"", ""stanford-postagger-full-2014-10-26/stanford-postagger.jar"", encoding=""utf-8"")


print french_postagger.tag(""siddhartha is a good boy"".split())
</code></pre>

<p>the result is as follows:</p>

<pre><code>[('', u'/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home')]
</code></pre>

<p>instead I need to see the words and their tags.</p>
";;1716866;4741;leekaiinthesky;0;29487046;"<p>The problem is this part of your code:</p>

<pre><code>java_path = ""/usr/libexec/java_home"" # replace this
os.environ['JAVAHOME'] = java_path
</code></pre>

<p>Where did that code from? It looks like you should replace it. If your setup is like mine, changing that first line to <code>java_path = ""/usr/bin/java""</code> fixes the problem. Actually, if your setup is like mine, just <strong>deleting those two lines completely</strong> fixes the problem (while including them reproduces it):</p>

<pre><code>from nltk.tag.stanford import POSTagger
french_postagger = POSTagger(""models/french.tagger"", ""stanford-postagger.jar"", encoding=""utf-8"")
english_postagger = POSTagger(""models/english-bidirectional-distsim.tagger"", ""stanford-postagger.jar"", encoding=""utf-8"")

print french_postagger.tag(""siddhartha is a good boy"".split())
&gt; [[(u'siddhartha', u'ADV'), (u'is', u'VPP'), (u'a', u'V'), (u'good', u'ET'), (u'boy', u'ET')]]
</code></pre>
"
4416307;1367;new_with_python;<python><python-2.7><machine-learning><nlp><scikit-learn>;27761803;1;Problems loading textual data with scikit-learn?;"<p>I'm using my own data to classify into two categories some data, so let:</p>

<pre><code>from sklearn.datasets import load_files
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

# Load the text data
categories = [
    'CLASS_1',
    'CLASS_2',
]

text_train_subset = load_files('train',
    categories=categories)

text_test_subset = load_files('test',
    categories=categories)

# Turn the text documents into vectors of word frequencies
vectorizer = CountVectorizer()
X_train = vectorizer.fit_transform(text_train_subset)
y_train = text_train_subset.target


classifier = MultinomialNB().fit(X_train, y_train)
print(""Training score: {0:.1f}%"".format(
    classifier.score(X_train, y_train) * 100))

# Evaluate the classifier on the testing set
X_test = vectorizer.transform(text_test_subset.data)
y_test = text_test_subset.target
print(""Testing score: {0:.1f}%"".format(
    classifier.score(X_test, y_test) * 100))
</code></pre>

<p>For the above code and the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html"" rel=""nofollow"">documentation</a>, I have the following directory schema:</p>

<pre><code>data_folder/

    train_folder/
        CLASS_1.txt CLASS_2.txt
    test_folder/
        test.txt
</code></pre>

<p>Then I get this error: </p>

<pre><code>    % (size, n_samples))
ValueError: Found array with dim 0. Expected 5
</code></pre>

<p>I also tried fit_transform but still the same. How can I solve this dimession problem?</p>
";27764579;419338;13702;mbatchkarov;3;27764579;"<p>The first problem is you've got the wrong directory structure. <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html"" rel=""nofollow"">You need it to be like</a> </p>

<pre><code>container_folder/
    CLASS_1_folder/
        file_1.txt, file_2.txt ... 
    CLASS_2_folder/
        file_1.txt, file_2.txt, ....
</code></pre>

<p>You need to have both the train and test set in this directory structure. Alternatively, you can have all data in one directory and use <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html"" rel=""nofollow"">train_test_split</a> to split it in two.</p>

<p>Secondly, </p>

<pre><code>X_train = vectorizer.fit_transform(text_train_subset)
</code></pre>

<p>needs to be </p>

<pre><code>X_train = vectorizer.fit_transform(text_train_subset.data) # added .data
</code></pre>

<p>Here is a complete and working example:</p>

<pre><code>from sklearn.datasets import load_files
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

text_train_subset = load_files('sample-data/web')
text_test_subset = text_train_subset # load your actual test data here

# Turn the text documents into vectors of word frequencies
vectorizer = CountVectorizer()
X_train = vectorizer.fit_transform(text_train_subset.data)
y_train = text_train_subset.target


classifier = MultinomialNB().fit(X_train, y_train)
print(""Training score: {0:.1f}%"".format(
    classifier.score(X_train, y_train) * 100))

# Evaluate the classifier on the testing set
X_test = vectorizer.transform(text_test_subset.data)
y_test = text_test_subset.target
print(""Testing score: {0:.1f}%"".format(
    classifier.score(X_test, y_test) * 100))
</code></pre>

<p>The directory structure of <code>sample-data/web</code> is</p>

<pre><code>sample-data/web
â”œâ”€â”€ de
â”‚Â Â  â”œâ”€â”€ apollo8.txt
â”‚Â Â  â”œâ”€â”€ fiv.txt
â”‚Â Â  â”œâ”€â”€ habichtsadler.txt
â””â”€â”€ en
    â”œâ”€â”€ elizabeth_needham.txt
    â”œâ”€â”€ equipartition_theorem.txt
    â”œâ”€â”€ sunderland_echo.txt
    â””â”€â”€ thespis.txt
</code></pre>
"
1413388;1605;Cjoerg;<ruby><machine-learning><cluster-analysis><k-means><hierarchical-clustering>;27771043;0;How to make one-dimensional k-means clustering using Ruby?;"<p><strong>My question:</strong></p>

<p>I have searched through available Ruby gems to find one that performs k-means clustering. I've found quite a few: <a href=""http://github.com/id774/kmeans"" rel=""nofollow noreferrer"">kmeans</a>, <a href=""https://github.com/vaneyckt/kmeans-clustering"" rel=""nofollow noreferrer"">kmeans-clustering</a>, <a href=""https://github.com/reddavis/K-Means"" rel=""nofollow noreferrer"">reddavis-k_means</a> and <a href=""https://github.com/ollie/k_means_pp"" rel=""nofollow noreferrer"">k_means_pp</a>. My problem is that none of the gems deals with one-dimensional k-means clustering. They all expect input like this:</p>

<pre><code>[[1, 2], [3, 4], [5, 6]]
</code></pre>

<p>My input looks like this:</p>

<pre><code>[1, 2, 3, 4, 5, 6]
</code></pre>

<p>Hence my question: <em>How do I perform a one-dimensional k-means clustering using Ruby?</em></p>

<p><strong>The context (my task):</strong></p>

<p>I have 100 input values:</p>

<p>0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 5, 5, 5, 5, 5, 8, 8, 10, 16, 18, 22, 22, 35, 50, 50</p>

<p>Each value represents a response time, i.e. the number of minutes it took for some customer service agent to respond to an email from a customer. So the first value 0 indicates that the customer only waited 0 minutes for a response.</p>

<p>I need to find out how many fast, medium-fast and slow response time instances there is. In other words, I want to cut my input values up in 3 pools, and then count how many there are in each pool.</p>

<p>The complicating factor is that I based on the overall slope steepness have to figure out where to make the cuts. There is no fixed definition of fast, medium-fast and slow. The first cut (between fast and medium-fast) should occur where the steepness of the slope starts to increase more drastically than before. The second cut (between medium-fast and slow) should occur when an even more dramatic steepness increase occur.</p>

<p>Here is a graphical representation of the input values.</p>

<p><a href=""https://i.imgur.com/pSEJ0SV.png"" rel=""nofollow noreferrer""><img src=""https://i.imgur.com/pSEJ0SV.png"" /></a></p>

<p>In the above example, common sense would probably define fast as 0-3, because there are many instances of 0, 1, 2, and 3. 4-8 or 4-10 looks like common sense choices for medium-fast. But how to determine something like this mathematically? If the response times were generally faster, then the customers would be expecting this, so an even smaller increase towards the end should trigger the cut.</p>

<p><strong>Finishing notes:</strong></p>

<p>I did find the gem <a href=""http://github.com/davidrichards/kmeans"" rel=""nofollow noreferrer"">davidrichards-kmeans</a> that deals with one-dimensional k-means clustering, but it don't seem to work properly (the example code raises a syntax error).</p>
";27771247;1060350;70512;Has QUIT--Anony-Mousse;2;27771247;"<p>k-means is the wrong tool for this job anyway.</p>

<p>It's not designed for fitting an exponential curve.</p>

<p>Here is a much more sound proposal for you:</p>

<p>Look at the plot, mark the three points, and then you have your three groups.</p>

<p>Or look at quantiles... Report the median response time, the 90% quantile, and the 99% quantile...</p>

<p>Clustering is about <strong>structure discovery</strong> in multivariate data. It's probably not what you want it to be, sorry.</p>

<p>If you insist on trying k-means, try encoding the data as</p>

<pre><code>[[1], [2], [3], [4], [5]]
</code></pre>

<p>and check if the results are at least a little bit what you want them to be (also remember that k-means is randomized. Running it multiple times may yield very different results).</p>
"
2241766;2138;lenhhoxung;<machine-learning><classification><backpropagation>;27772237;0;Use number of misclassificatios as objective function for back propagation;"<p>I'm new to machine learning (neutral network) and I have a question, please help me explain.
In back propagation, the objective function to be minimized is usually a sum of the squared error between the output and the target. However, in classification problems, the goal is often to minimize the total number of misclassifications. Why can this total number of misclassifications not be used directly as an objective function in back propagation?</p>
";27797690;1190430;5186;Artem Sobolev;0;27797690;"<p>Because of mathematics. We'd really like to minimize number of misclassifications, but this target would be non-<a href=""http://en.wikipedia.org/wiki/Smoothness"" rel=""nofollow"">smooth</a> (and not even <a href=""http://en.wikipedia.org/wiki/Continuous_function"" rel=""nofollow"">continuous</a>), and thus hard to optimize.</p>

<p>So, in order to optimize it, we use smooth ""proxies"": sum-of-squares penalizes your mistakes in a continuous way. Namely, a (very) tiny deviation in input parameters results in a tiny change of output. This would not be the case if non-continuous target function was used. </p>

<p>Also, note that to find a misclassification you need to compare output to the actual answer. Since you can't directly compare floating numbers for equality using <code>==</code>, you'll need to tolerate some errors. And, it's really not a big problem to miss answer by <code>0.001</code> when answers are of much bigger magnitude. So you want to push predictions as close to real answers as you can, and you do so by minimizing total distance from predictions to answers.</p>
"
4202221;453;Spu;<python-3.x><machine-learning><scipy><neural-network><classification>;27780868;1;Backpropagation neural network;"<p>I need to use Backpropagation Neural Netwrok for multiclass classification purposes in my application. I have found <a href=""http://danielfrg.com/blog/2013/07/03/basic-neural-network-python/#disqus_thread"" rel=""nofollow"">this code</a> and try to adapt it to my needs. It is based on the lections of Machine Learning in Coursera from Andrew Ng. 
I have tested it in IRIS dataset and achieved good results (accuracy of classification around 0.96), whereas on my real data I get terrible results. I assume there is some implementation error, because the data is very simple. But I cannot figure out what exactly is the problem.</p>

<p>What are the parameters that it make sense to adjust?
I tried with:</p>

<ul>
<li>number of units in hidden layer</li>
<li>generalization parameter (lambda)</li>
<li>number of iterations for minimization function</li>
</ul>

<p>Built-in minimization function used in this code is pretty much confusing me. It is used just once, as @goncalopp has mentioned in comment. Shouldn't it iteratively update the weights? How it can be implemented?</p>

<p>Here is my training data (target class is in the last column):</p>

<hr>

<pre><code>65535, 3670, 65535, 3885, -0.73, 1
65535, 3962, 65535, 3556, -0.72, 1
65535, 3573, 65535, 3529, -0.61, 1
3758, 3123, 4117, 3173, -0.21, 0
3906, 3119, 4288, 3135, -0.28, 0
3750, 3073, 4080, 3212, -0.26, 0
65535, 3458, 65535, 3330, -0.85, 2
65535, 3315, 65535, 3306, -0.87, 2
65535, 3950, 65535, 3613, -0.84, 2
65535, 32576, 65535, 19613, -0.35, 3
65535, 16657, 65535, 16618, -0.37, 3
65535, 16657, 65535, 16618, -0.32, 3
</code></pre>

<p>The dependencies are so obvious, I think it should be so easy to classify it...</p>

<p>But results are terrible. I get accuracy of 0.6 to 0.8. This is absolutely inappropriate for my application. Can someone please point out possible improvements I could make in order to achieve better results.</p>

<p>Here is the code:</p>

<pre><code>import numpy as np
from scipy import optimize

from sklearn import cross_validation
from sklearn.metrics import accuracy_score
import math

class NN_1HL(object):

    def __init__(self, reg_lambda=0, epsilon_init=0.12, hidden_layer_size=25, opti_method='TNC', maxiter=500):
        self.reg_lambda = reg_lambda
        self.epsilon_init = epsilon_init
        self.hidden_layer_size = hidden_layer_size
        self.activation_func = self.sigmoid
        self.activation_func_prime = self.sigmoid_prime
        self.method = opti_method
        self.maxiter = maxiter

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def sigmoid_prime(self, z):
        sig = self.sigmoid(z)
        return sig * (1 - sig)

    def sumsqr(self, a):
        return np.sum(a ** 2)

    def rand_init(self, l_in, l_out):
        self.epsilon_init = (math.sqrt(6))/(math.sqrt(l_in + l_out))
        return np.random.rand(l_out, l_in + 1) * 2 * self.epsilon_init - self.epsilon_init

    def pack_thetas(self, t1, t2):
        return np.concatenate((t1.reshape(-1), t2.reshape(-1)))

    def unpack_thetas(self, thetas, input_layer_size, hidden_layer_size, num_labels):
        t1_start = 0
        t1_end = hidden_layer_size * (input_layer_size + 1)
        t1 = thetas[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))
        t2 = thetas[t1_end:].reshape((num_labels, hidden_layer_size + 1))
        return t1, t2

    def _forward(self, X, t1, t2):
        m = X.shape[0]
        ones = None
        if len(X.shape) == 1:
            ones = np.array(1).reshape(1,)
        else:
            ones = np.ones(m).reshape(m,1)

        # Input layer
        a1 = np.hstack((ones, X))

        # Hidden Layer
        z2 = np.dot(t1, a1.T)
        a2 = self.activation_func(z2)
        a2 = np.hstack((ones, a2.T))

        # Output layer
        z3 = np.dot(t2, a2.T)
        a3 = self.activation_func(z3)
        return a1, z2, a2, z3, a3

    def function(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):
        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)

        m = X.shape[0]
        Y = np.eye(num_labels)[y]

        _, _, _, _, h = self._forward(X, t1, t2)
        costPositive = -Y * np.log(h).T
        costNegative = (1 - Y) * np.log(1 - h).T
        cost = costPositive - costNegative
        J = np.sum(cost) / m

        if reg_lambda != 0:
            t1f = t1[:, 1:]
            t2f = t2[:, 1:]
            reg = (self.reg_lambda / (2 * m)) * (self.sumsqr(t1f) + self.sumsqr(t2f))
            J = J + reg
        return J

    def function_prime(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):
        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)

        m = X.shape[0]
        t1f = t1[:, 1:]
        t2f = t2[:, 1:]
        Y = np.eye(num_labels)[y]

        Delta1, Delta2 = 0, 0
        for i, row in enumerate(X):
            a1, z2, a2, z3, a3 = self._forward(row, t1, t2)

            # Backprop
            d3 = a3 - Y[i, :].T
            d2 = np.dot(t2f.T, d3) * self.activation_func_prime(z2)

            Delta2 += np.dot(d3[np.newaxis].T, a2[np.newaxis])
            Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis])

        Theta1_grad = (1 / m) * Delta1
        Theta2_grad = (1 / m) * Delta2

        if reg_lambda != 0:
            Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (reg_lambda / m) * t1f
            Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (reg_lambda / m) * t2f

        return self.pack_thetas(Theta1_grad, Theta2_grad)

    def fit(self, X, y):
        num_features = X.shape[0]
        input_layer_size = X.shape[1]
        num_labels = len(set(y))

        theta1_0 = self.rand_init(input_layer_size, self.hidden_layer_size)
        theta2_0 = self.rand_init(self.hidden_layer_size, num_labels)
        thetas0 = self.pack_thetas(theta1_0, theta2_0)

        options = {'maxiter': self.maxiter}
        _res = optimize.minimize(self.function, thetas0, jac=self.function_prime, method=self.method, 
                                 args=(input_layer_size, self.hidden_layer_size, num_labels, X, y, 0), options=options)

        self.t1, self.t2 = self.unpack_thetas(_res.x, input_layer_size, self.hidden_layer_size, num_labels)

        np.savetxt(""weights_t1.txt"", self.t1, newline=""\n"")
        np.savetxt(""weights_t2.txt"", self.t2, newline=""\n"")

    def predict(self, X):
        return self.predict_proba(X).argmax(0)

    def predict_proba(self, X):
        _, _, _, _, h = self._forward(X, self.t1, self.t2)
        return h


##################
# IR data        #
##################
values = np.loadtxt('infrared_data.txt', delimiter=', ', usecols=[0,1,2,3,4])

targets = np.loadtxt('infrared_data.txt', delimiter=', ', dtype=(int), usecols=[5])

X_train, X_test, y_train, y_test = cross_validation.train_test_split(values, targets, test_size=0.4)
nn = NN_1HL()
nn.fit(values, targets)
print(""Accuracy of classification: ""+str(accuracy_score(y_test, nn.predict(X_test))))
</code></pre>
";27817013;1595865;18698;loopbackbee;0;27781347;"<p>The most obvious problem is that <strong>your training dataset is very small</strong>.</p>

<p>Since you're using <code>scipy.optimize.minimize</code> instead of the usual iterative gradient descent, I think it's also likely you're <a href=""https://en.wikipedia.org/wiki/Overfitting"" rel=""nofollow""><strong>overfitting</strong> your model to your training data</a>. Possibly a iterative algorithm works better, here. Don't forget to carefully <strong>monitor the validation error</strong>.</p>

<p>If you try backpropagation with gradient descent, notice that, depending on the parameters used on backpropagation, neural networks take a while to converge</p>

<p>You can try to feed the network the same training data multiple times or tweak the <a href=""https://en.wikipedia.org/wiki/Backpropagation#Phase_2:_Weight_update"" rel=""nofollow"">learning rate</a> but ideally you should use more diverse data.</p>
"
4202221;453;Spu;<python-3.x><machine-learning><scipy><neural-network><classification>;27780868;1;Backpropagation neural network;"<p>I need to use Backpropagation Neural Netwrok for multiclass classification purposes in my application. I have found <a href=""http://danielfrg.com/blog/2013/07/03/basic-neural-network-python/#disqus_thread"" rel=""nofollow"">this code</a> and try to adapt it to my needs. It is based on the lections of Machine Learning in Coursera from Andrew Ng. 
I have tested it in IRIS dataset and achieved good results (accuracy of classification around 0.96), whereas on my real data I get terrible results. I assume there is some implementation error, because the data is very simple. But I cannot figure out what exactly is the problem.</p>

<p>What are the parameters that it make sense to adjust?
I tried with:</p>

<ul>
<li>number of units in hidden layer</li>
<li>generalization parameter (lambda)</li>
<li>number of iterations for minimization function</li>
</ul>

<p>Built-in minimization function used in this code is pretty much confusing me. It is used just once, as @goncalopp has mentioned in comment. Shouldn't it iteratively update the weights? How it can be implemented?</p>

<p>Here is my training data (target class is in the last column):</p>

<hr>

<pre><code>65535, 3670, 65535, 3885, -0.73, 1
65535, 3962, 65535, 3556, -0.72, 1
65535, 3573, 65535, 3529, -0.61, 1
3758, 3123, 4117, 3173, -0.21, 0
3906, 3119, 4288, 3135, -0.28, 0
3750, 3073, 4080, 3212, -0.26, 0
65535, 3458, 65535, 3330, -0.85, 2
65535, 3315, 65535, 3306, -0.87, 2
65535, 3950, 65535, 3613, -0.84, 2
65535, 32576, 65535, 19613, -0.35, 3
65535, 16657, 65535, 16618, -0.37, 3
65535, 16657, 65535, 16618, -0.32, 3
</code></pre>

<p>The dependencies are so obvious, I think it should be so easy to classify it...</p>

<p>But results are terrible. I get accuracy of 0.6 to 0.8. This is absolutely inappropriate for my application. Can someone please point out possible improvements I could make in order to achieve better results.</p>

<p>Here is the code:</p>

<pre><code>import numpy as np
from scipy import optimize

from sklearn import cross_validation
from sklearn.metrics import accuracy_score
import math

class NN_1HL(object):

    def __init__(self, reg_lambda=0, epsilon_init=0.12, hidden_layer_size=25, opti_method='TNC', maxiter=500):
        self.reg_lambda = reg_lambda
        self.epsilon_init = epsilon_init
        self.hidden_layer_size = hidden_layer_size
        self.activation_func = self.sigmoid
        self.activation_func_prime = self.sigmoid_prime
        self.method = opti_method
        self.maxiter = maxiter

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def sigmoid_prime(self, z):
        sig = self.sigmoid(z)
        return sig * (1 - sig)

    def sumsqr(self, a):
        return np.sum(a ** 2)

    def rand_init(self, l_in, l_out):
        self.epsilon_init = (math.sqrt(6))/(math.sqrt(l_in + l_out))
        return np.random.rand(l_out, l_in + 1) * 2 * self.epsilon_init - self.epsilon_init

    def pack_thetas(self, t1, t2):
        return np.concatenate((t1.reshape(-1), t2.reshape(-1)))

    def unpack_thetas(self, thetas, input_layer_size, hidden_layer_size, num_labels):
        t1_start = 0
        t1_end = hidden_layer_size * (input_layer_size + 1)
        t1 = thetas[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))
        t2 = thetas[t1_end:].reshape((num_labels, hidden_layer_size + 1))
        return t1, t2

    def _forward(self, X, t1, t2):
        m = X.shape[0]
        ones = None
        if len(X.shape) == 1:
            ones = np.array(1).reshape(1,)
        else:
            ones = np.ones(m).reshape(m,1)

        # Input layer
        a1 = np.hstack((ones, X))

        # Hidden Layer
        z2 = np.dot(t1, a1.T)
        a2 = self.activation_func(z2)
        a2 = np.hstack((ones, a2.T))

        # Output layer
        z3 = np.dot(t2, a2.T)
        a3 = self.activation_func(z3)
        return a1, z2, a2, z3, a3

    def function(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):
        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)

        m = X.shape[0]
        Y = np.eye(num_labels)[y]

        _, _, _, _, h = self._forward(X, t1, t2)
        costPositive = -Y * np.log(h).T
        costNegative = (1 - Y) * np.log(1 - h).T
        cost = costPositive - costNegative
        J = np.sum(cost) / m

        if reg_lambda != 0:
            t1f = t1[:, 1:]
            t2f = t2[:, 1:]
            reg = (self.reg_lambda / (2 * m)) * (self.sumsqr(t1f) + self.sumsqr(t2f))
            J = J + reg
        return J

    def function_prime(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):
        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)

        m = X.shape[0]
        t1f = t1[:, 1:]
        t2f = t2[:, 1:]
        Y = np.eye(num_labels)[y]

        Delta1, Delta2 = 0, 0
        for i, row in enumerate(X):
            a1, z2, a2, z3, a3 = self._forward(row, t1, t2)

            # Backprop
            d3 = a3 - Y[i, :].T
            d2 = np.dot(t2f.T, d3) * self.activation_func_prime(z2)

            Delta2 += np.dot(d3[np.newaxis].T, a2[np.newaxis])
            Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis])

        Theta1_grad = (1 / m) * Delta1
        Theta2_grad = (1 / m) * Delta2

        if reg_lambda != 0:
            Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (reg_lambda / m) * t1f
            Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (reg_lambda / m) * t2f

        return self.pack_thetas(Theta1_grad, Theta2_grad)

    def fit(self, X, y):
        num_features = X.shape[0]
        input_layer_size = X.shape[1]
        num_labels = len(set(y))

        theta1_0 = self.rand_init(input_layer_size, self.hidden_layer_size)
        theta2_0 = self.rand_init(self.hidden_layer_size, num_labels)
        thetas0 = self.pack_thetas(theta1_0, theta2_0)

        options = {'maxiter': self.maxiter}
        _res = optimize.minimize(self.function, thetas0, jac=self.function_prime, method=self.method, 
                                 args=(input_layer_size, self.hidden_layer_size, num_labels, X, y, 0), options=options)

        self.t1, self.t2 = self.unpack_thetas(_res.x, input_layer_size, self.hidden_layer_size, num_labels)

        np.savetxt(""weights_t1.txt"", self.t1, newline=""\n"")
        np.savetxt(""weights_t2.txt"", self.t2, newline=""\n"")

    def predict(self, X):
        return self.predict_proba(X).argmax(0)

    def predict_proba(self, X):
        _, _, _, _, h = self._forward(X, self.t1, self.t2)
        return h


##################
# IR data        #
##################
values = np.loadtxt('infrared_data.txt', delimiter=', ', usecols=[0,1,2,3,4])

targets = np.loadtxt('infrared_data.txt', delimiter=', ', dtype=(int), usecols=[5])

X_train, X_test, y_train, y_test = cross_validation.train_test_split(values, targets, test_size=0.4)
nn = NN_1HL()
nn.fit(values, targets)
print(""Accuracy of classification: ""+str(accuracy_score(y_test, nn.predict(X_test))))
</code></pre>
";27817013;4202221;453;Spu;0;27817013;"<p>Correctly normalizing the data solved the problem. I used preprocessing module from sklearn. Here is example:</p>

<pre><code>from sklearn import preprocessing
import numpy as np

X_train = np.array([[ 1., -1.,  2.],
                    [ 2.,  0.,  0.],
                    [ 0.,  1., -1.]])


min_max_scaler = preprocessing.MinMaxScaler()
X_train_minmax = min_max_scaler.fit_transform(X_train)
print(X_train_minmax)

X_test = np.array([[ -3., -1.,  4.]])
X_test_minmax = min_max_scaler.transform(X_test)
print(X_test_minmax)
</code></pre>

<p>And the output is:</p>

<pre><code>[[ 0.5     0.      1.    ]
 [ 1.      0.5     0.3333]
 [ 0.      1.      0.    ]]


[[-1.5     0.      1.6667]]
</code></pre>
"
4420776;2669;David Heckmann;<r><machine-learning><r-caret>;27784008;0;caret: RFE with variable tuneGrid;"<p>I'm trying to use caret to fit a PLS model while optimizing the number of components 'ncomps':</p>

<pre><code>library(""caret"")
set.seed(342)
train &lt;- as.data.frame ( matrix( rnorm(1e4) , 100, 100 ) )

ctrl &lt;- rfeControl(functions = caretFuncs,                                                      
                   method = ""repeatedcv"",
                   number=2, 
                   repeats=1,
                   verbose =TRUE
)

pls.fit.rfe &lt;- rfe(V1 ~ .,
                   data = train,   
                   method = ""pls"",                    
                   sizes =  6,
                   tuneGrid = data.frame(ncomp = 7), 
                   rfeControl = ctrl
)
</code></pre>

<p><em>Error in { : 
  task 1 failed - ""final tuning parameters could not be determined""
In addition: There were 50 or more warnings (use warnings() to see the first 50)</em></p>

<p><em>Invalid number of components, ncomp</em></p>

<p>Setting sizes to 6 fixes the problem. It makes sense that I get an error when min(sizes) &lt; max(ncomp), but is there a way to vary ncomp depending on the number of features used in the RFE iteration, i.e. the sizes variable? I would simply like to optimize over a wide range of sizes and #components at the same time.</p>
";27788079;1078601;11384;topepo;2;27788079;"<p>Try using <code>tuneLength = 7</code> instead of <code>tuneGrid</code>. The former is more flexible and will use an appropriate <code>ncomp</code> given the size of the data set:</p>

<pre>
> pls.fit.rfe  pls.fit.rfe

Recursive feature selection

Outer resampling method: Cross-Validated (2 fold, repeated 1 times) 

Resampling performance over subset size:

 Variables   RMSE Rsquared  RMSESD RsquaredSD Selected
         6 1.0229  0.01684 0.04192  0.0155092         
        99 0.9764  0.00746 0.01096  0.0008339        *

The top 5 variables (out of 99):
</pre>

<p>If you'd rather not do that, you can always <a href=""http://topepo.github.io/caret/rfe.html#rfe"" rel=""nofollow"">write your own</a> fit function too.</p>

<p>Max</p>
"
574187;3765;ihadanny;<python><machine-learning><scikit-learn>;27784338;0;scikit-learn metrics on a subset of classes;"<p>We're using <code>scikit-learn==0.15.2</code> and training <code>LinearSVC</code> on 9 classes and a special 'others' class. The 'others' class contain anything in our dataset which does not fit into the 9 important classes we are trying to classify. </p>

<p>We would like to get average micro/macro precision/recall/f1 metrics on only the 9 classes, without the 'others' class, in order to get a performance estimation for our classifier. </p>

<p>We've failed to find any support for that in the built-in scikit <code>metrics</code> functions. And even the <code>classification_report</code> function has an issue when trying to restrict the labels to only the 9 (<a href=""https://github.com/scikit-learn/scikit-learn/issues/3123"" rel=""nofollow"">https://github.com/scikit-learn/scikit-learn/issues/3123</a>). </p>

<p>Is this lack of support indicating that our fundamental approach isn't correct? Should we include the 'others' when we measure performance?</p>

<p>EDIT: Note that our consumer uses our predictions only when we predict one of the 9 classes. If we predict 'others' our output is thrown away and another model is used. </p>
";;2658050;56586;lejlot;2;27788866;"<p>In short <strong>yes, you should include each class</strong>. Why would you ignore the (probably the biggest) class? Even if it is just noise it is fundamental to classifiers performance to actually be able to distinguish the noise from important classes. There might be situations where you are not interested in ""others"" class (in cases when False Positives are irrelevant) but these situations are quite rare and so are not directly implemented in scikit-learn's metrics module.</p>
"
574187;3765;ihadanny;<python><machine-learning><scikit-learn>;27784338;0;scikit-learn metrics on a subset of classes;"<p>We're using <code>scikit-learn==0.15.2</code> and training <code>LinearSVC</code> on 9 classes and a special 'others' class. The 'others' class contain anything in our dataset which does not fit into the 9 important classes we are trying to classify. </p>

<p>We would like to get average micro/macro precision/recall/f1 metrics on only the 9 classes, without the 'others' class, in order to get a performance estimation for our classifier. </p>

<p>We've failed to find any support for that in the built-in scikit <code>metrics</code> functions. And even the <code>classification_report</code> function has an issue when trying to restrict the labels to only the 9 (<a href=""https://github.com/scikit-learn/scikit-learn/issues/3123"" rel=""nofollow"">https://github.com/scikit-learn/scikit-learn/issues/3123</a>). </p>

<p>Is this lack of support indicating that our fundamental approach isn't correct? Should we include the 'others' when we measure performance?</p>

<p>EDIT: Note that our consumer uses our predictions only when we predict one of the 9 classes. If we predict 'others' our output is thrown away and another model is used. </p>
";;4370183;371;klubow;1;27793532;"<p>Why not use confusion matrix <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"" rel=""nofollow"">http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html</a> ? </p>

<p>Based on that matrix you can create your own metrics</p>
"
2443048;25;user2443048;<python><python-2.7><machine-learning><scikit-learn>;27789555;1;Issues with Pipelining in sklearn;"<p>I am brand new to sklearn. I am using Pipeline to use Vectorizer and Classifier together in a Text mining problem. Here is my code:</p>

<pre><code>def create_ngram_model():
tfidf_ngrams = TfidfVectorizer(ngram_range=(1, 3),
analyzer=""word"", binary=False)
clf = GaussianNB()
pipeline = Pipeline([('vect', tfidf_ngrams), ('clf', clf)])
return pipeline


def get_trains():
    data=open('../cleaning data/cleaning the sentences/cleaned_comments.csv','r').readlines()[1:]
    lines=len(data)
    features_train=[]
    labels_train=[]
    for i in range(lines):
        l=data[i].split(',')
        labels_train+=[int(l[0])]
        a=l[2]
        features_train+=[a]
    return features_train,labels_train

def train_model(clf_factory,features_train,labels_train):
    features_train,labels_train=get_trains()
    features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features_train, labels_train, test_size=0.1, random_state=42)
    clf=clf_factory()
    clf.fit(features_train,labels_train)
    pred = clf.predict(features_test)
    accuracy = accuracy_score(pred,labels_test)
    return accuracy

X,Y=get_trains()
print train_model(create_ngram_model,X,Y)
</code></pre>

<p>The features returned from get_trains() are strings.
I am getting this error.</p>

<pre><code>clf.fit(features_train,labels_train)
  File ""C:\Python27\lib\site-packages\sklearn\pipeline.py"", line 130, in fit
    self.steps[-1][-1].fit(Xt, y, **fit_params)
  File ""C:\Python27\lib\site-packages\sklearn\naive_bayes.py"", line 149, in fit
    X, y = check_arrays(X, y, sparse_format='dense')
  File ""C:\Python27\lib\site-packages\sklearn\utils\validation.py"", line 263, in check_arrays
    raise TypeError('A sparse matrix was passed, but dense '
TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.
</code></pre>

<p>I have come across this error many times. Then, I just changed the features to features_transformed.toarray() but since, here, I am using pipeline I am not able to do so as the transformed feature is returned automatically. I also tried making a new class which returns the features_transformed.toarray() but that too throwed the same error.
I have searched a lot but not getting it. Please help!!</p>
";27796481;1190430;5186;Artem Sobolev;0;27796481;"<p>There are 2 options:</p>

<ol>
<li><p>Use sparse-data-compatible classifier. For example, documentation says <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB"" rel=""nofollow"">Bernoulli Naive Bayes</a> and <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB"" rel=""nofollow"">Multinomial Naive Bayes</a> support sparse input for <code>fit</code>.</p></li>
<li><p>Add a ""densifier"" to the Pipeline. Apparently, you got it wrong, this one worked for me (when I needed to densify my sparse data along the way):</p>

<pre><code>class Densifier(object):
    def fit(self, X, y=None):
        pass
    def fit_transform(self, X, y=None):
        return self.transform(X)
    def transform(self, X, y=None):
        return X.toarray()
</code></pre>

<p>Make sure to put into pipeline it right before classificator.</p></li>
</ol>
"
3958112;83;Peter Li;<matlab><machine-learning><bayesian>;27790562;-2;"How to understand the ""calculate the priors based on occurence in the training set"" in the function";"<p>I have a function from a toolbox, I paste it here. I cannot understand the last part,
which begins from ""<strong><code>% // calculate the priors based on occurence in the training set</code></strong>"" ? Can
anybody explain it for me? Thank you so much!</p>

<pre><code>function [scratch] = train_gnb(trainpats,traintargs, in_args, cv_args)

% // Use a Gaussian Naive Bayes classifier to learn regressors.
%
% // [SCRATCH] = TRAIN_GNB(TRAINPATS, TRAINTARGS, IN_ARGS, CV_ARGS)
%
% // The Gaussian Naive Bayes classifier makes the assumption that
% // each data point is conditionally independent of the others, given
% // a class label, and that, furthermore, the likelihood function for
% // each class is normal.  The likelihood of a given data point X,
% // where Y is one of K labels, is thus:
%
% // Pr ( X | Y==K) = Product_N ( Normal(X_N | theta_K) ) 
% 
% // The GNB is trained by finding the Normal MLE's for each subset of
% // the training set that have the same label.  Each voxel has a
% // scalar mean and a scalar variance.
%
% // OPTIONAL ARGUMENTS:
%
% // UNIFORM_PRIOR (default = true): If uniform_prior is true,
% // then the algorithm will assume that no classes are
% // inherently more likely than others, and will use 1/K as
% // the prior probability for each of K classes.  If
% // uniform_prior is false, then train_gnb will estimate the
% // priors from the data using laplace smoothing: if N_k is
% // the number of times class k is observed in the training
% // set and N is the total number of training datapoints, then
% // Pr(Y == k) = (N_k + 1) / (N + K).  This way, no cluster is
% // ever assigned a 0 prior.

% // License:
% // =====================================================================
%
% // This is part of the Princeton MVPA toolbox, released under
% // the GPL. See http://www.csbmb.princeton.edu/mvpa for more
% // information.
% 
%  // The Princeton MVPA toolbox is available free and
% // unsupported to those who might find it useful. We do not
% // take any responsibility whatsoever for any problems that
% // you have related to the use of the MVPA toolbox.
%
% // ======================================================================

defaults.uniform_prior = true;

args = mergestructs(in_args, defaults);

nConds = size(traintargs,1);
[nVox nTimepoints] = size(trainpats);

% // find a gaussian distribution for each voxel for each category

scratch.mu = NaN(nVox, nConds);
scratch.sigma = NaN(nVox, nConds);

for k = 1:nConds

  % // grab the subset of the data with a label of category k
    k_idx = find(traintargs(k, :) == 1);

    if numel(k_idx) &lt; 1
      error('Condition %g has no data points.', k);
    end

    data = trainpats(:, k_idx);

    % calculate the maximum likelihood estimators (mean and variance)
    [ mu_hat, sigma_hat] = normfit(data');

    scratch.mu(:,k) = mu_hat;
    scratch.sigma(:,k) = sigma_hat;

end

% // calculate the priors based on occurence in the training set
scratch.prior = NaN(nConds, 1);
if (args.uniform_prior)
  scratch.prior = ones(nConds,1) / nConds;
else

  for k = 1:nConds  
    scratch.prior(k) = (1 + numel( find(traintargs(k, :) == 1))) / ...
        (nConds + nTimepoints);    
  end

end
</code></pre>
";27790969;4280372;1595;chipaudette;3;27790969;"<p>The ""prior"" is the ""prior distribution"", which is the distribution describing the likelihood of each class.  This is relevant when it comes time to look at a new data point and, based on your training data, to decide which class it is.  If you know a priori that one class is more likely to occur than another class, it will affect the decision on the class to which the new point belongs.</p>

<p>A common assumption for the prior distribution  is a ""uniform prior"" which means that, when you go to test a new data point, we assume that each class is as like as likely to occur as any other class.  A uniform prior is a good assumption, but may not model the data very well.  </p>

<p>A better model would be to assume that your training data is a good representation of all data.  You then measure the distribution of each class in your training data.  This becomes your prior.</p>

<p>So, back to your example code, your question is about the section of code that defines the prior.  This section of code is described in the block comments at the top of your code.  See the section that reads:</p>

<pre><code>% UNIFORM_PRIOR (default = true): If uniform_prior is true,
% then the algorithm will assume that no classes are
% inherently more likely than others, and will use 1/K as
% the prior probability for each of K classes.  If
% uniform_prior is false, then train_gnb will estimate the
% priors from the data using laplace smoothing: if N_k is
% the number of times class k is observed in the training
% set and N is the total number of training datapoints, then
% Pr(Y == k) = (N_k + 1) / (N + K).  This way, no cluster is
% ever assigned a 0 prior.
</code></pre>

<p>In the code itself, you see the initial <code>if (args.uniform_prior)</code> which determines whether you're assuming the uniform prior or not....</p>

<p>If you <em>are</em> assuming a uniform prior, then the line <code>scratch.prior = ones(nConds,1) / nConds;</code> sets the prior to all the same value...ie, a uniform distribution.  Apparently the number of classes is defined by <code>nConds</code> so that the likelihood of a new data point being in any one class is basically <code>1 / nConds</code>.</p>

<p>If you are <em>not</em> assuming a uniform prior, the <code>for</code> loop goes through your training data and counts the number of occurrences of each class...via the portion of the line <code>numel( find(traintargs(k, :) == 1))</code>.  The rest of this line of code normalizes and smooths this value, using (I guess) the laplace smoothing technique discussed in the block comment at the top.</p>

<p>I hope that this helps!</p>

<p>Chip</p>
"
3423045;978;Deepak;<opencv><machine-learning><svm><libsvm>;27792564;1;How Feature length depend on prediction in SVM classifier;"<p>Currently I am doing English alphabet classification using SVM classifier in opencv.
I have following doubts in doing above thing</p>

<ol>
<li><p>How length of feature vector depends on the classification ?
(What will happen if feature length increases (my current feature length is 125))</p></li>
<li><p>Is time taken for prediction depend on number of data used for training ? </p></li>
<li><p>Why we need normalization of feature vector (will this improve accuracy of prediction and time required for the prediction of the class) ?</p></li>
<li><p>How to determine best method for normalizing feature vector ?</p></li>
</ol>
";27793215;4370183;371;klubow;2;27793215;"<p>1) Length of features does not matter per se, what matters is predictive quality of features</p>

<p>2) No, it does not depend on number of samples, but it depends on number of features (prediction is generally very fast)</p>

<p>3) Normalization is required if features are in very different ranges of values</p>

<p>4) There are basically standarization (mean, stdev) and scaling (xmax -> +1, xmean -> -1 or 0) - you could do both and see which one is better</p>
"
3423045;978;Deepak;<opencv><machine-learning><svm><libsvm>;27792564;1;How Feature length depend on prediction in SVM classifier;"<p>Currently I am doing English alphabet classification using SVM classifier in opencv.
I have following doubts in doing above thing</p>

<ol>
<li><p>How length of feature vector depends on the classification ?
(What will happen if feature length increases (my current feature length is 125))</p></li>
<li><p>Is time taken for prediction depend on number of data used for training ? </p></li>
<li><p>Why we need normalization of feature vector (will this improve accuracy of prediction and time required for the prediction of the class) ?</p></li>
<li><p>How to determine best method for normalizing feature vector ?</p></li>
</ol>
";27793215;1858151;3363;stefan;2;28947067;"<p>when talking about classification the data consists of feature vectors with a number of features. in image processing there is also features which are mapped to classification feature vectors. so your ""feature length"" is actually the number of features or feature vector size.</p>

<p>1) the number of features matter. in principle more features allow better classification but also lead to overtraining. to avoid the latter you can add more samples (more feature vectors).</p>

<p>2) yes, as the prediction time depends on the number of support vectors and the size of the support vectors. but as prediction is very fast this is not an issue unless you have some real time requirements.</p>

<p>3) while SVM as a maximum margin classifier is quite robust against different feature value ranges a feature with a bigger value range would have more weight than one with a smaller range. this especially applies to penalty calculation if classes are not completely separable.</p>

<p>4) as SVM is quite robust against different value ranges (compared to cluster oriented algorithms) this is not the biggest issue. typically absolute min/max are scaled to -1/+1. if you know the expected range of your data you could scale that range and measurement errors in your data would not influence the scaling. a fixed range is also preferable when adding trraining data in an iterative process.</p>
"
3515225;453;Gunjan naik;<opencv><image-processing><machine-learning><pattern-matching>;27793348;10;How to define our own kernel for Pattern recognition in OPENCV?;"<p>I wanted to write my own kernel for Image classification on OpenCV.</p>

<p>But for SVM (Built in function for Opencv) the kernel is already defined.</p>

<p>My question is, is there anything in OpenCV that would allow me to define my kernel?</p>

<p>Actually,I wanted to implement multiple kernel learning for Image classification.</p>
";51817940;1698143;1886;Failed Scientist;2;51817940;"<p>I have looked around an answer and one of the workaround (mentioned in <a href=""https://stackoverflow.com/questions/9752402/build-a-custom-svm-kernel-matrix-with-opencv"">linked question</a> as well) is to use altenative SVM libraries like LibSVM, etc. (and LibSVM is really good one).</p>

<p>Though, if you want to stay in OpenCV only (and thats what appears by your question), then there is a <a href=""http://answers.opencv.org/question/67129/loadsave-svm-custom-kernel/"" rel=""nofollow noreferrer"">similar question posted on OpenCV forums</a> and somehow workaround is (copying verbatim):</p>

<blockquote>
  <p>This message means that SVM with custom kernel does not support loading from file. You can try following things:</p>
  
  <ol>
  <li><p>Use one of the standard kernels (obviously)</p></li>
  <li><p>Implement your kernel as standard and optionally contribute it to the mainline</p></li>
  <li><p>Set kernel to standard before saving to file and back to custom after loading from file (workaround)</p></li>
  <li><p>Implement mechanism for saving/loading custom kernels with parameters (can be hard)</p></li>
  </ol>
</blockquote>
"
4202221;453;Spu;<python-3.x><numpy><machine-learning><classification><knn>;27798767;0;Load data from file and normalize;"<p>How to normalize data loaded from file? Here what I have. Data looks kind of like this:</p>

<pre><code>65535, 3670, 65535, 3885, -0.73, 1
65535, 3962, 65535, 3556, -0.72, 1
</code></pre>

<p>Last value in each line is a target. I want to have the same structure of the data but with normalized values.</p>

<pre><code>import numpy as np
dataset = np.loadtxt('infrared_data.txt', delimiter=',')

# select first 5 columns as the data
X = dataset[:, 0:5]

# is that correct? Should I normalize along 0 axis?
normalized_X = preprocessing.normalize(X, axis=0)

y = dataset[:, 5]
</code></pre>

<p>Now the question is, how to pack correctly <code>normalized_X</code> and <code>y</code> back, that it has the structure:</p>

<pre><code>dataset = [[normalized_X[0], y[0]],[normalized_X[1], y[1]],...]
</code></pre>
";27802886;325565;237547;Joe Kington;1;27802886;"<p>It sounds like you're asking for <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.column_stack.html"" rel=""nofollow""><code>np.column_stack</code></a>.  For example, let's set up some dummy data:</p>

<pre><code>import numpy as np
x = np.arange(25).reshape(5, 5)
y = np.arange(5) + 1000
</code></pre>

<p>Which gives us:</p>

<pre><code>X:
array([[ 0,  1,  2,  3,  4],
       [ 5,  6,  7,  8,  9],
       [10, 11, 12, 13, 14],
       [15, 16, 17, 18, 19],
       [20, 21, 22, 23, 24]])
Y:
array([1000, 1001, 1002, 1003, 1004])
</code></pre>

<p>And we want:</p>

<pre><code>new = np.column_stack([x, y])
</code></pre>

<p>Which gives us:</p>

<pre><code>New:
array([[   0,    1,    2,    3,    4, 1000],
       [   5,    6,    7,    8,    9, 1001],
       [  10,   11,   12,   13,   14, 1002],
       [  15,   16,   17,   18,   19, 1003],
       [  20,   21,   22,   23,   24, 1004]])
</code></pre>

<p>If you'd prefer less typing, you can also use:</p>

<pre><code>In [4]: np.c_[x, y]
Out[4]:
array([[   0,    1,    2,    3,    4, 1000],
       [   5,    6,    7,    8,    9, 1001],
       [  10,   11,   12,   13,   14, 1002],
       [  15,   16,   17,   18,   19, 1003],
       [  20,   21,   22,   23,   24, 1004]])
</code></pre>

<p>However, I'd discourage using <code>np.c_</code> for anything other than interactive use, simply due to readability concerns.</p>
"
2130541;730;nkhuyu;<python-2.7><machine-learning><parameter-passing><scikit-learn><cross-validation>;27810855;19;(Python - sklearn) How to pass parameters to the customize ModelTransformer class by gridsearchcv;"<p>Below is my pipeline and it seems that I can't pass the parameters to my models by using the ModelTransformer class, which I take it from the link (<a href=""http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html"">http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html</a>)</p>

<p>The error message makes sense to me, but I don't know how to fix this. Any idea how to fix this? Thanks.</p>

<pre><code># define a pipeline
pipeline = Pipeline([
('vect', DictVectorizer(sparse=False)),
('scale', preprocessing.MinMaxScaler()),
('ess', FeatureUnion(n_jobs=-1, 
                     transformer_list=[
     ('rfc', ModelTransformer(RandomForestClassifier(n_jobs=-1, random_state=1,  n_estimators=100))),
     ('svc', ModelTransformer(SVC(random_state=1))),],
                     transformer_weights=None)),
('es', EnsembleClassifier1()),
])

# define the parameters for the pipeline
parameters = {
'ess__rfc__n_estimators': (100, 200),
}

# ModelTransformer class. It takes it from the link
(http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html)
class ModelTransformer(TransformerMixin):
    def __init__(self, model):
        self.model = model
    def fit(self, *args, **kwargs):
        self.model.fit(*args, **kwargs)
        return self
    def transform(self, X, **transform_params):
        return DataFrame(self.model.predict(X))

grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, refit=True)
</code></pre>

<p>Error Message:
ValueError: Invalid parameter n_estimators for estimator ModelTransformer.</p>
";27817446;1190430;5186;Artem Sobolev;21;27817446;"<p><code>GridSearchCV</code> has a special naming convention for nested objects. In your case <code>ess__rfc__n_estimators</code> stands for <code>ess.rfc.n_estimators</code>, and, according to the definition of the <code>pipeline</code>, it points to the property <code>n_estimators</code> of </p>

<pre><code>ModelTransformer(RandomForestClassifier(n_jobs=-1, random_state=1,  n_estimators=100)))
</code></pre>

<p>Obviously, <code>ModelTransformer</code> instances don't have such property.</p>

<p>The fix is easy: in order to access underlying object of <code>ModelTransformer</code> one needs to use <code>model</code> field. So, grid parameters become</p>

<pre><code>parameters = {
  'ess__rfc__model__n_estimators': (100, 200),
}
</code></pre>

<p><strong>P.S.</strong> it's not the only problem with your code. In order to use multiple jobs in GridSearchCV, you need to make all objects you're using copy-able. This is achieved by implementing methods <code>get_params</code> and <code>set_params</code>, you can borrow them from <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html#sklearn.base.BaseEstimator"" rel=""noreferrer""><code>BaseEstimator</code></a> mixin.</p>
"
4202221;453;Spu;<python><machine-learning><scipy><neural-network>;27815580;2;Use of scipy.optimize.minimize in Neural Network;"<p>Trying to use Backpropagation Neural Network for multiclass classification. I have found <a href=""http://danielfrg.com/blog/2013/07/03/basic-neural-network-python/#disqus_thread"" rel=""nofollow"">this code</a> and try to adapt it. It is based on the lections of <a href=""http://sun.stanford.edu/~couvidat/MachineLearning/ex4.pdf"" rel=""nofollow"">Machine Learning in Coursera from Andrew Ng</a>. </p>

<p>I don't understand exactly the implementation of <code>scipy.optimize.minimize</code> function here. It is used just once in the code. Is it iteratively updating the weights of the network? How can I visualize  (plot) it's performance to see when it converges?</p>

<p>Using this function what parameters I can adjust to achieve better performance? I found <a href=""http://en.wikibooks.org/wiki/Artificial_Neural_Networks/Neural_Network_Basics"" rel=""nofollow"">here</a> a list common parameters:</p>

<ul>
<li>Number of neurons in the hidden layer: this is <code>hidden_layer_size=25</code> in my code</li>
<li><strong>Learning rate: can I still adjust that using built-in minimization function?</strong></li>
<li><strong>Momentum:</strong> is that <code>reg_lambda=0</code> in my case? Regularization parameter to avoid overfitting, right?</li>
<li>Epoch: <code>maxiter=500</code></li>
</ul>

<p>Here is my training data (target class is in the last column):</p>

<hr>

<pre><code>65535, 3670, 65535, 3885, -0.73, 1
65535, 3962, 65535, 3556, -0.72, 1
65535, 3573, 65535, 3529, -0.61, 1
3758, 3123, 4117, 3173, -0.21, 0
3906, 3119, 4288, 3135, -0.28, 0
3750, 3073, 4080, 3212, -0.26, 0
65535, 3458, 65535, 3330, -0.85, 2
65535, 3315, 65535, 3306, -0.87, 2
65535, 3950, 65535, 3613, -0.84, 2
65535, 32576, 65535, 19613, -0.35, 3
65535, 16657, 65535, 16618, -0.37, 3
65535, 16657, 65535, 16618, -0.32, 3
</code></pre>

<p>The dependencies are so obvious, I think it should be so easy to classify it...</p>

<p>But results are terrible. I get accuracy of 0.6 to 0.8. This is absolutely inappropriate for my application. I know I need more data normally, but I would be already happy when I could at least fit the training data (without taking into account potential overfitting)</p>

<p>Here is the code:</p>

<pre><code>import numpy as np
from scipy import optimize

from sklearn import cross_validation
from sklearn.metrics import accuracy_score
import math

class NN_1HL(object):

    def __init__(self, reg_lambda=0, epsilon_init=0.12, hidden_layer_size=25, opti_method='TNC', maxiter=500):
        self.reg_lambda = reg_lambda
        self.epsilon_init = epsilon_init
        self.hidden_layer_size = hidden_layer_size
        self.activation_func = self.sigmoid
        self.activation_func_prime = self.sigmoid_prime
        self.method = opti_method
        self.maxiter = maxiter

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def sigmoid_prime(self, z):
        sig = self.sigmoid(z)
        return sig * (1 - sig)

    def sumsqr(self, a):
        return np.sum(a ** 2)

    def rand_init(self, l_in, l_out):
        self.epsilon_init = (math.sqrt(6))/(math.sqrt(l_in + l_out))
        return np.random.rand(l_out, l_in + 1) * 2 * self.epsilon_init - self.epsilon_init

    def pack_thetas(self, t1, t2):
        return np.concatenate((t1.reshape(-1), t2.reshape(-1)))

    def unpack_thetas(self, thetas, input_layer_size, hidden_layer_size, num_labels):
        t1_start = 0
        t1_end = hidden_layer_size * (input_layer_size + 1)
        t1 = thetas[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))
        t2 = thetas[t1_end:].reshape((num_labels, hidden_layer_size + 1))
        return t1, t2

    def _forward(self, X, t1, t2):
        m = X.shape[0]
        ones = None
        if len(X.shape) == 1:
            ones = np.array(1).reshape(1,)
        else:
            ones = np.ones(m).reshape(m,1)

        # Input layer
        a1 = np.hstack((ones, X))

        # Hidden Layer
        z2 = np.dot(t1, a1.T)
        a2 = self.activation_func(z2)
        a2 = np.hstack((ones, a2.T))

        # Output layer
        z3 = np.dot(t2, a2.T)
        a3 = self.activation_func(z3)
        return a1, z2, a2, z3, a3

    def function(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):
        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)

        m = X.shape[0]
        Y = np.eye(num_labels)[y]

        _, _, _, _, h = self._forward(X, t1, t2)
        costPositive = -Y * np.log(h).T
        costNegative = (1 - Y) * np.log(1 - h).T
        cost = costPositive - costNegative
        J = np.sum(cost) / m

        if reg_lambda != 0:
            t1f = t1[:, 1:]
            t2f = t2[:, 1:]
            reg = (self.reg_lambda / (2 * m)) * (self.sumsqr(t1f) + self.sumsqr(t2f))
            J = J + reg
        return J

    def function_prime(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):
        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)

        m = X.shape[0]
        t1f = t1[:, 1:]
        t2f = t2[:, 1:]
        Y = np.eye(num_labels)[y]

        Delta1, Delta2 = 0, 0
        for i, row in enumerate(X):
            a1, z2, a2, z3, a3 = self._forward(row, t1, t2)

            # Backprop
            d3 = a3 - Y[i, :].T
            d2 = np.dot(t2f.T, d3) * self.activation_func_prime(z2)

            Delta2 += np.dot(d3[np.newaxis].T, a2[np.newaxis])
            Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis])

        Theta1_grad = (1 / m) * Delta1
        Theta2_grad = (1 / m) * Delta2

        if reg_lambda != 0:
            Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (reg_lambda / m) * t1f
            Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (reg_lambda / m) * t2f

        return self.pack_thetas(Theta1_grad, Theta2_grad)

    def fit(self, X, y):
        num_features = X.shape[0]
        input_layer_size = X.shape[1]
        num_labels = len(set(y))

        theta1_0 = self.rand_init(input_layer_size, self.hidden_layer_size)
        theta2_0 = self.rand_init(self.hidden_layer_size, num_labels)
        thetas0 = self.pack_thetas(theta1_0, theta2_0)

        options = {'maxiter': self.maxiter}
        _res = optimize.minimize(self.function, thetas0, jac=self.function_prime, method=self.method, 
                                 args=(input_layer_size, self.hidden_layer_size, num_labels, X, y, 0), options=options)

        self.t1, self.t2 = self.unpack_thetas(_res.x, input_layer_size, self.hidden_layer_size, num_labels)

        np.savetxt(""weights_t1.txt"", self.t1, newline=""\n"")
        np.savetxt(""weights_t2.txt"", self.t2, newline=""\n"")

    def predict(self, X):
        return self.predict_proba(X).argmax(0)

    def predict_proba(self, X):
        _, _, _, _, h = self._forward(X, self.t1, self.t2)
        return h


##################
# IR data        #
##################
values = np.loadtxt('infrared_data.txt', delimiter=', ', usecols=[0,1,2,3,4])

targets = np.loadtxt('infrared_data.txt', delimiter=', ', dtype=(int), usecols=[5])

X_train, X_test, y_train, y_test = cross_validation.train_test_split(values, targets, test_size=0.4)
nn = NN_1HL()
nn.fit(values, targets)
print(""Accuracy of classification: ""+str(accuracy_score(y_test, nn.predict(X_test))))
</code></pre>
";27818014;1190430;5186;Artem Sobolev;1;27818014;"<p>In the given code <a href=""http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize"" rel=""nofollow""><code>scipy.optimize.minimize</code></a> iteratively minimizes function given it's derivative (Jacobi's matrix). According to the documentation, use can specify <code>callback</code> argument to a function that will be called after each iteration â€” this will let you measure performance, though I'm not sure if it'll let you halt the optimization process.</p>

<p>All parameters you listed are hyperparameters, it's hard to optimize them directly:</p>

<p><em>Number of neurons in the hidden layer</em> is a discrete valued parameters, and, thus, is not optimizable via gradient techniques. Moreover, it affects NeuralNet architecture, so you can't optimize it while training the net. What you can do, though, is to use some higher-level routine to search for possible options, like exhaustive grid search with cross-validation (for example look at <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV"" rel=""nofollow"">GridSearchCV</a>) or other tools for hyperparameter search (<a href=""https://github.com/hyperopt/hyperopt"" rel=""nofollow"">hyperopt</a>, <a href=""https://github.com/JasperSnoek/spearmint"" rel=""nofollow"">spearmint</a>, <a href=""https://github.com/Yelp/MOE"" rel=""nofollow"">MOE</a>, etc).</p>

<p><em>Learning rate</em> does not seem to be customizable for most of the optimization methods available. But, actually, learning rate in gradient descent is just a Newton's method with Hessian ""approximated"" by <code>1 / eta I</code> â€” diagonal matrix with inverted learning rates on the major diagonal. So you can try hessian-based methods with this heuristic.</p>

<p><em>Momentum</em> is completely unrelated to regularization. It's an optimization technique, and, since you use scipy for optimization, is unavailable for you.</p>
"
4202221;453;Spu;<machine-learning><artificial-intelligence><normalization>;27819923;4;Do I need to normalize targets for Neural Network?;"<p>I use backpropagation neural network for multiclass classification.</p>

<p>My data looks like this</p>

<pre><code>65535, 8710, 55641, 5396, 23.6056640625
65535, 8600, 65535, 5305, 10.0318359375
64539, 8664, 65535, 5305, 11.0232421875 
65535, 8674, 65535, 5257, 21.962109375
32018, 8661, 65535, 5313, 2.8986328125
35569, 8665, 65535, 5289, 2.8494140624999997
23652, 8656, 65535, 5260, 22.4806640625
42031, 8551, 65535, 5239, 2.7298828125
65535, 8573, 65535, 5232, 10.3728515625
</code></pre>

<p>Before I feed it to the network I scale the data to be in the range [0,1]</p>

<p>And the targets are:</p>

<pre><code>[0, 1, 1, 0, 2, 2, 0, 2, 1]
</code></pre>

<p>Do I need to normalize targets to be in the range [0,1]?</p>
";27820348;1190430;5186;Artem Sobolev;4;27820348;"<p>Normalizing targets makes sense only in <a href=""http://en.wikipedia.org/wiki/Regression_analysis"" rel=""nofollow"">regression</a> problems when your Network is asked to predict a (possibly, vector of) real value(s).</p>

<p>In your case targets are too ""round"" and, apparently, class indicators. Thus, solving regression problem would be incorrect and you need to go with <a href=""http://en.wikipedia.org/wiki/Statistical_classification"" rel=""nofollow"">classification</a> instead. Normalizing targets would be a total disaster in that case: you'd make targets incomparable (since limitations of computer floating arithmetic do not allow us to compare floats for equality) and will not simplify NN's (or any other ML algorthm's) work because numerical values of these classes aren't used at all.</p>
"
4154133;327;Dider;<machine-learning><chess>;27824546;2;Machine Learning applied to chess tutoring software;"<p>This may belong in the chess SE community but I am looking at the question from a programming perspective rather than pedagogical or even chess perspective.</p>

<p>I know of several studies and attempts to create chess engines which use some variant of machine learning to <strong>play</strong> chess (most of which are usually studies in the subject, rather than attempts to trump the brute force method, which is so far superior to other methods), but few attempts to apply machine learning to chess pedagogy. </p>

<p>One of the main reasons for a chess coach/tutor is the personalized attention and direction that the tutor provides. Is it then possible to create a chess program which uses machine learning to generate personalized ""lessons"" for the user based on their strengths and weaknesses?</p>

<p>The lessons need not be complex, even generating relevant positions from a database and asking the user to ""solve"" them, then giving a line or variation in response to an answer (correct or incorrect) is a great deal of instruction (for, even without explanations, the variations can often suffice)</p>

<p>The main questions are:</p>

<ol>
<li>How would the software be able to guage the user's skill level? (This is really where the ML algorithm would have to come in)</li>
<li>How can the software determine the difficulty or ""appropriateness"" of a test position? e.g suppose the software determines that the user has difficulty with tactical positions, (an issue most amateur players have) how can the program choose a position (from its database of games, lets suppose) with approriate tactical difficulty?</li>
<li>Finally, how will the software percieve and adapt to improvement by the user?</li>
</ol>

<p>I apologize if this question is to abstract or theoritical for SO, if so, I will move it elsewhere.</p>

<p>Thanks </p>
";;1832418;2150;Mr. Concolato;1;27824747;"<p>I would start by having the chess program dump out statistics to a CSV or JSON file showing:</p>

<blockquote>
  <ol>
  <li>Which pieces were moved and how often</li>
  <li>How many moves it took until a checkmate took place</li>
  <li>How many games over what period of time.</li>
  <li>How many pieces captured over time. etc</li>
  </ol>
</blockquote>

<p>You have a tremendous amount of flexibility over picking you data points of interest or features that will be used to train your ML algorithm. Once you have these data points together and a data file that can be made available to your algorithm, you can begin to train it and see what predictive results you get. Then will need to tweak your experiment until you get results that are indeed useful. </p>

<p>Here is a Python based Random Forrest algorithm along with <a href=""https://www.kaggle.com/c/digit-recognizer/forums/t/2299/getting-started-python-sample-code-random-forest"" rel=""nofollow"">a tutorial</a> to get you started:</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier
from numpy import genfromtxt, savetxt

def main():
    #create the training &amp; test sets, skipping the header row with [1:]
    dataset = genfromtxt(open('Data/train.csv','r'), delimiter=',', dtype='f8')[1:]    
    target = [x[0] for x in dataset]
    train = [x[1:] for x in dataset]
    test = genfromtxt(open('Data/test.csv','r'), delimiter=',', dtype='f8')[1:]

    #create and train the random forest
    #multi-core CPUs can use: rf = RandomForestClassifier(n_estimators=100, n_jobs=2)
    rf = RandomForestClassifier(n_estimators=100)
    rf.fit(train, target)

    savetxt('Data/submission2.csv', rf.predict(test), delimiter=',', fmt='%f')

if __name__==""__main__""
</code></pre>

<p>This could be quite interesting when you get it going. Getting the program to begin to anticipate weaknesses in a player would depend on what data you choose to collect. </p>

<p>Good luck.</p>
"
1115169;2518;pbu;<python><pandas><machine-learning><missing-data>;27824954;8;How to handle missing NaNs for machine learning in python;"<p><strong>How to handle missing values in datasets before applying machine learning algorithm??.</strong> </p>

<p>I noticed that it is not a smart thing to drop missing NAN values. I usually do interpolate (compute mean) using pandas and fill it up the data which is kind of works and improves the classification accuracy but may not be the best thing to do.</p>

<p>Here is a very important question. <strong>What is the best way to handle missing values in data set?</strong></p>

<p>For example if you see this dataset, only 30% has original data.</p>

<pre><code>Int64Index: 7049 entries, 0 to 7048
Data columns (total 31 columns):
left_eye_center_x            7039 non-null float64
left_eye_center_y            7039 non-null float64
right_eye_center_x           7036 non-null float64
right_eye_center_y           7036 non-null float64
left_eye_inner_corner_x      2271 non-null float64
left_eye_inner_corner_y      2271 non-null float64
left_eye_outer_corner_x      2267 non-null float64
left_eye_outer_corner_y      2267 non-null float64
right_eye_inner_corner_x     2268 non-null float64
right_eye_inner_corner_y     2268 non-null float64
right_eye_outer_corner_x     2268 non-null float64
right_eye_outer_corner_y     2268 non-null float64
left_eyebrow_inner_end_x     2270 non-null float64
left_eyebrow_inner_end_y     2270 non-null float64
left_eyebrow_outer_end_x     2225 non-null float64
left_eyebrow_outer_end_y     2225 non-null float64
right_eyebrow_inner_end_x    2270 non-null float64
right_eyebrow_inner_end_y    2270 non-null float64
right_eyebrow_outer_end_x    2236 non-null float64
right_eyebrow_outer_end_y    2236 non-null float64
nose_tip_x                   7049 non-null float64
nose_tip_y                   7049 non-null float64
mouth_left_corner_x          2269 non-null float64
mouth_left_corner_y          2269 non-null float64
mouth_right_corner_x         2270 non-null float64
mouth_right_corner_y         2270 non-null float64
mouth_center_top_lip_x       2275 non-null float64
mouth_center_top_lip_y       2275 non-null float64
mouth_center_bottom_lip_x    7016 non-null float64
mouth_center_bottom_lip_y    7016 non-null float64
Image                        7049 non-null object
</code></pre>
";27825523;2547739;5540;Paul Lo;12;27825523;"<pre><code>What is the best way to handle missing values in data set?
</code></pre>

<p>There is NO best way, each solution/algorithm has their own pros and cons (and you can even mix some of them together to create your own strategy and tune the related parameters to come up one best satisfy your data, there are many research/papers about this topic).  </p>

<p>For example, <strong>Mean Imputation</strong> is quick and simple, but it would underestimate the variance and the distribution shape is distorted by replacing NaN with the mean value, while <strong>KNN Imputation</strong> might not be ideal in a large data set in terms of time complexity, since it iterate over all the data points and perform calculation for each NaN value, and the assumption is that NaN attribute is correlated with other attributes.   </p>

<pre><code>How to handle missing values in datasets before applying machine learning algorithm??
</code></pre>

<p>In addition to <em>mean imputation</em> you mention, you could also take a look at <em>K-Nearest Neighbor Imputation</em> and <em>Regression Imputation</em>, and refer to the powerful <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html"" rel=""noreferrer"">Imputer</a> class in <a href=""http://scikit-learn.org/stable/"" rel=""noreferrer"">scikit-learn</a> to check existing APIs to use.</p>

<p><strong>KNN Imputation</strong></p>

<p>Calculate the mean of k nearest neighbors of this NaN point.</p>

<p><strong>Regression Imputation</strong></p>

<p>A regression model is estimated to predict observed values of a variable based on other variables, and that model is then used to impute values in cases where that variable is missing.</p>

<p><a href=""http://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values"" rel=""noreferrer"">Here</a> links to scikit's '<em>Imputation of missing values'</em> section. 
I have also heard of <a href=""http://orange.biolab.si/docs/latest/reference/rst/Orange.feature.imputation.html"" rel=""noreferrer"">Orange</a> library for imputation, but haven't had a chance to use it yet. </p>
"
1115169;2518;pbu;<python><pandas><machine-learning><missing-data>;27824954;8;How to handle missing NaNs for machine learning in python;"<p><strong>How to handle missing values in datasets before applying machine learning algorithm??.</strong> </p>

<p>I noticed that it is not a smart thing to drop missing NAN values. I usually do interpolate (compute mean) using pandas and fill it up the data which is kind of works and improves the classification accuracy but may not be the best thing to do.</p>

<p>Here is a very important question. <strong>What is the best way to handle missing values in data set?</strong></p>

<p>For example if you see this dataset, only 30% has original data.</p>

<pre><code>Int64Index: 7049 entries, 0 to 7048
Data columns (total 31 columns):
left_eye_center_x            7039 non-null float64
left_eye_center_y            7039 non-null float64
right_eye_center_x           7036 non-null float64
right_eye_center_y           7036 non-null float64
left_eye_inner_corner_x      2271 non-null float64
left_eye_inner_corner_y      2271 non-null float64
left_eye_outer_corner_x      2267 non-null float64
left_eye_outer_corner_y      2267 non-null float64
right_eye_inner_corner_x     2268 non-null float64
right_eye_inner_corner_y     2268 non-null float64
right_eye_outer_corner_x     2268 non-null float64
right_eye_outer_corner_y     2268 non-null float64
left_eyebrow_inner_end_x     2270 non-null float64
left_eyebrow_inner_end_y     2270 non-null float64
left_eyebrow_outer_end_x     2225 non-null float64
left_eyebrow_outer_end_y     2225 non-null float64
right_eyebrow_inner_end_x    2270 non-null float64
right_eyebrow_inner_end_y    2270 non-null float64
right_eyebrow_outer_end_x    2236 non-null float64
right_eyebrow_outer_end_y    2236 non-null float64
nose_tip_x                   7049 non-null float64
nose_tip_y                   7049 non-null float64
mouth_left_corner_x          2269 non-null float64
mouth_left_corner_y          2269 non-null float64
mouth_right_corner_x         2270 non-null float64
mouth_right_corner_y         2270 non-null float64
mouth_center_top_lip_x       2275 non-null float64
mouth_center_top_lip_y       2275 non-null float64
mouth_center_bottom_lip_x    7016 non-null float64
mouth_center_bottom_lip_y    7016 non-null float64
Image                        7049 non-null object
</code></pre>
";27825523;481326;410;Alex Rubinsteyn;2;35049578;"<p>There's no single best way to deal with missing data. The most rigorous approach is to model the missing values as additional parameters in a probabilistic framework like PyMC. This way you'll get a distribution over possible values, instead of just a single answer. Here's an example of dealing with missing data using PyMC: <a href=""http://stronginference.com/missing-data-imputation.html"" rel=""nofollow"">http://stronginference.com/missing-data-imputation.html</a></p>

<p>If you really want to plug those holes with point estimates, then you're looking to perform ""imputation"". I'd steer away from simple imputation methods like mean-filling since they really butcher the joint distribution of your features. Instead, try something like <a href=""https://web.stanford.edu/~hastie/swData/softImpute/vignette.html"" rel=""nofollow"">softImpute</a> (which tries you infer the missing value via low-rank approximation). The original version of softImpute is written for R but I've made a Python version (along with other methods like kNN imputation) here: <a href=""https://github.com/hammerlab/fancyimpute"" rel=""nofollow"">https://github.com/hammerlab/fancyimpute</a></p>
"
991607;1165;Jeyhun Karimov;<hadoop><mapreduce><machine-learning><cluster-computing>;27829629;1;Hadoop Mapreduce count distinct vector elements for big data;"<p>I have data consisting of <code>n-length</code> vector of <code>integer/real</code> numbers. Data is typically in GB level and feature size of a vector is more than 100. I want to count distinct elements of every vector feature. For example if I have data like:</p>

<pre><code>1.13211 22.33 1.00 ... 311.66
1.13211 44.44 4.52 ... 311.66
1.55555 22.33 5.11 ... 311.66
</code></pre>

<p>I want the result like <code>(2,2,3,...,1)</code> just one vector. Since there is 2 distinct value in first feature of a vector, 2 distinct value in second feature and etc.</p>

<p>The way I think to do it with mapreduce is , to send the values from mapper (""$val$+{feature_vector_num}"",1). For example like <code>(1.13211+1,1) or (2.33+2,1)</code>. And in reducer just sum them up and probably the second mapper and reducer to wrap up the all reducer results from previous step. </p>

<p>The problem is that, if I have data of size N, with my solution, its size sent to reducer will be 
<code>|V| * N</code> in worst case,(<code>|V|</code> is the length of feature vector) and this is also the number of reducers and number of keys at the same time. Therefore for big data, this is quite a bad solution.</p>

<p>Do you have any suggessions?
Thanks</p>
";;1969563;1195;bendaizer;2;27845504;"<p>Without considering any implementation detail (MapReduce or not), I would do it in 2 steps with a hashtable per feature (probably in Redis).</p>

<p>The first step would list all values and corresponding counts.</p>

<p>The second would then run through each vector and see if the element is unique or not in the hastable. If you have some margin for error, and want a light memory footprint, I would even go with a bloom filter.</p>

<p>The two steps are trivially parallelized.</p>
"
991607;1165;Jeyhun Karimov;<hadoop><mapreduce><machine-learning><cluster-computing>;27829629;1;Hadoop Mapreduce count distinct vector elements for big data;"<p>I have data consisting of <code>n-length</code> vector of <code>integer/real</code> numbers. Data is typically in GB level and feature size of a vector is more than 100. I want to count distinct elements of every vector feature. For example if I have data like:</p>

<pre><code>1.13211 22.33 1.00 ... 311.66
1.13211 44.44 4.52 ... 311.66
1.55555 22.33 5.11 ... 311.66
</code></pre>

<p>I want the result like <code>(2,2,3,...,1)</code> just one vector. Since there is 2 distinct value in first feature of a vector, 2 distinct value in second feature and etc.</p>

<p>The way I think to do it with mapreduce is , to send the values from mapper (""$val$+{feature_vector_num}"",1). For example like <code>(1.13211+1,1) or (2.33+2,1)</code>. And in reducer just sum them up and probably the second mapper and reducer to wrap up the all reducer results from previous step. </p>

<p>The problem is that, if I have data of size N, with my solution, its size sent to reducer will be 
<code>|V| * N</code> in worst case,(<code>|V|</code> is the length of feature vector) and this is also the number of reducers and number of keys at the same time. Therefore for big data, this is quite a bad solution.</p>

<p>Do you have any suggessions?
Thanks</p>
";;4233586;3677;yurgis;1;27852220;"<p>I would agree with lejlot is that 1GB would be much more optimally solvable using other means (e.g. memory algorithms such as hash map) and not with m/r.</p>
<p>However in case if your problem is 2-3+ orders of magnitude larger, or if you just want to practice with m/r, here is one of the possible solutions:</p>
<h1>Phase 1</h1>
<h2>Mapper</h2>
<h3>Params:</h3>
<ul>
<li>Input key: irrelevant (for TextInputFormat I think it is LongWritable that
represents a position in a file but you can just use Writable)</li>
<li>Input value: a single line with vector components separated by space (1.13211 22.33 1.00 ... 311.66)</li>
<li>Output key: a pair &lt;IntWritable, DoubleWritable&gt;
where IntWritable holds an index of the component, and DoubleWritable holds a value of the component.
Google for hadoop examples, specifically, SecondarySort.java which demonstrates how to implement a pair of IntWritable. You just need to rewrite this using DoubleWritable as a second component.</li>
<li>Output value: irrelevant, you can use NullWritable</li>
</ul>
<h3>Mapper Function</h3>
<ul>
<li>Tokenize the value</li>
<li>For each token, emit &lt;IntWritable, DoubleWritable&gt; key (you can create a custom writable pair class for that) and NullWritable value</li>
</ul>
<h2>Reducer</h2>
<p>The framework will call your reducer with &lt;IntWritable, DoubleWritable&gt; pair as keys, only one time for each key variation, effectively making dedupe. For example,  &lt;1, 1.13211&gt; key will come only once.</p>
<h3>Params</h3>
<ul>
<li>Input Key: Pair &lt;IntWritable, DoubleWritable&gt;</li>
<li>Input Value: Irrelevant (Writable or NullWritable)</li>
<li>Output Key: IntWritable (component index)</li>
<li>Output Value: IntWritable (count corresponding to the index)</li>
</ul>
<h3>Reducer Setup</h3>
<ul>
<li>initialize int[] counters array of size equal to your vector dimension.</li>
</ul>
<h3>Reducer Function</h3>
<ul>
<li>get an index from key.getFirst()</li>
<li>increment count for the index: counters[index]++</li>
</ul>
<h3>Reducer Cleanup</h3>
<ul>
<li>for each count in counters array emit, index of the array as a key, and value of the counter.</li>
</ul>
<h1>Phase 2</h1>
<p>This one is trivial and only needed if you have multiple reducers in the first phase. In this case the counts calculated above are partial.
You need to combine the outputs of your multiple reducers into a single output.
You need to set up a single-reducer job, where your reducer will just accumulate counts for corresponding indices.</p>
<h2>Mapper</h2>
<p>NO-OP</p>
<h2>Reducer</h2>
<h3>Params</h3>
<ul>
<li>Input key: IntWritable (position)</li>
<li>Input value: IntWritable (partial count)</li>
<li>Output key: IntWritable (position)</li>
<li>Output value: IntWritable (total count)</li>
</ul>
<h3>Reducer Function</h3>
<ul>
<li>for each input key
<ul>
<li>int counter = 0</li>
<li>iterate over the values
<ul>
<li>counter += value</li>
</ul>
</li>
<li>emit input key (as a key) and counter (as a value)</li>
</ul>
</li>
</ul>
<p>The resulting output file &quot;part-r-00000&quot; should have N records, where each record is a pair of values (position and distinct count) sorted by position.</p>
"
123991;1134;Sid;<machine-learning><simplecv><orange>;27830657;0;Hough Transform SimpleCV Feature Extractor;"<p>I'm trying to build a new SimpleCV FeatureExtractor for openCV's Hough Circle Transform but I'm running into an error during my machine learning script's training phase.</p>

<p>I've provided the error below. It is raised by the Orange machine learning library when creating the <code>self.mDataSetOrange</code> variable within SimpleCV's TreeClassifier.py. The size of the dataset does not match Orange's expectation for some reason. I looked into Orange's source code and the found that error is thrown <a href=""https://github.com/biolab/orange/blob/5eaa02a1ae167c7563b10d78ff15539886af6789/source/orange/cls_example.cpp#L43"" rel=""nofollow"">here</a>:</p>

<p>orange/source/orange/cls_example.cpp</p>

<pre><code>int const nvars = dom-&gt;variables-&gt;size() + dom-&gt;classVars-&gt;size();
if (Py_ssize_t(nvars) != PyList_Size(lst)) {
    PyErr_Format(PyExc_IndexError, ""invalid list size (got %i, expected %i items)"",
        PyList_Size(lst), nvars);
    return false;
}
</code></pre>

<p>Obviously, my feature extractor is not extracting the things as required by Orange but I can't pinpoint what the problem could be. I'm pretty new to SimpleCV and Orange so I'd be grateful if someone could point out any mistakes I'm making.</p>

<p>The error:</p>

<pre><code>Traceback (most recent call last):
  File ""MyClassifier.py"", line 113, in &lt;module&gt;
    MyClassifier.run(MyClassifier.TRAIN_RUN_TYPE, trainingPaths)
  File ""MyClassifier.py"", line 39, in run
    self.decisionTree.train(imgPaths, MyClassifier.CLASSES, verbose=True)
  File ""/usr/local/lib/python2.7/dist-packages/SimpleCV-1.3-py2.7.egg/SimpleCV/MachineLearning/TreeClassifier.py"", line 282, in train
    self.mDataSetOrange = orange.ExampleTable(self.mOrangeDomain,self.mDataSetRaw)
IndexError: invalid list size (got 266, expected 263 items) (at example 2)
</code></pre>

<p>HoughTransformFeatureExtractor.py</p>

<pre><code>class HoughTransformFeatureExtractor(FeatureExtractorBase):

    def extract(self, img):
        bitmap = img.getBitmap()
        cvMat = cv.GetMat(bitmap)
        cvImage = numpy.asarray(cvMat)

        height, width = cvImage.shape[:2]
        gray = cv2.cvtColor(cvImage, cv2.COLOR_BGR2GRAY)

        circles = cv2.HoughCircles(gray, cv2.cv.CV_HOUGH_GRADIENT, 2.0, width / 2)
        self.featuresLen = 0

        if circles is not None:
            circleFeatures = circles.ravel().tolist()
            self.featuresLen = len(circleFeatures)

            return circleFeatures
        else:
            return None

    def getFieldNames(self):
        retVal = []
        for i in range(self.featuresLen):
            name = ""Hough""+str(i)
            retVal.append(name)
        return retVal

    def getNumFields(self):
        return self.featuresLen
</code></pre>
";;123991;1134;Sid;0;27831442;"<p>So, I figured out my issue. Basically, the problem was with the size of list returned by the <code>extract</code> method. The size of the list varied for each processed image, which is what led to this error. So, here are some examples of the type of lists returned by the <code>extract</code> method:</p>

<pre><code>3 -&gt; [74.0, 46.0, 14.866068840026855]
3 -&gt; [118.0, 20.0, 7.071067810058594]
6 -&gt; [68.0, 8.0, 8.5440034866333, 116.0, 76.0, 13.03840446472168]
3 -&gt; [72.0, 44.0, 8.602325439453125]
9 -&gt; [106.0, 48.0, 15.81138801574707, 20.0, 52.0, 23.409399032592773, 90.0, 122.0, 18.0]
</code></pre>

<p>Once I made sure that the size of the list was consistent, no matter the image, the error went away. Hopefully, this will help anyone having similar issues in the future.</p>
"
2483127;11389;jeff;<matlab><machine-learning><data-mining><projection><pca>;27830783;1;How to find projection matrix for PCA in MATLAB?;"<p>I'm trying to reduce the dimensionality of my data with PCA.</p>

<p>So I call <code>[COEFF, SCORE] = princomp(data);</code> According to <a href=""https://stackoverflow.com/a/12689269/2483127"">this answer</a>, I can reconstruct my data with <code>SCORE * COEFF' + Mean</code>, and it works.</p>

<p>But I'm trying to find the projection matrix <code>P</code>, where any given vector <code>x</code> can be transformed to its projection in PCA space.</p>

<p>My intuition tells me that I should be able to project <code>x</code> by : </p>

<pre><code>proj = ((x-m) * inv(C)) + m
</code></pre>

<p>where <code>m</code> is the mean of my data.</p>

<p>so I test this by choosing <code>x</code> as the first observation of my data, and I expect <code>proj</code> should be very close to the first row of <code>SCORE</code>. However this is not the case.</p>

<p>So where am I doing wrong? And how can I find the projection matrix?</p>

<p>Thanks for any help!</p>
";27830910;2483127;11389;jeff;0;27830910;"<p>Oops, now I see my mistake.</p>

<p>First of all, COEFF is orthogonal (not sure) so <code>inv(COEFF) == COEFF'</code></p>

<p>and the projection is found by</p>

<pre><code>proj = COEFF' * (x-m)
</code></pre>
"
2039562;183;Shaun;<math><machine-learning><differentiation>;27831209;0;What are some error functions that take into account the sign of the error?;"<p>The mean square error squares the error as a result all the errors become positive. The information pertaining to the positivity or negetivity of the error is lost. Are there any error functions that take into account the sign of the error?</p>
";27834902;1190430;5186;Artem Sobolev;0;27834902;"<p>You can design a model-specific loss function if you want to penalize underestimations more than overestimations (and vice versa) or even to tolerate some underestimations.</p>

<p>This can be achieved with the help of piecewise-linear loss function, for example. Then you'll have to use some (sub)gradient optimization routine in order to learn a model.</p>
"
3306125;205;user3306125;<python><machine-learning><scikit-learn>;27836146;2;How to find new features in sklearn?;"<p>Does sk-learn have any method that finds new features by aggregating already existing features in dataset?
I mean something like this one: foobar=foo/bar</p>
";;1330293;33731;elyase;1;27840427;"<p>The only thing that comes to mind is <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures"" rel=""nofollow"">PolynomialFeatures</a>:</p>

<blockquote>
  <p>if an input sample is two dimensional and of the form [a, b], the
  degree-2 polynomial features are [1, a, b, a^2, ab, b^2]</p>
</blockquote>

<p>Example:</p>

<pre><code>&gt;&gt;&gt; X = np.array([[0, 1],
                  [2, 3],
                  [4, 5]])
&gt;&gt;&gt; poly = PolynomialFeatures(2)
&gt;&gt;&gt; poly.fit_transform(X)
array([[ 1,  0,  1,  0,  0,  1],
       [ 1,  2,  3,  4,  6,  9],
       [ 1,  4,  5, 16, 20, 25]])
</code></pre>
"
3782614;510;Haseeb Ahmed;<c#><machine-learning>;27837396;-4;Can't understand this part of C# code;"<p>This part of code has been taken from <a href=""http://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/Infer.NET%20Learners%20-%20Bayes%20Point%20Machine%20classifiers%20-%20Introduction.aspx"" rel=""nofollow"">this tutorial</a> regarding using Learners in Infer.NET library. I'm planning to use it for machine learning but unfortunately i can't get this code fragment. Please help me out.</p>

<pre><code>/// &lt;summary&gt;
/// A mapping for the Bayes Point Machine classifier tutorial.
/// &lt;/summary&gt;
public class ClassifierMapping 
    : IClassifierMapping&lt;IList&lt;Vector&gt;, int, IList&lt;string&gt;, string, Vector&gt;
{
    public IEnumerable&lt;int&gt; GetInstances(IList&lt;Vector&gt; featureVectors)
    {
        for (int instance = 0; instance &lt; featureVectors.Count; instance++)
        {
            yield return instance;
        }
    }

    public Vector GetFeatures(int instance, IList&lt;Vector&gt; featureVectors)
    {
        return featureVectors[instance];
    }

    public string GetLabel(
        int instance, IList&lt;Vector&gt; featureVectors, IList&lt;string&gt; labels)
    {
        return labels[instance];
    }

    public IEnumerable&lt;string&gt; GetClassLabels(
        IList&lt;Vector&gt; featureVectors = null, IList&lt;string&gt; labels = null)
    {
        return new[] { ""Female"", ""Male"" };
    }
}
</code></pre>

<p>Thank you in advance!</p>
";27837774;360211;50944;weston;2;27837774;"<p>The only remotely complicated bit is the <code>yield return</code>:</p>

<pre><code>public IEnumerable&lt;int&gt; GetInstances(IList&lt;Vector&gt; featureVectors)
{
    for (int instance = 0; instance &lt; featureVectors.Count; instance++)
    {
        yield return instance;
    }
}
</code></pre>

<p>Which could be replaced with:</p>

<pre><code>public IEnumerable&lt;int&gt; GetInstances(IList&lt;Vector&gt; featureVectors)
{
    var result = new List&lt;int&gt;();
    for (int instance = 0; instance &lt; featureVectors.Count; instance++)
    {
       result.Add(instance);
    }
    return result;
}
</code></pre>
"
3782614;510;Haseeb Ahmed;<c#><machine-learning>;27837396;-4;Can't understand this part of C# code;"<p>This part of code has been taken from <a href=""http://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/Infer.NET%20Learners%20-%20Bayes%20Point%20Machine%20classifiers%20-%20Introduction.aspx"" rel=""nofollow"">this tutorial</a> regarding using Learners in Infer.NET library. I'm planning to use it for machine learning but unfortunately i can't get this code fragment. Please help me out.</p>

<pre><code>/// &lt;summary&gt;
/// A mapping for the Bayes Point Machine classifier tutorial.
/// &lt;/summary&gt;
public class ClassifierMapping 
    : IClassifierMapping&lt;IList&lt;Vector&gt;, int, IList&lt;string&gt;, string, Vector&gt;
{
    public IEnumerable&lt;int&gt; GetInstances(IList&lt;Vector&gt; featureVectors)
    {
        for (int instance = 0; instance &lt; featureVectors.Count; instance++)
        {
            yield return instance;
        }
    }

    public Vector GetFeatures(int instance, IList&lt;Vector&gt; featureVectors)
    {
        return featureVectors[instance];
    }

    public string GetLabel(
        int instance, IList&lt;Vector&gt; featureVectors, IList&lt;string&gt; labels)
    {
        return labels[instance];
    }

    public IEnumerable&lt;string&gt; GetClassLabels(
        IList&lt;Vector&gt; featureVectors = null, IList&lt;string&gt; labels = null)
    {
        return new[] { ""Female"", ""Male"" };
    }
}
</code></pre>

<p>Thank you in advance!</p>
";27837774;3047225;274;amuz;0;27839779;"<p>This is creating a Mapping class which is implementing <code>IClassifierMapping</code> interface. </p>

<pre><code>http://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/Infer.NET%20Learners%20-%20Bayes%20Point%20Machine%20classifiers%20-%20API%20-%20Mappings%20-%20Standard%20Data%20Format%20Mapping.aspx
</code></pre>

<p>There are other mapping interfaces as well as can be seen here.</p>

<pre><code>http://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/Infer.NET%20Learners%20-%20Bayes%20Point%20Machine%20classifiers%20-%20API%20-%20Mappings.aspx
</code></pre>

<p>With this mapping class, you can now create a Bayes Point Machine classifier.</p>

<p>Regarding the complexity of the code, you can look at westons answer. </p>
"
2518644;4895;user123;<python><machine-learning><gearman>;27844279;0;Is it possible to run two different gearman client/worker on same port?;"<p>I have typical scenario, where I want to put two different worker in listening mode (trained using some data), and want to call from different client.</p>

<pre><code>for(100 times)
    w1 (listening) &lt;- c1 will call 
    result1
    w2 (listening) &lt;- c2 will call 
    result2
    compare (result1 and result2)
</code></pre>

<p>w1,w2 are trained using machine learning algo, on <strong>different data</strong> so takes time complete their process. So they need to be kept in listening mode, could not bear starting every time. </p>

<p>I tried to run them on different port, 4730 and 4731. But i guess it work on only one port, 4730.</p>

<p>client :</p>

<pre><code>class client_(object):
    def __init__(self):
        self.gm_client = gearman.GearmanClient(['localhost:4730'])    

    def callWorker(self, query):
        c_result = {}
        completed_job_request = self.gm_client.submit_job(""db_worker"", query)
        c_result =  completed_job_request.result
        c_result = json.loads(c_result)
</code></pre>

<p>worker : </p>

<pre><code>def __init__(self):
    self.gm_worker = gearman.GearmanWorker(['localhost:4730'])
    self.gm_worker.register_task('db_worker', self.testClassifier)
    self.root_dir = os.getcwd()
    self.trainClassifier()
</code></pre>

<p>How to manage this scenario? let me know if it is not clear.</p>
";27894420;3240686;497;drolando;1;27894420;"<p>You must also run the gearmand server!
The Client/Worker library cannot be used to send directly messages from a client to the workers. The jobs are sent to the server, stored in in-memory queues and delivered to the corresponding worker when it becomes available.</p>

<p>4730 is the default port used by the gearmand server.</p>

<p>You have to download gearmand with apt-get or from <a href=""https://launchpad.net/gearmand"" rel=""nofollow"">https://launchpad.net/gearmand</a> and start it.
The argument that you pass to the client and worker is the hostname and port of the server. The server uses the string passed to the submit_job and register_task (in your case ""db_worker"") to match the job and the corresponding workers.</p>

<p>This means that you can use the same server and port for both of your workers and you simply need to register them with two different function names.</p>

<p>You can have a look at the documentation at <a href=""http://gearman.org/"" rel=""nofollow"">http://gearman.org/</a> but it's pretty bad... Here you can find the instructions to download and run the server: <a href=""http://gearman.org/getting-started"" rel=""nofollow"">http://gearman.org/getting-started</a></p>
"
3735495;179;Vladimir;<python><machine-learning><scikit-learn><vectorization>;27845866;1;Spam filter using Python;"<p>I`m trying to make a simple spam filter using python 2.7 and scikit-learn. So, I have a set of letters for train and a set of letters for test. Firstly, I want to vectorize training set and fit logistic regression using it, then vectorize each letter in test set and put them into classifier separately.</p>

<pre><code>import codecs
import json
import os
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import linear_model

def classify(mail, vectorizer, logreg):
    vect_mail = vectorizer.transform(mail)
    res = logreg.predict(vect_mail)
    return res

def make_output(test_dir, vectorizer, logreg):
   with codecs.open('test.txt', 'w', 'utf-8') as out:
       for f in os.listdir(test_dir):
           mail = json.load(open(os.path.join(test_dir, f)), 'utf-8')
           result = classify(mail['body'].encode('ascii','ignore'), vectorizer, logreg)
           out.write(u'%s\t%s\n' % (f, result))

def read_train(train_dir):
    for f in os.listdir(train_dir):
        with open(os.path.join(train_dir, f), 'r') as fo:
            mail = json.load(fo, 'utf-8')
            yield mail

if __name__ == '__main__':
    train_mails = list(read_train('spam_data/train'))
    corpus = list()
    is_spam = list()

    for mail in train_mails:
        corpus.append(mail['body'].encode('ascii','ignore'))
        is_spam.append(mail['is_spam'])
    vectorizer = CountVectorizer()
    cnt_vect = vectorizer.fit_transform(corpus)
    logreg = linear_model.LogisticRegression()
    logreg.fit(cnt_vect, is_spam)
    make_output('spam_data/test', vectorizer, logreg)
</code></pre>

<p>But <code>res = logreg.predict(vect_mail)</code> returns a list, not one meaning. So, I guess, predictor interprets <code>vect_mail</code> like sample of documents of one word, not like a document with many words. How should I rewrite this code?</p>
";27848600;1190430;5186;Artem Sobolev;1;27848600;"<p>According to the sklearn's documentation, <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.transform"" rel=""nofollow""><code>CountVectorizer.transform</code></a> accepts not a single document to transform, but an <strong>iterable</strong> of documents. Since a string in Python is an iterable of its characters, <code>transform</code> generates as many ""documents"" as there are characters in the string.</p>

<p>In order to fix this issue, pass a single-element list to the <code>transform</code>:</p>

<pre><code>vect_mail = vectorizer.transform([mail])
</code></pre>
"
574187;3765;ihadanny;<machine-learning><scikit-learn><cross-validation>;27846542;2;cross validating a train set where the class variable has a different distribution than the actual population;"<p>(noob in ML, be patient) 
I want to test the performance of my scikit-learn SVMLinear classifier. My train-set has a different class distribution than the actual population, but my test-set is a representative, and distributes like the actual population. </p>

<p>I noticed that there's a class-weight parameter, and I want to try giving my classifier the actual population distribution, and see if it helps it perform better.</p>

<p>However - as my train-set distribution is different, so will be my validation set, right? So should I expect an improvement on the validation, or must I use my test-set to see the improvement? And if so - isn't it against the rules to calibrate using the test-set which will lead to burning the test-set or overfitting?</p>

<p>I've thought about bootstrap re-sampling of my train-set: making it distribute the same as the general population, and only then training and validating my model. Is this a good solution?</p>

<p>Thanks!</p>
";;3646384;557;Amin Suzani;1;27869099;"<p>It seems that you have some good ideas which are mostly worth trying. The answers mostly depend on the application and the size of your train/test set. </p>

<p>It is against the rules to calibrate based on test set and again use the whole test set for evaluation. However, if your test set is large enough, you can always divide your test set to two sets: validation set, and actual test set. Then, your final evaluation will be based on a smaller test set, which might be still acceptable depending on the application. </p>

<p>For your training set that you believe it has different class distribution than the actual population, there might be several things worth trying. Usually the most acceptable approach is to use a classifier that can handle these differences (usually with fewer parameters to avoid over-fitting). There is a whole topic of classification and regression on skewed datasets that you can look through. Other than the classifier, provided that you did not derive the actual population from your test set, the methods below might help too:
1- One of them can be (as you said) bootstrap re-sampling in case that your training set is large enough for that. 
2- Another approach can be generating more training samples by adding some noise to the current samples of the training set. For example if you are classifying images of birds, you can randomly make images darker or brighter, or randomly move them a few pixels to the sides or up and down (select values randomly in a small enough range). This way, you can add to the training set in a way to get the desired distribution. </p>
"
3474956;7458;kilojoules;<python><machine-learning>;27852199;-1;Python Machine Learning Algorithm to Recognize Known Events;"<p>I have two sets of data. These data are logged voltages of two points A and B in a circuit. Voltage A is the main component of the circuit, and B is a sub-circuit. Every positive voltage in B is (1) considered a B event and (2) known to be composite of A. I have included sample data where there is a B voltage event, <code>4,4,0,0,4,4</code>. A real training data set would have many more available data.</p>

<p>How can I train a Python machine learning algorithm to recognize B events given only A data? </p>

<p>Example data:</p>

<pre><code>V(A), V(B)
0, 0
2, 0
5, 4
3, 4
1, 0
3, 4
4, 4
1, 0
0, 0
2, 0
5, 0
7, 0
2, 0
5, 4
9, 4
3, 0
5, 0
4, 4
6, 4
3, 0
2, 0
</code></pre>
";27852335;1330293;33731;elyase;1;27852335;"<p>An idea:</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier

n = 5
X = [df.A.iloc[i:i+n] for i in df.index[:-n+1]] 
labels = (df.B &gt; 0)[n-1:]

model = RandomForestClassifier()
model.fit(X, labels)
model.predict(X)
</code></pre>

<p>What this does is, it takes the previous <code>n</code> observations as predictors for the 'B' value. On this small data set it achieves 0.94 accuracy (could be overfitting).</p>

<p>EDIT: Corrected a small alignment error.</p>
"
3073054;73;Jim GB;<python><numpy><machine-learning><scikit-learn>;27856705;2;Categorical data transformation in Scikit-Learn;"<p>I have a 40 million x 22 numpy array of integer data for a classification task.
Most of the features are categorical data that use different integer values to represent different categories. For example, in the column ""Color"": 0 means blue, 1 means red and so on. I have preprocessed the data using LabelEncoder. </p>

<ol>
<li>Does it make sense to fit those data into any classification model in SK-learn? I tried to fit the data into Random Forest model but got extremely bad accuracy. I also tried One Hot Encoding to transform the data into dummy variables, but my computer can only deal with a sparse matrix after using One Hot Encoding, the problem is that Random Forest can only take a dense matrix, which will exceed my computer's memory. </li>
<li>What's the correct strategy to deal with categorical data in SK-learn?</li>
</ol>
";27857428;1190430;5186;Artem Sobolev;1;27857428;"<p><code>LabelEncoder</code> is useless in your case, since output numbers do not make any sense as numbers (i.e. it's meaningless to perform arithmetic operations on them). <code>OneHotEncoder</code> is essential when dealing with categorical data.</p>

<p>Recently sklearn <a href=""https://github.com/scikit-learn/scikit-learn/pull/3173"" rel=""nofollow"">got support for sparse input</a> in Random Forests and Decision Trees, so you might want to check out the latest version. Also, other methods like <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"" rel=""nofollow"">LogisticRegression</a> support sparse data.</p>

<p>Moreover, I don't think you need to use all 40M of examples to get a decent accuracy. It should be enough to randomly sample, say, 100k of them (this number depends on number of features after OneHotEncoding, their variability, and number of target classes).</p>
"
4436207;9;CEM Boys;<java><machine-learning><classification><weka><prediction>;27857486;0;using the saved model for predicting using weka (Eclipse+Java);"<p>I was confused with the arguments of the lines ""Instances originalTrain="" can anyone please help me to correct this error since I was new to this weka. We are creating a disease prediction system using weka in java.</p>

<pre><code>import weka.classifiers.Classifier;
import weka.core.Instances;

public class Main {

    public static void main(String[] args) throws Exception
    {
        String rootPath=""/some/where/""; 
        Instances originalTrain= //instances here (don't know to complete this statement)

        //load model
        Classifier cls = (Classifier) weka.core.SerializationHelper.read(rootPath+""tree.model"");

        //predict instance class values
        Instances originalTrain= //load or create Instances to predict (This statement too)

        //which instance to predict class value
        int s1=0;

        //perform your prediction
        double value=cls.classifyInstance(originalTrain.instance(s1));

        //get the prediction percentage or distribution
        double[] percentage=cls.distributionForInstance(originalTrain.instance(s1));

        //get the name of the class value
        String prediction=originalTrain.classAttribute().value((int)value); 

        System.out.println(""The predicted value of instance ""+
                            Integer.toString(s1)+
                            "": ""+prediction); 

        //Format the distribution
        String distribution="""";
        for(int i=0; i &lt;percentage.length; i=i+1)
        {
            if(i==value)
            {
                distribution=distribution+""*""+Double.toString(percentage[i])+"","";
            }
            else
            {
                distribution=distribution+Double.toString(percentage[i])+"","";
            }
        }
        distribution=distribution.substring(0, distribution.length()-1);

        System.out.println(""Distribution:""+ distribution);
    }

}
</code></pre>
";27861498;628499;2731;Walter;0;27861498;"<p>For completeness, the code snippet in the question originates from <a href=""https://stackoverflow.com/questions/21674522/get-prediction-percentage-in-weka-using-own-java-code-and-a-model/21678307#21678307"">Get prediction percentage in WEKA using own Java code and a model</a>.</p>

<p>originalTrain should be your training instances.  There are two ways that I know to add instances to originalTrain.</p>

<ol>
<li><p>This method loads data from an .arff file and is based on instructions found <a href=""http://weka.wikispaces.com/Use+WEKA+in+your+Java+code"" rel=""nofollow noreferrer"">here</a>.<br>
<pre><code>// rootPath should be where the .arff file is held
// filename should hold the complete name of the .arff file
public static Instances instanceData(String rootPath, String filename) throws Exception
{ 
  // initialize source 
  DataSource source = null;
  Instances data = null;
  source = new DataSource(rootPath + filename);
  data = source.getDataSet();<br>
  // set the class to the last attribute of the data (may need to tweak) 
  if (data.classIndex() == -1)
   data.setClassIndex(data.numAttributes() -1 );
  return data;
}
</pre></code></p></li>
<li><p>You can create and add instance manually as described in this answer <a href=""https://stackoverflow.com/questions/20093977/define-input-data-for-clustering-using-weka-api/20109536#20109536"">Define input data for clustering using WEKA API</a> .  </p></li>
</ol>
"
2419777;46;RJ Rajan;<java><c#><machine-learning><weka>;27859167;1;Combining multiple saved Classifier in weka;"<p>Have huge distributed datasets which are trained to produce classifiers.All the datasets have identical attributes and the training is done using a single algorithm J48.
The problem I am facing is as to how would  combine these classifiers to have a single classifier which can be used for testing and predicting data.
I am using weka tool for the code.Have converted the weka jar to dll.Using C# language.
Any help in C# or Java would be of great help.
If any additional information is needed you are free to ask.
Thanks </p>
";;968064;2363;Rushdi Shams;0;28175002;"<p>I don't think it is possible if you create N classifiers on N training sets and then combine N classifiers to generate a single one. Because first, the data are different; second, so the models will be different. Instead, what I would do is if I were happy with the N results, I would combine all N datasets and develop a single model from it to test and predict unseen data.</p>
"
2419777;46;RJ Rajan;<java><c#><machine-learning><weka>;27859167;1;Combining multiple saved Classifier in weka;"<p>Have huge distributed datasets which are trained to produce classifiers.All the datasets have identical attributes and the training is done using a single algorithm J48.
The problem I am facing is as to how would  combine these classifiers to have a single classifier which can be used for testing and predicting data.
I am using weka tool for the code.Have converted the weka jar to dll.Using C# language.
Any help in C# or Java would be of great help.
If any additional information is needed you are free to ask.
Thanks </p>
";;3562232;498;shirowww;1;32564591;"<p>It is perfectly possible to do what you are asking for. You could build N different classifiers from N different but compatible datasets and combine their outputs to form a new dataset of higher order. Its a hierarchical way of combining classifiers and there is a great variety in ways of doing that. Its called 'ensembling' or 'classifier ensemble'. There are a large number of technical articles detailing how to do it.</p>

<p>One approach would be:
1. Train/get N different classifiers.
2. Build a new dataset with its probability output for a known set of instances, one instance per row, the set-of-output-probalities per set of columns. And the right/known class.
3. Throw away the old attributes and retain only the output probs calculated and known class.
4. Train a new model/classifier with this higher order dataset (don't need to use the whole data, only a moderate subsample).
5. For every new instance, get lower level probabilities (using N classifiers), as previously done, and apply higher level classifier over these newly constructed instance.</p>

<p>Hope to have helped.</p>
"
1452759;6648;user1452759;<python><machine-learning><scikit-learn>;27860302;2;In Sklearn machine learning, is there any way to classify text without target labels?;"<p>I was wondering if there was any way to classify text data into different groups/categories based on the words in the text using a combination of Python and Sklearn Machine Learning?</p>

<p>For example:</p>

<pre><code>text = [[""request approval for access"", ""request approval to enter premises"", ""Laptop not working""], [""completed bw table loading""]]
</code></pre>

<p>So can I get categories like:</p>

<pre><code>category_label = [[0,0,2], [1]]
categories = [[""approval request"", ""approval request"", ""Laptop working""], [""bw table""]]
</code></pre>

<p>where </p>

<pre><code>      0 = approval request
      2 = laptop working
      1 = bw table
</code></pre>

<p>Basically the above would imply that there is no labelled training data or target labels. </p>
";27867351;1330293;33731;elyase;1;27861026;"<p>You can try a clustering method but there is no guarantee that the clusters you get will correspond to the categories you want, because you haven't clearly explained the algorithm what you want.</p>

<p>What I would do is manually label some data (how long can it take to label 300 samples?) and train on that, so that your algo can learn the words the are correlated with each class. </p>

<p>If this is impossible then your best bet is to calculate a cosine similarity between one sample and each class description, rank them, and then assign the closest class. But in my opinion by the time you finish to code this, you could have manually labeled some samples and trained a standard algo with a much better precision.</p>
"
1452759;6648;user1452759;<python><machine-learning><scikit-learn>;27860302;2;In Sklearn machine learning, is there any way to classify text without target labels?;"<p>I was wondering if there was any way to classify text data into different groups/categories based on the words in the text using a combination of Python and Sklearn Machine Learning?</p>

<p>For example:</p>

<pre><code>text = [[""request approval for access"", ""request approval to enter premises"", ""Laptop not working""], [""completed bw table loading""]]
</code></pre>

<p>So can I get categories like:</p>

<pre><code>category_label = [[0,0,2], [1]]
categories = [[""approval request"", ""approval request"", ""Laptop working""], [""bw table""]]
</code></pre>

<p>where </p>

<pre><code>      0 = approval request
      2 = laptop working
      1 = bw table
</code></pre>

<p>Basically the above would imply that there is no labelled training data or target labels. </p>
";27867351;1615070;800;404pio;0;27862639;"<p>@user1452759</p>

<p>Your problem is more specific than general machine learning, and you should use package NLTK instead of sklearn. Look at classifying text with nltk <a href=""http://www.nltk.org/book/ch06.html"" rel=""nofollow"">http://www.nltk.org/book/ch06.html</a></p>
"
1452759;6648;user1452759;<python><machine-learning><scikit-learn>;27860302;2;In Sklearn machine learning, is there any way to classify text without target labels?;"<p>I was wondering if there was any way to classify text data into different groups/categories based on the words in the text using a combination of Python and Sklearn Machine Learning?</p>

<p>For example:</p>

<pre><code>text = [[""request approval for access"", ""request approval to enter premises"", ""Laptop not working""], [""completed bw table loading""]]
</code></pre>

<p>So can I get categories like:</p>

<pre><code>category_label = [[0,0,2], [1]]
categories = [[""approval request"", ""approval request"", ""Laptop working""], [""bw table""]]
</code></pre>

<p>where </p>

<pre><code>      0 = approval request
      2 = laptop working
      1 = bw table
</code></pre>

<p>Basically the above would imply that there is no labelled training data or target labels. </p>
";27867351;1745038;2367;AN6U5;1;27867351;"<p>This is readily possible in <a href=""http://scikit-learn.org/stable/"" rel=""nofollow"">Scikit-Learn</a> as well as in <a href=""http://www.nltk.org/"" rel=""nofollow"">NLTK</a>.</p>

<p>The features that you list:</p>

<pre><code>0 = approval request
2 = laptop working
1 = bw table
</code></pre>

<p>are not ones that a clustering algorithm would naturally choose and its worthwhile to caution you against the possible mistake of clouding your statistical learning algorithm with heuristics.  I suggest that you first try some clustering and classification and then consider <a href=""http://scikit-learn.org/stable/modules/label_propagation.html"" rel=""nofollow"">semi-supervised learning methods</a> whereby you can label your clusters and propagate those labels.</p>
"
2572645;4438;Andy K;<machine-learning><nlp><word2vec>;27860652;97;word2vec: negative sampling (in layman term)?;"<p>I'm reading the paper below and I have some trouble , understanding the concept of negative sampling.</p>

<p><a href=""http://arxiv.org/pdf/1402.3722v1.pdf"">http://arxiv.org/pdf/1402.3722v1.pdf</a></p>

<p>Can anyone help , please?</p>
";27864657;419338;13702;mbatchkarov;171;27864657;"<p>The idea of <code>word2vec</code> is to maximise the similarity (dot product) between the vectors for words which appear close together (in the context of each other) in text, and minimise the similarity of words that do not. In equation (3) of the paper you link to, ignore the exponentiation for a moment. You have </p>

<pre><code>      v_c * v_w
 -------------------
   sum(v_c1 * v_w)
</code></pre>

<p>The numerator is basically the similarity between words <code>c</code> (the context) and <code>w</code> (the target) word. The denominator computes the similarity of all other contexts <code>c1</code> and the target word <code>w</code>. Maximising this ratio ensures words that appear closer together in text have more similar vectors than words that do not. However, computing this can be very slow, because there are many contexts <code>c1</code>. Negative sampling is one of the ways of addressing this problem- just select a couple of contexts <code>c1</code> at random. The end result is that if <code>cat</code> appears in the context of <code>food</code>, then the vector of <code>food</code> is more similar to the vector of <code>cat</code> (as measures by their dot product) than the vectors of <strong>several other randomly chosen words</strong> (e.g. <code>democracy</code>, <code>greed</code>, <code>Freddy</code>), instead of <strong>all other words in language</strong>. This makes <code>word2vec</code> much much faster to train.</p>
"
2572645;4438;Andy K;<machine-learning><nlp><word2vec>;27860652;97;word2vec: negative sampling (in layman term)?;"<p>I'm reading the paper below and I have some trouble , understanding the concept of negative sampling.</p>

<p><a href=""http://arxiv.org/pdf/1402.3722v1.pdf"">http://arxiv.org/pdf/1402.3722v1.pdf</a></p>

<p>Can anyone help , please?</p>
";27864657;1462770;13671;Amir;41;41319421;"<p>Computing <strong><em>Softmax</em></strong> (Function to determine which words are similar to the current target word) is expensive since requires summing over all words in <strong>V</strong> (denominator), which is generally very large.</p>

<p><a href=""https://i.stack.imgur.com/Akfej.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Akfej.png"" alt=""enter image description here""></a></p>

<p><em>What can be done?</em></p>

<p>Different strategies have been proposed to <strong>approximate</strong> the softmax. These approaches can be grouped into <strong>softmax-based</strong> and <strong>sampling-based</strong> approaches. <strong><em>Softmax-based</em></strong> approaches are methods that keep the softmax layer intact, but modify its architecture to improve its efficiency (e.g hierarchical softmax). <strong><em>Sampling-based</em></strong> approaches on the other hand completely do away with the softmax layer and instead optimise some other loss function that approximates the softmax (They do this by approximating the normalization in the denominator of the softmax with some other loss that is cheap to compute like negative sampling).</p>

<p>The loss function in Word2vec is something like:</p>

<p><a href=""https://i.stack.imgur.com/4s4f6.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4s4f6.png"" alt=""enter image description here""></a></p>

<p>Which logarithm can decompose into:</p>

<p><a href=""https://i.stack.imgur.com/6RDai.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/6RDai.png"" alt=""enter image description here""></a></p>

<p>With some mathematic and gradient formula (See more details at <a href=""http://sebastianruder.com/word-embeddings-softmax/"" rel=""noreferrer"">6</a>) it converted to:</p>

<p><a href=""https://i.stack.imgur.com/fua4s.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fua4s.png"" alt=""enter image description here""></a></p>

<p>As you see it converted to binary classification task (y=1 positive class, y=0 negative class). As we need labels to perform our binary classification task, we designate all context words <em>c</em> as true labels (y=1, positive sample), and <em>k</em> randomly selected from corpora as false labels (y=0, negative sample).</p>

<hr>

<p>Look at the following paragraph. Assume our target word is ""<strong>Word2vec</strong>"". With window of 3, our context words are: <code>The</code>, <code>widely</code>, <code>popular</code>, <code>algorithm</code>, <code>was</code>, <code>developed</code>. These context words consider as positive labels. We also need some negative labels. We randomly pick some words from corpus (<code>produce</code>, <code>software</code>, <code>Collobert</code>, <code>margin-based</code>, <code>probabilistic</code>) and consider them as negative samples. This technique that we picked some randomly example from corpus is called negative sampling.  </p>

<p><a href=""https://i.stack.imgur.com/nnnQX.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/nnnQX.png"" alt=""enter image description here""></a></p>

<p><strong>Reference</strong> :</p>

<ul>
<li>(1) C. Dyer, <em>""Notes on Noise Contrastive Estimation and Negative Sampling""</em>, 2014</li>
<li>(2) <a href=""http://sebastianruder.com/word-embeddings-softmax/"" rel=""noreferrer"">http://sebastianruder.com/word-embeddings-softmax/</a></li>
</ul>
"
2572645;4438;Andy K;<machine-learning><nlp><word2vec>;27860652;97;word2vec: negative sampling (in layman term)?;"<p>I'm reading the paper below and I have some trouble , understanding the concept of negative sampling.</p>

<p><a href=""http://arxiv.org/pdf/1402.3722v1.pdf"">http://arxiv.org/pdf/1402.3722v1.pdf</a></p>

<p>Can anyone help , please?</p>
";27864657;7552761;1967;Eric Kim;24;56401065;"<p>I wrote an tutorial article about negative sampling <a href=""https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling"" rel=""noreferrer"">here</a>.</p>

<p><strong>Why do we use negative sampling?</strong> -> to reduce computational cost</p>

<p>The cost function for vanilla Skip-Gram (SG) and Skip-Gram negative sampling (SGNS) looks like this:</p>

<p><a href=""https://i.stack.imgur.com/8bUdX.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/8bUdX.png"" alt=""enter image description here""></a></p>

<p>Note that <code>T</code> is the number of all vocabs. It is equivalent to <code>V</code>. In the other words, <code>T</code> = <code>V</code>.</p>

<p>The probability distribution <code>p(w_t+j|w_t)</code> in SG is computed for all <code>V</code> vocabs in the corpus with:</p>

<p><a href=""https://i.stack.imgur.com/3Lc8N.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3Lc8N.png"" alt=""enter image description here""></a></p>

<p><code>V</code> can easily exceed tens of thousand when training Skip-Gram model. The probability needs to be computed <code>V</code> times, making it computationally expensive. Furthermore, the normalization factor in the denominator requires extra <code>V</code> computations. </p>

<p>On the other hand, the probability distribution in SGNS is computed with:</p>

<p><a href=""https://i.stack.imgur.com/Tl4ST.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Tl4ST.png"" alt=""enter image description here""></a></p>

<p><code>c_pos</code> is a word vector for positive word, and <code>W_neg</code> is word vectors for all <code>K</code> negative samples in the output weight matrix. With SGNS, the probability needs to be computed only <code>K + 1</code> times, where <code>K</code> is typically between 5 ~ 20. Furthermore, no extra iterations are necessary to compute the normalization factor in the denominator. </p>

<p>With SGNS, only a fraction of weights are updated for each training sample, whereas SG updates all millions of weights for each training sample.</p>

<p><a href=""https://i.stack.imgur.com/0bPat.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/0bPat.png"" alt=""enter image description here""></a></p>

<p><strong>How does SGNS achieve this?</strong> -> by transforming multi-classification task into binary classification task.</p>

<p>With SGNS, word vectors are no longer learned by predicting context words of a center word. It learns to differentiate the actual context words (positive) from randomly drawn words (negative) from the noise distribution.</p>

<p><a href=""https://i.stack.imgur.com/qbZaH.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/qbZaH.png"" alt=""enter image description here""></a></p>

<p>In real life, you don't usually observe <code>regression</code> with random words like <code>Gangnam-Style</code>, or <code>pimples</code>. The idea is that if the model can distinguish between the likely (positive) pairs vs unlikely (negative) pairs, good word vectors will be learned.</p>

<p><a href=""https://i.stack.imgur.com/7V7EO.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7V7EO.png"" alt=""enter image description here""></a></p>

<p>In the above figure, current positive word-context pair is (<code>drilling</code>, <code>engineer</code>). <code>K=5</code> negative samples are <a href=""https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#neg_drawn"" rel=""noreferrer"">randomly drawn</a> from the <a href=""https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#noise_dist"" rel=""noreferrer"">noise distribution</a>: <code>minimized</code>, <code>primary</code>, <code>concerns</code>, <code>led</code>, <code>page</code>. As the model iterates through the training samples, weights are optimized so that the probability for positive pair will output <code>p(D=1|w,c_pos)â‰ˆ1</code>, and probability for negative pairs will output <code>p(D=1|w,c_neg)â‰ˆ0</code>. </p>
"
4437095;11;Farshid;<machine-learning><neural-network><classification><svm><decision-tree>;27862165;1;online/incremental learning for classifiers;"<p>I understand that in online/incremental learning it is possible that SVM or NN learn incrementally, as the new data becomes available over time. What if instead of new cases, just new features/variables for the existing cases become available over time. Is there any technique that can handle this kind of training for classifiers/predictions?  </p>
";;4415339;103;zsoltc;1;27864304;"<p>In the case of neural networks I would go with this method:</p>

<p>Take the already trained network. Add new input neurons for the new features. Optionally add new neurons to the hidden layer(s). Initialize the weights for the new connections with zeros or random values. Retrain the network.</p>

<p>It should probably be faster than training a new network from scratch.</p>
"
4379528;43;Lei;<r><machine-learning><statistics><svm>;27864446;3;An Error from SVM in R;"<p>I am new to R and try to retrieve data from a text, and then apply it in SVM for classification. Here is the code:</p>

<pre><code>train&lt;-read.table(""training.txt"")
train[which(train==""?"",arr.ind=TRUE)]&lt;-NA
train=unique(train)
y=train[,length(train)]

classifier&lt;-svm(y~.,data=train[,-length(train)],scale=F)
classifier&lt;-svm(x=train[,-length(train)],y=factor(y),scale=F)
</code></pre>

<p>I try the 2 different ways to invoke svm, for the 1st one <code>(svm(y~.,data=train[,-length(train)],scale=F))</code> seems ok, but the 2nd one has problems, it reported:</p>

<pre><code>Error in svm.default(x = train[, length(train)], y = factor(y), scale = F) : 
  NA/NaN/Inf in foreign function call (arg 1)
In addition: Warning message:
In svm.default(x = train[, length(train)], y = factor(y), scale = F) :
  NAs introduced by coercion
</code></pre>

<p>Here is a sample of the <code>training.txt</code>, the last column is the target</p>

<pre><code>39,State-gov,77516,Bachelors,13,Never-married,Adm-clerical,Not-in-family,White,Male,2174,0,40,United-States,0
50,Self-emp-not-inc,83311,Bachelors,13,Married-civ-spouse,Exec-managerial,Husband,White,Male,0,0,13,United-States,0
38,Private,215646,HS-grad,9,Divorced,Handlers-cleaners,Not-in-family,White,Male,0,0,40,United-States,0
53,Private,234721,11th,7,Married-civ-spouse,Handlers-cleaners,Husband,Black,Male,0,0,40,United-States,0
28,Private,338409,Bachelors,13,Married-civ-spouse,Prof-specialty,Wife,Black,Female,0,0,40,Cuba,0
37,Private,284582,Masters,14,Married-civ-spouse,Exec-managerial,Wife,White,Female,0,0,40,United-States,0
49,Private,160187,9th,5,Married-spouse-absent,Other-service,Not-in-family,Black,Female,0,0,16,Jamaica,0
52,Self-emp-not-inc,209642,HS-grad,9,Married-civ-spouse,Exec-managerial,Husband,White,Male,0,0,45,United-States,1
31,Private,45781,Masters,14,Never-married,Prof-specialty,Not-in-family,White,Female,14084,0,50,United-States,1
42,Private,159449,Bachelors,13,Married-civ-spouse,Exec-managerial,Husband,White,Male,5178,0,40,United-States,1
37,Private,280464,Some-college,10,Married-civ-spouse,Exec-managerial,Husband,Black,Male,0,0,80,United-States,1
30,State-gov,141297,Bachelors,13,Married-civ-spouse,Prof-specialty,Husband,Asian-Pac-Islander,Male,0,0,40,India,1
23,Private,122272,Bachelors,13,Never-married,Adm-clerical,Own-child,White,Female,0,0,30,United-States,0
32,Private,205019,Assoc-acdm,12,Never-married,Sales,Not-in-family,Black,Male,0,0,50,United-States,0
40,Private,121772,Assoc-voc,11,Married-civ-spouse,Craft-repair,Husband,Asian-Pac-Islander,Male,0,0,40,NA,1
</code></pre>

<p>Any idea about it? thanks in advance!</p>
";27864621;4130044;33919;LyzandeR;5;27864621;"<p>From documentation:</p>

<p>For the <code>x</code> argument:</p>

<pre><code>a data matrix, a vector, or a sparse matrix (object of class Matrix
provided by the Matrix package,or of class matrix.csr provided by the
SparseM package, or of class simple_triplet_matrix provided by the slam package).
</code></pre>

<p>For the <code>y</code> argument:</p>

<pre><code>a response vector with one label for each row/component of x. Can be
either a factor (for classification tasks) or a numeric vector (for regression).
</code></pre>

<p>When you type: <code>x=train[,-length(train)]</code> in the second function you are practically using a <code>data.frame</code> which is not supported and it crashes.</p>

<p>The <code>svm</code> function works with a numeric matrix <strong>only</strong></p>

<pre><code>library(e1071)
train[which(train==""?"",arr.ind=TRUE)]&lt;-NA
train=unique(train)
y=factor(train[,length(train)])
train &lt;- data.frame(lapply(train,as.numeric)) #convert to numeric. factors are integer fields anyway behind the scenes.

train &lt;- as.matrix(train[-length(train)])

classifier&lt;-svm(x= train ,y=y,scale=F)
</code></pre>

<p>Output:</p>

<pre><code>&gt; summary(classifier)

Call:
svm.default(x = train, y = y, scale = F)


Parameters:
   SVM-Type:  C-classification 
 SVM-Kernel:  radial 
       cost:  1 
      gamma:  0.07142857 

Number of Support Vectors:  14

 ( 9 5 )


Number of Classes:  2 

Levels: 
 0 1
</code></pre>
"
4415339;103;zsoltc;<machine-learning><genetic-algorithm><evolutionary-algorithm><genetic-programming>;27875980;4;How can we implement loop constructs in genetic programming?;"<p>I've been playing around with genetic programming for some time and started wondering how to implement looping constructs.</p>
<p>In the case of for loops I can think of 3 parameters:</p>
<ul>
<li><strong>start</strong>: starting value of counter</li>
<li><strong>end</strong>: counter upper limit</li>
<li><strong>expression</strong>:  the expression to execute while counter &lt; end</li>
</ul>
<p>Now the tricky part is the <em>expression</em> because it generates the same value in every iteration unless <em>counter</em> is somehow injected into it. So I could allow the symbol for <em>counter</em> to be present in the expressions but then how do I prevent it from appearing outside of for loops?</p>
<p>Another problem is using the result of the expression. I could have a for loop which sums the results, another one that multiplies them together but that's limiting and doesn't seem right. I would like a general solution, not one for every operator.</p>
<p>So does anyone know a good method to implement loops in genetic programming?</p>
";27877867;461202;6611;zegkljan;1;27877867;"<p>Well, that's tricky. Genetic programming (the original Koza-style GP) is best suited for functional-style programming, i.e. there is no internal execution state and every node is a function that returns (and maybe takes) values, like lisp. That is a problem when the node is some loop - it is not clear what the node should return.</p>
<p>You could also design your loop node as a binary node. One parameter is a boolean expression that will be called before every loop and if true is returned, the loop will be executed. The second parameter would be the loop expression.</p>
<p>The problem you already mentioned, that there is no way of changing the loop expression. You can solve this by introducing a concept of some internal state or variables. But that leaves you with another problems like the need to define the number of variables. A variable can be realized e.g. by a tuple of functions - a setter (one argument, no return value, or it can return the argument) and getter (no arguments, returns the value of the variable).</p>
<p>Regarding the way of handling the loop result processing, you could step from GP to strongly typed GP or STGP for short. It is essentialy a GP with types. Your loop could then be effectively a function that returns a list of values (e.g. numbers) and you could have other functions that take such lists and calculate other values...</p>
<p>There is another GP algorithm (my favourite), called Grammatical Evolution (or GE) which uses context-free grammar to generate the programs. It can be used to encode type information like in STGP. You could also define the grammar in a way that classical c-like for and while loops can be generated. Some extensions to it, like Dynamically Defined Functions, could be used to implement variables dynamically.</p>
<p>If there is anything unclear, just comment on the answer and I'll try to clarify it.</p>
"
4415339;103;zsoltc;<machine-learning><genetic-algorithm><evolutionary-algorithm><genetic-programming>;27875980;4;How can we implement loop constructs in genetic programming?;"<p>I've been playing around with genetic programming for some time and started wondering how to implement looping constructs.</p>
<p>In the case of for loops I can think of 3 parameters:</p>
<ul>
<li><strong>start</strong>: starting value of counter</li>
<li><strong>end</strong>: counter upper limit</li>
<li><strong>expression</strong>:  the expression to execute while counter &lt; end</li>
</ul>
<p>Now the tricky part is the <em>expression</em> because it generates the same value in every iteration unless <em>counter</em> is somehow injected into it. So I could allow the symbol for <em>counter</em> to be present in the expressions but then how do I prevent it from appearing outside of for loops?</p>
<p>Another problem is using the result of the expression. I could have a for loop which sums the results, another one that multiplies them together but that's limiting and doesn't seem right. I would like a general solution, not one for every operator.</p>
<p>So does anyone know a good method to implement loops in genetic programming?</p>
";27877867;2036035;3529;hythis;1;47723172;"<p>The issue with zegkjan's answer is that there is more than one way to skin a cat. </p>

<p>Theres actually a simpler, and at times, better solution to creating GP datastructures than koza trees, instead using stacks. </p>

<p>This method is called <a href=""https://pdfs.semanticscholar.org/d992/1698680ca2327d7c242ebdb530707f170a4e.pdf"" rel=""nofollow noreferrer"">Stack Based Genetic Programming</a>, which is quite old (1993).  This method of genetic programming removes trees entirely, you have a instruction list, and a data stack (where your functional and terminal set remain the same).   You iterate through your instruction list, pushing values to the data stack, and pulling values to consume them, and returning a new value/values to the stack given your instruction.  For example, consider the following genetic program.</p>

<pre><code>0: PUSH TERMINAL X
1: PUSH TERMINAL X
2: MULTIPLY (A,B)
</code></pre>

<p>Iterating through this program will give you:</p>

<pre><code>step1: DATASTACK X
step2: DATASTACK X, X
step3: DATASTACK X^2
</code></pre>

<p>Once you have executed all program list statements, you then just take off the number of elements from the stack that you care about (you can get multiple values out of the GP program this way).   This ends up being a fast and extremely flexible method (memory locality, number of parameters doesn't matter, nor number of elements returned) you can implement fairly quickly.</p>

<p>To loop in this method, you can create a separate stack, an execution stack, where new special operators are used to push and pop multiple statements from the execution stack at once to be executed afterwards. </p>

<p>Additionally you can simply include a jump statement to move backwards in your program list, make a loop statement, or have a separate stack holding loop information.  With this a genetic program could theoretically develop its own for loop. </p>

<pre><code>0: PUSH TERMINAL X
1: START_LOOP 2
2: PUSH TERMINAL X
3: MULTIPLY (A, B)
4: DECREMENT_LOOP_NOT_ZERO

step1: DATASTACK X
       LOOPSTACK 
step2: DATASTACK X
       LOOPSTACK [1,2]
step3: DATASTACK X, X
       LOOPSTACK [1,2]
step4: DATASTACK X^2
       LOOPSTACK [1,2]
step5: DATASTACK X^2
       LOOPSTACK [1,1]
step6: DATASTACK X^2, X
       LOOPSTACK [1,1]
step7: DATASTACK X^3
       LOOPSTACK [1,1]
step8: DATASTACK X^3
       LOOPSTACK [1,0]
</code></pre>

<p>Note however, with any method, it may be difficult for a GP program to actually evolve a member that has a loop, and even if it does, its likely that such a mechanism would result in a poor fitness evaluation at the start, likely removing it from the population any way.   To fix this type of problem (potentially good innovations dying early due to poor early fitness), you'll need to include the concepts of <a href=""https://en.wikipedia.org/wiki/Deme_(biology)"" rel=""nofollow noreferrer"">demes</a> in your genetic program to isolate genetically disparate populations. </p>
"
4309200;240;pevadi;<matlab><audio><machine-learning><mp3><large-files>;27878190;0;Read and represent mp3 files using memmapfile in matlab;"<p>I have to analyze bio acoustic audiofiles using matlab. Eventually I want to be able to find anomalies in the audio. That's the reason I need to find a way to represent the audio in a way I can extract and compare features.  I'm dealing with mp3 files up to 150 mb. These files are too large for matlab to read in to it's memory. Therefore I want to use the memmapfile() function. I used the following code and a small mp3 file to find out how it actually works.    </p>

<pre><code>[testR, ~] = audioread('test.mp3');
testM = memmapfile('test.mp3');
disp(testM.Data);
disp(testR);   
</code></pre>

<p>The actual values of the testM.Data and testR are different. Audioread() returns a 7483391 x 2 matrix and memmapfile() a 4113874 x 1 matrix.
I'm not really sure how memmapfile() works, I expected this to be equal to each other. Is there a way to read mp3 files in the same format audioread() does using memmapfile()? And what does memmapfile actually return in case of an audio file? Maybe it's also usable in the vector format in the case of anomaly detection?</p>

<p>Thanks in advance!</p>

<p>NOTE: The original files were in wav IMA ADPCM format with sizes from 1.5 up to 2.5 gb. Since Matlab can't deal with that format and the size of the files I converted them to 8bit mp3 files. </p>
";;4312376;1;francesco quiri;0;27882518;"<p>I think that the problem is mammapfile by default read data in uint8 format, while audioread function read data in another way.
How you can see <a href=""http://uk.mathworks.com/help/matlab/ref/memmapfile.html"" rel=""nofollow"">here</a> you can specify the format of data when you read it with memmapfile, so try to ""play"" with different values. From the <a href=""http://uk.mathworks.com/help/matlab/ref/audioread.html"" rel=""nofollow"">documentation</a> I read that you can read data in double format, so try to modify the memmapfile data format and audioread data format.<br>
Last thing, memmapfile always organize the data in matrix like ""somenumbers x 1"", so if you want the original one you need to use something like reshape.
Anyway if you work with big data I suggest you to try with something different instead memmapfile, because it is very very slow </p>
"
1330926;1803;Ryo;<machine-learning><svm><libsvm>;27883413;0;The meaning of the output from grid.py in libsvm;"<p>I'm a newbie in SVM, and have several questions regarding a tool in <a href=""https://github.com/cjlin1/libsvm"" rel=""nofollow"">libsvm</a>.</p>

<p>There's tools/grid.py which tools/README explains as ""parameter selection tool for C-SVM classification using  47 the RBF (radial basis function) kernel"".</p>

<p>I have 2 questions regarding this tool. </p>

<ol>
<li>What this tool does is: given sets of label/feature_parameters, chooses the most ""efficient"" and ""minimum"" feature_parameters by performing grid search. Am I correct?</li>
</ol>

<p>e.g. Given a dataset like following, whose the label is only dependent on param1,</p>

<pre><code>label, param1, param2, param3
0    , 0     , 61    , 2     
0    , 0     , 92    , 6
1    , 1     , 10    , 32
1    , 1     , 83    , 10
</code></pre>

<p>If we apply grid.py to this dataset, does it tell me that most ""efficient"" (in the way that it precisely identifies the class of a test data) and ""minimum"" (in the way that only that no trivial parameters are included) parameter is param1.</p>

<ol start=""2"">
<li>If the answer of the question above is YES, how can I know which parameters are efficient and minimum? I see some output files but doens't make sense for me. If it's NO, are there any de facto standard method for doing what I want?</li>
</ol>
";27982247;1330926;1803;Ryo;0;27982247;"<p>Probably found the answer.</p>

<hr>

<h3>Question 1. What this tool does is: given sets of label/feature_parameters, chooses the most ""efficient"" and ""minimum"" feature_parameters by performing grid search. Am I correct?</h3>

<p>The answer is No.
grid.py performs <a href=""http://scikit-learn.org/stable/modules/grid_search.html"" rel=""nofollow"">grid search</a> and estimates the best <code>cost</code> and <code>gamma</code> value. So it helps making SVM ""efficient"" anyway, but not doesn't help finding minimum set of features (well, there's no ""absolute minimum"" probably, because accuracy and the number of features is probably in proportion).</p>

<h3>Question 2. If the answer of the question above is YES, how can I know which parameters are efficient and minimum? I see some output files but doesn't make sense for me. If it's NO, are there any de facto standard method for doing what I want?</h3>

<p>There probably isn't any de facto standard method. Perhaps when deciding feature parameters, we should first take approach from domain's perspective, and then adjust them in mathematical approach. (e.g. if you're making SVM to descriminate malwares and benign apps, you should think of the behaviour and tendency of malwares, and decide the ""parameter candidates"" first, then apply mathematical approach like calculating average and deviation of each features.)</p>
"
1330926;1803;Ryo;<machine-learning><svm><libsvm>;27883413;0;The meaning of the output from grid.py in libsvm;"<p>I'm a newbie in SVM, and have several questions regarding a tool in <a href=""https://github.com/cjlin1/libsvm"" rel=""nofollow"">libsvm</a>.</p>

<p>There's tools/grid.py which tools/README explains as ""parameter selection tool for C-SVM classification using  47 the RBF (radial basis function) kernel"".</p>

<p>I have 2 questions regarding this tool. </p>

<ol>
<li>What this tool does is: given sets of label/feature_parameters, chooses the most ""efficient"" and ""minimum"" feature_parameters by performing grid search. Am I correct?</li>
</ol>

<p>e.g. Given a dataset like following, whose the label is only dependent on param1,</p>

<pre><code>label, param1, param2, param3
0    , 0     , 61    , 2     
0    , 0     , 92    , 6
1    , 1     , 10    , 32
1    , 1     , 83    , 10
</code></pre>

<p>If we apply grid.py to this dataset, does it tell me that most ""efficient"" (in the way that it precisely identifies the class of a test data) and ""minimum"" (in the way that only that no trivial parameters are included) parameter is param1.</p>

<ol start=""2"">
<li>If the answer of the question above is YES, how can I know which parameters are efficient and minimum? I see some output files but doens't make sense for me. If it's NO, are there any de facto standard method for doing what I want?</li>
</ol>
";27982247;1858151;3363;stefan;1;28943084;"<p><em>[...] given sets of label/feature_parameters, chooses the most ""efficient"" and ""minimum"" feature_parameters [...]</em></p>

<p><strong>No</strong>. Grid.py is a tool that does a grid search for the best training parameters on a given feature vector set. These parameters c, g are subsequently used for training. Do not confuse data (called feature vectors) and parameters for the training tools. You actually named the features in your table ""params"" which is misleading.</p>

<p><em>[...] dataset like following, whose the label is only dependent on param1 [...]</em>.</p>

<p><strong>No</strong>. Of course there is a perfect correlation to the first feature(badly named param1). But there is also a nice correlation to the third feature (named param3). SVM will make use of all input features. only constant features are guaranteed to be useless. they are omitted in scaling/training. </p>

<p><em>If we apply grid.py to this dataset, does it tell me that most ""efficient"" (in the way that it precisely identifies the class of a test data) and ""minimum"" (in the way that only that no trivial parameters are included) parameter is param1.</em></p>

<p><strong>No</strong>. What you are speaking of is called <a href=""http://en.wikipedia.org/wiki/Feature_selection"" rel=""nofollow"">feature selection/reduction</a>.</p>
"
2085886;319;neutralino;<python><arrays><numpy><matrix><machine-learning>;27884413;1;Numpy: select value at a particular row for each column of a matrix;"<p>I have a 2D matrix <code>X = ((a11, a12, .. a1n), (a21 .. a2n) .. (am1, .. amn))</code> and a 1D vector <code>y = [y1, ..., yn]</code> each <code>yi</code> is between <code>1</code> and <code>m</code>. For each column <code>i</code> of <code>X</code> I want to pick out the element at row <code>yi</code>. That is, I want to pick out the vector <code>z = (a_(y1 1), ... a_(yn n))</code>. </p>

<p>Is there a vectorized way to do this?</p>
";27884515;1078084;67300;Akavall;1;27884515;"<p>How about this: </p>

<pre><code>In [39]: x = np.arange(12).reshape(4,3)

In [40]: y = np.array([0,3,2])

In [41]: x[y[None, :], np.arange(len(y))[None,:]][0]
Out[41]: array([ 0, 10,  8])

In [42]: x
Out[42]: 
array([[ 0,  1,  2],
       [ 3,  4,  5],
       [ 6,  7,  8],
       [ 9, 10, 11]])
</code></pre>
"
2085886;319;neutralino;<python><arrays><numpy><matrix><machine-learning>;27884413;1;Numpy: select value at a particular row for each column of a matrix;"<p>I have a 2D matrix <code>X = ((a11, a12, .. a1n), (a21 .. a2n) .. (am1, .. amn))</code> and a 1D vector <code>y = [y1, ..., yn]</code> each <code>yi</code> is between <code>1</code> and <code>m</code>. For each column <code>i</code> of <code>X</code> I want to pick out the element at row <code>yi</code>. That is, I want to pick out the vector <code>z = (a_(y1 1), ... a_(yn n))</code>. </p>

<p>Is there a vectorized way to do this?</p>
";27884515;3923281;131023;Alex Riley;1;27886919;"<p>As an alternative solution, <code>np.choose</code> is useful for making the selections.</p>

<pre><code>&gt;&gt;&gt; x = np.arange(16).reshape(4,4)
</code></pre>

<p>So <code>x</code> looks like this:</p>

<pre><code>array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11],
       [12, 13, 14, 15]])
</code></pre>

<p>Now the selection of the value at a particular row <code>y</code> in each column can be done like this:</p>

<pre><code>&gt;&gt;&gt; y = np.array([3, 0, 2, 1])
&gt;&gt;&gt; np.choose(y, x)
array([12, 1, 10,  7])
</code></pre>
"
4273266;4775;Elizabeth Susan Joseph;<machine-learning><scikit-learn>;27884601;1;Error when I pass a different form of K in knearest neighbours(Sci kit learn);"<p>When running k nearest neighbors in scikit learn, When I set k as 21 I get value error. but when I set k as <code>k=np.arange(20) +1</code> I dont get an error, so what is the difference between these two? </p>

<pre><code>k = np.arange(21)

parameters = {'n_neighbors': k}
knn = sklearn.neighbors.KNeighborsClassifier()

clf = sklearn.grid_search.GridSearchCV(knn, parameters, cv=10)
clf.fit(X_train, Y_train)

ValueError: Invalid shape in axis 1: 0.
</code></pre>

<p>Also can some one explain me what is the </p>

<pre><code>a = clf.grid_scores_
scores = [b.cv_validation_scores for b in a]
</code></pre>

<p>Out put of scores is as follows :</p>

<pre><code>array([ 1.        ,  0.90909091,  1.        ,  0.72727273,  0.9       ,
         1.        ,  1.        ,  1.        ,  1.        ,  0.88888889]),
</code></pre>

<p><code>clf.grid_scores_</code> is accuracy of the classifier but what is that <code>cv_validation</code> scores?? </p>
";27885788;1190430;5186;Artem Sobolev;1;27885788;"<p><code>GridSearchCV</code> expects a list of values for each parameter to search over. If you want to set just one value for the search, put it into list:</p>

<pre><code>parameters = {'n_neighbors': [21]} # ok
parameters = {'n_neighbors': np.range(21)} # error, the first value is 0
parameters = {'n_neighbors': np.range(21) + 1} # ok
</code></pre>

<p>Notice that each value in that list should be valid value for that parameter. For example, using <code>np.range(p)</code> as a list of parameters for <code>n_neighbors</code> is wrong since it has <code>0</code> which is invalid value for number of neighbors.</p>

<p>According to the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn-grid-search-gridsearchcv"" rel=""nofollow"">documentation</a>,</p>

<blockquote>
  <p><code>cv_validation_scores</code> [is] the list of scores for each fold</p>
</blockquote>

<p>Thus <code>grid_scores_</code> must be averages of corresponding <code>cv_validation_scores</code>.</p>
"
4265201;45;Axe;<machine-learning><scikit-learn><nltk>;27896726;0;importing own data for document classification;"<p>I have folders each having  multiple text files. the folder are named on the class of the text files. How do i import these folders and files for document classification in nltk/ scikit learn. I am planning to use Bi normal separartion for feature selection and SVM for classification. any help will be appreciated</p>
";27901659;1330293;33731;elyase;0;27901659;"<p>Take a look at <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html"" rel=""nofollow"">load_files</a> which serves this exact purpose. Here you can also find some  <a href=""http://scikit-learn.org/stable/datasets/twenty_newsgroups.html"" rel=""nofollow"">examples</a>.</p>
"
4265201;45;Axe;<machine-learning><scikit-learn><nltk>;27896726;0;importing own data for document classification;"<p>I have folders each having  multiple text files. the folder are named on the class of the text files. How do i import these folders and files for document classification in nltk/ scikit learn. I am planning to use Bi normal separartion for feature selection and SVM for classification. any help will be appreciated</p>
";27901659;4265201;45;Axe;0;28128372;"<p>This code works</p>

<pre><code>from sklearn.datasets import load_files

dt=load_files('C:/test4',load_content=True)
print dt.target_names
X, y = dt.data, dt.target
</code></pre>
"
3141533;475;modarwish;<python><machine-learning><nltk>;27897591;1;Python NLTK Naive Bayes Classifier: What is the underlying computation that this classifier uses to classifiy input?;"<p>I use the Naive Bayes classifier in Python NLTK to compute the probability distribution for the following example:</p>

<pre><code>import nltk

def main():
    train = [(dict(feature=1), 'class_x'), (dict(feature=0), 'class_x'),   (dict(feature=0), 'class_y'), (dict(feature=0), 'class_y')]

    test = [dict(feature=1)]

    classifier = nltk.classify.NaiveBayesClassifier.train(train)

    print(""classes available: "", sorted(classifier.labels()))

    print (""input assigned to: "", classifier.classify_many(test))

    for pdist in classifier.prob_classify_many(test):
        print (""probability distribution: "")
        print ('%.4f %.4f' % (pdist.prob('class_x'), pdist.prob('class_y')))

if __name__ == '__main__':
    main()
</code></pre>

<p>There are two classes (class_x and class_y) in the training dataset. Two inputs are given to each of the classes. For class_x, the first input feature has a value of 1, and the second a value of 0. For class_y, both input features have a value of 0. The test dataset is made up of one input, with a value of 1.</p>

<p>When I run the code, the output is:</p>

<pre><code>classes available:  ['class_x', 'class_y']
input assigned to:  ['class_x']
0.7500 0.2500
</code></pre>

<p>To get the probabilities, or likelihoods, for each class, the classifier should multiply the prior of the class (in this case, 0.5) by the probabilities of each of the features in the class. Smoothing should be considered. </p>

<p>I usually use a formula similar to this (or a similar variant):</p>

<p>P(feature|class) = prior of class * frequency of feature in class <strong>+1</strong> / total features in class + <strong>Vocabulary size</strong>. Smoothing can vary and slightly changes the outcome. </p>

<p>In the example code above, how exactly does the classifier compute the probability distribution? What is the formula used? </p>

<p>I checked <a href=""http://www.nltk.org/howto/classify.html"" rel=""nofollow"">here</a> and <a href=""http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/"" rel=""nofollow"">here</a>, but could not get any information as to exactly how the computation is done. </p>

<p>Thanks in advance.</p>
";30143801;1277335;2263;daviddeath;2;30143801;"<p>From the source code</p>

<p><a href=""https://github.com/nltk/nltk/blob/develop/nltk/classify/naivebayes.py#L9yo"" rel=""nofollow"">https://github.com/nltk/nltk/blob/develop/nltk/classify/naivebayes.py#L9yo</a></p>

<pre><code>|                       P(label) * P(features|label)
|  P(label|features) = ------------------------------
|                              P(features)
</code></pre>
"
3423045;978;Deepak;<opencv><math><image-processing><machine-learning><svm>;27898181;1;Correct way to do Min-Max normalization;"<p>I am implementing alphabet classification using opencv svm. 
I have doubt in normalizing feature vector. 
I have two ways of normalizing feature vector, 
I need to find which is logically correct normalization method ??</p>

<p><strong>Method 1</strong></p>

<p>Suppose I have 3 feature vector as follows</p>

<pre><code>[2,  3,  8, 5 ] -&gt; image 1
[3,  5,  2, 5 ] -&gt; image 2
[9,  3,  8, 5 ] -&gt; image 3
</code></pre>

<p>And each value in feature vector is obtained by convolving the pixel with a kernal.</p>

<p>Currently I am finding maximum and minimum value of the each column and doing normalization based on that.</p>

<p>In the above case first column is <code>[2, 3, 9]</code></p>

<pre><code>min = 2
max = 9
</code></pre>

<p>and normalization of first column is done based on that. Likewise all other columns are normalized</p>

<p><strong>Method 2</strong></p>

<p>If the kernal is as follows </p>

<pre><code>[-1   0  1]
[-1   0  1]
[-1   0  1]
</code></pre>

<p>then maximum and minimum value can obtained by convolving with above kernel is as follows (8 bit image- Intensity range: 0-255) </p>

<pre><code>max val = 765
min val = -765
</code></pre>

<p>And normalize every value with above max min ?</p>

<p>Which is logically correct way to do normalization (method-1 or method-2) ?</p>
";27911434;3214668;650;Adrien Descamps;1;27911434;"<p>The standard way to do it is method-1 (see the answer to <a href=""https://stackoverflow.com/questions/15436367/svm-scaling-input-values"">this question</a>). I also recommend you to read <a href=""http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf"" rel=""nofollow noreferrer"">this paper</a> for a good reference about svm training.</p>

<p>However, in you case, the range of all features computed with the same kernel will be similar , and method-1 may hurt more than it helps (for example by increasing noise of almost constant features).</p>

<p>So my advice would be : test both methods, and evalute performances to see what works best in your case.</p>
"
4171438;23;Mohammad Alqudah;<machine-learning><data-mining><apriori>;27908105;2;Association Rule Mining on Categorical Data with a High Number of Values for Each Attribute;"<p>I am struggling with association rule mining for a data set, the data set has a lot of binary attributes but also has a lot of categorical attributes. Converting the categorical to binary is theoretically possible but not practical. I am searching for a technique to overcome this issue.</p>

<p>Example of data for a car specifications, to execute association rule mining, the car color attribute should be a binary, and in the case of colors, we have a a lot of colors to be transferred to binary (My data set is insurance claims and its much worse than this example).</p>
";27921330;1060350;70512;Has QUIT--Anony-Mousse;2;27921330;"<p>Association rule mining doesn't use ""attributes"". It processes <strong>market basket</strong> type of data.
It does not make sense to preprocess it to binary attributes. Because you would need to convert the binary attributes into items again (worst case, you would then tranlate your ""color=blue"" item into ""color_red=0, color_black=0, ... color_blue=1"" if you are also looking for negative rules.</p>

<p>Different algorithms - and different implementations of the theoretically same algorithm, unfortunately - will scale very differently.</p>

<p>APRIORI is designed to scale well with the number of transactions, but not very well with the number of different items that have minimum support; in particular if you are expecting short itemsets to be frequent only. Other algorithms such as Eclat and FP-Growth may be much better there. But YMMV.</p>

<p>First, try to convert the data set into a market basket format, in a way that <em>you</em> consider every item to be relevant. Discard everything else. Then start with a high minimum support, until you start getting results. Running with a too low minimum support may just run out of memory, or may take a long time.</p>

<p>Also, make sure to get a good implementation. A lot of things that claim to be APRIORI are only half of it, and are incredibly slow.</p>
"
447458;3859;Fernando;<python><machine-learning>;27909622;2;Predicting time series data with python;"<p>I'm starting with machine learning and so far have only tested scikit-learn but I couldn't find the right algorithm or an example similar to my problem.</p>

<p>I have a time series showing where an event happened. The location of the event is identified with an integer between 1 and 25 ( including ). At a certain date, an event cannot happen at the same place twice and it always happens in 5 places.</p>

<p>My data looks like this:</p>

<pre><code>2015-01-01,1,3,5,8,9,10
2015-01-03,23,16,3,5,9
2015-01-05,22,16,6,13,11
</code></pre>

<p>The first column is the date and the others are the places. Dates aren't included if nothing happened.</p>

<p>Do you have any recommendations on which algorithm should I take a look to try to predict the numbers ( places ) in the next time series?</p>

<p>An algorithm that is available in a Python library like scikit-learn would be perfect!</p>
";27909790;1330293;33731;elyase;1;27909790;"<p>One idea would to treat it as a multi-class problem. You can imagine this as your target <code>y</code> having 25 rows (actually 24 but forget about it for now) where each column is 1 or 0 representing wether the event happened or not.</p>

<p>As predictors for your <code>X</code> you can chose some lagged average or the last lets say <code>3</code> observations. See <a href=""https://stackoverflow.com/questions/27852199/python-machine-learning-algorithm-to-recognize-known-events/27852335#27852335"">this question</a> for more details.</p>

<p>Some code:</p>

<pre><code>from io import StringIO
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer()

s=""""""
2015-01-01,1,2,3
2015-01-03,1,2,4
2015-01-05,1,2,4
2015-01-07,1,4,3
""""""
df = pd.read_csv(StringIO(s), index_col=0, parse_dates=True, header=None)

mlb = MultiLabelBinarizer()
labels = mlb.fit_transform(df.values)
labels
[[1 1 1 0]
 [1 1 0 1]
 [1 1 0 1]
 [1 0 1 1]]
</code></pre>

<p>We have 4 classes and 4 examples so we get a 4x4 matrix. Columns represent classes/locations and rows are events.</p>

<p>Now we will use the first 3 observations to predict the fourth one:</p>

<pre><code>X = labels[:-1]   
[[1 1 1 0]
 [1 1 0 1]
 [1 1 0 1]]
</code></pre>

<p>We get 4 classes and 3 observations. We need to make it a vector because this is only a sample:</p>

<pre><code>&gt;&gt;&gt; X.flatten()
[1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1]
</code></pre>

<p>Each column here is a feature/predictor that can be interpreted in the following way: A 1 in the first column means that class one was present 3 days a go. A 0 in the 7th column means that class 3 was not present in 2 days ago, and so on.</p>

<p>So now we have one sample/event (one row of the final <code>X</code> matrix) and the corresponding label(one row of the target <code>y</code>):</p>

<pre><code>&gt;&gt;&gt; labels[-1]
[1 0 1 1]
</code></pre>

<p>If you follow this procedure you will be able to get a training set that can be fed to a classifier.</p>
"
4140027;3985;tumbleweed;<machine-learning><scikit-learn><libsvm>;27912872;31;What is the difference between SVC and SVM in scikit-learn?;"<p>From the <a href=""http://scikit-learn.org/stable/modules/svm.html"">documentation</a> scikit-learn implements SVC, NuSVC and LinearSVC which are classes capable of performing multi-class classification on a dataset. By the other hand I also read about that scikit learn also uses libsvm for support vector machine algorithm. I'm a bit confused about what's the difference between SVC and libsvm versions, by now I guess the difference is that SVC is the support vector machine algorithm fot the multiclass problem and libsvm is for the binary class problem. Could anybody help me to understad the difference between this?.</p>
";27913126;1330293;33731;elyase;29;27913126;"<p>They are just different implementations of the same algorithm. The SVM module (SVC, NuSVC, etc) is a wrapper around the <a href=""http://www.csie.ntu.edu.tw/~cjlin/libsvm/"" rel=""noreferrer"">libsvm</a> library and supports different kernels while <code>LinearSVC</code> is based on <a href=""http://www.csie.ntu.edu.tw/~cjlin/liblinear/"" rel=""noreferrer"">liblinear</a> and only supports a linear kernel. So:</p>

<pre><code>SVC(kernel = 'linear')
</code></pre>

<p>is in theory ""equivalent"" to:</p>

<pre><code>LinearSVC()
</code></pre>

<p>Because the implementations are different in practice you will get different results, the most important ones being that LinearSVC only supports a linear kernel, is faster and can scale a lot better.</p>
"
4140027;3985;tumbleweed;<machine-learning><scikit-learn><libsvm>;27912872;31;What is the difference between SVC and SVM in scikit-learn?;"<p>From the <a href=""http://scikit-learn.org/stable/modules/svm.html"">documentation</a> scikit-learn implements SVC, NuSVC and LinearSVC which are classes capable of performing multi-class classification on a dataset. By the other hand I also read about that scikit learn also uses libsvm for support vector machine algorithm. I'm a bit confused about what's the difference between SVC and libsvm versions, by now I guess the difference is that SVC is the support vector machine algorithm fot the multiclass problem and libsvm is for the binary class problem. Could anybody help me to understad the difference between this?.</p>
";27913126;12295283;39;nickz;1;64274403;"<p>This is a snapshot from the book - <strong>Hands-on Machine Learning</strong></p>
<p><img src=""https://i.stack.imgur.com/BtPP2.png"" alt=""This is a snapshot from the book - Hands-on Machine Learning"" /></p>
<p>I hope you may find it useful.</p>
"
4448562;23;sohail siddique;<machine-learning><computer-vision><artificial-intelligence><deep-learning><dbn>;27918774;-2;how to design feature extraction layer for DBN for face recognition;"<p>I am trying to use deep belief networks for face recognition. But I am a beginner in this area, I have read the research papers and documentations available on the Internet and I understood the basic concept for binary images. But still when I sit down to code I find great difficulty because nothing is explained from a programmers perspective all you find is energy functions and all that stuff.
Can some body help me design(code) hidden layer for a gray scale Face image ? (To be more specific what should my hidden layer be should it be an array of different filters or something else ....) </p>
";27932427;4451148;26;Brian S. Penn;-1;27932427;"<p>I have experience with Neural Networks and Self-Organizing Maps dating back to the late-1980s, but I too find Energy-based Restricted Boltzmann Machines somewhat daunting to just sit down and implement.  I found the following websites with either Matlab code (Octave?) and C.  They're from the Netflix competition (winner from University of Toronto), but it's a good winning example and should provide some insight.</p>

<p><a href=""http://imonad.com/rbm/restricted-boltzmann-machine/"" rel=""nofollow"">http://imonad.com/rbm/restricted-boltzmann-machine/</a></p>

<p><a href=""https://code.google.com/p/nprizeadditions/source/browse/trunk/rbm.c"" rel=""nofollow"">https://code.google.com/p/nprizeadditions/source/browse/trunk/rbm.c</a></p>

<p>I might also suggest taking Andrew Ng's Coursera on Machine Learning from Stanford (it's free and a new session starts on Jan 19, 2015)  I've viewed a number of the lectures and they are very good.  Hope this helps.</p>
"
4334098;109;Schweigerama;<python><machine-learning><scikit-learn>;27924292;1;Python scikit-learn Predictionfail;"<p>I'm new to Python and Machine Learning. I try to implement a simple Machine Learning script to predict the Topic of a Text, e.g. Texts about Barack Obama should be Mapped to Politicians.</p>

<p>I think i make the right moves to do that, but im not 100% sure so i ask you guys.</p>

<p>First of all here is my little script:</p>

<pre><code>#imports
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
#dictionary for mapping the targets
categories_dict = {'0' : 'politiker','1' : 'nonprofit org'}

import glob
#get filenames from docs
filepaths = glob.glob('Data/*.txt')
print(filepaths)

docs = []

for path in filepaths:
doc = open(path,'r')
docs.append(doc.read())
#print docs


count_vect = CountVectorizer()
#train Data
X_train_count = count_vect.fit_transform(docs)
#print X_train_count.shape

#tfidf transformation (occurences to frequencys)
tfdif_transform = TfidfTransformer()
X_train_tfidf = tfdif_transform.fit_transform(X_train_count)

#get the categories you want to predict in a set, these must be in the order the train        docs are!
categories = ['0','0','0','1','1']
clf = MultinomialNB().fit(X_train_tfidf,categories)

#try to predict
to_predict = ['Barack Obama is the President of the United States','Greenpeace']

#transform(not fit_transform) the new data you want to predict
X_pred_counts = count_vect.transform(to_predict)
X_pred_tfidf = tfdif_transform.transform(X_pred_counts)
print X_pred_tfidf

#predict
predicted = clf.predict(X_pred_tfidf)

for doc,category in zip(to_predict,predicted):
    print('%r =&gt; %s' %(doc,categories_dict[category]))
</code></pre>

<p>Im sure about the general Workflow that is required to use this, but im not sure how i map the categories to the docs i use to train the classifier. I know that they must be in correct order and i think i got that but it doesn't output the right category.</p>

<p>Is that because my Documents i use to Train the Classifier are bad, or do i make a certain mistake im not aware of?</p>

<p>He predicts that both new Texts are about Target 0 (Politicians)</p>

<p>Thanks in advance.</p>
";27925025;1330293;33731;elyase;1;27925025;"<p>It looks like the model hyper parameters are not rightly tuned. It is difficult to make conclusions with so little data but if you use:</p>

<pre><code>model = MultinomialNB(0.5).fit(X, y)
# or
model = LogisticRegression().fit(X, y)
</code></pre>

<p>you will get the expected results, at least for words like ""Greenpeace"", ""Obama"", ""President"" which are so obviously correlated with its corresponding class. I took a quick look at the coefficients of the model and it seems to be doing the right thing.</p>

<p>For a more sophisticated approach to topic modeling I recommend you take a look at <a href=""https://radimrehurek.com/gensim/tutorial.html"" rel=""nofollow"">gensim</a>.</p>
"
961627;10989;user961627;<python><machine-learning><computer-vision><classification><logarithm>;27926804;1;Classifier scores normalized for comparison, using log scale;"<p>What's a good method to compare the scored results of different classifiers?</p>

<p>For example, let's say I have a classifier that deals with 12 different classes of fruit. 
Out of these 12 classes, 4 of them represent different views of an apple, 4 represent views of a banana, and 4 represent a pineapple. This is a multi-class classifier, and given an image, it assigns a score to all of the possible 12 fruit classes that the image might belong to. The class that got the highest score is selected as THE class of the image.</p>

<p>Now in addition to that - I have 3 individual classifiers, one for apples, one for bananas, and one for pineapples. Each classifier deals with 4 different views of an individual fruit.</p>

<p>I want to compare whether using the single 12-class classifier gives better results than using a combination of the individual 4-class classifiers.</p>

<p>When I run the 12-class classifier on images of apples, the results are indeed less accurate than the results of running the individual apple classifier, and the same goes for bananas and pineapples.</p>

<p>What I want to do now is build a combination of the 3 classifiers. So my program will run all 3 classifiers on a single image and tell me what the most likely class is. </p>

<p>The problem is - how can I normalize the scores across the different classifiers, so that I can make a comparison among the the 3 classifiers' classes and choose the class with the highest score? Although the method used to train the individual classifiers was the same, I doubt I can directly compare their scores without some kind of normalization.</p>

<p>Would it be practical to simply convert all the scores to the log scale and then compare them?</p>
";;628538;2059;Bharat;1;27937924;"<p>A popular method to normalize classifiers is done by Platt-Scaling, which is implemented in libSVM. Its a straight forward normalization as described in the following link.</p>

<p><a href=""http://en.wikipedia.org/wiki/Platt_scaling"" rel=""nofollow"">http://en.wikipedia.org/wiki/Platt_scaling</a></p>
"
492372;24872;London guy;<machine-learning><nlp><topic-modeling><text-analysis><mallet>;27927556;5;What do the parameters of the csvIterator mean in Mallet?;"<p>I am using mallet topic modelling sample code and though it runs fine, I would like to know what the parameters of this statement actually mean?</p>

<pre><code>instances.addThruPipe(new CsvIterator(new FileReader(dataFile),
                                      ""(\\w+)\\s+(\\w+)\\s+(.*)"",
                                      3, 2, 1)  // (data, target, name) field indices                    
                     );
</code></pre>
";27929358;419338;13702;mbatchkarov;8;27929358;"<p>From the <a href=""http://mallet.cs.umass.edu/api/"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p>This iterator, perhaps more properly called a Line Pattern Iterator, reads through a file and returns one instance per line, based on a regular expression.</p>
  
  <p>If you have data of the form</p>
  
  <p>[name]  [label]  [data]</p>
</blockquote>

<p>The call you are interested in is</p>

<pre><code>CsvIterator(java.io.Reader input, java.lang.String lineRegex, 
            int dataGroup, int targetGroup, int uriGroup) 
</code></pre>

<p>The first parameter is how data is read in, like a file reader or a string reader. The second parameter is the regex that is used to extract data from each line that's read from the reader. In your example, you've got <code>(\\w+)\\s+(\\w+)\\s+(.*)</code> which translates to:</p>

<ul>
<li>1 or more alphanumeric characters (capture group, this is the name of the instance), followed by</li>
<li>1 or more whitespace character (tab, space, ..), followed by</li>
<li>1 or more alphanumeric characters (capture group, this is the label/target), followed by</li>
<li>1 or more whitespace character (tab, space, ..), followed by</li>
<li>0 or more characters (this is the data)</li>
</ul>

<p>The numbers <code>3, 2, 1</code> indicate the data comes last, the target comes second, and the name comes first. The regex basically ensures the format of each line is as stated in the documentation:</p>

<pre><code>test1 spam Wanna buy viagra?
test2 not-spam Hello, are you busy on Sunday?
</code></pre>

<p><code>CsvIterator</code> is a terrible name, because it is not actually comma-separated values that this class reads in, it is whitespace-separated (space, tab, ...) values. </p>
"
492372;24872;London guy;<machine-learning><nlp><topic-modeling><text-analysis><mallet>;27927556;5;What do the parameters of the csvIterator mean in Mallet?;"<p>I am using mallet topic modelling sample code and though it runs fine, I would like to know what the parameters of this statement actually mean?</p>

<pre><code>instances.addThruPipe(new CsvIterator(new FileReader(dataFile),
                                      ""(\\w+)\\s+(\\w+)\\s+(.*)"",
                                      3, 2, 1)  // (data, target, name) field indices                    
                     );
</code></pre>
";27929358;4413371;310;drp;0;39545089;"<p>Explanation given in above answer is way too good.</p>

<p>However one point is missing. Sequence of regular expression(regex) for each one of the data, label and name fields of input instances in Line regex needs to be in correspondence to the way instances are provided in input file i.e. if say you are providing name as 1st field , data as second field and label as 3rd field in your input file then you have to provide regex of name first followed by regex of data and then at last regex of label. Example is as shown below :</p>

<p>Input instance : Mail67(tab space)TCC problems. Hi there, For some reason no administrators in the Old Master Paintings department have been able to get information from TCC. It appears to be going through on JDE, but nothing appears when searched on TCC. Any help or guidance you can offer to f....(tab space)Inc</p>

<p>CsvIterator Parameters: CsvIterator(new FileReader(Path to file), ""(\w+)\t(.*)\t(\w+)"",2,3,1)</p>
"
4285011;167;Owen Kavanagh;<math><machine-learning><probability><bayesian>;27927700;-3;Probability: the one true fish;"<p>I have exams in Machine Learning coming up and I need help answering this question.</p>

<blockquote>
  <p>There are a million identical fish in a lake, one of which has
  swallowed the One True Ring. You must get it back! After months of
  effort, you catch another random fish and pass your metal detector
  over it, and the detector beeps! It is the best metal detector money
  can buy, and has a very low error rate: it fails to beep when near the
  ring only one in a billion times, and it beeps incorrectly only one in
  ten thousand times. What is the probability that, at long last, youâ€™ve
  found your precious ring?</p>
</blockquote>

<p>This is my answer I worked out:</p>

<p><img src=""https://i.stack.imgur.com/bztZM.gif"" alt=""enter image description here""></p>

<p>Is this the right way to work out this type of question and is that somewhat the correct answer?</p>
";27928164;3777428;5331;eigenchris;1;27928164;"<p>What you want is the probability of having the right fish given that the detector beeps, which is <code>P(A|B)</code>.</p>

<p>The <code>P(B|A) = 9999/10000</code> is the probability of the detector beeping given you have the right fish. However, we don't know if the fish you have is the right one. All you know is that the detector beeps, and you can't tell if it's a true positive with probability <code>P(B|A)</code> or a false positive with probability <code>P(B|not A)</code>.</p>

<p>Bayes' theorem lets you switch between <code>P(B|A)</code> and <code>P(A|B)</code>, so the other information isn't useless fluff. You do in need it all to solve the problem.</p>
"
3214477;143;Explorer;<machine-learning><scikit-learn><classification>;27936778;1;Classifier is giving different results every time it is trained even though the training and test data is the same;"<p>The training and test data are pickled and loaded. But the results (accuracy and f-measure) seem to vary every time even though the classifier is trained with the same training data and tested with the same test data. How is this happening? The classifier I'm referring to is Extreme Learning Machine.</p>
";27938100;628538;2059;Bharat;2;27938100;"<p>Depends on the classification algorithm you chose. If you chose something like random forests, each time a new model would be learnt, so such an observation is possible.</p>
"
3214477;143;Explorer;<machine-learning><scikit-learn><classification>;27936778;1;Classifier is giving different results every time it is trained even though the training and test data is the same;"<p>The training and test data are pickled and loaded. But the results (accuracy and f-measure) seem to vary every time even though the classifier is trained with the same training data and tested with the same test data. How is this happening? The classifier I'm referring to is Extreme Learning Machine.</p>
";27938100;486262;2385;Leon palafox;0;27967404;"<p>How different are the weights of the output? Do you get different parameters?</p>

<p>Bear in mind many algorithms start with different random seeds, which make them behave different for different sets of experiments.</p>

<p>How different are the outputs? Are they wildly different? If that is the case, perhaps you need to let it run longer.</p>

<p>As someone said, you have to use cross validation to find the best set of parameters and to do that you need to rotate your training and dataset.</p>
"
4435720;1;Stephen;<machine-learning><neural-network><gradient-descent>;27942621;0;Gradient decent on the inputs of a pre-trained neural network to achieve a target y-value;"<p>I have a trained neural network which suitably maps my inputs to my outputs. Is it then possible to specify a desired y output and then use a gradient decent method to determine the optimum input values to get that output?</p>

<p>When using backpropegation, the partial derivative of a weight is used with error function to proportionally adjust the weights; is there a way to do something similar with the input values themselves and a target y value?</p>
";;672018;11366;Andrzej Gis;0;27986201;"<p>A neural network is basically a complex mathematical function. By adjusting the weights you basically adjust that function's parameters. Given that, your question is if you can easily and automatically <a href=""http://en.wikipedia.org/wiki/Inverse_function"" rel=""nofollow"">invert the function</a>. I don't think this can be done easily.</p>

<p>I think that the only thing you can do is to create another inverted network and train it with inverted data.</p>
"
961627;10989;user961627;<python><numpy><machine-learning><scikit-learn><confusion-matrix>;27944762;1;Invalid index to scalar variable in scikit confusion matrix;"<p>I'm using a confusion matrix that is working just fine until I get to a specific part of my numpy arrays.</p>

<p>The ground truth results are stored in an array called <code>y_test</code>, while the classfiers' results are stored in <code>r</code>.</p>

<p>When I use the confusion matrix for the whole set of results, there are no problems. </p>

<p>But I want to divide the results from my experiment.
I have 3 specific classifiers' results which are stored in arrays called <code>c</code>, <code>b</code> and <code>t</code>.</p>

<p>Now I want to compare the results of these 3 specific classifiers against some specific indices of the overall results. For example, I want to highlight the confusion matrix for the results for classifier <code>C</code> specifically from indices 91 to 180 of the overall results.</p>

<p>For classifier <code>B</code> I want to see the confusion matrix of the results from indices 1 to 90.
And so on.</p>

<p>This is my code, below. For the first 2 confusion matrices, there are no problems. They show up fine.</p>

<pre><code>cm_c = confusion_matrix(y_test[91:80],c[91:80])
plt.matshow(cm_c)
plt.title('Confusion matrix')
plt.colorbar()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

cm_b = confusion_matrix(y_test[1:90],b[1:90])
plt.matshow(cm_b)
plt.title('Confusion matrix')
plt.colorbar()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

cm_t = confusion_matrix(y_test[228:317,t[228:317])
plt.matshow(cm_t)
plt.title('Confusion matrix')
plt.colorbar()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()
</code></pre>

<p>But for the last set of results above (using the results array from classifier <code>T</code>), I get the following error:</p>

<pre><code>cm_t = confusion_matrix(y_test[228:317], t[228:317])
IndexError: invalid index to scalar variable
</code></pre>

<p>I don't know what's wrong.</p>
";27945255;1330293;33731;elyase;1;27945255;"<p>In your line:</p>

<pre><code>cm_t = confusion_matrix(y_test[228:317,t[228:317])
</code></pre>

<p>you are missing a bracket. It should be:</p>

<pre><code>cm_t = confusion_matrix(y_test[228:317],t[228:317])
</code></pre>
"
4178757;167;Denny Dharmawan;<python><machine-learning><theano>;27948324;3;Implementing LeCun Local Contrast Normalization with Theano;"<p>I'm trying to use the code that I found to implement the LeCun Local Contrast Normalization but I get incorrect result. The code is in Python and uses the <code>theano</code> library.</p>

<pre><code>def lecun_lcn(input, img_shape, kernel_shape, threshold=1e-4):
    """"""
    Yann LeCun's local contrast normalization
    Orginal code in Theano by: Guillaume Desjardins
    """"""
    input = input.reshape(input.shape[0], 1, img_shape[0], img_shape[1])
    X = T.matrix(dtype=theano.config.floatX)
    X = X.reshape(input.shape)

    filter_shape = (1, 1, kernel_shape, kernel_shape)
    filters = gaussian_filter(kernel_shape).reshape(filter_shape)

    convout = conv.conv2d(input=X,
                             filters=filters,
                             image_shape=(input.shape[0], 1, img_shape[0], img_shape[1]),
                             filter_shape=filter_shape,
                             border_mode='full')

    # For each pixel, remove mean of 9x9 neighborhood

    mid = int(np.floor(kernel_shape / 2.))
    centered_X = X - convout[:, :, mid:-mid, mid:-mid]
    # Scale down norm of 9x9 patch if norm is bigger than 1
    sum_sqr_XX = conv.conv2d(input=centered_X ** 2,
                             filters=filters,
                             image_shape=(input.shape[0], 1, img_shape[0], img_shape[1]),
                             filter_shape=filter_shape,
                             border_mode='full')

    denom = T.sqrt(sum_sqr_XX[:, :, mid:-mid, mid:-mid])
    per_img_mean = denom.mean(axis=[1, 2])
    divisor = T.largest(per_img_mean.dimshuffle(0, 'x', 'x', 1), denom)
    divisor = T.maximum(divisor, threshold)

    new_X = centered_X / divisor
    new_X = new_X.dimshuffle(0, 2, 3, 1)
    new_X = new_X.flatten(ndim=3)

    f = theano.function([X], new_X)
    return f(input)
</code></pre>

<p>Here is the testing code:</p>

<pre><code>x_img_origin = plt.imread(""..//data//Lenna.png"")
x_img = plt.imread(""..//data//Lenna.png"")
x_img_real_result = plt.imread(""..//data//Lenna_Processed.png"")

x_img = x_img.reshape(1, x_img.shape[0], x_img.shape[1], x_img.shape[2])
for d in range(3):
    x_img[:, :, :, d] = tools.lecun_lcn(x_img[:, :, :, d], (x_img.shape[1], x_img.shape[2]), 9)
x_img = x_img[0]

pylab.subplot(1, 3, 1); pylab.axis('off'); pylab.imshow(x_img_origin)
pylab.gray()
pylab.subplot(1, 3, 2); pylab.axis('off'); pylab.imshow(x_img)
pylab.subplot(1, 3, 3); pylab.axis('off'); pylab.imshow(x_img_real_result)
pylab.show()
</code></pre>

<p>Here is the result: </p>

<p><img src=""https://i.stack.imgur.com/UnCq9.jpg"" alt=""Example image processing results""></p>

<p><em>(left to right: origin, my result, the expected result)</em> </p>

<p>Could someone tell me what I did wrong with the code?</p>
";;4810149;11;Hobinson;1;29746766;"<p>I think these two lines may have some mistakes on the matrix axes:</p>

<pre><code>per_img_mean = denom.mean(axis=[1, 2])
divisor = T.largest(per_img_mean.dimshuffle(0, 'x', 'x', 1), denom)
</code></pre>

<p>and it should be rewritten as:</p>

<pre><code>per_img_mean = denom.mean(axis=[2, 3])
divisor = T.largest(per_img_mean.dimshuffle(0, 1, 'x', 'x'), denom)
</code></pre>
"
4178757;167;Denny Dharmawan;<python><machine-learning><theano>;27948324;3;Implementing LeCun Local Contrast Normalization with Theano;"<p>I'm trying to use the code that I found to implement the LeCun Local Contrast Normalization but I get incorrect result. The code is in Python and uses the <code>theano</code> library.</p>

<pre><code>def lecun_lcn(input, img_shape, kernel_shape, threshold=1e-4):
    """"""
    Yann LeCun's local contrast normalization
    Orginal code in Theano by: Guillaume Desjardins
    """"""
    input = input.reshape(input.shape[0], 1, img_shape[0], img_shape[1])
    X = T.matrix(dtype=theano.config.floatX)
    X = X.reshape(input.shape)

    filter_shape = (1, 1, kernel_shape, kernel_shape)
    filters = gaussian_filter(kernel_shape).reshape(filter_shape)

    convout = conv.conv2d(input=X,
                             filters=filters,
                             image_shape=(input.shape[0], 1, img_shape[0], img_shape[1]),
                             filter_shape=filter_shape,
                             border_mode='full')

    # For each pixel, remove mean of 9x9 neighborhood

    mid = int(np.floor(kernel_shape / 2.))
    centered_X = X - convout[:, :, mid:-mid, mid:-mid]
    # Scale down norm of 9x9 patch if norm is bigger than 1
    sum_sqr_XX = conv.conv2d(input=centered_X ** 2,
                             filters=filters,
                             image_shape=(input.shape[0], 1, img_shape[0], img_shape[1]),
                             filter_shape=filter_shape,
                             border_mode='full')

    denom = T.sqrt(sum_sqr_XX[:, :, mid:-mid, mid:-mid])
    per_img_mean = denom.mean(axis=[1, 2])
    divisor = T.largest(per_img_mean.dimshuffle(0, 'x', 'x', 1), denom)
    divisor = T.maximum(divisor, threshold)

    new_X = centered_X / divisor
    new_X = new_X.dimshuffle(0, 2, 3, 1)
    new_X = new_X.flatten(ndim=3)

    f = theano.function([X], new_X)
    return f(input)
</code></pre>

<p>Here is the testing code:</p>

<pre><code>x_img_origin = plt.imread(""..//data//Lenna.png"")
x_img = plt.imread(""..//data//Lenna.png"")
x_img_real_result = plt.imread(""..//data//Lenna_Processed.png"")

x_img = x_img.reshape(1, x_img.shape[0], x_img.shape[1], x_img.shape[2])
for d in range(3):
    x_img[:, :, :, d] = tools.lecun_lcn(x_img[:, :, :, d], (x_img.shape[1], x_img.shape[2]), 9)
x_img = x_img[0]

pylab.subplot(1, 3, 1); pylab.axis('off'); pylab.imshow(x_img_origin)
pylab.gray()
pylab.subplot(1, 3, 2); pylab.axis('off'); pylab.imshow(x_img)
pylab.subplot(1, 3, 3); pylab.axis('off'); pylab.imshow(x_img_real_result)
pylab.show()
</code></pre>

<p>Here is the result: </p>

<p><img src=""https://i.stack.imgur.com/UnCq9.jpg"" alt=""Example image processing results""></p>

<p><em>(left to right: origin, my result, the expected result)</em> </p>

<p>Could someone tell me what I did wrong with the code?</p>
";;5896264;91;Jos van de Wolfshaar;6;35259710;"<p>Here is how I implemented local contrast normalization as reported in Jarrett et al (<a href=""http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf"">http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf</a>). You can use it as a separate layer.</p>

<p>I tested it on the code from the LeNet tutorial of theano in which I applied LCN to the input and to each convolutional layer which yields slightly better results.</p>

<p>You can find the full code here:
<a href=""https://github.com/jostosh/theano_utils/blob/master/lcn.py"">https://github.com/jostosh/theano_utils/blob/master/lcn.py</a> </p>

<pre><code>class LecunLCN(object):
def __init__(self, X, image_shape, threshold=1e-4, radius=9, use_divisor=True):
    """"""
    Allocate an LCN.

    :type X: theano.tensor.dtensor4
    :param X: symbolic image tensor, of shape image_shape

    :type image_shape: tuple or list of length 4
    :param image_shape: (batch size, num input feature maps,
                         image height, image width)
    :type threshold: double
    :param threshold: the threshold will be used to avoid division by zeros

    :type radius: int
    :param radius: determines size of Gaussian filter patch (default 9x9)

    :type use_divisor: Boolean
    :param use_divisor: whether or not to apply divisive normalization
    """"""

    # Get Gaussian filter
    filter_shape = (1, image_shape[1], radius, radius)

    self.filters = theano.shared(self.gaussian_filter(filter_shape), borrow=True)

    # Compute the Guassian weighted average by means of convolution
    convout = conv.conv2d(
        input=X,
        filters=self.filters,
        image_shape=image_shape,
        filter_shape=filter_shape,
        border_mode='full'
    )

    # Subtractive step
    mid = int(numpy.floor(filter_shape[2] / 2.))

    # Make filter dimension broadcastable and subtract
    centered_X = X - T.addbroadcast(convout[:, :, mid:-mid, mid:-mid], 1)

    # Boolean marks whether or not to perform divisive step
    if use_divisor:
        # Note that the local variances can be computed by using the centered_X
        # tensor. If we convolve this with the mean filter, that should give us
        # the variance at each point. We simply take the square root to get our
        # denominator

        # Compute variances
        sum_sqr_XX = conv.conv2d(
            input=T.sqr(centered_X),
            filters=self.filters,
            image_shape=image_shape,
            filter_shape=filter_shape,
            border_mode='full'
        )


        # Take square root to get local standard deviation
        denom = T.sqrt(sum_sqr_XX[:,:,mid:-mid,mid:-mid])

        per_img_mean = denom.mean(axis=[2,3])
        divisor = T.largest(per_img_mean.dimshuffle(0, 1, 'x', 'x'), denom)
        # Divisise step
        new_X = centered_X / T.maximum(T.addbroadcast(divisor, 1), threshold)
    else:
        new_X = centered_X

    self.output = new_X


def gaussian_filter(self, kernel_shape):
    x = numpy.zeros(kernel_shape, dtype=theano.config.floatX)

    def gauss(x, y, sigma=2.0):
        Z = 2 * numpy.pi * sigma ** 2
        return  1. / Z * numpy.exp(-(x ** 2 + y ** 2) / (2. * sigma ** 2))

    mid = numpy.floor(kernel_shape[-1] / 2.)
    for kernel_idx in xrange(0, kernel_shape[1]):
        for i in xrange(0, kernel_shape[2]):
            for j in xrange(0, kernel_shape[3]):
                x[0, kernel_idx, i, j] = gauss(i - mid, j - mid)

    return x / numpy.sum(x)
</code></pre>
"
1835351;1271;user1835351;<python><numpy><machine-learning><vectorization>;27948363;13;Numpy Broadcast to perform euclidean distance vectorized;"<p>I have matrices that are 2 x 4 and 3 x 4. I want to find the euclidean distance across rows, and get a 2 x 3 matrix at the end. Here is the code with one for loop that computes the euclidean distance for every row vector in a against all b row vectors. How do I do the same without using for loops?</p>

<pre><code> import numpy as np
a = np.array([[1,1,1,1],[2,2,2,2]])
b = np.array([[1,2,3,4],[1,1,1,1],[1,2,1,9]])
dists = np.zeros((2, 3))
for i in range(2):
      dists[i] = np.sqrt(np.sum(np.square(a[i] - b), axis=1))
</code></pre>
";27948463;1409938;18860;gg349;19;27948463;"<p>Simply use <code>np.newaxis</code> at the right place:</p>

<pre><code> np.sqrt((np.square(a[:,np.newaxis]-b).sum(axis=2)))
</code></pre>
"
1835351;1271;user1835351;<python><numpy><machine-learning><vectorization>;27948363;13;Numpy Broadcast to perform euclidean distance vectorized;"<p>I have matrices that are 2 x 4 and 3 x 4. I want to find the euclidean distance across rows, and get a 2 x 3 matrix at the end. Here is the code with one for loop that computes the euclidean distance for every row vector in a against all b row vectors. How do I do the same without using for loops?</p>

<pre><code> import numpy as np
a = np.array([[1,1,1,1],[2,2,2,2]])
b = np.array([[1,2,3,4],[1,1,1,1],[1,2,1,9]])
dists = np.zeros((2, 3))
for i in range(2):
      dists[i] = np.sqrt(np.sum(np.square(a[i] - b), axis=1))
</code></pre>
";27948463;2476444;11959;Oliver W.;4;27953679;"<p>This functionality is already included in <a href=""http://docs.scipy.org/doc/scipy/reference/spatial.distance.html"" rel=""nofollow"">scipy's spatial module</a> and I recommend using it as it will be vectorized and highly optimized under the hood. But, as evident by the other answer, there are ways you can do this yourself.</p>

<pre><code>import numpy as np
a = np.array([[1,1,1,1],[2,2,2,2]])
b = np.array([[1,2,3,4],[1,1,1,1],[1,2,1,9]])
np.sqrt((np.square(a[:,np.newaxis]-b).sum(axis=2)))
# array([[ 3.74165739,  0.        ,  8.06225775],
#       [ 2.44948974,  2.        ,  7.14142843]])
from scipy.spatial.distance import cdist
cdist(a,b)
# array([[ 3.74165739,  0.        ,  8.06225775],
#       [ 2.44948974,  2.        ,  7.14142843]])
</code></pre>
"
1835351;1271;user1835351;<python><numpy><machine-learning><vectorization>;27948363;13;Numpy Broadcast to perform euclidean distance vectorized;"<p>I have matrices that are 2 x 4 and 3 x 4. I want to find the euclidean distance across rows, and get a 2 x 3 matrix at the end. Here is the code with one for loop that computes the euclidean distance for every row vector in a against all b row vectors. How do I do the same without using for loops?</p>

<pre><code> import numpy as np
a = np.array([[1,1,1,1],[2,2,2,2]])
b = np.array([[1,2,3,4],[1,1,1,1],[1,2,1,9]])
dists = np.zeros((2, 3))
for i in range(2):
      dists[i] = np.sqrt(np.sum(np.square(a[i] - b), axis=1))
</code></pre>
";27948463;5348749;561;Han Qiu;23;35814006;"<p>I had the same problem recently working with deep learning(stanford cs231n,Assignment1),but when I used</p>

<pre><code> np.sqrt((np.square(a[:,np.newaxis]-b).sum(axis=2)))
</code></pre>

<p>There was a error</p>

<pre><code>MemoryError
</code></pre>

<p>That means I ran out of memory(In fact,that produced a array of 500*5000*1024 in the middle.It's so huge!)</p>

<p>To prevent that error,we can use a formula to simplify:</p>

<p><img src=""https://latex.codecogs.com/gif.latex?(a-b)%5E2&space;=&space;a%5E2&space;-&space;2ab&plus;b%5E2"" title=""(a-b)^2 = a^2 - 2ab+b^2"" /></p>

<p>code:</p>

<pre><code>import numpy as np
aSumSquare = np.sum(np.square(a),axis=1);
bSumSquare = np.sum(np.square(b),axis=1);
mul = np.dot(a,b.T);
dists = np.sqrt(aSumSquare[:,np.newaxis]+bSumSquare-2*mul)
</code></pre>
"
1835351;1271;user1835351;<python><numpy><machine-learning><vectorization>;27948363;13;Numpy Broadcast to perform euclidean distance vectorized;"<p>I have matrices that are 2 x 4 and 3 x 4. I want to find the euclidean distance across rows, and get a 2 x 3 matrix at the end. Here is the code with one for loop that computes the euclidean distance for every row vector in a against all b row vectors. How do I do the same without using for loops?</p>

<pre><code> import numpy as np
a = np.array([[1,1,1,1],[2,2,2,2]])
b = np.array([[1,2,3,4],[1,1,1,1],[1,2,1,9]])
dists = np.zeros((2, 3))
for i in range(2):
      dists[i] = np.sqrt(np.sum(np.square(a[i] - b), axis=1))
</code></pre>
";27948463;4561314;28810;stackoverflowuser2010;34;37903795;"<p>Here are the original input variables:</p>

<pre><code>A = np.array([[1,1,1,1],[2,2,2,2]])
B = np.array([[1,2,3,4],[1,1,1,1],[1,2,1,9]])
A
# array([[1, 1, 1, 1],
#        [2, 2, 2, 2]])
B
# array([[1, 2, 3, 4],
#        [1, 1, 1, 1],
#        [1, 2, 1, 9]])
</code></pre>

<p>A is a 2x4 array.
B is a 3x4 array.</p>

<p>We want to compute the Euclidean distance matrix operation in one entirely vectorized operation, where <code>dist[i,j]</code> contains the distance between the ith instance in A and jth instance in B. So <code>dist</code> is 2x3 in this example.</p>

<p>The distance </p>

<p><a href=""https://i.stack.imgur.com/syuCq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/syuCq.png"" alt=""enter image description here""></a></p>

<p>could ostensibly be written with numpy as</p>

<pre><code>dist = np.sqrt(np.sum(np.square(A-B))) # DOES NOT WORK
# Traceback (most recent call last):
#   File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
# ValueError: operands could not be broadcast together with shapes (2,4) (3,4)
</code></pre>

<p>However, as shown above, the problem is that the element-wise subtraction operation <code>A-B</code> involves incompatible array sizes, specifically the 2 and 3 in the first dimension.</p>

<pre><code>A has dimensions 2 x 4
B has dimensions 3 x 4
</code></pre>

<p>In order to do element-wise subtraction, we have to pad either A or B to satisfy numpy's broadcast rules. I'll choose to pad A with an extra dimension so that it becomes 2 x 1 x 4, which allows the arrays' dimensions to line up for broadcasting. For more on numpy broadcasting, see the <a href=""http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"" rel=""noreferrer"">tutorial in the scipy manual</a> and the final example in <a href=""http://scipy.github.io/old-wiki/pages/EricsBroadcastingDoc"" rel=""noreferrer"">this tutorial</a>.</p>

<p>You can perform the padding with either <code>np.newaxis</code> value or with the <code>np.reshape</code> command. I show both below:</p>

<pre><code># First approach is to add the extra dimension to A with np.newaxis
A[:,np.newaxis,:] has dimensions 2 x 1 x 4
B has dimensions                     3 x 4

# Second approach is to reshape A with np.reshape
np.reshape(A, (2,1,4)) has dimensions 2 x 1 x 4
B has dimensions                          3 x 4
</code></pre>

<p>As you can see, using either approach will allow the dimensions to line up. I'll use the first approach with <code>np.newaxis</code>. So now, this will work to create A-B, which is a 2x3x4 array:</p>

<pre><code>diff = A[:,np.newaxis,:] - B
# Alternative approach:
# diff = np.reshape(A, (2,1,4)) - B
diff.shape
# (2, 3, 4)
</code></pre>

<p>Now we can put that difference expression into the <code>dist</code> equation statement to get the final result:</p>

<pre><code>dist = np.sqrt(np.sum(np.square(A[:,np.newaxis,:] - B), axis=2))
dist
# array([[ 3.74165739,  0.        ,  8.06225775],
#        [ 2.44948974,  2.        ,  7.14142843]])
</code></pre>

<p>Note that the <code>sum</code> is over <code>axis=2</code>, which means take the sum over the 2x3x4 array's third axis (where the axis id starts with 0).</p>

<p>If your arrays are small, then the above command will work just fine. However, if you have large arrays, then you may run into memory issues. Note that in the above example, numpy internally created a 2x3x4 array to perform the broadcasting. If we generalize A to have dimensions <code>a x z</code> and B to have dimensions <code>b x z</code>, then numpy will internally create an <code>a x b x z</code> array for broadcasting.</p>

<p>We can avoid creating this intermediate array by doing some mathematical manipulation. Because you are computing the Euclidean distance as a sum-of-squared-differences, we can take advantage of the mathematical fact that sum-of-squared-differences can be rewritten.</p>

<p><a href=""https://i.stack.imgur.com/Hf8Gj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Hf8Gj.png"" alt=""enter image description here""></a></p>

<p>Note that the middle term involves the sum over <strong>element-wise</strong> multiplication. This sum over multiplcations is better known as a dot product. Because A and B are each a matrix, then this operation is actually a matrix multiplication. We can thus rewrite the above as:</p>

<p><a href=""https://i.stack.imgur.com/QxNaK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/QxNaK.png"" alt=""enter image description here""></a></p>

<p>We can then write the following numpy code:</p>

<pre><code>threeSums = np.sum(np.square(A)[:,np.newaxis,:], axis=2) - 2 * A.dot(B.T) + np.sum(np.square(B), axis=1)
dist = np.sqrt(threeSums)
dist
# array([[ 3.74165739,  0.        ,  8.06225775],
#        [ 2.44948974,  2.        ,  7.14142843]])
</code></pre>

<p>Note that the answer above is exactly the same as the previous implementation. Again, the advantage here is the we do not need to create the intermediate 2x3x4 array for broadcasting.</p>

<p>For completeness, let's double-check that the dimensions of each summand in <code>threeSums</code> allowed broadcasting.</p>

<pre><code>np.sum(np.square(A)[:,np.newaxis,:], axis=2) has dimensions 2 x 1
2 * A.dot(B.T) has dimensions                               2 x 3
np.sum(np.square(B), axis=1) has dimensions                 1 x 3
</code></pre>

<p>So, as expected, the final <code>dist</code> array has dimensions 2x3.</p>

<p>This use of the dot product in lieu of sum of element-wise multiplication is also discussed in <a href=""http://eli.thegreenplace.net/2015/broadcasting-arrays-in-numpy/"" rel=""noreferrer"">this tutorial</a>.</p>
"
1835351;1271;user1835351;<python><numpy><machine-learning><vectorization>;27948363;13;Numpy Broadcast to perform euclidean distance vectorized;"<p>I have matrices that are 2 x 4 and 3 x 4. I want to find the euclidean distance across rows, and get a 2 x 3 matrix at the end. Here is the code with one for loop that computes the euclidean distance for every row vector in a against all b row vectors. How do I do the same without using for loops?</p>

<pre><code> import numpy as np
a = np.array([[1,1,1,1],[2,2,2,2]])
b = np.array([[1,2,3,4],[1,1,1,1],[1,2,1,9]])
dists = np.zeros((2, 3))
for i in range(2):
      dists[i] = np.sqrt(np.sum(np.square(a[i] - b), axis=1))
</code></pre>
";27948463;570918;41455;merv;2;42184755;"<p>Using <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html"" rel=""nofollow noreferrer"">numpy.linalg.norm</a> also works well with broadcasting. Specifying an integer value for <code>axis</code> will use a vector norm, which defaults to Euclidean norm.</p>

<pre><code>import numpy as np

a = np.array([[1,1,1,1],[2,2,2,2]])
b = np.array([[1,2,3,4],[1,1,1,1],[1,2,1,9]])
np.linalg.norm(a[:, np.newaxis] - b, axis = 2)

# array([[ 3.74165739,  0.        ,  8.06225775],
#       [ 2.44948974,  2.        ,  7.14142843]])
</code></pre>
"
3370773;395;user3370773;<google-analytics><machine-learning><google-analytics-api>;27951742;0;Google-analytics framework for predictive analysis;"<p>I'm trying to use the google-analytics framework to create predictive analysis tools. For example I would like to cluster my webpage visitors, etc.
In general, is there any list of machine learning algorithms implemented by this framework? for example: regression, clustering, classification, feature selection, etc.
Thank you for any help</p>
";;2113279;717;georgez;0;28252234;"<p>When you say GA framework I'll assume you're referring to the set of Google Analytics APIs listed <a href=""https://developers.google.com/analytics/"" rel=""nofollow"">here</a>. The framework by itself doesn't provide machine learning capabilities. It merely provides access to the processed and aggregated GA data stored in Google's servers. You can use the API and feed the data to a machine learning application/system/program that does all of the stuff you mentioned.</p>
"
3370773;395;user3370773;<google-analytics><machine-learning><google-analytics-api>;27951742;0;Google-analytics framework for predictive analysis;"<p>I'm trying to use the google-analytics framework to create predictive analysis tools. For example I would like to cluster my webpage visitors, etc.
In general, is there any list of machine learning algorithms implemented by this framework? for example: regression, clustering, classification, feature selection, etc.
Thank you for any help</p>
";;2285514;303;kushan_s;1;28346141;"<p>Depending upon your language of choice, you might want to export your Google Analytics Metrics to flat files or a database and then start experimenting with ML models. Two popular languages with stable ML Implementations are Python and R. R's <a href=""http://caret.r-forge.r-project.org/"" rel=""nofollow"">caret</a> package includes tools for building a predictive model pipeline. Python's <a href=""http://scikit-learn.org/stable/"" rel=""nofollow"">scikit-learn</a> also contains implementations of all major classes of ML algorithms.</p>
"
4178757;167;Denny Dharmawan;<machine-learning><computer-vision><feature-extraction>;27955695;3;What's the difference between standardization and global contrast normalization? (Image preprocessing);"<p>I'm trying to understand the article located <a href=""https://caglarift6266.wordpress.com/"" rel=""nofollow"">here</a></p>

<p>I have 2 questions:</p>

<ol>
<li><p>What's the difference between standardization and global contrast normalization?
To the best of my understanding, I think they mean the same thing in which we subtracts each pixel of an image with the global mean and divides by the global standard deviation.</p></li>
<li><p>What's the purpose of them in machine learning or feature extraction topic?</p></li>
</ol>

<p>Thx</p>
";28054241;2705625;433;Mozglubov;0;28054241;"<p>1.) It is not completely clear from the post you shared, but it looks like standardization is normalizing each image column separately (while global contrast normalization does this over all pixels in the image). My guess is that both use the Standardize function from this <a href=""http://deeplearning.net/software/pylearn2/library/datasets.html"" rel=""nofollow"">pylearn2 documentation</a> but the first step has the two global flags set to false, while the second step sets them to true.</p>

<p>2.) Normalization is a fairly standard step in machine learning/feature extraction. It basically attempts to convert data into a common input space, and can at least partially deal with external sources of data variation like illumination levels. A good start would be to read the Wikipedia article on <a href=""http://en.wikipedia.org/wiki/Feature_scaling"" rel=""nofollow"">feature scaling</a>.</p>
"
707887;141;Neal;<image><machine-learning><apache-spark><distributed-computing>;27959831;2;How does Spark read 100K images effeciently?;"<p>Currently, I'm programming something on image classification with Spark. I need to read all the images into memory as RDD and my method is as following:  </p>

<pre><code>val images = spark.wholeTextFiles(""hdfs://imag-dir/"")  
</code></pre>

<p><strong>imag-dir</strong> is the target image storing directory on hdfs. With this method, all the images will be   loaded into memory and every image will be organized as ""image name, image content"" pair. However, I find this process is time consuming, is there any better way to load large image data set into spark? </p>
";27960634;47978;11485;Francois G;4;27960634;"<p>I suspect that may be because you have a lot of <em>small</em> files on HDFS, which is a problem as such (the 'small files problem'). <a href=""http://blog.cloudera.com/blog/2009/02/the-small-files-problem/"" rel=""nofollow"">Here</a> you'll find a few suggestions in addressing the issue. </p>

<p>You may also want to set the number of partitions (the <code>minpartitions</code> argument of <code>wholetextFiles</code>) to a reasonable number : at least 2x the number of cores in your cluster (look <a href=""https://www.youtube.com/watch?v=dmL0N3qfSc8"" rel=""nofollow"">there</a> for details).</p>

<p>But in sum, apart from the 2 ideas above, the way you're loading those is ok and <em>not</em> where your problem lies (assuming <code>spark</code> is your Spark context).</p>
"
1115169;2518;pbu;<cuda><machine-learning>;27971328;0;CUDA Error - Kernel execution failed with invalid device function;"<p>i am trying to run CIFAR10 after successfully compiling cuda-convnet2, i am getting this error</p>

<pre><code>src/nvmatrix.cu(394) : getLastCudaError() CUDA error : kSetupCurand: Kernel execution failed : (8) invalid device function .
</code></pre>

<p>i am running linux on Zotak Nvidia geforce 750ti GPU. Here is the log output</p>

<pre><code>$ python convnet.py --data-provider cifar --test-range 6 --train-range 1-5 --data-path cifar/cifar-10-py-colmajor --inner-size 24 --save-path cifar/save/ --gpu 0 --layer-def layers/layers-cifar10-11pct.cfg --layer-params layers/layer-params-cifar10-11pct.cfg
python: can't open file 'convnet.py': [Errno 2] No such file or directory
pbu@pbu-OptiPlex-740-Enhanced:~/Desktop$ cd cuda-convnet2
pbu@pbu-OptiPlex-740-Enhanced:~/Desktop/cuda-convnet2$ python convnet.py --data-provider cifar --test-range 6 --train-range 1-5 --data-path cifar/cifar-10-py-colmajor --inner-size 24 --save-path cifar/save/ --gpu 0 --layer-def layers/layers-cifar10-11pct.cfg --layer-params layers/layer-params-cifar10-11pct.cfg
Initialized data layer 'data', producing 1728 outputs
Initialized data layer 'labels', producing 1 outputs
Initialized convolutional layer 'conv1' on GPUs 0, producing 24x24 64-channel output
Initialized max-pooling layer 'pool1' on GPUs 0, producing 12x12 64-channel output
Initialized cross-map response-normalization layer 'rnorm1' on GPUs 0, producing 12x12 64-channel output
Initialized convolutional layer 'conv2' on GPUs 0, producing 12x12 64-channel output
Initialized cross-map response-normalization layer 'rnorm2' on GPUs 0, producing 12x12 64-channel output
Initialized max-pooling layer 'pool2' on GPUs 0, producing 6x6 64-channel output
Initialized locally-connected layer 'local3' on GPUs 0, producing 6x6 64-channel output
Initialized locally-connected layer 'local4' on GPUs 0, producing 6x6 32-channel output
Initialized fully-connected layer 'fc10' on GPUs 0, producing 10 outputs
Initialized softmax layer 'probs' on GPUs 0, producing 10 outputs
Initialized logistic regression cost 'logprob' on GPUs 0
Initialized neuron layer 'conv2_neuron' on GPUs 0, producing 9216 outputs
Initialized neuron layer 'conv1_neuron' on GPUs 0, producing 36864 outputs
Initialized neuron layer 'local4_neuron' on GPUs 0, producing 1152 outputs
Initialized neuron layer 'local3_neuron' on GPUs 0, producing 2304 outputs
Layer local4_neuron using acts from layer local4
Layer conv2_neuron using acts from layer conv2
Layer local3_neuron using acts from layer local3
Layer conv1_neuron using acts from layer conv1
=========================
Importing cudaconvnet._ConvNet C++ module
Fwd terminal: logprob
found bwd terminal conv1[0] in passIdx=0
=========================
Training ConvNet
Add PCA noise to color channels with given scale                        : 0     [DEFAULT]
Check gradients and quit?                                               : 0     [DEFAULT]
Conserve GPU memory (slower)?                                           : 0     [DEFAULT]
Convert given conv layers to unshared local                             :       
Cropped DP: crop size (0 = don't crop)                                  : 24    
Cropped DP: test on multiple patches?                                   : 0     [DEFAULT]
Data batch range: testing                                               : 6-6   
Data batch range: training                                              : 1-5   
Data path                                                               : cifar/cifar-10-py-colmajor 
Data provider                                                           : cifar 
Force save before quitting                                              : 0     [DEFAULT]
GPU override                                                            : 0     
Layer definition file                                                   : layers/layers-cifar10-11pct.cfg 
Layer file path prefix                                                  :       [DEFAULT]
Layer parameter file                                                    : layers/layer-params-cifar10-11pct.cfg 
Load file                                                               :       [DEFAULT]
Logreg cost layer name (for --test-out)                                 :       [DEFAULT]
Minibatch size                                                          : 128   [DEFAULT]
Number of epochs                                                        : 50000 [DEFAULT]
Output test case predictions to given path                              :       [DEFAULT]
Save file override                                                      :       
Save path                                                               : cifar/save/ 
Subtract this scalar from image (-1 = don't)                            : -1    [DEFAULT]
Test and quit?                                                          : 0     [DEFAULT]
Test on one batch at a time?                                            : 1     [DEFAULT]
Testing frequency                                                       : 57    [DEFAULT]
Unshare weight matrices in given layers                                 :       
Write test data features from given layer                               :       [DEFAULT]
Write test data features to this path (to be used with --write-features):       [DEFAULT]
=========================
Running on CUDA device(s) 0
Current time: Thu Jan 15 20:15:50 2015
Saving checkpoints to cifar/save/ConvNet__2015-01-15_20.15.47
=========================
src/nvmatrix.cu(394) : getLastCudaError() CUDA error : kSetupCurand: Kernel execution failed : (8) invalid device function .
</code></pre>
";28390324;3503121;28;oneg;1;28390324;"<p>You may need to modify the <code>Makefile</code>:</p>

<ul>
<li>cudaconv3/Makefile</li>
<li>cudaconvnet/Makefile</li>
<li>nvmatrix/Makefile</li>
</ul>

<p>and change</p>

<pre><code>GENCODE_SM35    := -gencode arch=compute_35,code=sm_35
GENCODE_FLAGS   := $(GENCODE_SM35)
</code></pre>

<p>to</p>

<pre><code>GENCODE_SM35    := -gencode arch=compute_35,code=sm_35
GENCODE_SM50    := -gencode arch=compute_50,code=sm_50
GENCODE_FLAGS   := $(GENCODE_SM50)
</code></pre>

<p>since 750Ti is with Compute Capability 5.0.</p>
"
4140027;3985;tumbleweed;<python><machine-learning><nlp><scikit-learn>;27978507;0;How to select hyper parameters for SVC estimator in scikit learn?;"<p>I'm classifiying some texts with SVC and I would like to run a grid search so I followed the example provided at the <a href=""http://scikit-learn.org/stable/auto_examples/grid_search_text_feature_extraction.html#example-grid-search-text-feature-extraction-py"" rel=""nofollow"">documentation</a>. In the example they are using SGDClassifier with the following parameters:</p>

<pre><code>parameters = {
    'vect__max_df': (0.5, 0.75, 1.0),
    #'vect__max_features': (None, 5000, 10000, 50000),
    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams
    #'tfidf__use_idf': (True, False),
    #'tfidf__norm': ('l1', 'l2'),
    'clf__alpha': (0.00001, 0.000001),
    'clf__penalty': ('l2', 'elasticnet'),
    #'clf__n_iter': (10, 50, 80),
}
</code></pre>

<p>My issue is, what kind of parameters should I try for SVC classifier, tfidf, hashing vectorizer and CountVectorizer?. How should I select this parameters if this is a multi class classification problem?.</p>
";27980068;4370183;371;klubow;2;27980068;"<p>You should read what those parameters mean and decide for yourself.</p>

<p>I optimize following: 
SVC: gamma and C
TFIDF: max_features, max_df, min_df</p>

<p>That really depends on your data and model. If you don't know run very wide optimization on many parameters and many possible values to narrow it down.
This is time consuming so you may want to subsample your data.</p>
"
4420776;2669;David Heckmann;<r><machine-learning><r-caret>;27980793;0;caret: RFE with tuneLength and rerank;"<p>When using rfe() with tuneLength set and rerank = TRUE, is the parameter in tuneLength optimized when rankings are recalculated on the subset of features?
Here is an exmple of the analysis I am doing:</p>

<pre><code>library(""caret"")
set.seed(342)
train &lt;- as.data.frame ( matrix( rnorm(1e4) , 100, 100 ) )

ctrl &lt;- rfeControl(functions = caretFuncs,        
               rerank = TRUE,
               method = ""repeatedcv"",
               number=2, 
               repeats=1,
               verbose =TRUE
)

pls.fit.rfe &lt;- rfe(V1 ~ .,
               data = train,   
               method = ""pls"",                    
               sizes =  c(2,5),
               tuneLength = 5, 
               rfeControl = ctrl
)
</code></pre>
";;1078601;11384;topepo;0;27987421;"<p>The re-ranking happens after the each PLS fit. I don't think that the rankings have any direct impact on the model tuning. </p>

<p>Max</p>
"
4141061;1299;MLMLTL;<c++><opencv><machine-learning><svm>;27984427;2;C++ OpenCV SVM Predict Not Working;"<p>Trying to create a functional <code>SVM</code>. I have 114 training images, 60 Positive/54 Negative, and 386 testing images for the <code>SVM</code> to predict against.</p>

<p>I read in the training image features to <code>float</code> like this:</p>

<pre><code>trainingDataFloat[i][0] = trainFeatures.rows;
trainingDataFloat[i][1] = trainFeatures.cols;
</code></pre>

<p>And the same for the testing images too:</p>

<pre><code>testDataFloat[i][0] = testFeatures.rows;
testDataFloat[i][2] = testFeatures.cols;
</code></pre>

<p>Then, using Micka's <a href=""https://stackoverflow.com/questions/27981571/c-opencv-turn-a-mat-into-a-1-dimensional-array""><strong>answer to this question</strong></a>, I turn the <code>testDataFloat</code> into a 1 Dimensional Array, and feed it to a <code>Mat</code> like this so to predict on the <code>SVM</code>:</p>

<pre><code>float* testData1D = (float*)testDataFloat;
Mat testDataMat1D(height*width, 1, CV_32FC1, testData1D);
float testPredict = SVMmodel.predict(testDataMat1D);
</code></pre>

<p>Once this was all in place, there is the Debug Error of:</p>

<p><strong>Sizes of input arguments do not match (the sample size is different from what has been used for training) in cvPreparePredictData</strong></p>

<p>Looking at <a href=""https://stackoverflow.com/questions/25555556/java-opencv-training-svm-error-input-sample-must-be-1-dimensional-vector""><strong>this post</strong></a> I found (Thanks to berak) that:</p>

<p><em>""all images (used in training &amp; prediction) have to be the same size""</em></p>

<p>So I included a re-size function that would re-size the images to be all square at whatever size you wished (<code>100x100, 200x200, 1000, 1000 etc.</code>)</p>

<p>Run it again with the images re-sized to a new directory that the program now loads the images in from, and I get the exact same error as before of:</p>

<p><strong>Sizes of input arguments do not match (the sample size is different from what has been used for training) in cvPreparePredictData</strong></p>

<p>I just have no idea anymore on what to do. Why is it still throwing that error?</p>

<p><strong>EDIT</strong></p>

<p>I changed</p>

<pre><code>Mat testDataMat1D(TestDFheight*TestDFwidth, 1, CV_32FC1, testData1D);
</code></pre>

<p>to</p>

<pre><code>Mat testDataMat1D(1, TestDFheight*TestDFwidth, CV_32FC1, testData1D);
</code></pre>

<p>and placed the <code>.predict</code> inside the loop that the <code>features</code> are given to the <code>float</code> so that each image is given to the <code>.predict</code> individually because of <a href=""https://stackoverflow.com/questions/14694810/using-opencv-and-svm-with-images""><strong>this question</strong></a>. With the to <code>int</code> swapped so that <code>.cols</code> = 1 and <code>.rows</code> = <code>TestDFheight*TestDFwidth</code> the program seems to actually run, but then stops on image 160 (<code>.exe has stopped working</code>)... So that's a new concern.</p>

<p><strong>EDIT 2</strong></p>

<p>Added a simple</p>

<pre><code>std::cout &lt;&lt; testPredict;
</code></pre>

<p>To view the determined output of the SVM, and it seems to be positively matching everything until Image 160, where it stops running:</p>

<p><img src=""https://i.stack.imgur.com/gdU9V.png"" alt=""""></p>
";27989164;4141975;347;Daniel Moodie;2;27989164;"<p>Please check your training and test feature vector.</p>

<p>I'm assuming your feature data is some form of cv::Mat containing features on each row.
In which case you want your training matrix to be a concatenation of each feature matrix from each image.
These line doesn't look right:</p>

<pre><code>trainingDataFloat[i][0] = trainFeatures.rows;
trainingDataFloat[i][1] = trainFeatures.cols;
</code></pre>

<p>This is setting an element of a 2d matrix to the number of rows and columns in trainFeatures.  This has nothing to do with the actual data that is in the trainFeatures matrix.</p>

<p>What are you trying to detect?  If each image is a positive and negative example, then are you trying to detect something in an image?  What are your features?</p>

<p>If you're trying to detect an object in the image on a per image basis, then you need a feature vector describing the whole image in one vector.  In which case you'd do something like this with your training data:</p>

<pre><code>int N; // Set to number of images you plan on using for training
int feature_size; // Set to the number of features extracted in each image.  Should be constant across all images.

cv::Mat X = cv::Mat::zeros(N, feature_size, CV_32F); // Feature matrix
cv::Mat Y = cv::Mat::zeros(N, 1, CV_32F); // Label vector
// Now use a for loop to copy data into X and Y, Y = +1 for positive examples and -1 for negative examples
for(int i = 0; i &lt; trainImages.size(); ++i)
{
  X.row(i) = trainImages[i].features; // Where features is a cv::Mat row vector of size N of the extracted features
  Y.row(i) = trainImages[i].isPositive ? 1:-1; 
}
// Now train your cv::SVM on X and Y.
</code></pre>
"
492372;24872;London guy;<java><machine-learning><weka><svm>;27985072;1;What does this exception mean when running LibSVM using Weka?;"<p>I am writing this code to do a 5-fold cross validation using LibSVM using Weka.</p>

<pre><code>    LibSVM svm = new LibSVM();
    svm.setKernelType(new SelectedTag(2, LibSVM.TAGS_KERNELTYPE));
    svm.setDegree(2);
    //Run a cross validation to select the right parameters
    CVParameterSelection ps = new CVParameterSelection();
    ps.setClassifier(svm);
    ps.setNumFolds(5);  // using 5-fold CV
    ps.addCVParameter(""G 1 10 .1"");

    // build and output best options
    //I get exception below -&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
    ps.buildClassifier(isTrainingSet);
    System.out.println(Utils.joinOptions(ps.getBestClassifierOptions()));
</code></pre>

<p>But it throws an exception saying</p>

<pre><code>Exception in thread ""main"" java.lang.Exception: Error: gamma &lt; 0
at weka.classifiers.functions.LibSVM.buildClassifier(LibSVM.java:1690)
at weka.classifiers.meta.CVParameterSelection.findParamsByCrossValidation(CVParameterSelection.java:377)
at weka.classifiers.meta.CVParameterSelection.findParamsByCrossValidation(CVParameterSelection.java:354)
at weka.classifiers.meta.CVParameterSelection.buildClassifier(CVParameterSelection.java:628)
at ExtractTopics.main(ExtractTopics.java:164)
</code></pre>

<p>I think the parameter gamme never is less than zero, right? the initial value is 1 and final value is 10 with 10 steps. So, why is this exception then?</p>
";;492372;24872;London guy;3;27985412;"<p>Answering my own question below.</p>

<p>The last parameter among the three values is not the amount of step size to take. It is the number of steps to perform. So it was incorrect to say above that I want to perform 0.1 steps. It should have been 100 steps so that Weka can infer that it needs to take 0.1 step size internally.</p>

<p>Using [1 10 100] helped and it ran fine.  </p>

<p>The answer I found is here - <a href=""http://weka.8497.n7.nabble.com/Problem-evaluating-classifier-C-lt-0-error-td31971.html"" rel=""nofollow"">http://weka.8497.n7.nabble.com/Problem-evaluating-classifier-C-lt-0-error-td31971.html</a></p>
"
4462028;63;Qingxin Zheng;<machine-learning><neural-network><deep-learning><caffe>;27986339;4;Where can I find the label map between trained model like googleNet's output to there real class label?;"<p>everyone, I am new to caffe. Currently, I try to use the trained GoogleNet which was downloaded from model zoo to classify some images. However, the network's output seem to be a vector rather than real label(like dog, cat). 
Where can I find the label-map between trained model like googleNet's output to their real class label?
Thanks.</p>
";28009396;1714410;92418;Shai;7;28009396;"<p>If you got <code>caffe</code> from git you should find in <code>data/ilsvrc12</code> folder a shell script <code>get_ilsvrc_aux.sh</code>.<br>
This script should download several files used for ilsvrc (sub set of imagenet used for the large scale image recognition challenge) training.  </p>

<p>The most interesting file (for you) that will be downloaded is <code>synset_words.txt</code>, this file has 1000 lines, one line per class identified by the net.<br>
The format of the line is</p>

<blockquote>
  <p>nXXXXXXXX description of class</p>
</blockquote>
"
492372;24872;London guy;<machine-learning><weka><svm><libsvm>;27986849;1;How to do a grid search programmatically for SVM in Weka;"<p>What is the piece of code in Java that I can use to do a grid search for SVM parameters using LibSVM in Weka?</p>

<p>Currently, I am able to search for a good value for only 1 parameter using the following piece of code:</p>

<pre><code>LibSVM svm = new LibSVM();
    svm.setKernelType(new SelectedTag(2, LibSVM.TAGS_KERNELTYPE));
    svm.setDegree(2);
    //Run a cross validation to select the right parameters
    CVParameterSelection ps = new CVParameterSelection();
    ps.setClassifier(svm);
    ps.setNumFolds(5);  // using 5-fold CV
    ps.addCVParameter(""G 0.1 10 100"");
</code></pre>
";;4462487;110;Phil Gabardo;0;27991386;"<p>This seems like a reliable API:
<a href=""http://java-ml.sourceforge.net/api/0.1.6/libsvm/GridSearch.html"" rel=""nofollow"">http://java-ml.sourceforge.net/api/0.1.6/libsvm/GridSearch.html</a></p>

<p>The GridSearch constructor accepts a LibSVM object, Dataset object and an integer representing the number of folds to use for the grid search. The search method can then be called on the GridSearch object. The inputs for this method are the initial SVM parameters (including C and Gamma) and the outputs are the optimal C and Gamma values.</p>
"
3785667;135;Neil;<azure><machine-learning><cors><azure-machine-learning-studio>;27987910;12;Azure Machine Learning - CORS;"<p>I've searched for hours for this and can't find a single thing that answers the question. I've created and published a new Azure Machine Learning service, and have created an endpoint. I can call the service using the Postman REST CLient, but accessing it via a JavaScript webpage returns a console log saying that CORS is enabled for the service. Now, for the life of me, I can't figure out how to disable CORS for Azure Machine Learning services. Any help would be much appreciated, thanks!</p>
";35145088;896697;4930;Jochen van Wylick;1;28189546;"<p>You have to start your browser with <code>--disable-web-security</code> ( Chrome that is ). Here's some jQuery that allowed me to call the service AFTER re-starting my browser with --disable-web-security:</p>

<pre><code>$(document).ready(function () {
    var ajaxData = ""-- the request body "";
    var serviceUrl = ""https://ussouthcentral.services.azureml.net/workspaces/00e36959fc3e4673a32eae9f9b184346/--whatever"";

    $.ajax({
        type: ""POST"",
        url: serviceUrl,
        data: ajaxData,
        headers: {
            ""Authorization"": ""Bearer --API KEY HERE--"",
            ""Content-Type"": ""application/json;charset=utf-8""
        }
    }).done(function (data) {
        console.log(data);
    });
});
</code></pre>

<p>That returned the data. NOTE: Be sure you see that warning in Chrome. I didn't at fist, because some Chrome processes were still running in the background. After killing those, restarting with that flag, seeing the message, it worked. ( Chrome v40.something )</p>

<p>See: <a href=""https://stackoverflow.com/a/6083677/896697"">https://stackoverflow.com/a/6083677/896697</a> </p>
"
3785667;135;Neil;<azure><machine-learning><cors><azure-machine-learning-studio>;27987910;12;Azure Machine Learning - CORS;"<p>I've searched for hours for this and can't find a single thing that answers the question. I've created and published a new Azure Machine Learning service, and have created an endpoint. I can call the service using the Postman REST CLient, but accessing it via a JavaScript webpage returns a console log saying that CORS is enabled for the service. Now, for the life of me, I can't figure out how to disable CORS for Azure Machine Learning services. Any help would be much appreciated, thanks!</p>
";35145088;4589073;1231;neerajkh;4;35145088;"<p>Currently, we don't support disabling CORS on API side but you can either use the above option or you can use the API management service to disable CORS. The links below should help you with this</p>

<p>Here are the links: <a href=""http://azure.microsoft.com/en-us/documentation/articles/api-management-get-started/"" rel=""noreferrer"">step by step</a> guide, also this <a href=""http://channel9.msdn.com/Blogs/AzureApiMgmt/Last-mile-Security"" rel=""noreferrer"">video</a> on setting headers, and <a href=""https://msdn.microsoft.com/en-us/library/azure/dn894084.aspx#JSONP"" rel=""noreferrer"">this doc</a> on policies.</p>

<p>API Management service allow CORS by enabling it in the API configuration page</p>
"
3785667;135;Neil;<azure><machine-learning><cors><azure-machine-learning-studio>;27987910;12;Azure Machine Learning - CORS;"<p>I've searched for hours for this and can't find a single thing that answers the question. I've created and published a new Azure Machine Learning service, and have created an endpoint. I can call the service using the Postman REST CLient, but accessing it via a JavaScript webpage returns a console log saying that CORS is enabled for the service. Now, for the life of me, I can't figure out how to disable CORS for Azure Machine Learning services. Any help would be much appreciated, thanks!</p>
";35145088;9929041;550;Alibek Jakupov;1;52111270;"<p>Just an excerpt from the Azure ML Book (one may find it useful):</p>

<blockquote>
  <p>This CORS restriction really means that if you wish to fully exploit
  Azure Machine Learning web services for deployment, testing, and
  production for a wide variety of (web) clients, you will need to host
  your own server-side applications. You basically have two choices.</p>
</blockquote>

<ol>
<li>Host a web application, such as an ASP.NET webpage, and invoke the Azure Machine Learning web service server-side to conform to the current Azure Machine Learning CORS restrictions.</li>
<li>Host your own web service that does provide CORS support and can in turn invoke the Azure Machine Learning web service on behalf of a wide variety of web and mobile clients via modern protocols and data formats like REST and JSON.</li>
</ol>
"
2102122;1112;Potaito;<machine-learning><kernel-density><probability-density>;27998945;0;Calculate Bias of Parzen WIndows analytically;"<p>I'm still having some trouble understanding what Bias and Variance for a specific estimator actually are. </p>

<p>I'm working with the definition of Bias as it is found on Wikipedia:</p>

<p><img src=""https://i.stack.imgur.com/piQ4C.png"" alt=""Bias of an estimator""></p>

<p>If we define kernel-density-estimates as</p>

<p><img src=""https://i.stack.imgur.com/tohFa.png"" alt=""Wikipedia definition of kernel density estimate""></p>

<p>But how can I apply this to kernel density estimation, or to be more exact Parzen Windows? Can someone at least give me an idea how the estimated density f_hat(x) relates to Bias (and Variance)?</p>

<p>Qualitative I can already tell, that a box-window containing the whole data space will have maximum bias and no variance as the estimated density will simply be the average of the whole training data set. </p>
";27999150;2102122;1112;Potaito;0;27999150;"<p>I think I just figured it out myself. The parameter theta in the case of density estimation is .. drumroll... the density function f(x). So the bias is defined as </p>

<p><strong>Bias = E[f_hat(x)] - f(x)</strong></p>

<p>The E[f_hat(x)] term is the expected value or the <em>mean</em> of the window function. Calculating it involves a simple integral. </p>

<p><em>f(x)</em> is the <em>true</em> density function of the data, which in reality is likely to be unknown. </p>
"
3070617;240;Bit Manipulator;<machine-learning><classification><svm><svmlight>;28000132;1;How to provide cost for balancing training by imbalanced train dataset as available in svmlight?;"<p>Cost in e1071's SVM doesn't seems same as svmlight's Cost. The manual of e1071 library states the following definition for its cost parameter:</p>

<pre><code>cost of constraints violation (default: 1)â€”it is the â€˜Câ€™-constant of the regular-
ization term in the Lagrange formulation
</code></pre>

<p>This is basically the allowance of miss-classification. There is one weight as provided by svmlight, described in its manual as:</p>

<pre><code>Cost: cost-factor, by which training errors on
      positive examples outweight errors on negative
      examples (default 1)
</code></pre>

<p>This cost is basically to allow balancing in case the train data doesn't has equal number of positive and negative data points. Is there anything similar in e1071's SVM implementation?</p>
";28001773;4465180;26;David Meyer;0;28001773;"<p>You probably want to look at the argument: class.weights (which is explained on the help page).</p>

<p>Best
David</p>
"
4240359;113;Nidhi jain;<machine-learning><pos-tagger>;28002136;0;POS tagger and chunker;"<p>I want to make a POS tagger and chunker using JAVA. But I am unable to figure out that from where should I start. What all libraries would be required?</p>
";;4464328;33;Kim;0;28002388;"<p>I think you should read articles or reports to know what they did. I' m working with Vietnamese processing, not know what libraries use in English. But I saw Stanford nlp while using Vietnamese tagger libraries.
I think grammar and lexicon are very important. 
This is NLP Stanford, try this. <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tagger.shtml</a></p>
"
4240359;113;Nidhi jain;<machine-learning><pos-tagger>;28002136;0;POS tagger and chunker;"<p>I want to make a POS tagger and chunker using JAVA. But I am unable to figure out that from where should I start. What all libraries would be required?</p>
";;2798955;7958;Buru;1;28047388;"<p>you can use various libraries</p>

<ul>
<li><a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">Stanford Core NLP</a> </li>
<li><a href=""https://opennlp.apache.org/"" rel=""nofollow"">OpenNLP</a></li>
<li><a href=""https://gate.ac.uk/gate/doc/plugins.html"" rel=""nofollow"">Gate</a></li>
</ul>

<p>I used OpenNLP in my project. I think this instructions will help you to go through OpenNLP Library. Follow this <a href=""https://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html"" rel=""nofollow"">document</a></p>

<ol>
<li>First download the models from this <a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow"">page</a></li>
<li>Then add those into your project</li>
<li>You also need <code>Tokenizer</code> model to break sentences into tokens. Then pass these tokens into POS Tagger.</li>
</ol>

<p>Code Samples</p>

<hr>

<p><strong>Load Model</strong></p>

<pre><code>InputStream modelIn = null;

try {
  modelIn = new FileInputStream(""en-pos-maxent.bin"");
  POSModel model = new POSModel(modelIn);
}
catch (IOException e) {
  // Model loading failed, handle the error
  e.printStackTrace();
}
finally {
  if (modelIn != null) {
    try {
      modelIn.close();
    }
    catch (IOException e) {
    }
  }
}
</code></pre>

<hr>

<p><strong>Instantiate POSTaggerME</strong> </p>

<pre><code>POSTaggerME tagger = new POSTaggerME(model);
</code></pre>

<hr>

<p><strong>Generate TAGS</strong></p>

<pre><code>    String sent[] = new String[]{""Most"", ""large"", ""cities"", ""in"", ""the"", ""US"", ""had"",
                                 ""morning"", ""and"", ""afternoon"", ""newspapers"", "".""};
//This is manual String tokens of a sentence. To Generate word token use [Tokenizer Model][6]         
    String tags[] = tagger.tag(sent);
</code></pre>

<p>Links</p>

<ul>
<li>Implement <a href=""https://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html#tools.tokenizer"" rel=""nofollow"">Tokenizer Model</a></li>
<li>Implement <a href=""https://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html#tools.chunker"" rel=""nofollow"">Chunker</a></li>
</ul>
"
356729;6254;dukebody;<machine-learning><scikit-learn><random-forest>;28002991;2;How to use whole training example to estimate class probabilities in sklearn RandomForest;"<p>I want to use scikit-learn RandomForestClassifier to estimate the probabilities of a given example to belong to a set of classes, after prior training of course.</p>

<p>I know I can get the class probabilities using the <a href=""http://scikit-learn.org/dev/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba"" rel=""nofollow noreferrer""><code>predict_proba</code></a> method, that calculates them as </p>

<blockquote>
  <p>[...] the mean predicted class probabilities of the trees in the forest.</p>
</blockquote>

<p>In <a href=""https://stackoverflow.com/a/14193569/356729"">this question</a> it is mentioned that:</p>

<blockquote>
  <p>The probabilities returned by a single tree are the normalized class
  histograms of the leaf a sample lands in.</p>
</blockquote>

<p>Now, I've been reading some papers on probability estimation and realized there isn't a trivial solution. According to <a href=""http://people.dsv.su.se/~henke/papers/bostrom07c.pdf"" rel=""nofollow noreferrer"">Estimating Class Probabilities in Random Forests (BÃ¶strom)</a>:</p>

<blockquote>
  <p>using the same examples to both grow the trees and estimate the
  probabilities, [...] by necessity will lead to pure (and therefore
  small) estimation sets</p>
</blockquote>

<p>And this is bad. The solution appears to be to use all the examples in the training set, instead of only the ones in the bootstrap sample used to grow the tree.</p>

<p>Scikit-learn does use only the bootstrap sample for each tree to calculate the probability estimate of each class, right? <strong>Does somebody have any pointers about how to proceed to make the class probabilities come from the whole training set of the RandomForest instead?</strong></p>

<p>I assume this would need some special <code>Tree</code> subclassing that doesn't assign class probabilities to the leaves of the trees and then some procedure to assign them from the RandomForest classifier using the whole training set.</p>
";28367138;676634;23217;Andreas Mueller;2;28367138;"<blockquote>
  <p>Scikit-learn does use only the bootstrap sample for each tree to calculate the probability estimate of each class, right? </p>
</blockquote>

<p>No, it uses only the in-sample part, and therefore will not give very calibrated probability outputs (which I guess is what the paper suggests).</p>

<p>You could get better probability estimates using the out-of-sample estimates, and maybe that would even be done easily with the current code base. Maybe it would be better to use a calibration method as post-processing (using the out-of-bag samples).</p>

<p>Anyhow, what you want to achieve is the default.</p>
"
969294;1431;Tumetsu;<python><machine-learning><scikit-learn><classification>;28003222;1;Classifying text with scikit;"<p>I'm learning Scikit machine-learning for a project and while I'm beginning to grasp the general process the details are a bit fuzzy still. </p>

<p>Earlier I managed to build a classifier, train it and test it with test set. I saved it to disk with cPickle. Now I want to create a class which loads this classifier and lets user to classify single tweets with it.</p>

<p>I thought this would be trivial but I seem to get ValueError('dimension mismatch') from X_new_tfidf = <em>self.tfidf_transformer.fit_transform(fitTweetVec)</em> line with following code:</p>

<pre><code>class TweetClassifier:

classifier = None
vect = TfidfVectorizer()
tfidf_transformer = TfidfTransformer()

#open the classifier saved to disk to be utilized later
def openClassifier(self, name):
    with open(name+'.pkl', 'rb') as fid:
        return cPickle.load(fid)

def __init__(self, classifierName):
    self.classifier = self.openClassifier(classifierName)
    self.classifyTweet(np.array([u""Helvetin vittu miksi aina pitÃ¤Ã¤ sataa vettÃ¤???""]))

def classifyTweet(self, tweetText):

    fitTweetVec = self.vect.fit_transform(tweetText)
    print self.vect.get_feature_names()
    X_new_tfidf = self.tfidf_transformer.fit_transform(fitTweetVec)
    print self.classifier.predict(X_new_tfidf)
</code></pre>

<p>What I'm doing wrong here? I used similar code while I made the classifier and ran test set for it. Have I forgotten some important step here?</p>

<p>Now I admit that I don't fully understand yet the fitting and transforming here since I found the Scikit's tutorial a bit ambiguous about it. If someone knows an as clear explanation of them as possible, I'm all for links :)</p>
";28004192;1330293;33731;elyase;2;28004192;"<p>The problem is that your classifier was trained with a fixed number of features (the length of the vocabulary of your previous data) and now when you <code>fit_transform</code> the new tweet, the <code>TfidfTransformer</code> will produce a new vocabulary and a new number of features, and will represent the new tweet in this space.</p>

<p>The solution is to also save the previously fitted <code>TfidfTransformer</code> (which contains the old vocabulary), load it with the classifier and <code>.transform</code> (not <code>fit_transform</code> because it was already fitted to the old data) the new tweet in this same representation. </p>

<p>You can also use a <code>Pipeline</code> that contains both the <code>TfidfTransformer</code> and the <code>Classifier</code> and pickle the <code>Pipeline</code>, this is easier and recommended.</p>
"
4403855;313;Isaac GS;<python><machine-learning><scikit-learn>;28005307;7;GridSearchCV no reporting on high verbosity;"<p>Okay, I'm just going to say starting out that I'm entirely new to SciKit-Learn and data science. But here is the issue and my current research on the problem. Code at the bottom.</p>

<h2>Summary</h2>

<p>I'm trying to do type recognition (like digits, for example) with a BernoulliRBM and I'm trying to find the correct parameters with GridSearchCV. However I don't see anything going on. With a lot of examples using verbosity settings I see output and progress, but with mine it just says,</p>

<pre><code>Fitting 3 folds for each of 15 candidates, totalling 45 fits
</code></pre>

<p>Then it sits there and does nothing....forever (or 8 hours, the longest I've waited with high verbosity settings).</p>

<p>I have a pretty large data set (1000 2D arrays each of size 428 by 428), so this might be the problem but I've also set the verbosity to 10 so I feel like I should be seeing some kind of output or progress. Also, in terms of my ""target"", it is just either a 0 or a 1, either it is the object I'm looking for (1), or it isn't (0).</p>

<h2>Previous Research</h2>

<ul>
<li>I looked into sklearn.preprocessing to see if that was necessary, it doesn't seem to be the issue (but again, I'm entirely new to this).</li>
<li>I increased verbosity</li>
<li>I switched from using a 3D list of data to using a list of scipy csr matrices.</li>
<li>I waited 8 hours with high verbosity settings, I still don't see anything happening.</li>
<li>I switched from not using a pipeline, to using a pipeline</li>
<li><p>I tampered with various parameters of gridsearchcv and tried creating fake (smaller) data sets to practice on.</p>

<pre><code>def network_trainer(self, data, files):
    train_x, test_x, train_y, test_y = train_test_split(data, files, test_size=0.2, random_state=0)

    parameters = {'learning_rate':np.arange(.25, .75, .1), 'n_iter':[5, 10, 20]}
    model = BernoulliRBM(random_state=0, verbose=True)
    model.cv = 2
    model.n_components = 2

    logistic = linear_model.LogisticRegression()
    pipeline = Pipeline(steps=[('model', model), ('clf', logistic)])

    gscv = grid_search.GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=10)
    gscv.fit(train_x, train_y)
    print gscv.best_params_
</code></pre></li>
</ul>

<p>I'd really appreciate a nudge in the right direction here. Thanks for considering my issue.</p>
";28056038;4370183;371;klubow;1;28021176;"<p>I noticed that GridSearch does not output anything when run with more than 1 thread. 
When all threads are finished it prints out everything.
And multithreading does not work in Windows - use n_jobs > 1 only in Linux.</p>
"
4403855;313;Isaac GS;<python><machine-learning><scikit-learn>;28005307;7;GridSearchCV no reporting on high verbosity;"<p>Okay, I'm just going to say starting out that I'm entirely new to SciKit-Learn and data science. But here is the issue and my current research on the problem. Code at the bottom.</p>

<h2>Summary</h2>

<p>I'm trying to do type recognition (like digits, for example) with a BernoulliRBM and I'm trying to find the correct parameters with GridSearchCV. However I don't see anything going on. With a lot of examples using verbosity settings I see output and progress, but with mine it just says,</p>

<pre><code>Fitting 3 folds for each of 15 candidates, totalling 45 fits
</code></pre>

<p>Then it sits there and does nothing....forever (or 8 hours, the longest I've waited with high verbosity settings).</p>

<p>I have a pretty large data set (1000 2D arrays each of size 428 by 428), so this might be the problem but I've also set the verbosity to 10 so I feel like I should be seeing some kind of output or progress. Also, in terms of my ""target"", it is just either a 0 or a 1, either it is the object I'm looking for (1), or it isn't (0).</p>

<h2>Previous Research</h2>

<ul>
<li>I looked into sklearn.preprocessing to see if that was necessary, it doesn't seem to be the issue (but again, I'm entirely new to this).</li>
<li>I increased verbosity</li>
<li>I switched from using a 3D list of data to using a list of scipy csr matrices.</li>
<li>I waited 8 hours with high verbosity settings, I still don't see anything happening.</li>
<li>I switched from not using a pipeline, to using a pipeline</li>
<li><p>I tampered with various parameters of gridsearchcv and tried creating fake (smaller) data sets to practice on.</p>

<pre><code>def network_trainer(self, data, files):
    train_x, test_x, train_y, test_y = train_test_split(data, files, test_size=0.2, random_state=0)

    parameters = {'learning_rate':np.arange(.25, .75, .1), 'n_iter':[5, 10, 20]}
    model = BernoulliRBM(random_state=0, verbose=True)
    model.cv = 2
    model.n_components = 2

    logistic = linear_model.LogisticRegression()
    pipeline = Pipeline(steps=[('model', model), ('clf', logistic)])

    gscv = grid_search.GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=10)
    gscv.fit(train_x, train_y)
    print gscv.best_params_
</code></pre></li>
</ul>

<p>I'd really appreciate a nudge in the right direction here. Thanks for considering my issue.</p>
";28056038;4403855;313;Isaac GS;7;28056038;"<p>Okay, so just to summarize everything I've figured out about it over the past few days.</p>

<ul>
<li>On Windows 8.1 don't set n_jobs to anything other than 1 if you still want it to be verbose.</li>
<li>In my case, even though I only have n_jobs = 1, all of my processor cores were still involved in the calculations, so either this is a bug or should be better documented.</li>
<li>I made the horrible mistake of using a list of csr matrices, so basically, read the documentation and then read it again before you ask questions.</li>
</ul>

<p>Again I'd like to thank @Barmaley.exe for the initial tip.</p>
"
4403855;313;Isaac GS;<python><machine-learning><scikit-learn>;28005307;7;GridSearchCV no reporting on high verbosity;"<p>Okay, I'm just going to say starting out that I'm entirely new to SciKit-Learn and data science. But here is the issue and my current research on the problem. Code at the bottom.</p>

<h2>Summary</h2>

<p>I'm trying to do type recognition (like digits, for example) with a BernoulliRBM and I'm trying to find the correct parameters with GridSearchCV. However I don't see anything going on. With a lot of examples using verbosity settings I see output and progress, but with mine it just says,</p>

<pre><code>Fitting 3 folds for each of 15 candidates, totalling 45 fits
</code></pre>

<p>Then it sits there and does nothing....forever (or 8 hours, the longest I've waited with high verbosity settings).</p>

<p>I have a pretty large data set (1000 2D arrays each of size 428 by 428), so this might be the problem but I've also set the verbosity to 10 so I feel like I should be seeing some kind of output or progress. Also, in terms of my ""target"", it is just either a 0 or a 1, either it is the object I'm looking for (1), or it isn't (0).</p>

<h2>Previous Research</h2>

<ul>
<li>I looked into sklearn.preprocessing to see if that was necessary, it doesn't seem to be the issue (but again, I'm entirely new to this).</li>
<li>I increased verbosity</li>
<li>I switched from using a 3D list of data to using a list of scipy csr matrices.</li>
<li>I waited 8 hours with high verbosity settings, I still don't see anything happening.</li>
<li>I switched from not using a pipeline, to using a pipeline</li>
<li><p>I tampered with various parameters of gridsearchcv and tried creating fake (smaller) data sets to practice on.</p>

<pre><code>def network_trainer(self, data, files):
    train_x, test_x, train_y, test_y = train_test_split(data, files, test_size=0.2, random_state=0)

    parameters = {'learning_rate':np.arange(.25, .75, .1), 'n_iter':[5, 10, 20]}
    model = BernoulliRBM(random_state=0, verbose=True)
    model.cv = 2
    model.n_components = 2

    logistic = linear_model.LogisticRegression()
    pipeline = Pipeline(steps=[('model', model), ('clf', logistic)])

    gscv = grid_search.GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=10)
    gscv.fit(train_x, train_y)
    print gscv.best_params_
</code></pre></li>
</ul>

<p>I'd really appreciate a nudge in the right direction here. Thanks for considering my issue.</p>
";28056038;3052835;1291;Dennis Sakva;0;36575885;"<p>Are you using ipython notebook and Python 2.x? If yes then multiprocessing module doesn't work with this combination. You can export (save as) your ipython notebook as a regular .py file and run it with regular python interpreter. Then you can use n_jobs=-1</p>
"
46190;7601;aneccodeal;<machine-learning><neural-network><conv-neural-network>;28005462;6;How to determine the number of feature maps to use in a convolutional neural network layer?;"<p>I've been doing a lot of reading on Conv Nets and even some playing using Julia's Mocha.jl package (which looks a lot like Caffe, but you can play with it in the Julia REPL).</p>

<p>In a Conv net, Convolution layers are followed by ""feature map"" layers. What I'm wondering is how does one determine how many feature maps a network needs to have to solve some particular problem? Is there any science to this or is it more art? I can see that if you're trying to make a classification at least that last layer should have number of feature maps == number of classes (unless you've got a fully connected MLP at the top of the network, I suppose).</p>

<p>In my case, I 'm not doing a classification so much as trying to come up with a value for every pixel in an image (I suppose this could be seen as a classification where the classes are from 0 to 255).</p>

<p>Edit: as pointed out in the comments, I'm trying to solve a regression problem where the outputs are in a range from 0 to 255 (grayscale in this case). Still, the question remains: How does one determine how many feature maps to use at any given convolution layer? Does this differ for a regression problem vs. a classification problem?</p>
";28021139;4367179;1031;Denis Tarasov;5;28021139;"<p>Basically, like any other hyperparameter - by evaluting results on separate development set and finding what number works best. It also worth checking publications that deal with similar problem and finding what number of feature maps they were using. </p>
"
46190;7601;aneccodeal;<machine-learning><neural-network><conv-neural-network>;28005462;6;How to determine the number of feature maps to use in a convolutional neural network layer?;"<p>I've been doing a lot of reading on Conv Nets and even some playing using Julia's Mocha.jl package (which looks a lot like Caffe, but you can play with it in the Julia REPL).</p>

<p>In a Conv net, Convolution layers are followed by ""feature map"" layers. What I'm wondering is how does one determine how many feature maps a network needs to have to solve some particular problem? Is there any science to this or is it more art? I can see that if you're trying to make a classification at least that last layer should have number of feature maps == number of classes (unless you've got a fully connected MLP at the top of the network, I suppose).</p>

<p>In my case, I 'm not doing a classification so much as trying to come up with a value for every pixel in an image (I suppose this could be seen as a classification where the classes are from 0 to 255).</p>

<p>Edit: as pointed out in the comments, I'm trying to solve a regression problem where the outputs are in a range from 0 to 255 (grayscale in this case). Still, the question remains: How does one determine how many feature maps to use at any given convolution layer? Does this differ for a regression problem vs. a classification problem?</p>
";28021139;3634699;94;sam_sach;1;41476572;"<p>More art. The only difference between imagenet winners that use conv-nets has been changing the structure of layers and maybe some novel ways of training.</p>

<p>VGG is a neat example. Begins with filter sizes beginning with 2^7, then 2^8, then 2^9 followed by fully connected layers, then an output layer which will give you your classes. Your maps and layer depths can be completely unrelated to the number of output classes.</p>

<p>You would not want a fully connected layer at the top. That kind of defeats the purpose that convolutional nets were designed to solve (overfitting and optimizing hundreds of thousands of weights per neuron)</p>

<p>Training on big sets will require some heavy computational resources. If you're working with imagenet - there's a set of pre-trained models with caffe that you could build on top of <a href=""http://caffe.berkeleyvision.org/model_zoo.html"" rel=""nofollow noreferrer"">http://caffe.berkeleyvision.org/model_zoo.html</a></p>

<p>I'm not sure if you can port these to mocha. There's a port to tensor flow though if you're interested in that <a href=""https://github.com/ethereon/caffe-tensorflow"" rel=""nofollow noreferrer"">https://github.com/ethereon/caffe-tensorflow</a></p>
"
3378649;4188;user3378649;<python><machine-learning><scikit-learn>;28005537;3;How do I avoid re-training machine learning models;"<p>self-learner here. </p>

<p>I am building a web application that predict events. </p>

<p>Let's consider this quick example. </p>

<pre><code>X = [[0], [1], [2], [3]]
y = [0, 0, 1, 1]
from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit(X, y) 

print(neigh.predict([[1.1]]))
</code></pre>

<p>How can I keep the state of <code>neigh</code> so when I enter a new value like <code>neigh.predict([[1.2]])</code> I don't need to re-train the model. Is there any good practice, or hint to start solving the problem  ? </p>
";28008058;577088;124140;senderle;10;28006451;"<p>You've chosen a slightly confusing example for a couple of reasons. First, when you say <code>neigh.predict([[1.2]])</code>, you aren't adding a new training point, you're just doing a new prediction, so that doesn't require any changes at all. Second, KNN algorithms aren't really ""trained"" -- KNN is an <a href=""http://en.wikipedia.org/wiki/Instance-based_learning"" rel=""noreferrer"">instance-based</a> algorithm, which means that ""training"" amounts to storing the training data in a suitable structure. As a result, this question has two different answers. I'll try to answer the KNN question first.</p>

<p><strong>K Nearest Neighbors</strong></p>

<p>For KNN, adding new training data amounts to appending new data points to the structure. However, it appears that <code>scikit-learn</code> doesn't provide any such functionality. (That's reasonable enough -- since KNN explicitly stores <em>every</em> training point, you can't just keep giving it new training points indefinitely.) </p>

<p>If you aren't using many training points, a simple list might be good enough for your needs! In that case, you could skip <code>sklearn</code> altogether, and just append new data points to your list. To make a prediction, do a linear search, saving the <code>k</code> nearest neighbors, and then make a prediction based on a simple ""majority vote"" -- if out of five neighbors, three or more are red, then return red, and so on. But keep in mind that every training point you add will slow the algorithm.</p>

<p>If you need to use many training points, you'll want to use a more efficient structure for nearest neighbor search, like a <a href=""http://en.wikipedia.org/wiki/K-d_tree"" rel=""noreferrer"">K-D Tree</a>. There's a <a href=""http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.KDTree.html"" rel=""noreferrer""><code>scipy</code></a> K-D Tree implementation that ought to work. The <code>query</code> method allows you to find the <code>k</code> nearest neighbors. It will be more efficient than a list, but it will still get slower as you add more training data.</p>

<p><strong>Online Learning</strong></p>

<p>A more general answer to your question is that you are (unbeknownst to yourself) trying to do something called <a href=""http://en.wikipedia.org/wiki/Online_machine_learning"" rel=""noreferrer"">online learning</a>. Online learning algorithms allow you to use individual training points as they arrive, and discard them once they've been used. For this to make sense, you need to be storing not the training points themselves (as in KNN) but a set of parameters, which you optimize.</p>

<p>This means that some algorithms are better suited to this than others. <code>sklearn</code> provides just a few algorithms <a href=""http://scikit-learn.org/stable/modules/scaling_strategies.html"" rel=""noreferrer"">capable of online learning</a>. These all have a <code>partial_fit</code> method that will allow you to pass training data in batches. The <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier"" rel=""noreferrer""><code>SKDClassifier</code></a> with <code>'hinge'</code> or <code>'log'</code> loss is probably a good starting point. </p>
"
3378649;4188;user3378649;<python><machine-learning><scikit-learn>;28005537;3;How do I avoid re-training machine learning models;"<p>self-learner here. </p>

<p>I am building a web application that predict events. </p>

<p>Let's consider this quick example. </p>

<pre><code>X = [[0], [1], [2], [3]]
y = [0, 0, 1, 1]
from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit(X, y) 

print(neigh.predict([[1.1]]))
</code></pre>

<p>How can I keep the state of <code>neigh</code> so when I enter a new value like <code>neigh.predict([[1.2]])</code> I don't need to re-train the model. Is there any good practice, or hint to start solving the problem  ? </p>
";28008058;4370183;371;klubow;6;28008058;"<p>Or maybe you just want to save your model after fitting</p>

<pre><code>joblib.dump(neigh, FName)
</code></pre>

<p>and load it when needed</p>

<pre><code>neigh = joblib.load(FName)
neigh.predict([[1.1]])
</code></pre>
"
4462426;159;bzkrtmurat;<machine-learning><artificial-intelligence><tic-tac-toe><reinforcement-learning><q-learning>;28021734;9;Q Learning Algorithm for Tic Tac Toe;"<p>I could not understand how to update Q values for tic tac toe game. I read all about that but I could not imagine how to do this. I read that Q value is updated end of the game, but I haven't understand that if there is Q value for each action ?</p>
";28022044;1478624;4868;Tudor Berariu;6;28022044;"<p>You have a <code>Q</code> value for each state-action pair. You update one <code>Q</code> value after every action you perform. More precisely, if applying action <code>a1</code> from state <code>s1</code> gets you into state <code>s2</code> and brings you some reward <code>r</code>, then you update <code>Q(s1, a1)</code> as follows:</p>

<pre><code>Q(s1, a1) = Q(s1, a1) + learning_rate * (r + discount_factor * max Q(s2, _) - Q(s1, a1))
</code></pre>

<p>In many games, such as tic-tac-toe you don't get rewards until the end of the game, that's why you have to run the algorithm through several episodes. That's how information about utility of final states is propagated to other states.</p>
"
4462426;159;bzkrtmurat;<machine-learning><artificial-intelligence><tic-tac-toe><reinforcement-learning><q-learning>;28021734;9;Q Learning Algorithm for Tic Tac Toe;"<p>I could not understand how to update Q values for tic tac toe game. I read all about that but I could not imagine how to do this. I read that Q value is updated end of the game, but I haven't understand that if there is Q value for each action ?</p>
";28022044;468305;3510;vonjd;2;36529143;"<p>The problem with the standard Q Learning algorithm is that it just takes too long to propagate the values from the final to the first move because you only know the outcome of the game at the end of it.</p>

<p>Therefore the Q Learning algorithm should be modified. The following paper gives some details on possible modifications:</p>

<ol>
<li>a non negative reward is given after the game ends (except for draw), then the Q updates is not performed at every action step (which changes nothing), but
only after the end of the game</li>
<li>the Q updates is performed by propagating its new value from the last move
backward to the first move</li>
<li>another update formula is incorporated that also considers the opponent point of view because of the turn-taking nature of two-player game</li>
</ol>

<p>Abstract:</p>

<blockquote>
  <p>This paper reports our experiment on applying Q Learning algorithm for
  learning to play Tic-tac-toe. The original algorithm is modified by
  updating the Q value only when the game terminates, propagating the
  update process from the final move backward to the first move, and
  incorporating a new update rule. We evaluate the agent performance
  using full-board and partial-board representations. In this
  evaluation, the agent plays the tic-tac-toe game against human
  players. The evaluation results show that the performance of modified
  Q Learning algorithm with partial-board representation is comparable
  to that of human players.</p>
</blockquote>

<p><a href=""http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=5254776&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F5235860%2F5254768%2F05254776.pdf%3Farnumber%3D5254776"" rel=""nofollow"">Learning to Play Tic-Tac-Toe (2009) by Dwi H. Widyantoro &amp; Yus G. Vembrina</a></p>

<p>(Unfortunately it is behind a paywall. Either you have access to the IEEE archive or you can ask the authors to provide a copy on researchgate: <a href=""https://www.researchgate.net/publication/251899151_Learning_to_play_Tic-tac-toe"" rel=""nofollow"">https://www.researchgate.net/publication/251899151_Learning_to_play_Tic-tac-toe</a>)</p>
"
2795062;152;imkhan;<python><machine-learning><scikit-learn><auc>;28022081;2;How to calculate AUC for One Class SVM in python?;"<p>I have difficulty in plotting OneClassSVM's AUC plot in python (I am using sklearn which generates confusion matrix like <code>[[tp, fp],[fn,tn]]</code> with <code>fn=tn=0</code>. </p>

<pre><code>from sklearn.metrics import roc_curve, auc
fpr, tpr, thresholds = roc_curve(y_test, y_nb_predicted)
roc_auc = auc(fpr, tpr) # this generates ValueError[1]
print ""Area under the ROC curve : %f"" % roc_auc
plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)
</code></pre>

<p>I want to handle error [1] and plot <code>AUC</code> for <code>OneClassSVM</code>. </p>

<pre><code>[1] ValueError: Input contains NaN, infinity or a value too large for dtype('float64').
</code></pre>
";49942754;567620;63048;ely;3;49942754;"<p>Please see <a href=""https://stackoverflow.com/a/49942654/567620"">my answer</a> on a similar question. The gist is:</p>

<ul>
<li><p>OneClassSVM fundamentally doesn't support converting a decision into a probability score, so you cannot pass the necessary scores into functions that require varying a score threshold, such as for ROC or Precision-Recall curves and scores.</p></li>
<li><p>You can approximate this type of score by computing the max value of your OneClassSVM's decision function across your input data, call it <code>MAX</code>, and then score the prediction for a given observation <code>y</code> by computing <code>y_score = MAX - decision_function(y)</code>.</p></li>
<li><p>Use these scores to pass as <code>y_score</code> to functions such as <code>average_precision_score</code>, etc., which will accept non-thresholded scores instead of probabilities.</p></li>
<li><p>Finally, keep in mind that ROC will make less physical sense for OneClassSVM specifically because OneClassSVM is intended for situations where there is an expected and huge class imbalance (outliers vs. non-outliers), and ROC will not accurately up-weight the relative success on the small amount of outliers.</p></li>
</ul>
"
2795062;152;imkhan;<python><machine-learning><scikit-learn><auc>;28022081;2;How to calculate AUC for One Class SVM in python?;"<p>I have difficulty in plotting OneClassSVM's AUC plot in python (I am using sklearn which generates confusion matrix like <code>[[tp, fp],[fn,tn]]</code> with <code>fn=tn=0</code>. </p>

<pre><code>from sklearn.metrics import roc_curve, auc
fpr, tpr, thresholds = roc_curve(y_test, y_nb_predicted)
roc_auc = auc(fpr, tpr) # this generates ValueError[1]
print ""Area under the ROC curve : %f"" % roc_auc
plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)
</code></pre>

<p>I want to handle error [1] and plot <code>AUC</code> for <code>OneClassSVM</code>. </p>

<pre><code>[1] ValueError: Input contains NaN, infinity or a value too large for dtype('float64').
</code></pre>
";49942754;7117716;571;Shaina Raza;0;65330488;"<p>Use the predprobs function to calculate the scores or probabilities/scores as asked in the auc(y_true, y_score), the issue is because of y_score. you can convert it as shown in the following line of code</p>
<pre><code># Classifier - Algorithm - SVM
# fit the training dataset on the classifier
SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto',probability=True)
SVM.fit(Train_X_Tfidf,Train_Y)
# predict the labels on validation dataset
predictions_SVM = SVM.predict(Test_X_Tfidf)
# Use accuracy_score function to get the accuracy
print(&quot;SVM Accuracy Score -&gt; &quot;,accuracy_score(predictions_SVM, Test_Y))

probs = SVM.predict_proba(Test_X_Tfidf)
preds = probs[:,1]
fpr, tpr, threshold = roc_curve(Test_Y, preds)
print(&quot;SVM Area under curve -&gt; &quot;,auc(fpr, tpr))
</code></pre>
<p>see the difference between the accuracy_score and the auc(), you need the scores of predictions.</p>
<p>share  edit  delete  flag</p>
"
194106;3262;Uri Cohen;<machine-learning><classification><svm><libsvm>;28023554;0;max-margin linear separator using libsvm;"<p>I have a set of N data points X with +/- labels for which I'd like to calculate the max-margin linear separator (aka classifier, hyperplane) or fail if no such linear separator exist. </p>

<p>I do not want to avoid overfitting in the context of this question, as I do so elsewhere. So no slack variables ; no cross-validation ; no limits on the number of support vectors ; just find max-margin separator or fail.</p>

<p>How to I use libsvm to do so? I believe you can't give c=0 in C-SVM and you can't give nu=1 in nu-svm.</p>

<p>Related question (which I think didn't provide an answer):
<a href=""https://stackoverflow.com/questions/9375502/which-of-the-parameters-in-libsvm-is-the-slack-variable"">Which of the parameters in LibSVM is the slack variable?</a></p>
";28044905;682624;133;Daniel Moraes;1;28044905;"<p>In the case of C-SVM, you should use a linear kernel and a very large C value (or nu = 0.999... for nu-SVM). If you still have slacks with this setting, probably your data is not linearly separable.</p>

<p>Quick explanation: the C-SVM optimization function tries to find the hyperplane having maximum margin and lowest misclassification costs at the same time. The misclassification costs in the C-SVM formulation is defined by: distance from the misclassified point to its correct side of the hyperplane, multiplied by C. If you increase the C value (or nu value for nu-SVM), every misclassified point will be too costly and an hyperplane that separates the data perfectly will be preferable for the optimization function.</p>
"
4469858;21;RoiG;<python><machine-learning><scikit-learn>;28024191;2;Using precomputed Gram matrix in sklearn linear models (Lasso, Lars, etc);"<p>I'm trying to train a linear model on a very large dataset. 
The feature space is small but there are too many samples to hold in memory. 
I'm calculating the Gram matrix on-the-fly and trying to pass it as an argument to sklearn Lasso (or other algorithms) but, when I call fit, it needs the actual X and y matrices. </p>

<p>Any idea how to use the 'precompute' feature without storing the original matrices?</p>
";;1403102;176;Robert Schwarz;0;28024414;"<p>(My answer is based on the usage of svm.SVC, Lasso may be different.)</p>

<p>I think that you are supposed pass the Gram matrix instead of X to the fit method.</p>

<p>Also, the Gram matrix has shape (n_samples, n_samples) so it should also be too large for memory in your case, right?</p>
"
2602163;205;Yaeli778;<matlab><machine-learning><pca><face-recognition><reduction>;28027104;0;How to calculate MDA for face recognition in matlab;"<p>I want to do PCA and then MDA (Multiple Discriminative Analysis) in order to reduce the dimensions of the dataset from 99^2 to 49 (face recognition).</p>

<p>My first step was reducing dimensions from 99^2 to 50 by PCA. Now I want to use MDA to reduce from c to c-1 -> from 50 to 49. 
I've tried this code but I get complex values in the 'Answer', which is wrong. </p>

<pre><code>%   calculate PCA
mat_mean = mean(trainData(:));
normalized_train = trainData - mat_mean;
A = normalized_train/std(normalized_train(:));
S1 = A * A';
[V,Z] = eigs(S2,50);
Wpca = A'*V*Z;


%    calculate MDA
[Sb,Sw] = scattermat(Wpca);
Sb1=Wpca*Sb*Wpca';
Sw1=Wpca*Sw*Wpca';
[Answer,ready1] = eigs(Sb1,Sw1,49);
</code></pre>

<p>Any suggestions what am I doing wrong?</p>
";28031226;2602163;205;Yaeli778;0;28031226;"<p>The reason is that ""eigs"" calculates the eigenvalues of the matrix, which includes SQRT in it... and I have negative values in Sb,Sw</p>
"
2590727;359;user2590727;<java><machine-learning><predictionio>;28027507;1;EasyRec vs PredictionIO vs Apache Mahout;"<p>I would like to develop real-time analytic tools for my website ( engineering project). My application will be written in JAVA-EE and maven. I have found three tools (topic). I knew that PredictionIO is using Apache Mahout. But I can't decide which algorithm use. If someone knew what are the pros and cons of this algorithms let them write. What do you think, which will be the best ?</p>
";28248817;2212075;46;tstonez;2;28248817;"<p>As of version 0.8 of PredictionIO the stack has been rebuilt on Apache Spark and now integrates <a href=""https://spark.apache.org/docs/0.9.1/mllib-guide.html"" rel=""nofollow"">MLlib</a> library. It is hard to advise about pros and cons of different algos without knowing your use case and in more detail feel free to ask on our PredictionIO support forum.</p>

<p>Databricks (creators of Apache Spark) also have some <a href=""https://databricks.com/blog/2014/07/23/scalable-collaborative-filtering-with-spark-mllib.html"" rel=""nofollow"">performance benchmarks for MLlib versus Mahout</a>.</p>
"
1458453;87;lakhujanivijay;<machine-learning><classification><cluster-analysis><supervised-learning><unsupervised-learning>;28028017;0;Is supervised learning synonymous to classification and unsupervised learning synonymous to clustering?;"<p>I am a beginner in machine learning and recently read about supervised and unsupervised machine learning. It looks like supervised learning is synonymous to classification and unsupervised learning is  synonymous to clustering, is it so?</p>
";28028879;1190430;5186;Artem Sobolev;3;28028879;"<p>No.</p>

<p>Supervised learning is when you know correct answers (targets). Depending on their type, it might be <a href=""http://en.wikipedia.org/wiki/Statistical_classification"" rel=""nofollow"">classification</a> (categorical targets), <a href=""http://en.wikipedia.org/wiki/Regression_analysis"" rel=""nofollow"">regression</a> (numerical targets) or <a href=""http://en.wikipedia.org/wiki/Learning_to_rank"" rel=""nofollow"">learning to rank</a> (ordinal targets) (this list is by no means complete, there might be other types that I either forgot or unaware of).</p>

<p>On the contrary, in unsupervised learning setting we don't know correct answers, and we try to infer, learn some structure from data. Be it cluster number or low-dimensional approximation (<a href=""http://en.wikipedia.org/wiki/Dimensionality_reduction"" rel=""nofollow"">dimensionality reduction</a>, actually, one might think of clusterization as of extreme 1D case of dimensionality reduction). Again, this might be far away from completeness, but the general idea is about hidden structure, that we try to discover from data.</p>
"
1458453;87;lakhujanivijay;<machine-learning><classification><cluster-analysis><supervised-learning><unsupervised-learning>;28028017;0;Is supervised learning synonymous to classification and unsupervised learning synonymous to clustering?;"<p>I am a beginner in machine learning and recently read about supervised and unsupervised machine learning. It looks like supervised learning is synonymous to classification and unsupervised learning is  synonymous to clustering, is it so?</p>
";28028879;1060350;70512;Has QUIT--Anony-Mousse;1;28033089;"<p><strong>Supervised learning</strong> is when you have <em>labeled training data</em>. In other words, you have a well-defined target to optimize your method for.</p>

<p>Typical (supervised) learning tasks are classification and regression: learning to predict categorial (classification), numerical (regression) values or ranks (learning to rank).</p>

<p><strong>Unsupservised learning</strong> is an odd term. Because most of the time, the methods aren't ""<em>learning</em>"" anything. Because what would they learn from? You don't have training data?</p>

<p>There are plenty of <strong>unsupervised methods</strong> that don't fit the ""learning"" paradigm well. This includes dimensionality reduction methods such as PCA (which by far predates any ""machine learning"" - PCA was proposed in 1901, long before the computer!). Many of these are just data-driven statistics (as opposed to parameterized statistics). This includes most cluster analysis methods, outlier detection, ... for understanding these, it's better to step out of the ""learning"" mindset. Many people have trouble understanding these approaches, because they always think in the ""minimize objective function f"" mindset common in learning.</p>

<p>Consider for example DBSCAN. One of the most popular clustering algorithms. It does not fit the learning paradigm well. It can nicely be interpreted as a graph-theoretic construct: (density-) connected components. But it doesn't optimize any objective function. It computes the transitive closure of a relation; but there is no function maximized or minimized.</p>

<p>Similarly APRIORI finds frequent itemsets; combinations of items that occur more than minsupp times, where minsupp is a user parameter. It's an extremely simple definition; but the search space can be painfully large when you have large data. The brute-force approach just doesn't finish in acceptable time. So APRIORI uses a clever search strategy to avoid unnecessary hard disk accesses, computations, and memory. But there is no ""worse"" or ""better"" result as in learning. Either the result is correct (complete) or not - nothing to optimize on the result (only on the algorithm runtime).</p>

<p>Calling these methods ""unsupervised learning"" is squeezing them into a mindset that they don't belong into. They are not ""learning"" anything. Neither optimizes a function, or uses labels, or uses any kind of feedback. They just SELECT a certain set of objects from the database: APRIORI selects columns that frequently have a 1 at the same time; DBSCAN select connected components in a density graph. Either the result is correct, or not.</p>

<p>Some (but by far not all) unsupervised methods can be formalized as an optimization problem. At which point they become similar to popular supervised learning approaches. For example k-means is a minimization problem. PCA is a minimization problem, too - closely related to linear regression, actually. But it is the other way around. Many machine learning tasks are transformed into an optimization problem; and can be solved with general purpose statistical tools, which just happen to be highly popular in machine learning (e.g. linear programming). All the ""learning"" part is then wrapped into the way the data is transformed prior to feeding it into the optimizer. And in some cases, like for PCA, a non-iterative way to compute the optimum solution was found (in 1901). So in these cases, you don't need the usual optimization hammer at all.</p>
"
821806;4054;zergylord;<machine-learning><classification><detection>;28031513;1;Tricks for classification problems where one class is predominant?;"<p>Imagine you have two classes A and B, and you need to distinguish between them. Normally I'd just train a normal classifier on the data (e.g. logistic regression, backprop net) and be done with it. But I notice that A occurs several orders of magnitude more frequently than than B does! Should present the classifier with the data distribution as is, or change it such that the classes are equally frequent? Or would more drastic changes be helpful? I imagine this case has been studied a lot previously (e.g. face detection), but I'm somewhat unfamiliar with how much the techniques differ from regular old classification.</p>
";;840460;13642;Backlin;1;28044992;"<p>There are a number of ways to deal with so called ""imbalanced datasets"", e.g. oversampling, and custom costs matrices and class priors.</p>

<p>I generally prefer to keep the problem as it is, but be sure to study the performance classwise. If a classifier for instance does 20% errors on a dataset with two classes, one of which only accounts for 25% of the observations, you need the classwise errors to tell if it is doing a good overall job or just neglecting the minor class.</p>

<p>A good introduction can be found in <a href=""https://books.google.se/books?id=S-XvEQWABeUC"" rel=""nofollow"">Data Mining and Knowledge Discovery Handbook</a>, chapter 40, <a href=""https://www3.nd.edu/~dial/papers/SPRINGER05.pdf"" rel=""nofollow"">Data mining for imbalanced datasets: An overview</a>. (NB, I think the pdf obtained with the latter link is publicly available, but it is hard to tell from within the university network I'm working from.)</p>
"
3393459;8752;exp1orer;<python><numpy><pandas><machine-learning><scikit-learn>;28035216;9;Ordered Logit in Python?;"<p>I'm interested in running an ordered logit regression in python (using pandas, numpy, sklearn, or something that ecosystem). But I cannot find any way to do this. Is my google-skill lacking? Or is this not something that's been implemented in a standard package?</p>
";32007463;744239;1056;dmh;5;32007463;"<p>If you're looking for Ordered Logistic Regression, it looks like you can find it in <a href=""https://github.com/fabianp/minirank/blob/master/minirank/logistic.py"" rel=""noreferrer"">Fabian Pedregosa's <code>minirank</code> repo on GitHub</a>.</p>

<p>(Hattip to @elyase, who originally provided the link in a comment on the question.)</p>
"
2028043;5367;USB;<machine-learning><classification><nearest-neighbor><knn>;28046735;1;KNN choosing classlabel when k=4;"<p>In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). </p>

<ol>
<li>If k = 1, then the object is simply assigned to the class of that single nearest neighbor.</li>
<li>If k=3, and the classlabels are Good =2 Bad=1,then the predicted classlabel will be Good,which contains the magority vote.</li>
<li><strong>If k=4, and the classlabels are Good =2 Bad=2, What will be the classlabel?</strong> </li>
</ol>
";28046982;2328763;11346;Alper;2;28046982;"<p>There are different approaches. For example Matlab uses 'random' or 'nearest' as documented <a href=""http://de.mathworks.com/help/bioinfo/ref/knnclassify.html"" rel=""nofollow"">here</a>.</p>

<blockquote>
  <p>When classifying to more than two groups or when using an even value
  for k, it might be necessary to break a tie in the number of nearest
  neighbors. Options are 'random', which selects a random tiebreaker,
  and 'nearest', which uses the nearest neighbor among the tied groups
  to break the tie.</p>
</blockquote>
"
2028043;5367;USB;<machine-learning><classification><nearest-neighbor><knn>;28046735;1;KNN choosing classlabel when k=4;"<p>In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). </p>

<ol>
<li>If k = 1, then the object is simply assigned to the class of that single nearest neighbor.</li>
<li>If k=3, and the classlabels are Good =2 Bad=1,then the predicted classlabel will be Good,which contains the magority vote.</li>
<li><strong>If k=4, and the classlabels are Good =2 Bad=2, What will be the classlabel?</strong> </li>
</ol>
";28046982;1060350;70512;Has QUIT--Anony-Mousse;0;28055307;"<p>This problem is <strong>not specific to k=4</strong>.</p>

<p>Consider a data set with 3 classes. At k=2, two different classes may arise. At k=3, three different classes may arise, at k=4, it may be 0,2,2... any k beyond 1 bears the risk of a <em>tie</em>.</p>

<p>Choose one at random, or use weighting (i.e. give the 1NN more weight than the 2nd nearest neighbor etc.) to further reduce the risk of ties.</p>
"
3016483;586;Illuminati0x5B;<matlab><opencv><machine-learning><computer-vision><mex>;28048441;1;Running Stretchable Models for Human parsing Code;"<p>I'm trying to run the code given in the following link on Ubuntu 13.04 (64 bit):</p>

<p><a href=""https://github.com/bensapp/Stretchable-Models-for-Motion-Parsing"" rel=""nofollow"">https://github.com/bensapp/Stretchable-Models-for-Motion-Parsing</a></p>

<p>I'm getting the following error:</p>

<p>INVALID MEX FILE: 'filepath/cps/utils/mex_opencv_boosting.mexa64': filepath/cps/utils/mex_opencv_bosting.mexa64: undefined symbol: _ZN7CvBoostC1EPKc.</p>

<p>Can someone help me with fixing this error?</p>

<p>Thank you</p>

<hr>

<p>When I run the command lld mex_opencv_boosting.mexa64, I get the following output. Everything seems to be fine.</p>

<pre><code>
linux-vdso.so.1 =>  (0x00007fff7c588000)
    libcxcore.so.4 => filepath/cps/thirdparty/OpenCV-2.0.0/lib/libcxcore.so.4 (0x00007ff8f63b0000)
    libcv.so.4 => filepath/cps/thirdparty/OpenCV-2.0.0/lib/libcv.so.4 (0x00007ff8f5f2b000)
    libml.so.4 => filepath/cps/thirdparty/OpenCV-2.0.0/lib/libml.so.4 (0x00007ff8f5c9e000)
    libhighgui.so.4 => filepath/cps/thirdparty/OpenCV-2.0.0/lib/libhighgui.so.4 (0x00007ff8f5a70000)
    libmx.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmx.so (0x00007ff8f5748000)
    libmex.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmex.so (0x00007ff8f5523000)
    libmat.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmat.so (0x00007ff8f52d3000)
    libstdc++.so.6 => /usr/local/MATLAB/R2014a/sys/os/glnxa64/libstdc++.so.6 (0x00007ff8f4fcb000)
    libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007ff8f4c9a000)
    libgcc_s.so.1 => /usr/local/MATLAB/R2014a/sys/os/glnxa64/libgcc_s.so.1 (0x00007ff8f4a85000)
    libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007ff8f4867000)
    libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007ff8f449f000)
    librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007ff8f4297000)
    libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007ff8f407d000)
    libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007ff8f3e79000)
    libgomp.so.1 => /usr/lib/x86_64-linux-gnu/libgomp.so.1 (0x00007ff8f3c6a000)
    libpng12.so.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libpng12.so.0 (0x00007ff8f3a43000)
    libmwresource_core.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwresource_core.so (0x00007ff8f3841000)
    libmwi18n.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwi18n.so (0x00007ff8f3567000)
    libut.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libut.so (0x00007ff8f32ad000)
    libmwfl.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwfl.so (0x00007ff8f2eb1000)
    libmwMATLAB_res.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwMATLAB_res.so (0x00007ff8f28ab000)
    libboost_date_time.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_date_time.so.1.49.0 (0x00007ff8f269a000)
    libboost_signals.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_signals.so.1.49.0 (0x00007ff8f2482000)
    libboost_system.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_system.so.1.49.0 (0x00007ff8f227e000)
    libboost_thread.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_thread.so.1.49.0 (0x00007ff8f2063000)
    libmwcpp11compat.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwcpp11compat.so (0x00007ff8f1e56000)
    libboost_log.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_log.so.1.49.0 (0x00007ff8f1b77000)
    libboost_log_setup.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_log_setup.so.1.49.0 (0x00007ff8f1665000)
    libicudata.so.49 => /usr/local/MATLAB/R2014a/bin/glnxa64/libicudata.so.49 (0x00007ff8f0345000)
    libicuuc.so.49 => /usr/local/MATLAB/R2014a/bin/glnxa64/libicuuc.so.49 (0x00007ff8effbc000)
    libicui18n.so.49 => /usr/local/MATLAB/R2014a/bin/glnxa64/libicui18n.so.49 (0x00007ff8efbad000)
    libicuio.so.49 => /usr/local/MATLAB/R2014a/bin/glnxa64/libicuio.so.49 (0x00007ff8ef9a0000)
    libtbb.so.2 => /usr/local/MATLAB/R2014a/bin/glnxa64/libtbb.so.2 (0x00007ff8ef859000)
    libtbbmalloc.so.2 => /usr/local/MATLAB/R2014a/bin/glnxa64/libtbbmalloc.so.2 (0x00007ff8ef724000)
    libmwservices.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwservices.so (0x00007ff8ef158000)
    libmwmpath.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwmpath.so (0x00007ff8eef11000)
    libmwm_dispatcher.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwm_dispatcher.so (0x00007ff8eec3d000)
    libboost_filesystem.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_filesystem.so.1.49.0 (0x00007ff8eea1d000)
    libhdf5_hl.so.6 => /usr/local/MATLAB/R2014a/bin/glnxa64/libhdf5_hl.so.6 (0x00007ff8ee7f0000)
    libhdf5.so.6 => /usr/local/MATLAB/R2014a/bin/glnxa64/libhdf5.so.6 (0x00007ff8ee371000)
    /lib64/ld-linux-x86-64.so.2 (0x00007ff8f6a53000)
    libexpat.so.1 => /usr/local/MATLAB/R2014a/bin/glnxa64/libexpat.so.1 (0x00007ff8ee149000)
    libcrypt.so.1 => /lib/x86_64-linux-gnu/libcrypt.so.1 (0x00007ff8edf0f000)
    libboost_chrono.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_chrono.so.1.49.0 (0x00007ff8edd08000)
    libboost_regex.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_regex.so.1.49.0 (0x00007ff8ed9ec000)
    libboost_serialization.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_serialization.so.1.49.0 (0x00007ff8ed774000)
    libunwind.so.8 => /usr/local/MATLAB/R2014a/bin/glnxa64/libunwind.so.8 (0x00007ff8ed556000)
    libmwregexp.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwregexp.so (0x00007ff8ed30d000)
    libmwmlutil.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwmlutil.so (0x00007ff8ecca9000)
    libmwsettingscore.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwsettingscore.so (0x00007ff8ec83b000)
    libmwms.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwms.so (0x00007ff8ec296000)
    libmwMATLAB_settings_res.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwMATLAB_settings_res.so (0x00007ff8ec073000)
    libmwnativedisplay.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwnativedisplay.so (0x00007ff8ebe6a000)
    libmwopccore.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwopccore.so (0x00007ff8ebc0b000)
    libmwopcmodel.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwopcmodel.so (0x00007ff8eb953000)
    libmwopczippackage.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwopczippackage.so (0x00007ff8eb737000)
    libmwopcmwservices.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwopcmwservices.so (0x00007ff8eb4dd000)
    libboost_iostreams.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_iostreams.so.1.49.0 (0x00007ff8eb2c7000)
    libxerces-c.so.27 => /usr/local/MATLAB/R2014a/bin/glnxa64/libxerces-c.so.27 (0x00007ff8eace4000)
    libncurses.so.5 => /lib/x86_64-linux-gnu/libncurses.so.5 (0x00007ff8eaac1000)
    libmwxmlcore.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwxmlcore.so (0x00007ff8ea84c000)
    libminizip.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libminizip.so (0x00007ff8ea63f000)
    libtinfo.so.5 => /lib/x86_64-linux-gnu/libtinfo.so.5 (0x00007ff8ea415000)</code></pre>
";;6094282;1;Sancha;0;36135062;"<p>Try the following</p>

<pre><code>export LD_LIBRARY_PATH=filepath/cps/thirdparty/opencv/lib:$LD_LIBRARY_PATH
</code></pre>

<p>After this run MATLAB from terminal. Your code should run. </p>

<p>What I have found is that the symbol: _ZN7CvBoostC1EPKc is there in filepath/cps/thirdparty/opencv/lib/libml.so.4</p>

<pre><code>nm filepath/cps/thirdparty/opencv/lib/libml.so.4 | grep _ZN7CvBoostC1EPKc
</code></pre>

<p>Not in the file filepath/cps/thirdparty/OpenCV-2.0.0/lib/libml.so.4 and somehow setting the environment variable from the MATLAB is not working. Exporting LD_LIBRARY_PATH before running MATLAB is necessary.</p>
"
1115169;2518;pbu;<machine-learning><deep-learning><caffe><kaggle>;28057957;11;Multi label regression in Caffe;"<p>i am extracting 30 facial keypoints (x,y) from an input image as per kaggle facialkeypoints competition.</p>

<p>How do i setup caffe to run a regression and produce 30 dimensional output??. </p>

<pre><code>Input: 96x96 image
Output: 30 - (30 dimensions).
</code></pre>

<p>How do i setup caffe accordingly?. I am using EUCLIDEAN_LOSS (sum of squares) to get the regressed output. Here is a simple logistic regressor model using caffe but it is not working. Looks accuracy layer cannot handle multi-label output.</p>

<pre><code>I0120 17:51:27.039113  4113 net.cpp:394] accuracy &lt;- label_fkp_1_split_1
I0120 17:51:27.039135  4113 net.cpp:356] accuracy -&gt; accuracy
I0120 17:51:27.039158  4113 net.cpp:96] Setting up accuracy
F0120 17:51:27.039201  4113 accuracy_layer.cpp:26] Check failed: bottom[1]-&gt;channels() == 1 (30 vs. 1) 
*** Check failure stack trace: ***
    @     0x7f7c2711bdaa  (unknown)
    @     0x7f7c2711bce4  (unknown)
    @     0x7f7c2711b6e6  (unknown)
</code></pre>

<p>Here is the layer file:</p>

<pre><code>name: ""LogReg""
layers {
  name: ""fkp""
  top: ""data""
  top: ""label""
  type: HDF5_DATA
  hdf5_data_param {
   source: ""train.txt""
   batch_size: 100
  }
    include: { phase: TRAIN }

}

layers {
  name: ""fkp""
  type: HDF5_DATA
  top: ""data""
  top: ""label""
  hdf5_data_param {
    source: ""test.txt""
    batch_size: 100
  }

  include: { phase: TEST }
}

layers {
  name: ""ip""
  type: INNER_PRODUCT
  bottom: ""data""
  top: ""ip""
  inner_product_param {
    num_output: 30
  }
}
layers {
  name: ""loss""
  type: EUCLIDEAN_LOSS
  bottom: ""ip""
  bottom: ""label""
  top: ""loss""
}

layers {
  name: ""accuracy""
  type: ACCURACY
  bottom: ""ip""
  bottom: ""label""
  top: ""accuracy""
  include: { phase: TEST }
}
</code></pre>
";28369288;1115169;2518;pbu;4;28369288;"<p>i found it :)</p>

<p>I replaced the SOFTLAYER to EUCLIDEAN_LOSS function and changed the number of outputs. It worked.</p>

<pre><code>layers {
  name: ""loss""
  type: EUCLIDEAN_LOSS
  bottom: ""ip1""
  bottom: ""label""
  top: ""loss""
}
</code></pre>

<p>HINGE_LOSS is also another option.</p>
"
4156402;233;Mike Miller;<matlab><image-processing><machine-learning><computer-vision><classification>;28068485;0;Classification from a feature vector;"<p>I'm quite new to this; I'm try to classify textures as defective or non-defective. I've used a Gabor filter bank with Matlab which outputs a column vector of the Gabor features of an image. I have a data set of non-defective images and defective images. </p>

<p>My question is, what can I now do with this (or these) feature vectors to classify the texture? I've read about many types of classification, but couldn't find any similar types of implementation to help me get an idea of what I'm doing. Many thanks. </p>
";28070101;3584765;4322;Eypros;1;28068723;"<p>There are many ways to go if you have extracted your feature vectors.</p>

<ul>
<li><p>For example you can use an svm approach on your samples from your two classes.</p></li>
<li><p>Simpler approaches include nearest neighbor, nearest centroid etc</p></li>
</ul>

<p>Edit:</p>

<p>I thought this would be a comment but it's getting too big to fit.</p>

<p>As regards the separability of your samples:</p>

<ul>
<li>One way to determine the linear separability is to use linear svm as a boundary (unless you are concerned about time efiiciency so you are stuck with linear anyway). This svm model does not overtrain and can give a clue about separability.</li>
<li>Other options include a pca that will project your samples to fewer dimensions and these reduced dimensional samples can be easily plotted to examine it visually. This approach has the advantage of visual examination but it depends from the pca step how well it represent the separability of your samples. Maybe the separability lie in a non-principal component (i.e. dimension of your samples) and then pca just fails.</li>
<li>As a rough approximation I often plot random dimensions of my samples together to get a quick (and maybe inaccurate of course) look of them. If for example you have samples of 100 dimension you can plot the first two dimensions only (as if you had 2-D samples) to see if your two classes collide in a large degree. If they do, then you can check other dimensions but if they don't then you know that they are separable at least to some dimensions.</li>
</ul>
"
4156402;233;Mike Miller;<matlab><image-processing><machine-learning><computer-vision><classification>;28068485;0;Classification from a feature vector;"<p>I'm quite new to this; I'm try to classify textures as defective or non-defective. I've used a Gabor filter bank with Matlab which outputs a column vector of the Gabor features of an image. I have a data set of non-defective images and defective images. </p>

<p>My question is, what can I now do with this (or these) feature vectors to classify the texture? I've read about many types of classification, but couldn't find any similar types of implementation to help me get an idea of what I'm doing. Many thanks. </p>
";28070101;4436318;537;articuno;1;28070101;"<p>You can either use Support Vector Machine(SVM) or Neural Networks. SVM is widely used and gives great results. An example of how you can use it in Matlab.</p>

<ol>
<li>First of all, you need to divide your data into 'Training' and 'Testing' set.</li>
<li>'Training' set is the one about which you know i.e. in your case you know which textures are defective and which are non-defective.</li>
<li>'Testing' set is the one on which you want to test your method of classification.</li>
</ol>

<p>Lets say <code>training</code> matrix contains the Gabor features of all training set images where each row corresponds to feature vector of an image (transposed column vector). Lets assume that first 25 are non-defective and next 25 are defective. Now, you need to create a <code>group</code> matrix which tells SVM which are defective and which are not. So,</p>

<pre><code>group = [ones(25,1); -1*ones(25,1)]; // non-defective = 1, defective = -1    
SVMStruct = svmtrain(training, group);
</code></pre>

<p><code>SVMStruct</code> is the support vector which you will use for classifying 'Testing' data. Lets say <code>testing</code> matrix contains Gabor features as previous.</p>

<pre><code>results = svmclassify(SVMStruct, testing);
</code></pre>

<p><code>results</code> is the final decision matrix which contains 1 or -1 depending upon the decision made.</p>
"
3096792;75;Akash;<java><machine-learning><nlp><tokenize><opennlp>;28073147;1;finding token probabilies in a text in nlp;"<p>I came across this class TokenizerME in opennlp documentation page(<a href=""http://opennlp.apache.org/documentation/manual/opennlp.html"" rel=""nofollow"">http://opennlp.apache.org/documentation/manual/opennlp.html</a>). I am not getting how is it calculating the probabilies. I tested it with different inputs, still not understanding. Can someone help me understand the algorithm behind it?
I wrote this sample code</p>

<pre><code>public void tokenizerDemo(){
    try {
        InputStream modelIn = new FileInputStream(""en-token.bin"");
        TokenizerModel model = new TokenizerModel(modelIn);
        Tokenizer tokenizer = new TokenizerME(model);
        String tokens[] = tokenizer.tokenize(""This is is book"");
        for(String t:tokens){
            System.out.println(""Token : ""+t);
        }
        double tokenProbs[] = ((TokenizerME) tokenizer).getTokenProbabilities();
        for(double tP : tokenProbs){
            System.out.println(""Token Prob : ""+tP);
        }
    }
    catch (IOException e) {
      e.printStackTrace();
    }
}
</code></pre>

<p>I got this output</p>

<p>Token : This</p>

<p>Token : is</p>

<p>Token : is</p>

<p>Token : book</p>

<p>Token Prob : 1.0</p>

<p>Token Prob : 1.0</p>

<p>Token Prob : 1.0</p>

<p>Token Prob : 1.0</p>

<p>I want the token ""is"" to be counted twice and its probability should have been slightly higher than other tokens. Confused.</p>
";28087872;461847;6117;aab;1;28087872;"<p>The tokenizer probabilities relate to the tokenizer's confidence in identifying the token spans themselves: whether this string of characters in this context is a token or not according to the tokenizer model.  ""This"" at the beginning of a string with a following space is a very probable token for English, while ""Thi"" with a following ""s"" would not be.</p>

<p>The probabilities do not relate to how often a particular token content has been seen, just whether this sequence of characters is a probable token.  The string ""is is is is is is is"" is easy to tokenize for English because ""is"" is a common word and spaces are good token boundaries.  That's it.</p>

<p>If you are interested in calculating n-gram probabilities, you should look at language models instead.  (You'll still need to tokenize your text first, obviously.)</p>
"
3013195;121;eternalmothra;<python><machine-learning><scikit-learn><decision-tree><pruning>;28073351;0;Hacking/cloning sklearn to support pruning Decision Trees?;"<p>I wanted to create a decision tree and then prune it in python. However, sklearn does not support pruning by itself.
With an internet search, I found this:
<a href=""https://github.com/sgenoud/scikit-learn/blob/4a75a4aaebd45e864e28cfca897121d1199e41d9/sklearn/tree/tree.py"" rel=""nofollow"">https://github.com/sgenoud/scikit-learn/blob/4a75a4aaebd45e864e28cfca897121d1199e41d9/sklearn/tree/tree.py</a></p>

<p>But I don't know how to use the file. I tried:</p>

<pre><code>from sklearn.datasets import load_iris
import tree

clf = tree.DecisionTreeClassifier()
iris = load_iris()

clf = clf.fit(iris.data, iris.target)
</code></pre>

<p>But I get the error ValueError: Attempted relative import in non-package.
Is that not how I import? Do I need to save the files in a different way? Thank you.</p>
";;4385912;3207;cleros;0;28119965;"<p>In Python, Modules (=Packages in other languages) oftentimes define routines that are interdependent. In these cases, you cannot only download one .py file and put it into your Workspace (i.e. the directory where your sources are located). Instead, download the entire package into that folder, and import relatively, i.e. like this:</p>

<pre><code># a general import, should only be used if you are absolutely certain that there will be no namespace conflicts
from sklearn.tree.tree import * 
# a more ""safe"" way is to import the classes/functions you need explicitely
from sklearn.tree.tree import DecisionTreeClassifier
</code></pre>
"
3013195;121;eternalmothra;<python><machine-learning><scikit-learn><decision-tree><pruning>;28073351;0;Hacking/cloning sklearn to support pruning Decision Trees?;"<p>I wanted to create a decision tree and then prune it in python. However, sklearn does not support pruning by itself.
With an internet search, I found this:
<a href=""https://github.com/sgenoud/scikit-learn/blob/4a75a4aaebd45e864e28cfca897121d1199e41d9/sklearn/tree/tree.py"" rel=""nofollow"">https://github.com/sgenoud/scikit-learn/blob/4a75a4aaebd45e864e28cfca897121d1199e41d9/sklearn/tree/tree.py</a></p>

<p>But I don't know how to use the file. I tried:</p>

<pre><code>from sklearn.datasets import load_iris
import tree

clf = tree.DecisionTreeClassifier()
iris = load_iris()

clf = clf.fit(iris.data, iris.target)
</code></pre>

<p>But I get the error ValueError: Attempted relative import in non-package.
Is that not how I import? Do I need to save the files in a different way? Thank you.</p>
";;202229;25726;smci;-1;58244858;"<p>If you really want to use <a href=""https://github.com/sgenoud/scikit-learn/tree/4a75a4aaebd45e864e28cfca897121d1199e41d9"" rel=""nofollow noreferrer"">sgenoud's 7-year-old fork</a> of <a href=""https://github.com/scikit-learn/scikit-learn"" rel=""nofollow noreferrer"">scikit-learn</a> from back in 2012, <code>git clone</code> on the base directory of the repo, don't just try to copy/clone individual files (of course you'll be losing any improvements/fixes since 2012; way back on <a href=""https://scikit-learn.org/stable/whats_new/older_versions.html#version-0-11"" rel=""nofollow noreferrer"">v 0.12</a>)</p>

<p>But that idea sounds misconceived: <strong>you can get shallower/pruned trees by changing parameters to get early stopping <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier"" rel=""nofollow noreferrer""><code>DecisionTreeClassifier</code> parameters max_depth, min_samples, min_samples_leaf, min_impurity_decrease, min_impurity_split</a></strong>. See the doc and play around with the parameters, they do what you're asking for. I've done ML for >10 years and never once seen a need to hack the DT source. There are tons of good reasons not to do this and no good reasons to.</p>

<p>(And if you try to play with the DecisionTreeClassifier parameters and still can't get what you want, post a reproducible code example here using an open-source dataset like iris etc.)</p>
"
3013195;121;eternalmothra;<python><machine-learning><scikit-learn><decision-tree><pruning>;28073351;0;Hacking/cloning sklearn to support pruning Decision Trees?;"<p>I wanted to create a decision tree and then prune it in python. However, sklearn does not support pruning by itself.
With an internet search, I found this:
<a href=""https://github.com/sgenoud/scikit-learn/blob/4a75a4aaebd45e864e28cfca897121d1199e41d9/sklearn/tree/tree.py"" rel=""nofollow"">https://github.com/sgenoud/scikit-learn/blob/4a75a4aaebd45e864e28cfca897121d1199e41d9/sklearn/tree/tree.py</a></p>

<p>But I don't know how to use the file. I tried:</p>

<pre><code>from sklearn.datasets import load_iris
import tree

clf = tree.DecisionTreeClassifier()
iris = load_iris()

clf = clf.fit(iris.data, iris.target)
</code></pre>

<p>But I get the error ValueError: Attempted relative import in non-package.
Is that not how I import? Do I need to save the files in a different way? Thank you.</p>
";;10817844;581;yzerman;1;59597982;"<p>Scikit-learn version 0.22 introduced pruning in DecisionTreeClassifier. A new hyperparameter called <code>ccp_alpha</code> lets you calibrate the amount of pruning. See the documentation <a href=""https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#"" rel=""nofollow noreferrer"">here</a>. </p>
"
4479897;161;JacquesH;<apache-spark><machine-learning><scikit-learn><linear-regression>;28076232;11;Is Apache Spark less accurate than Scikit Learn?;"<p>I've recently been trying to get to know Apache Spark as a replacement for Scikit Learn, however it seems to me that even in simple cases, Scikit converges to an accurate model far faster than Spark does.
For example I generated 1000 data points for a very simple linear function (z=x+y) with the following script:</p>

<pre><code>from random import random

def func(in_vals):
    '''result = x (+y+z+w....)'''
    result = 0
    for v in in_vals:
        result += v
    return result

if __name__ == ""__main__"":
    entry_count = 1000
    dim_count = 2
    in_vals = [0]*dim_count
    with open(""data_yequalsx.csv"", ""w"") as out_file:
        for entry in range(entry_count):
            for i in range(dim_count):
                in_vals[i] = random()
            out_val = func(in_vals)
            out_file.write(','.join([str(x) for x in in_vals]))
            out_file.write("",%s\n"" % str(out_val))
</code></pre>

<p>I then ran the following Scikit script:</p>

<pre><code>import sklearn
from sklearn import linear_model

import numpy as np

data = []
target = []
with open(""data_yequalsx.csv"") as inFile:
    for row in inFile:
        vals = row.split("","")
        data.append([float(x) for x in vals[:-1]])
        target.append(float(vals[-1]))

test_samples= len(data)/10

train_data = [0]*(len(data) - test_samples)
train_target = [0]*(len(data) - test_samples)
test_data = [0]*(test_samples)
test_target = [0]*(test_samples)
train_index = 0
test_index = 0
for j in range(len(data)):
    if j &gt;= test_samples:
        train_data[train_index] = data[j]
        train_target[train_index] = target[j]
        train_index += 1
    else:
        test_data[test_index] = data[j]
        test_target[test_index] = target[j]
        test_index += 1

model = linear_model.SGDRegressor(n_iter=100, learning_rate=""invscaling"", eta0=0.0001, power_t=0.5, penalty=""l2"", alpha=0.0001, loss=""squared_loss"")
model.fit(train_data, train_target)
print(model.coef_)
print(model.intercept_)

result = model.predict(test_data)
mse = np.mean((result - test_target) ** 2)
print(""Mean Squared Error = %s"" % str(mse))
</code></pre>

<p>And then this Spark script: (with spark-submit , no other arguments)</p>

<pre><code>from pyspark.mllib.regression import LinearRegressionWithSGD, LabeledPoint
from pyspark import SparkContext

sc = SparkContext (appName=""mllib_simple_accuracy"")

raw_data = sc.textFile (""data_yequalsx.csv"", minPartitions=10) #MinPartitions doesnt guarantee that you get that many partitions, just that you wont have fewer than that many partitions
data = raw_data.map(lambda line: [float(x) for x in line.split ("","")]).map(lambda entry: LabeledPoint (entry[-1], entry[:-1])).zipWithIndex()
test_samples= data.count()/10

training_data = data.filter(lambda (entry, index): index &gt;= test_samples).map(lambda (lp,index): lp)
test_data = data.filter(lambda (entry, index): index &lt; test_samples).map(lambda (lp,index): lp)

model = LinearRegressionWithSGD.train(training_data, step=0.01, iterations=100, regType=""l2"", regParam=0.0001, intercept=True)
print(model._coeff)
print(model._intercept)

mse = (test_data.map(lambda lp: (lp.label - model.predict(lp.features))**2 ).reduce(lambda x,y: x+y))/test_samples;
print(""Mean Squared Error: %s"" % str(mse))

sc.stop ()
</code></pre>

<p>Strangely though, the error given by spark is an order of magnitude larger than that given by Scikit (0.185 and 0.045 respectively) despite the two models having a nearly identical setup (as far as I can tell)
I understand that this is using SGD with very few iterations and so the results may differ but I wouldn't have thought that it would be anywhere near such a large difference or such a large error, especially given the exceptionally simple data.</p>

<hr>

<p>Is there something I'm misunderstanding in Spark? Is it not correctly configured? Surely I should be getting a smaller error than that?</p>
";;3403718;2320;StackG;2;29869324;"<p>Because Spark is parallelized, each node needs to be able to work independently of the other nodes when the computation is under-way to avoid [time-]expensive shuffles between the nodes. Consequently, it uses a procedure called Stochastic Gradient Descent to approach a minimum, which follows local gradients downwards.</p>

<p>The 'exact' way to solve a [simple, least-squares] regression problem involves solving a matrix equation. This is probably what Scikit-Learn is doing, so in this case it will be more accurate.</p>

<p>The trade-off is that solving matrix equations generally scales as N^3 for a size-N square matrix, which rapidly becomes unfeasible for large datasets. Spark swaps accuracy for computational power. As with any machine-learning procedure, you should build in LOTS of sanity checks throughout your algorithms to make sure that the results of the previous step make any sense.</p>

<p>Hope this helps!</p>
"
4479897;161;JacquesH;<apache-spark><machine-learning><scikit-learn><linear-regression>;28076232;11;Is Apache Spark less accurate than Scikit Learn?;"<p>I've recently been trying to get to know Apache Spark as a replacement for Scikit Learn, however it seems to me that even in simple cases, Scikit converges to an accurate model far faster than Spark does.
For example I generated 1000 data points for a very simple linear function (z=x+y) with the following script:</p>

<pre><code>from random import random

def func(in_vals):
    '''result = x (+y+z+w....)'''
    result = 0
    for v in in_vals:
        result += v
    return result

if __name__ == ""__main__"":
    entry_count = 1000
    dim_count = 2
    in_vals = [0]*dim_count
    with open(""data_yequalsx.csv"", ""w"") as out_file:
        for entry in range(entry_count):
            for i in range(dim_count):
                in_vals[i] = random()
            out_val = func(in_vals)
            out_file.write(','.join([str(x) for x in in_vals]))
            out_file.write("",%s\n"" % str(out_val))
</code></pre>

<p>I then ran the following Scikit script:</p>

<pre><code>import sklearn
from sklearn import linear_model

import numpy as np

data = []
target = []
with open(""data_yequalsx.csv"") as inFile:
    for row in inFile:
        vals = row.split("","")
        data.append([float(x) for x in vals[:-1]])
        target.append(float(vals[-1]))

test_samples= len(data)/10

train_data = [0]*(len(data) - test_samples)
train_target = [0]*(len(data) - test_samples)
test_data = [0]*(test_samples)
test_target = [0]*(test_samples)
train_index = 0
test_index = 0
for j in range(len(data)):
    if j &gt;= test_samples:
        train_data[train_index] = data[j]
        train_target[train_index] = target[j]
        train_index += 1
    else:
        test_data[test_index] = data[j]
        test_target[test_index] = target[j]
        test_index += 1

model = linear_model.SGDRegressor(n_iter=100, learning_rate=""invscaling"", eta0=0.0001, power_t=0.5, penalty=""l2"", alpha=0.0001, loss=""squared_loss"")
model.fit(train_data, train_target)
print(model.coef_)
print(model.intercept_)

result = model.predict(test_data)
mse = np.mean((result - test_target) ** 2)
print(""Mean Squared Error = %s"" % str(mse))
</code></pre>

<p>And then this Spark script: (with spark-submit , no other arguments)</p>

<pre><code>from pyspark.mllib.regression import LinearRegressionWithSGD, LabeledPoint
from pyspark import SparkContext

sc = SparkContext (appName=""mllib_simple_accuracy"")

raw_data = sc.textFile (""data_yequalsx.csv"", minPartitions=10) #MinPartitions doesnt guarantee that you get that many partitions, just that you wont have fewer than that many partitions
data = raw_data.map(lambda line: [float(x) for x in line.split ("","")]).map(lambda entry: LabeledPoint (entry[-1], entry[:-1])).zipWithIndex()
test_samples= data.count()/10

training_data = data.filter(lambda (entry, index): index &gt;= test_samples).map(lambda (lp,index): lp)
test_data = data.filter(lambda (entry, index): index &lt; test_samples).map(lambda (lp,index): lp)

model = LinearRegressionWithSGD.train(training_data, step=0.01, iterations=100, regType=""l2"", regParam=0.0001, intercept=True)
print(model._coeff)
print(model._intercept)

mse = (test_data.map(lambda lp: (lp.label - model.predict(lp.features))**2 ).reduce(lambda x,y: x+y))/test_samples;
print(""Mean Squared Error: %s"" % str(mse))

sc.stop ()
</code></pre>

<p>Strangely though, the error given by spark is an order of magnitude larger than that given by Scikit (0.185 and 0.045 respectively) despite the two models having a nearly identical setup (as far as I can tell)
I understand that this is using SGD with very few iterations and so the results may differ but I wouldn't have thought that it would be anywhere near such a large difference or such a large error, especially given the exceptionally simple data.</p>

<hr>

<p>Is there something I'm misunderstanding in Spark? Is it not correctly configured? Surely I should be getting a smaller error than that?</p>
";;3569090;2914;Oussama;3;29896953;"<p>SGD, which stands for Stochastic Gradient Descent, is an online convex optimization algorithm, and therefore very difficult to parallelize, since it makes one update per iteration (there are smarter variantes such as SGD with mini-batches, but still not very good for parallel environment. </p>

<p>On the other hand, batch algorithms, such as L-BFGS, hwich I advise you to use with Spark (LogigisticRegressionWithLBFGS), can be easily parallelized, since it makes on iteration per epoch (it need to see all datapoints, calculate the value and gradient of the loss function of each point, then performs the aggregation to calculate the full gradient).</p>

<p>Python is ran in a single machine, therefore SGD performs well.</p>

<p>By the way, if you look in MLlib code, the equivalent of scikit learn's lambda is lambda/size of the dataset (mllib optimizes <code>1/n*sum(l_i(x_i,f(y_i)) + lambda</code> while scikit learn optimizes <code>sum(l_i(x_i,f(y_i)) + lambda</code>  </p>
"
4273266;4775;Elizabeth Susan Joseph;<python><machine-learning><scikit-learn>;28082422;1;Why should we perform a Kfold cross validation on test set??;"<p>I was working on a knearest neighbours problem set. I couldn't understand why are they performing <code>K fold cross validation</code> on test set?? Cant we directly test how well our best parameter K performed on the entire test data? rather than doing a cross validation?</p>

<pre><code>iris = sklearn.datasets.load_iris()

X = iris.data  
Y = iris.target

X_train, X_test, Y_train, Y_test = sklearn.cross_validation.train_test_split(
    X, Y, test_size=0.33, random_state=42)

k = np.arange(20)+1

parameters = {'n_neighbors': k}
knn = sklearn.neighbors.KNeighborsClassifier()
clf = sklearn.grid_search.GridSearchCV(knn, parameters, cv=10)
clf.fit(X_train, Y_train)

def computeTestScores(test_x, test_y, clf, cv):
    kFolds = sklearn.cross_validation.KFold(test_x.shape[0], n_folds=cv)

    scores = []
    for _, test_index in kFolds:
        test_data = test_x[test_index]
        test_labels = test_y[test_index]
        scores.append(sklearn.metrics.accuracy_score(test_labels, clf.predict(test_data)))
    return scores

scores = computeTestScores(test_x = X_test, test_y = Y_test, clf=clf, cv=5)
</code></pre>
";28084276;3687256;71;Chewie;0;28083527;"<p>If you make a program that adapts to input, then it will be optimal for the input you adapted it to.</p>

<p>This leads to a problem known as overfitting.</p>

<p>In order to see if you have made a good or a bad model, you need to test it on some other data that is not what you used to make the model. This is why you separate your data into 2 parts.</p>
"
4273266;4775;Elizabeth Susan Joseph;<python><machine-learning><scikit-learn>;28082422;1;Why should we perform a Kfold cross validation on test set??;"<p>I was working on a knearest neighbours problem set. I couldn't understand why are they performing <code>K fold cross validation</code> on test set?? Cant we directly test how well our best parameter K performed on the entire test data? rather than doing a cross validation?</p>

<pre><code>iris = sklearn.datasets.load_iris()

X = iris.data  
Y = iris.target

X_train, X_test, Y_train, Y_test = sklearn.cross_validation.train_test_split(
    X, Y, test_size=0.33, random_state=42)

k = np.arange(20)+1

parameters = {'n_neighbors': k}
knn = sklearn.neighbors.KNeighborsClassifier()
clf = sklearn.grid_search.GridSearchCV(knn, parameters, cv=10)
clf.fit(X_train, Y_train)

def computeTestScores(test_x, test_y, clf, cv):
    kFolds = sklearn.cross_validation.KFold(test_x.shape[0], n_folds=cv)

    scores = []
    for _, test_index in kFolds:
        test_data = test_x[test_index]
        test_labels = test_y[test_index]
        scores.append(sklearn.metrics.accuracy_score(test_labels, clf.predict(test_data)))
    return scores

scores = computeTestScores(test_x = X_test, test_y = Y_test, clf=clf, cv=5)
</code></pre>
";28084276;3751373;693;Laurence Billingham;3;28084276;"<h3>TL;DR</h3>
<p>Did you ever have a science teacher who said, 'any measurement without error bounds is meaningless?'</p>
<p>You might worry that the score on using your fitted, hyperparameter optimized, estimator on your test set is a fluke. By doing a number of tests on a randomly chosen subsample of the test set you get a range of scores; you can report their mean and standard deviation etc. This is, hopefully, a better proxy for how the estimator will perform on new data from the wild.</p>
<hr />
<p>The following conceptual model may not apply to all estimators but it is a useful to bear in mind. You end up needing 3 subsets of your data. You can skip to the final paragraph if the numbered points are things you are already happy with.</p>
<ol>
<li>Training your estimator will fit some internal parameters that you need not ever see directly. You optimize these by training on the training set.</li>
<li>Most estimators also have hyperparameters (number of neighbours, alpha for Ridge, ...). Hyperparameters also need to be optimized. You need to fit them to a different subset of your data; call it the validation set.</li>
<li>Finally, when you are happy with the fit of both the estimator's internal parameters and the hyperparmeters, you want to see how well the fitted estimator predicts on new data. You need a final subset (the test set) of your data to figure out how well the training and hyperparameter optimization went.</li>
</ol>
<p>In <em>lots</em> of cases the partitioning your data into 3 means you don't have enough samples in each subset. One way around this is to randomly split the training set a number of times, fit hyperparameters and aggregate the results. This also helps stop your hyperparameters being over-fit to a particular validation set. K-fold cross-validation is one strategy.</p>
<p>Another use for this splitting a data set at random is to get a range of results for how your final estimator did. By splitting the test set and computing the score you get a range of answers to 'how might we do on new data'. The hope is that this is more representative of what you might see as real-world novel data performance. You can also get a standard deviation for you final score. This appears to be what the Harvard cs109 gist is doing.</p>
"
3122060;479;Norrec;<algorithm><machine-learning><data-mining><prediction><reinforcement-learning>;28082922;0;Keyword association learning algorithm;"<p>To model my problem, I'll use a dating site as an example (although this is not the actual case). My problem is I have a set of keywords that a user can input that they like. Say ""Tall, dark hair, blue eyes"", etc. and I want to map them to other users that fit that criteria. More than that however, I need to be able to learn from data I get back to make better predictions that are not-so-exact matches. </p>

<p>For example, if other users that are looking for people with 'dark hair' like users with 'black hair', or have a height of 6'4 but don't mention they are tall. I want to be able to make an association between those similar keywords and be able to also suggest those as well so it best returns what the user wants, even if it wasn't exactly what they asked for. </p>

<p>My question is what algorithm/approach is best suited for this? I've been looking into areas like:</p>

<ul>
<li>decision trees, but those seem to break down when no keywords match. </li>
<li>naive bayes, which seem a bit more tolerant to missing connections, but require some prior knowledge about the connections, and since keywords can be anything, this seems like a road bloack</li>
<li>ANN, but these don't seem to do well with text input</li>
<li>KNN, but I'm not sure how to handle the possibly infinite user classifications?</li>
<li>Some sort of A* map search, where each time a user1 likes a user2, I make a map connection between user1's likes and user2's traits, if that connection already exists, I just shorten it, then find the closest N users. I'm just not sure how scalable this is.</li>
</ul>

<p>Any input is appreciated,
Thanks!</p>
";28086400;270287;41194;IVlad;2;28086400;"<p>This sounds like a rather classic application of <a href=""http://en.wikipedia.org/wiki/Association_rule_learning"" rel=""nofollow"">association rule learning</a>: basically, if people looking for partners with 'dark hair' like a lot of 'black hair' accounts, then you have an association rule between the two. There are algorithms that can detect this.</p>

<p>As for your suggestions, have you tried an ANN? ANNs don't work at all with text input, but for most machine learning + text tasks, you turn the text into numeric data (see the bag of words model, for example). Once you have your numeric features, they shouldn't do too badly. </p>

<p>For example, you'd want your network trained to return adequate recommendations based on profile settings, right? You would feed it the profile settings, and if you have training data that shows users looking for people with 'dark hair' liking users with 'black hair', the ANN should learn that relationship.</p>

<p>Association rules sound like the way to go though.</p>
"
3096792;75;Akash;<java><machine-learning><nlp><opennlp><porter-stemmer>;28083377;0;Built-in Porter Stemmer in java opennlp toolkit;"<p>Like there are built in Stemmers like porter Stemmer in python nltk (<a href=""https://stackoverflow.com/a/10369407/3096792"">https://stackoverflow.com/a/10369407/3096792</a> ), I want to know if there is any built-in Porter Stemmer in java apache opennlp though there is an interface under this package [opennlp.tools.stemmer.Stemmer]. Since I dont know or need to know the algorithm behind stemming as of now,I need a standard package under apache opennlp toolkit. If not present, whats the alternative in java ?</p>
";28094077;2798955;7958;Buru;2;28094077;"<p>You can use Apache Lucene Snowball Library <a href=""http://www.java2s.com/Code/JarDownload/lucene-snowball/lucene-snowball-3.0.2.jar.zip"" rel=""nofollow"">here</a></p>

<p>Just put this jar file inside your build path and use it</p>

<pre><code>PorterStemmer stemmer = new PorterStemmer();
stemmer.setCurrent(input); //set string you need to stem
stemmer.stem();  //stem the word
stemmer.getCurrent();//get the stemmed word
</code></pre>
"
3436624;605;user3436624;<machine-learning><nltk>;28083560;3;confidence level with crfsuite predictions;"<p>I am using the CRFSuite package here 
<a href=""http://www.chokkan.org/software/crfsuite/tutorial.html"" rel=""nofollow"">http://www.chokkan.org/software/crfsuite/tutorial.html</a></p>

<p>and I have successfully used it to build a classifier and tag text. However, I'm wondering if I can get a confidence value for each prediction it makes? </p>

<p>It doesn't seem so. What I would really like is to get the probability of a word being each type of tag ('PER', 'LOC', 'MISC', etc), rather than just the prediction itself.</p>
";;3781385;415;prateek05;0;32640915;"<p>The API provides extracting conditional probabilities. I guess you mean the crfsuite binary does not have that as option. You could edit the source and add the option yourself</p>
"
3436624;605;user3436624;<machine-learning><nltk>;28083560;3;confidence level with crfsuite predictions;"<p>I am using the CRFSuite package here 
<a href=""http://www.chokkan.org/software/crfsuite/tutorial.html"" rel=""nofollow"">http://www.chokkan.org/software/crfsuite/tutorial.html</a></p>

<p>and I have successfully used it to build a classifier and tag text. However, I'm wondering if I can get a confidence value for each prediction it makes? </p>

<p>It doesn't seem so. What I would really like is to get the probability of a word being each type of tag ('PER', 'LOC', 'MISC', etc), rather than just the prediction itself.</p>
";;7636396;35;Proton Boss;0;63908149;"<p>I hope this serves as an answer.   Sklearn crfsuite provides probability for each label.</p>
<pre><code>predict_marginals(X)
Make a prediction.

Parameters: X (list of lists of dicts) â€“ feature dicts in python-crfsuite format
Returns:    y â€“ predicted probabilities for each label at each position
Return type:    list of lists of dicts
</code></pre>
<p>Source: <a href=""https://sklearn-crfsuite.readthedocs.io/en/latest/_modules/sklearn_crfsuite/estimator.html#CRF.predict_marginals"" rel=""nofollow noreferrer"">https://sklearn-crfsuite.readthedocs.io/en/latest/_modules/sklearn_crfsuite/estimator.html#CRF.predict_marginals</a></p>
"
3096792;75;Akash;<java><python><entity-framework><machine-learning><nlp>;28085154;0;How to find a name Entity in a text in java;"<p>Following is the code written using opennlp in java to identify name entities</p>

<pre><code>try {
        System.out.println(""Input : Pierre Vinken is 61 years old"");
        InputStream modelIn = new FileInputStream(""en-ner-person.bin"");
        TokenNameFinderModel model = new TokenNameFinderModel(modelIn);
        NameFinderME nameFinder = new NameFinderME(model);
        String[] sentence = new String[]{
                ""Pierre"",
                ""Vinken"",
                ""is"",
                ""61"",
                ""years"",
                ""old"",
                "".""
                };

            Span nameSpans[] = nameFinder.find(sentence);
            for(Span s: nameSpans)
                System.out.println(""Name Entity : ""+s.toString());
    }
    catch (IOException e) {
      e.printStackTrace();
    }
</code></pre>

<p>This gives output :</p>

<p>Input : Pierre Vinken is 61 years old</p>

<p>Name Entity : [0..2) person</p>

<p>But for any other entities like GPE (geo-graphical and political entity),its not identifying</p>

<p>Eg </p>

<p>Input : Taj Mahal is in India</p>

<p>It is neither identifying Taj Mahal nor India. What can be done ?</p>
";28087839;461847;6117;aab;2;28087839;"<p>As suggested in the filename, <code>en-ner-person.bin</code> only identifies persons.  There are other available OpenNLP NER models for other kinds of entities.</p>
"
3787253;2595;smatthewenglish;<machine-learning><prolog><nlp><wordnet>;28088226;1;use WordNet to generalize specific word to higher-order concept;"<p>Does <a href=""http://wordnet.princeton.edu/"" rel=""nofollow"">WordNet</a> have ""higher order"" concepts? How to generate them for a given word?</p>

<p>I have a corpus of data in the form of prolog 'facts'. I want to generalize the conceptual components, i.e. <code>'contains'('oranges', 'vitamin c').</code> and <code>'contains'('spinach','iron').</code> would be generalized to <code>'contains'(&lt;food&gt;, &lt;nutrient&gt;).</code> </p>

<p>I don't know WordNet very well, so one thing I was thinking about was just to generate all possible hypernyms and then combinatorially elaborate every possible new rule, but this is sort of a 'brute force' approach. </p>

<p>Does WordNet store higher order concepts such as &lt;food&gt; for instance? That might make it easier, because then I can just create one new rule with the higher order concept of that particular variable, assuming that there is one in WordNet, as opposed to perhaps fifty or one hundred if I do it the brute force way.</p>

<p>So what I actually want to know is: is there a command to generate the higher order concepts for each of the three components within a given 'fact'? Or maybe just for the two that are inside the parentheses. If such a command exits, what is it? </p>

<p>Below is some of the data I'm working with for reference. </p>

<pre><code>'be'('mr jiang', 'representing china').
'be'('hrh', 'britain').
'be more than'('# distinguished guests', 'the principal representatives').
'end with'('the playing of the british national anthem', 'hong kong').
'follow at'('the stroke of midnight', 'this').
'take part in'('the ceremony', 'both countries').
'start at about'('# pm', 'the ceremony').
'end about'('# am', 'the ceremony').
'lower'('the british hong kong flag', '# royal hong kong police officers').
'raise'('the sar flag', 'another #').
'leave for'('the royal yacht britannia', 'the #').
'hold by'('the chinese and british governments', 'the handover of hong kong').
'rise over'('this land', 'the regional flag of the hong kong special administrative region of the people \'s republic of china').
'cast eye on'('hong kong', 'the world').
'hold on'('schedule', 'the # governments').
'be festival for'('the chinese nation', 'this').
'go in'('the annals of history', 'july # , #').
'become master of'('this chinese land', 'the hong kong compatriots').
'enter era of'('development', 'hong kong').
'remember'('mr deng xiaoping', 'history').
'be along'('the course', 'it').
'resolve'('the hong kong question', 'we').
'wish to express thanks to'('all the personages', 'i').
'contribute to'('the settlement of the hong kong', 'both china and britain').
'support'('hong kong \'s return', 'the world').
</code></pre>
";;1727392;19358;duhaime;1;29578455;"<p>Wordnet refers to higher-order concepts as ""hypernyms"". A hypernym for the color ""green,"" for instance, is ""chromatic color"", because the color green belongs to the higher-order class of chromatic colors. </p>

<p>One should note that Wordnet differentiates between ""words"" (strings of characters) and ""sysnets"" (the meaning we associate with a given string of characters). Just as one word can have multiple meanings, one string can have multiple synsets. If you want to retrieve all of the higher-order meanings for a given word, you can run these lines in Python:</p>

<pre><code>from nltk import wordnet as wn

# If you are using nltk version 3.0.1, the following will tell you all the synsets for ""green"" and will thenn find all of their hypernyms. If you're running nltk 3.0.0, you can change the first line to `for synset in wn.synsets('bank'):
for synset in wn.wordnet.synsets('green'):
    for hypernym in synset.hypernyms():
        print synset, hypernym
</code></pre>
"
2571955;63;Stefan Olaru;<matlab><machine-learning>;28091399;0;Nearest Prototype Matlab implementation using a database;"<p>I am new to matlab an what i want to do is this:  I have a database <a href=""http://archive.ics.uci.edu/ml/machine-learning-databases/wine/"" rel=""nofollow"">http://archive.ics.uci.edu/ml/machine-learning-databases/wine/</a> and i want to be able to implement the NP, Nearest Prototype algorithm in matlab. Is there anyone that can give me a peace of advice or some link to a NP example usage? </p>
";28094425;4480030;130;Yang Zhang;1;28094425;"<p>Here is a sample code, I assume this dataset's first column is the label the rest are its features</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>data = load('wine.data');

% split the dataset to training and testing
data = data(randperm(end), :);
train = data(1:floor(0.7*size(data, 1)), :);
test = data(floor(0.7*size(data, 1))+1:end, :);



% training phase
% --------------------------------------------------------
% initialize the centroid, the first column is the label

centroid = [unique(data(:, 1)) zeros(size(unique(data(:, 1)), 1), size(data, 2)-1)];


for label = unique(train(:, 1))'
    % collect all the data of under the label
    train(train(:, 1) == label, 2:end)
    % compute the centroid for the label
    centroid(centroid(:, 1) == label, 2:end) = mean(train(train(:, 1) == label, 2:end));

end

% testing phase
% --------------------------------------------------------
% initialize the prediction result
pre_result = zeros(size(test, 1), 1);
for i = 1:size(test, 1)
    dist = pdist2(test(i, 2:end), centroid(:, 2:end));
    [~, templabel] = min(dist);
    pre_result(i) = centroid(templabel, 1);

end</code></pre>
</div>
</div>
</p>
"
1182556;351;user1182556;<python><machine-learning><scikit-learn><svm>;28093516;6;Difference between .score() and .predict in the sklearn library?;"<p>I have instantiated a SVC object using the sklearn library with the following code:</p>

<p><code>clf = svm.SVC(kernel='linear', C=1, cache_size=1000, max_iter = -1, verbose = True)</code></p>

<p>I then fit data to it using:</p>

<p><code>model = clf.fit(X_train, y_train)</code></p>

<p>Where X_train is a (301,60) and y_train is (301,) ndarray (y_train consisting of class labels ""1"", ""2"" and ""3"").</p>

<p>Now, before I stumbled across the .score() method, to determine the accuracy of my model on the training set i was using the following:</p>

<p><code>prediction = np.divide((y_train == model.predict(X_train)).sum(), y_train.size, dtype = float)</code></p>

<p>which gives a result of approximately 62%.</p>

<p>However, when using the model.score(X_train, y_train) method I get a result of approximately 83%.</p>

<p>Therefore, I was wondering if anyone could explain to me why this should be the case because as far as I understand, they should return the same result?  </p>

<p>ADDENDUM:</p>

<p>The first 10 values of y_true are:</p>

<ul>
<li>2, 3, 1, 3, 2, 3, 2, 2, 3, 1, ...</li>
</ul>

<p>Whereas for y_pred (when using model.predict(X_train)), they are:</p>

<ul>
<li>2, 3, 3, 2, 2, 3, 2, 3, 3, 3, ...</li>
</ul>
";28102506;676634;23217;Andreas Mueller;5;28102506;"<p>Because your <code>y_train</code> is <code>(301, 1)</code> and not <code>(301,)</code> numpy does broadcasting, so</p>

<pre><code>(y_train == model.predict(X_train)).shape == (301, 301)
</code></pre>

<p>which is not what you intended.
The correct version of your code would be</p>

<pre><code>np.mean(y_train.ravel() == model.predict(X_train))
</code></pre>

<p>which will give the same result as</p>

<pre><code>model.score(X_train, y_train)
</code></pre>
"
1115169;2518;pbu;<machine-learning><deep-learning><caffe>;28093753;0;Convolution issue in Caffe;"<p>i have 96x96 pixel images in grayscale format stored in HDF5 files. i am trying to do multi output regression using caffe however convolution is not working. What exactly is the problem here? Why is convolutions not working?</p>

<pre><code>I0122 17:18:39.474860  5074 net.cpp:67] Creating Layer fkp
I0122 17:18:39.474889  5074 net.cpp:356] fkp -&gt; data
I0122 17:18:39.474930  5074 net.cpp:356] fkp -&gt; label
I0122 17:18:39.474967  5074 net.cpp:96] Setting up fkp
I0122 17:18:39.474987  5074 hdf5_data_layer.cpp:57] Loading filename from train.txt
I0122 17:18:39.475103  5074 hdf5_data_layer.cpp:69] Number of files: 1
I0122 17:18:39.475131  5074 hdf5_data_layer.cpp:29] Loading HDF5 filefacialkp-train.hd5
I0122 17:18:40.337786  5074 hdf5_data_layer.cpp:49] Successully loaded 4934 rows
I0122 17:18:40.337862  5074 hdf5_data_layer.cpp:81] output data size: 100,9216,1,1
I0122 17:18:40.337906  5074 net.cpp:103] Top shape: 100 9216 1 1 (921600)
I0122 17:18:40.337929  5074 net.cpp:103] Top shape: 100 30 1 1 (3000)
I0122 17:18:40.337971  5074 net.cpp:67] Creating Layer conv1
I0122 17:18:40.338001  5074 net.cpp:394] conv1 &lt;- data
I0122 17:18:40.338069  5074 net.cpp:356] conv1 -&gt; conv1
I0122 17:18:40.338109  5074 net.cpp:96] Setting up conv1
F0122 17:18:40.599761  5074 blob.cpp:13] Check failed: height &gt;= 0 (-3 vs. 0) 
</code></pre>

<p>My prototxt layer file is like this</p>

<pre><code>name: ""LogReg""
layers {
  top: ""data""
  top: ""label""
  name: ""fkp""
  type: HDF5_DATA
  hdf5_data_param {
    source: ""train.txt""
    batch_size: 100
  }
  include {
    phase: TRAIN
  }
}
layers {
  bottom: ""data""
  top: ""conv1""
  name: ""conv1""
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    kernel_size: 5
    stride: 1
    weight_filler {
      type: ""xavier""
    }
    bias_filler {
      type: ""constant""
    }
  }
}
layers {
  bottom: ""conv1""
  top: ""pool1""
  name: ""pool1""
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: ""pool1""
  top: ""conv2""
  name: ""conv2""
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 256
    kernel_size: 5
    stride: 1
    weight_filler {
      type: ""xavier""
    }
    bias_filler {
      type: ""constant""
    }
  }
}
layers {
  bottom: ""conv2""
  top: ""pool2""
  name: ""pool2""
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: ""pool2""
  top: ""ip1""
  name: ""ip1""
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 500
    weight_filler {
      type: ""xavier""
    }
    bias_filler {
      type: ""constant""
    }
  }
}
layers {
  bottom: ""ip1""
  top: ""ip1""
  name: ""relu1""
  type: RELU
}
layers {
  bottom: ""ip1""
  top: ""ip2""
  name: ""ip2""
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 30
    weight_filler {
      type: ""xavier""
    }
    bias_filler {
      type: ""constant""
    }
  }
}
layers {
  bottom: ""ip2""
  bottom: ""label""
  top: ""loss""
  name: ""loss""
  type: EUCLIDEAN_LOSS
}
</code></pre>
";28544226;4572256;46;amrhein;3;28544226;"<p>the lines</p>

<pre>I0122 17:18:40.337906  5074 net.cpp:103] Top shape: 100 9216 1 1 (921600)
I0122 17:18:40.337929  5074 net.cpp:103] Top shape: 100 30 1 1 (3000)</pre>

<p>suggest that your input data is not in the correct shape. For an input of 100 batchs of 96x96 grey-scale image the shape should be: 100 1 96 96.
Try to change this. (my guess is that for shape: N C H W, where N number of batches, c channels, h height, w weight)</p>
"
1655585;876;Bhanu Kaushik;<python><machine-learning><scikit-learn><classification><pmml>;28097247;2;moving classification to production environment;"<p>I am designing the architecture of an analytics system . I have a classification ensemble model developed in scikit learn . I want to move this to the production environment so the new incoming data can be classified on the fly using this model. Ideally the system should support a manual upload of ""model"" into production system. I don't have any experience with analytics production systems.  Any suggestions would be very helpful</p>

<p>I have checked out Py2PMML But it does not support all the models. 
Primarily I am looking for Boosted tree regressions.
PS: I am not asking for code or samples. Just the right direction.</p>
";28101032;1330293;33731;elyase;2;28101032;"<p>At the moment there isn't an <a href=""https://github.com/scikit-learn/scikit-learn/issues/1596"" rel=""nofollow"">official way</a> to export scikit models to PMML. The recommended way is to use pickle or <code>joblib.dump</code>. Please refer to the <a href=""http://scikit-learn.org/stable/modules/model_persistence.html"" rel=""nofollow"">model persistence</a> section of the docs. The idea is to save the model to disk with:</p>

<pre><code>&gt;&gt;&gt; from sklearn.externals import joblib
&gt;&gt;&gt; joblib.dump(model, 'saved_model.pkl') 
</code></pre>

<p>Then upload it to your server in production and load it with:</p>

<pre><code>&gt;&gt;&gt; model = joblib.load('saved_model.pkl')
</code></pre>

<p>It is important that you try to have similar environments, models saved in one version of scikit-learn might not load in another version.</p>
"
1084353;4680;Popcorn;<machine-learning><training-data>;28097371;0;Is it unsupervised learning if I don't have labeled data to train my models on;"<p>I would like to collect user information to determine whether they are male or female. I have zero labeled data for my users, but I know some features that can easily predict their gender. An example would be texts created by the users that contain words strongly associated with one gender (ex: Male: beer, football game, boxers. Female: facial, makeup, bra).</p>

<p>Would this be considered unsupervised learning, since <a href=""https://stackoverflow.com/a/1854449/545127"">I don't have labelled data to train my models on</a>? </p>
";;496223;43815;dynamic;1;28097447;"<p>This is neither supervised nor unsupervised. You are just applying some predefined rules to classify between male/fame.</p>

<p>This is also not machine learning, because you don't use any learning method...</p>
"
1084353;4680;Popcorn;<machine-learning><training-data>;28097371;0;Is it unsupervised learning if I don't have labeled data to train my models on;"<p>I would like to collect user information to determine whether they are male or female. I have zero labeled data for my users, but I know some features that can easily predict their gender. An example would be texts created by the users that contain words strongly associated with one gender (ex: Male: beer, football game, boxers. Female: facial, makeup, bra).</p>

<p>Would this be considered unsupervised learning, since <a href=""https://stackoverflow.com/a/1854449/545127"">I don't have labelled data to train my models on</a>? </p>
";;9492900;252;DoubleDouble;0;49436616;"<p>A supervised learning method would use all of the text used by the users and allow the machine to determine which words are important and by how much by trying to guess the user's gender and then correcting itself with the label.</p>
<p>An unsupervised method would be to provide the machine with all the text by the users and allow it to try and create different pattern groups out of it. However, there are many more ways to group users than 'male' and 'female' so this is not exactly an ideal unsupervised problem.</p>
<p>Telling the system which words are important and separating the system into groups based on that would just be a regular <strong>program</strong> and can be accomplished by any programming language that can match text and provide an output.</p>
<blockquote>
<p>proÂ·gram</p>
<p>noun
1.
a planned series of future events, items, or performances.</p>
</blockquote>
"
4122928;2622;giliev;<machine-learning><recommendation-engine>;28098989;5;Implementing recommendation system for unsupervised learning;"<p>I have been looking at papers and books about recommendation systems and the approaches suggested to build them. In many of them the Netflix competition was given as an example. On Netflix users rate movies (from 1 to 5). In that competition, the competitors were given a database of movies and corresponding ratings by the users, and they were supposed to implement a system which would best predict the rating of the movies and using that rating would suggest movies to the users.</p>

<p>For evaluation they suggest cross validation using measures which use the predicted and real ratings as arguments. Predicted rating is calculated using the history of the user and his ratings for the movies.</p>

<p>I am trying to build a news recommendation system. The problem I am facing now is that the news are relevant just for a short time and almost nobody would give a rating to the news. So, I only have implicit feedback (views) and no explicit feedback (rating). Also in the Netflix problem they are provided with a database. I am wondering how to cope with the cold start problem, because at the start no news would be read (viewed).</p>

<p>I will be so thankful if You could suggest me how to avoid the cold start problem and once I will have an algorithm how could I test if it works fine.</p>

<p>Thank you!</p>
";;2929041;457;macmania314;1;29718632;"<p>To get started with this project you're undertaking, I would suggest clustering for finding the pattern on news that are relevant/popular items. The more features that you incorporate in such a way that it adds value to your results (this part needs careful selection, study and statistical analysis). </p>

<p>For news recommendation - you can have layered approach, so let's say first part would be scan articles that are 'positive'/contain certain keywords from people that commented on that article. </p>

<p>Then perhaps the second layered approach would be to cross reference twitter's response to that article, to facebook's like/traffic, to how many pinterest user's pin that article, etc... </p>

<p>You might also check trending keywords from google, bing, etc... on particular topics so that's how to ensure that the article you are showing is 'relevant'</p>

<p>I also suggest starting small cause there are so many articles in the web - maybe look into focusing on one topic and then generalize it. As you may notice, an 'articles' popularity is kinda linked to certain voices that people follow so that's another way of finding the relevance of that article.</p>

<p>Here's more info on unsupervised learning: 
<a href=""http://en.wikipedia.org/wiki/Unsupervised_learning"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/Unsupervised_learning</a></p>

<p>You might also want to look into Expectation Maximization to find which variables would improve the unobserved data you've obtained. Here's a full explanation of EM
<a href=""https://stats.stackexchange.com/questions/72774/numerical-example-to-understand-expectation-maximization"">https://stats.stackexchange.com/questions/72774/numerical-example-to-understand-expectation-maximization</a> </p>
"
4122928;2622;giliev;<machine-learning><recommendation-engine>;28098989;5;Implementing recommendation system for unsupervised learning;"<p>I have been looking at papers and books about recommendation systems and the approaches suggested to build them. In many of them the Netflix competition was given as an example. On Netflix users rate movies (from 1 to 5). In that competition, the competitors were given a database of movies and corresponding ratings by the users, and they were supposed to implement a system which would best predict the rating of the movies and using that rating would suggest movies to the users.</p>

<p>For evaluation they suggest cross validation using measures which use the predicted and real ratings as arguments. Predicted rating is calculated using the history of the user and his ratings for the movies.</p>

<p>I am trying to build a news recommendation system. The problem I am facing now is that the news are relevant just for a short time and almost nobody would give a rating to the news. So, I only have implicit feedback (views) and no explicit feedback (rating). Also in the Netflix problem they are provided with a database. I am wondering how to cope with the cold start problem, because at the start no news would be read (viewed).</p>

<p>I will be so thankful if You could suggest me how to avoid the cold start problem and once I will have an algorithm how could I test if it works fine.</p>

<p>Thank you!</p>
";;5021307;310;Dan Jarratt;2;31789410;"<p>Movies are an excellent use case for classic collaborative filtering: they're items people are interested in for a long time, there are relatively few of them, many people have overlapping interests, and star ratings make sense. News stories are completely different. Rather than collaborative filtering, look at content-based filtering. That's where people's interests align with content identifiers (which could be keywords about the news story, or the publisher, or metadata about time of day or region of the world). View counts are your best bet for information about people's preferences, and they also allow you to use some data mining techniques like association rule mining.</p>

<p>While you'll still have the user cold start problem -- where a new user in your system has given you no information about her preferences, unless you bootstrap it from mining her tweets or Facebook interests or something of the sort -- you can avoid the item cold start problem. Instead of relying on news stories read through your community as the only way to get item similarities, you can use another corpus. In particular, try Wikipedia, and check out WikiBrain (<a href=""https://github.com/shilad/wikibrain"" rel=""nofollow"">https://github.com/shilad/wikibrain</a>). That's an API through which you can get the similarity of one concept to another, and apply it to your recommendation needs.</p>
"
2805564;675;ukejoe;<python><numpy><machine-learning><scipy><scikit-learn>;28099401;1;Generating numpy arrays for scikit linear regression model;"<p>I have a large dataset with multiple variables: item, location, quality (scale of 1-10), and a range of dates containing ""no"" if the item did not sell that day and the price if it did sell that day.</p>

<p>I want to create a linear regression model to be able to predict the price given a location and quality. I read through the scikit-learn tutorials, but I'm really confused as to what my input should be for the fit. Can someone help me out?</p>
";28100928;1330293;33731;elyase;3;28100928;"<p>You need to convert your data to a numeric representation that models can work with. The only problematic feature is the location (categorical variable), but we can represent it with one column for each location, and 0s and 1s (so called OneHotEncoding). An example to get you started:</p>

<h3>Preprocessing</h3>

<pre><code>from sklearn.feature_extraction import DictVectorizer

data  = [
        {'location': 'store 1', 'quality': 8},
        {'location': 'store 1', 'quality': 9},
        {'location': 'store 2', 'quality': 2},
        {'location': 'store 2', 'quality': 3},
        ]
prices = [100.00, 99.9, 11.25, 9.99]
vec = DictVectorizer()
X = vec.fit_transform(data)
y = prices
</code></pre>

<p>Now <code>X</code> will look like this:</p>

<pre><code>â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•—
â•‘ location=store1 â•‘ location=store2 â•‘ quality â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•£
â•‘               1 â•‘               0 â•‘       8 â•‘
â•‘               1 â•‘               0 â•‘       9 â•‘
â•‘               0 â•‘               1 â•‘       2 â•‘
â•‘               0 â•‘               1 â•‘       3 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•
</code></pre>

<h3>Model training</h3>

<p>This matrix can now be feed to a model:</p>

<pre><code>from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X, y)
</code></pre>

<h3>Prediction</h3>

<p>The new data will also need to be converted into numeric form using the same <code>DictVectorizer</code>. Note that now we use <code>.transform</code> instead of <code>.fit_transform</code>:</p>

<pre><code>&gt;&gt;&gt; test_data = [{'location': 'store 2', 'quality': 3}]
&gt;&gt;&gt; X_test = vec.transform(test_data)
&gt;&gt;&gt; model.predict(X_test)
array([ 10.28])
</code></pre>

<p>By the way, I would approach this problem as a classification problem(sold/not sold) and then I would use regression to determine the price only on the sold items.</p>
"
1288527;71;Rafael Cardoso;<c#><algorithm><machine-learning><id3>;28101810;0;Modify ID3 Algorithm C# to use 4 Classes instead original 2 (error c# Object reference not set to an instanc);"<p>I am trying to use modifying the original code which uses 2 classes (true,false) to use 4 classes (unacc, acc, good, vgood). The original code assume the ""boolean"", so to workaround it I need to compare string to string. </p>

<p>For some reason, it is returning the following error object reference not set to an instance of an object. I have google it and this happens when I don't have something initiated which is not the case.</p>

<p>Error:</p>

<pre><code>Unhandled Exception: System.NullReferenceException: Object reference not set to an instance of an object.
   at AA.TreeNode..ctor(Attribute attribute) in c:\VS\Program.cs:line 117
   at AA.DecisionTreeID3.internalMountTree(DataTable samples, String targetAttribute, Attribute[] attributes) in c:\VS\Program.cs:line 574
   at AA.DecisionTreeID3.mountTree(DataTable samples, String targetAttribute, Attribute[] attributes) in c:\VS\Program.cs:line 626
   at AA.DecisionTreeID3.internalMountTree(DataTable samples, String targetAttribute, Attribute[] attributes) in c:\VS\Program.cs:line 607
   at AA.DecisionTreeID3.mountTree(DataTable samples, String targetAttribute, Attribute[] attributes) in c:\VS\Program.cs:line 626
   at AA.DecisionTreeID3.internalMountTree(DataTable samples, String targetAttribute, Attribute[] attributes) in c:\VS\Program.cs:line 607
   at AA.DecisionTreeID3.mountTree(DataTable samples, String targetAttribute, Attribute[] attributes) in c:\VS\Program.cs:line 626
   at AA.ID3Sample.Main(String[] args) in c:\VS\Program.cs:line 718
</code></pre>

<p>The original code/solution is available <a href=""http://www.codeproject.com/Articles/5276/ID-Decision-Tree-Algorithm-in-C"" rel=""nofollow"">here</a></p>

<p>I leave my code below. 
Could be my error for comparing two strings? 
It's possible to create a function as string and return values in the end with strings right? If not, I could change the function to ""int"" and return it as values 0,1,2 and 3.</p>

<p>Could anyone help me or share ideas how I could solve this?</p>

<pre><code>using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Collections;
using System.Data;

namespace AA
{
    /// &lt;summary&gt;
    /// Classe que representa um atributo utilizado na classe de decisÃ£o
    /// &lt;/summary&gt;
    public class Attribute
    {
        ArrayList mValues;
        string mName;
        object mLabel;

        /// &lt;summary&gt;
        /// Inicializa uma nova instÃ¢ncia de uma classe Atribute
        /// &lt;/summary&gt;
        /// &lt;param name=""name""&gt;Indica o nome do atributo&lt;/param&gt;
        /// &lt;param name=""values""&gt;Indica os valores possÃ­veis para o atributo&lt;/param&gt;
        public Attribute(string name, string[] values)
        {
            mName = name;
            mValues = new ArrayList(values);
            mValues.Sort();
        }

        public Attribute(object Label)
        {
            mLabel = Label;
            mName = string.Empty;
            mValues = null;
        }

        /// &lt;summary&gt;
        /// Indica o nome do atributo
        /// &lt;/summary&gt;
        public string AttributeName
        {
            get
            {
                return mName;
            }
        }

        /// &lt;summary&gt;
        /// Retorna um array com os valores do atributo
        /// &lt;/summary&gt;
        public string[] values
        {
            get
            {
                if (mValues != null)
                    return (string[])mValues.ToArray(typeof(string));
                else
                    return null;
            }
        }

        /// &lt;summary&gt;
        /// Indica se um valor Ã© permitido para este atributo
        /// &lt;/summary&gt;
        /// &lt;param name=""value""&gt;&lt;/param&gt;
        /// &lt;returns&gt;&lt;/returns&gt;
        public bool isValidValue(string value)
        {
            return indexValue(value) &gt;= 0;
        }

        /// &lt;summary&gt;
        /// Retorna o Ã­ndice de um valor
        /// &lt;/summary&gt;
        /// &lt;param name=""value""&gt;Valor a ser retornado&lt;/param&gt;
        /// &lt;returns&gt;O valor do Ã­ndice na qual a posiÃ§Ã£o do valor se encontra&lt;/returns&gt;
        public int indexValue(string value)
        {
            if (mValues != null)
                return mValues.BinarySearch(value);
            else
                return -1;
        }

        /// &lt;summary&gt;
        /// 
        /// &lt;/summary&gt;
        /// &lt;returns&gt;&lt;/returns&gt;
        public override string ToString()
        {
            if (mName != string.Empty)
            {
                return mName;
            }
            else
            {
                return mLabel.ToString();
            }
        }
    }

    /// &lt;summary&gt;
    /// Classe que representarÃ¡ a arvore de decisÃ£o montada;
    /// &lt;/summary&gt;
    public class TreeNode
    {
        private ArrayList mChilds = null;
        private Attribute mAttribute;

        /// &lt;summary&gt;
        /// Inicializa uma nova instÃ¢ncia de TreeNode
        /// &lt;/summary&gt;
        /// &lt;param name=""attribute""&gt;Atributo ao qual o node estÃ¡ ligado&lt;/param&gt;
        public TreeNode(Attribute attribute)
        {
            if (attribute.values != null)
            {
                mChilds = new ArrayList(attribute.values.Length);
                for (int i = 0; i &lt; attribute.values.Length; i++)
                    mChilds.Add(null);
            }
            else
            {
                mChilds = new ArrayList(1);
                mChilds.Add(null);
            }
            mAttribute = attribute;
        }

        /// &lt;summary&gt;
        /// Adiciona um TreeNode filho a este treenode no galho de nome indicicado pelo ValueName
        /// &lt;/summary&gt;
        /// &lt;param name=""treeNode""&gt;TreeNode filho a ser adicionado&lt;/param&gt;
        /// &lt;param name=""ValueName""&gt;Nome do galho onde o treeNode Ã© criado&lt;/param&gt;
        public void AddTreeNode(TreeNode treeNode, string ValueName)
        {
            int index = mAttribute.indexValue(ValueName);
            mChilds[index] = treeNode;
        }

        /// &lt;summary&gt;
        /// Retorna o nro total de filhos do nÃ³
        /// &lt;/summary&gt;
        public int totalChilds
        {
            get
            {
                return mChilds.Count;
            }
        }

        /// &lt;summary&gt;
        /// Retorna o nÃ³ filho de um nÃ³
        /// &lt;/summary&gt;
        /// &lt;param name=""index""&gt;Indice do nÃ³ filho&lt;/param&gt;
        /// &lt;returns&gt;Um objeto da classe TreeNode representando o nÃ³&lt;/returns&gt;
        public TreeNode getChild(int index)
        {
            return (TreeNode)mChilds[index];
        }

        /// &lt;summary&gt;
        /// Atributo que estÃ¡ conectado ao NÃ³
        /// &lt;/summary&gt;
        public Attribute attribute
        {
            get
            {
                return mAttribute;
            }
        }

        /// &lt;summary&gt;
        /// Retorna o filho de um nÃ³ pelo nome do galho que leva atÃ© ele
        /// &lt;/summary&gt;
        /// &lt;param name=""branchName""&gt;Nome do galho&lt;/param&gt;
        /// &lt;returns&gt;O nÃ³&lt;/returns&gt;
        public TreeNode getChildByBranchName(string branchName)
        {
            int index = mAttribute.indexValue(branchName);
            return (TreeNode)mChilds[index];
        }
    }

    /// &lt;summary&gt;
    /// Classe que implementa uma Ã¡rvore de DecisÃ£o usando o algoritmo ID3
    /// &lt;/summary&gt;
    public class DecisionTreeID3
    {
        private DataTable mSamples;
        private int mTotalUnacc = 0;
        private int mTotalAcc = 0;
        private int mTotalGood = 0;
        private int mTotalVgood = 0;
        private int mTotal = 0;
        private string mTargetAttribute = ""result"";
        private double mEntropySet = 0.0;

        /// &lt;summary&gt;
        /// Retorna o total de amostras positivas em uma tabela de amostras
        /// &lt;/summary&gt;
        /// &lt;param name=""samples""&gt;DataTable com as amostras&lt;/param&gt;
        /// &lt;returns&gt;O nro total de amostras positivas&lt;/returns&gt;
        /// 

        private int countTotalUnacc(DataTable samples)
        {
            int result = 0;

            foreach (DataRow aRow in samples.Rows)
            {
                if (aRow[mTargetAttribute].ToString().ToUpper().Trim() == ""UNACC"")
                    result++;
            }

            return result;
        }

        private int countTotalAcc(DataTable samples)
        {
            int result = 0;

            foreach (DataRow aRow in samples.Rows)
            {
                if (aRow[mTargetAttribute].ToString().ToUpper().Trim() == ""ACC"")
                    result++;
            }

            return result;
        }

        private int countTotalGood(DataTable samples)
        {
            int result = 0;

            foreach (DataRow aRow in samples.Rows)
            {
                if (aRow[mTargetAttribute].ToString().ToUpper().Trim() == ""GOOD"")
                    result++;
            }

            return result;
        }

        private int countTotalVgood(DataTable samples)
        {
            int result = 0;

            foreach (DataRow aRow in samples.Rows)
            {
                if (aRow[mTargetAttribute].ToString().ToUpper().Trim() == ""VGOOD"")
                    result++;
            }

            return result;
        }

        /// &lt;summary&gt;
        /// Calcula a entropia dada a seguinte fÃ³rmula
        /// -p+log2p+ - p-log2p-
        /// 
        /// onde: p+ Ã© a proporÃ§Ã£o de valores positivos
        ///       p- Ã© a proporÃ§Ã£o de valores negativos
        /// &lt;/summary&gt;
        /// &lt;param name=""positives""&gt;Quantidade de valores positivos&lt;/param&gt;
        /// &lt;param name=""negatives""&gt;Quantidade de valores negativos&lt;/param&gt;
        /// &lt;returns&gt;Retorna o valor da Entropia&lt;/returns&gt;
        private double calcEntropy(int unacc, int acc,int good,int vgood)
        {
            //int total = positives + negatives;
            int total = unacc + acc + good + vgood;
            double ratioUnacc = (double)unacc / total;
            double ratioAcc = (double)acc / total;
            double ratioGood = (double)good / total;
            double ratioVgood = (double)vgood / total;

            if (ratioUnacc != 0)
                ratioUnacc = -(ratioUnacc) * System.Math.Log(ratioUnacc, 2);

            if (ratioAcc != 0)
                ratioAcc = -(ratioAcc) * System.Math.Log(ratioAcc, 2);

            if (ratioGood != 0)
                ratioGood = -(ratioGood) * System.Math.Log(ratioGood, 2);

            if (ratioVgood != 0)
                ratioVgood = -(ratioVgood) * System.Math.Log(ratioVgood, 2);

            double result = ratioUnacc + ratioAcc + ratioGood + ratioVgood;

            return result;
        }

        /// &lt;summary&gt;
        /// Varre tabela de amostras verificando um atributo e se o resultado Ã© positivo ou negativo
        /// &lt;/summary&gt;
        /// &lt;param name=""samples""&gt;DataTable com as amostras&lt;/param&gt;
        /// &lt;param name=""attribute""&gt;Atributo a ser pesquisado&lt;/param&gt;
        /// &lt;param name=""value""&gt;valor permitido para o atributo&lt;/param&gt;
        /// &lt;param name=""positives""&gt;ConterÃ¡ o nro de todos os atributos com o valor determinado com resultado positivo&lt;/param&gt;
        /// &lt;param name=""negatives""&gt;ConterÃ¡ o nro de todos os atributos com o valor determinado com resultado negativo&lt;/param&gt;
         private void getValuesToAttribute(DataTable samples, Attribute attribute, string value, out int unacc, out int acc, out int good, out int vgood)    
    {

            unacc = 0;
            acc = 0;
            good = 0;
            vgood = 0;

            foreach (DataRow aRow in samples.Rows)
            {
                if (
                    ((string)aRow[attribute.AttributeName] == value))
                    if ((string)aRow[mTargetAttribute] == ""unacc"")
                        unacc++;
                    else if ((string)aRow[mTargetAttribute] == ""acc"")
                        acc++;
                    else if ((string)aRow[mTargetAttribute] == ""good"")
                    good++;
                    else
                        vgood++;

            }
        }

        /// &lt;summary&gt;
        /// Calcula o ganho de um atributo
        /// &lt;/summary&gt;
        /// &lt;param name=""attribute""&gt;Atributo a ser calculado&lt;/param&gt;
        /// &lt;returns&gt;O ganho do atributo&lt;/returns&gt;
        private double gain(DataTable samples, Attribute attribute)
        {
            string[] values = attribute.values;
            double sum = 0.0;

            for (int i = 0; i &lt; values.Length; i++)
            {
                int unacc, acc, good, vgood;
                unacc = acc = good = vgood = 0;
                getValuesToAttribute(samples, attribute, values[i], out unacc, out acc, out good, out vgood);
                double entropy = calcEntropy(unacc, acc,good,vgood);
                sum += -(double)(unacc + acc + good + vgood) / mTotal * entropy;
            }
            return mEntropySet + sum;
        }

        /// &lt;summary&gt;
        /// Retorna o melhor atributo.
        /// &lt;/summary&gt;
        /// &lt;param name=""attributes""&gt;Um vetor com os atributos&lt;/param&gt;
        /// &lt;returns&gt;Retorna o que tiver maior ganho&lt;/returns&gt;
        private Attribute getBestAttribute(DataTable samples, Attribute[] attributes)
        {
            double maxGain = 0.0;
            Attribute result = null;

            foreach (Attribute attribute in attributes)
            {
                double aux = gain(samples, attribute);
                if (aux &gt; maxGain)
                {
                    maxGain = aux;
                    result = attribute;
                }
            }
            return result;
        }

        /// &lt;summary&gt;
        /// Retorna true caso todos os exemplos da amostragem sÃ£o positivos
        /// &lt;/summary&gt;
        /// &lt;param name=""samples""&gt;DataTable com as amostras&lt;/param&gt;
        /// &lt;param name=""targetAttribute""&gt;Atributo (coluna) da tabela a qual serÃ¡ verificado&lt;/param&gt;
        /// &lt;returns&gt;True caso todos os exemplos da amostragem sÃ£o positivos&lt;/returns&gt;
        private string allSamplesUnacc(DataTable samples, string targetAttribute)
        {
            foreach (DataRow row in samples.Rows)
            { //alterar
                if (row[targetAttribute].ToString() == ""acc"")
                    return ""acc"";
                if (row[targetAttribute].ToString() == ""good"")
                    return ""good"";
                if (row[targetAttribute].ToString() == ""vgood"")
                    return ""vgood"";
            }

            return ""unacc"";
        }

        private string allSamplesAcc(DataTable samples, string targetAttribute)
        {
            foreach (DataRow row in samples.Rows)
            { //alterar
                if (row[targetAttribute].ToString() == ""unacc"")
                    return ""unacc"";
                if (row[targetAttribute].ToString() == ""good"")
                    return ""good"";
                if (row[targetAttribute].ToString() == ""vgood"")
                    return ""vgood"";
            }

            return ""acc"";
        }
        private string allSamplesGood(DataTable samples, string targetAttribute)
        {
            foreach (DataRow row in samples.Rows)
            { //alterar
                if (row[targetAttribute].ToString() == ""unacc"")
                    return ""unacc"";
                if (row[targetAttribute].ToString() == ""acc"")
                    return ""acc"";
                if (row[targetAttribute].ToString() == ""vgood"")
                    return ""vgood"";
            }

            return ""good"";
        }
        private string allSamplesVgood(DataTable samples, string targetAttribute)
        {
            foreach (DataRow row in samples.Rows)
            { //alterar
                if (row[targetAttribute].ToString() == ""unacc"")
                    return ""unacc"";
                if (row[targetAttribute].ToString() == ""acc"")
                    return ""acc"";
                if (row[targetAttribute].ToString() == ""good"")
                    return ""good"";
            }

            return ""vgood"";
        }

        /// &lt;summary&gt;
        /// Retorna uma lista com todos os valores distintos de uma tabela de amostragem
        /// &lt;/summary&gt;
        /// &lt;param name=""samples""&gt;DataTable com as amostras&lt;/param&gt;
        /// &lt;param name=""targetAttribute""&gt;Atributo (coluna) da tabela a qual serÃ¡ verificado&lt;/param&gt;
        /// &lt;returns&gt;Um ArrayList com os valores distintos&lt;/returns&gt;
        private ArrayList getDistinctValues(DataTable samples, string targetAttribute)
        {
            ArrayList distinctValues = new ArrayList(samples.Rows.Count);

            foreach (DataRow row in samples.Rows)
            {
                if (distinctValues.IndexOf(row[targetAttribute]) == -1)
                    distinctValues.Add(row[targetAttribute]);
            }

            return distinctValues;
        }

        /// &lt;summary&gt;
        /// Retorna o valor mais comum dentro de uma amostragem
        /// &lt;/summary&gt;
        /// &lt;param name=""samples""&gt;DataTable com as amostras&lt;/param&gt;
        /// &lt;param name=""targetAttribute""&gt;Atributo (coluna) da tabela a qual serÃ¡ verificado&lt;/param&gt;
        /// &lt;returns&gt;Retorna o objeto com maior incidÃªncia dentro da tabela de amostras&lt;/returns&gt;
        private object getMostCommonValue(DataTable samples, string targetAttribute)
        {
            ArrayList distinctValues = getDistinctValues(samples, targetAttribute);
            int[] count = new int[distinctValues.Count];

            foreach (DataRow row in samples.Rows)
            {
                int index = distinctValues.IndexOf(row[targetAttribute]);
                count[index]++;
            }

            int MaxIndex = 0;
            int MaxCount = 0;

            for (int i = 0; i &lt; count.Length; i++)
            {
                if (count[i] &gt; MaxCount)
                {
                    MaxCount = count[i];
                    MaxIndex = i;
                }
            }

            return distinctValues[MaxIndex];
        }

        /// &lt;summary&gt;
        /// Monta uma Ã¡rvore de decisÃ£o baseado nas amostragens apresentadas
        /// &lt;/summary&gt;
        /// &lt;param name=""samples""&gt;Tabela com as amostragens que serÃ£o apresentadas para a montagem da Ã¡rvore&lt;/param&gt;
        /// &lt;param name=""targetAttribute""&gt;Nome da coluna da tabela que possue o valor true ou false para 
        /// validar ou nÃ£o uma amostragem&lt;/param&gt;
        /// &lt;returns&gt;A raiz da Ã¡rvore de decisÃ£o montada&lt;/returns&gt;&lt;/returns?&gt;
        private TreeNode internalMountTree(DataTable samples, string targetAttribute, Attribute[] attributes)
        {
            //alterar
            if (allSamplesUnacc(samples, targetAttribute) == ""unacc"")
                return new TreeNode(new Attribute(""unacc""));

            if (allSamplesAcc(samples, targetAttribute) == ""acc"")
                return new TreeNode(new Attribute(""acc""));

            if (allSamplesGood(samples, targetAttribute) == ""good"")
                return new TreeNode(new Attribute(""good""));

            if (allSamplesVgood(samples, targetAttribute) == ""vgood"")
                return new TreeNode(new Attribute(""vgood""));

            if (attributes.Length == 0)
                return new TreeNode(new Attribute(getMostCommonValue(samples, targetAttribute)));

            mTotal = samples.Rows.Count;
            mTargetAttribute = targetAttribute;
            mTotalUnacc = countTotalUnacc(samples);
            mTotalAcc = countTotalAcc(samples);
            mTotalGood = countTotalGood(samples);
            mTotalVgood = countTotalVgood(samples);

            mEntropySet = calcEntropy(mTotalUnacc, mTotalAcc, mTotalGood, mTotalVgood);

            Attribute bestAttribute = getBestAttribute(samples, attributes);

            TreeNode root = new TreeNode(bestAttribute);

            DataTable aSample = samples.Clone();

            foreach (string value in bestAttribute.values)
            {
                // Seleciona todas os elementos com o valor deste atributo              
                aSample.Rows.Clear();

                DataRow[] rows = samples.Select(bestAttribute.AttributeName + "" = "" + ""'"" + value + ""'"");

                foreach (DataRow row in rows)
                {
                    aSample.Rows.Add(row.ItemArray);
                }
                // Seleciona todas os elementos com o valor deste atributo              

                // Cria uma nova lista de atributos menos o atributo corrente que Ã© o melhor atributo               
                ArrayList aAttributes = new ArrayList(attributes.Length - 1);
                for (int i = 0; i &lt; attributes.Length; i++)
                {
                    if (attributes[i].AttributeName != bestAttribute.AttributeName)
                        aAttributes.Add(attributes[i]);
                }
                // Cria uma nova lista de atributos menos o atributo corrente que Ã© o melhor atributo

                if (aSample.Rows.Count == 0)
                {
                    return new TreeNode(new Attribute(getMostCommonValue(aSample, targetAttribute)));
                }
                else
                {
                    DecisionTreeID3 dc3 = new DecisionTreeID3();
                    TreeNode ChildNode = dc3.mountTree(aSample, targetAttribute, (Attribute[])aAttributes.ToArray(typeof(Attribute)));
                    root.AddTreeNode(ChildNode, value);
                }
            }

            return root;
        }


        /// &lt;summary&gt;
        /// Monta uma Ã¡rvore de decisÃ£o baseado nas amostragens apresentadas
        /// &lt;/summary&gt;
        /// &lt;param name=""samples""&gt;Tabela com as amostragens que serÃ£o apresentadas para a montagem da Ã¡rvore&lt;/param&gt;
        /// &lt;param name=""targetAttribute""&gt;Nome da coluna da tabela que possue o valor true ou false para 
        /// validar ou nÃ£o uma amostragem&lt;/param&gt;
        /// &lt;returns&gt;A raiz da Ã¡rvore de decisÃ£o montada&lt;/returns&gt;&lt;/returns?&gt;
        public TreeNode mountTree(DataTable samples, string targetAttribute, Attribute[] attributes)
        {
            mSamples = samples;
            return internalMountTree(mSamples, targetAttribute, attributes);
        }
    }

    /// &lt;summary&gt;
    /// Classe que exemplifica a utilizaÃ§Ã£o do ID3
    /// &lt;/summary&gt;
    class ID3Sample
    {

        public static void printNode(TreeNode root, string tabs)
        {
            Console.WriteLine(tabs + '|' + root.attribute + '|');

            if (root.attribute.values != null)
            {
                for (int i = 0; i &lt; root.attribute.values.Length; i++)
                {
                    Console.WriteLine(tabs + ""\t"" + ""&lt;"" + root.attribute.values[i] + ""&gt;"");
                    TreeNode childNode = root.getChildByBranchName(root.attribute.values[i]);
                    printNode(childNode, ""\t"" + tabs);
                }
            }
        }


        static DataTable getDataTable()
        {
            DataTable result = new DataTable(""samples"");
            DataColumn column = result.Columns.Add(""buying"");
            column.DataType = typeof(string);

            column = result.Columns.Add(""maint"");
            column.DataType = typeof(string);

            column = result.Columns.Add(""doors"");
            column.DataType = typeof(string);

            column = result.Columns.Add(""persons"");
            column.DataType = typeof(string);

            column = result.Columns.Add(""lugboot"");
            column.DataType = typeof(string);

            column = result.Columns.Add(""safety"");
            column.DataType = typeof(string);

            column = result.Columns.Add(""result"");
            column.DataType = typeof(string);

            result.Rows.Add(new object[] { ""vhigh"", ""med"", ""2"", ""4"", ""small"", ""high"", ""acc"" });
            result.Rows.Add(new object[] { ""vhigh"", ""med"", ""2"", ""4"", ""med"", ""low"", ""unacc"" });
            result.Rows.Add(new object[] { ""vhigh"", ""med"", ""2"", ""4"", ""med"", ""med"", ""unacc"" });
            result.Rows.Add(new object[] { ""vhigh"", ""med"", ""2"", ""4"", ""med"", ""high"", ""acc"" });
            result.Rows.Add(new object[] { ""vhigh"", ""med"", ""2"", ""4"", ""big"", ""low"", ""unacc"" });
            result.Rows.Add(new object[] { ""vhigh"", ""med"", ""2"", ""4"", ""big"", ""med"", ""acc"" });
            result.Rows.Add(new object[] { ""vhigh"", ""med"", ""2"", ""4"", ""big"", ""high"", ""acc"" });
            result.Rows.Add(new object[] { ""med"", ""low"", ""2"", ""4"", ""big"", ""high"", ""vgood"" });
            result.Rows.Add(new object[] { ""med"", ""low"", ""2"", ""more"", ""small"", ""low"", ""unacc"" });
            result.Rows.Add(new object[] { ""med"", ""low"", ""2"", ""more"", ""small"", ""med"", ""unacc"" });
            result.Rows.Add(new object[] { ""med"", ""low"", ""2"", ""more"", ""small"", ""high"", ""unacc"" });
            result.Rows.Add(new object[] { ""med"", ""low"", ""2"", ""more"", ""med"", ""low"", ""unacc"" });
            result.Rows.Add(new object[] { ""med"", ""low"", ""2"", ""more"", ""med"", ""med"", ""acc"" });
            result.Rows.Add(new object[] { ""med"", ""low"", ""2"", ""more"", ""med"", ""high"", ""good"" });
            result.Rows.Add(new object[] { ""med"", ""low"", ""2"", ""more"", ""big"", ""low"", ""unacc"" });
            result.Rows.Add(new object[] { ""med"", ""low"", ""2"", ""more"", ""big"", ""med"", ""good"" });
            result.Rows.Add(new object[] { ""med"", ""low"", ""2"", ""more"", ""big"", ""high"", ""vgood"" });

            return result;

        }

        /// &lt;summary&gt;
        /// The main entry point for the application.
        /// &lt;/summary&gt;
        /// 
        [STAThread]
        static void Main(string[] args)
        {

            Attribute buying = new Attribute(""buying"", new string[] { ""vhigh"", ""high"", ""med"", ""low"" });
            Attribute maint = new Attribute(""maint"", new string[] { ""vhigh"", ""high"", ""med"", ""low"" });
            Attribute doors = new Attribute(""doors"", new string[] { ""2"", ""3"", ""4"", ""5more"" });
            Attribute persons = new Attribute(""persons"", new string[] { ""2"", ""4"", ""more"" });
            Attribute lugboot = new Attribute(""lugboot"", new string[] { ""small"", ""med"", ""big"" });
            Attribute safety = new Attribute(""safety"", new string[] { ""low"", ""med"", ""high"" });

            Attribute[] attributes = new Attribute[] { buying, maint, doors, persons, lugboot, safety };

            DataTable samples = getDataTable();

            DecisionTreeID3 id3 = new DecisionTreeID3();
            TreeNode root = id3.mountTree(samples, ""result"", attributes);

            printNode(root, """");

        }
    }

}
</code></pre>
";;446649;6009;Steve Howard;0;28101858;"<p>Your stack trace has the information you need:</p>

<pre><code>AA.TreeNode..ctor(Attribute attribute) in c:\VS\Program.cs:line 117
</code></pre>

<p>'ctor' is fancy compiler slang for constructor, so we'll take a look at TreeNode(Attribute).</p>

<p>This likely means that <code>attribute</code> (the argument to the constructor) is null.  How did that happen?  Since we have all the code out in front of us, let's focus on the one call that doesn't explicitly create a new Attribute object to pass to the TreeNode constructor:</p>

<pre><code>Attribute bestAttribute = getBestAttribute(samples, attributes);
TreeNode root = new TreeNode(bestAttribute);
</code></pre>

<p><code>getBestAttribute</code> returns null if it none of the attributes have a gain greater than zero, or if there are no attributes.  To fix the exception, you'll need to fix either the input or the logic on that method.</p>
"
1319915;3316;trianta2;<python><machine-learning><scikit-learn><data-mining><data-analysis>;28114717;5;how can I combine training set specific learned parameters with sklearn online (out-of-core) learning;"<p>My dataset is getting too large and I'm looking for online learning solutions in sklearn, which they refer to as out-of-core learning.</p>

<p>They offer some classes which use a partial fit API, which basically lets you keep a subset of your data in memory and operate on it. However, a lot of preprocessing stages (such as data scaling) retain parameters during their fit stage on training data, which is then used for transformations.</p>

<p>For example, if you use a min-max scaler to bound features to [-1, 1] or standardize your data, the parameters they learn and eventually use to transform data are learned from <strong>a subset</strong> of the training data they happen to be operating on in a given iteration. </p>

<p>This means it's possible that the parameters learned during the fit stage on one subset of training data could be different from another subset of training data, since they are training set specific. And there lies the heart of my question:</p>

<p>How can you combine parameters learned during the fit stage of a preprocessing step when using online/out-of-core learning, when the learned parameters are a function of the training data?</p>
";28115258;163740;35373;ogrisel;5;28115258;"<p>You can fit the <code>StandardScaler</code> instance on a large enough subset that fit in RAM at once (say a couple of GB of data) and then re-use the same fixed instance of the scaler to transform the rest of the data one batch at a time. You should be able to get a good estimate of the mean and std values of each feature on a couple of thousands of samples so there is no need to compute the actual fit on the full data just for the scaler.</p>

<p>It would still be nice to add a <code>partial_fit</code> method to the <code>StandardScaler</code> class, implementing <a href=""http://www.johndcook.com/blog/standard_deviation/"" rel=""nofollow"">streaming mean &amp; variance estimation</a> for completeness.</p>

<p>But even if <code>StandardScaler</code> had a <code>partial_fit</code> method you would still need to do several path of the data (and optionally store the preprocessed data on the drive for later reuse):</p>

<ul>
<li>first pass: call <code>standard_scaler.partial_fit()</code> on all the original data chunks</li>
<li>second pass: call <code>standard_scaler.transform</code> on each chunk of the original data then pass the result to the <code>model.partial_fit</code> method.</li>
</ul>
"
4178757;167;Denny Dharmawan;<machine-learning><rbm><pylearn>;28115561;1;"What's the reason behind ""extracting 8x8 patches"" in Restricted Boltzman Machine?";"<p>I came accros this documentation on pyLearn2 (Machine Learning library) example of RBM. Could someone tell me why it is easier?</p>

<pre><code># First we want to pull out small patches of the images, since it's easier
# to train an RBM on these
pipeline.items.append(
    preprocessing.ExtractPatches(patch_shape=(8, 8), num_patches=150000)
)
</code></pre>

<p>For what it's worth I'm not well-informed about RBM so please bear with me. For the complete code, please refer to <a href=""https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/scripts/tutorials/grbm_smd/make_dataset.py"" rel=""nofollow"">this link</a></p>
";;860196;8365;runDOSrun;2;28126696;"<p>Simply put, as with any algorithm your complexity increases with larger input numbers. Dividing the problem into smaller sub-problems and subsequently combining these may prove faster (called divide and conquer algorithms).</p>

<p>Now, with these type of Machine Learning algorithms there's an additional need for abstraction in the features. You neither wanna input every single pixel at once (having only local information), nor want to represent the whole image by a single number/symbol (having only global information). A number of approaches combine these sorts of data into hierarchical representations (mostly called Deep Learning).</p>

<p>If you bring together these two concepts, it should be clear(er) that processing small image patches first gives you a bigger amount of local information which you can then combine to infer into global information at a later stage. So ""because it's easier"" is not the full reasoning behind it. It also makes everything perform better/more accurate. </p>

<p>I hope this answers your question without being too vague (a thorough answer would become too long). For a more detailed introduction on RBMs have a look at e.g. chapter 7 on this <a href=""http://image.diku.dk/igel/paper/TRBMAI.pdf"" rel=""nofollow"">page</a></p>
"
788619;737;wbg;<regex><machine-learning><nlp><scikit-learn><tokenize>;28129365;0;"Regex / ""token_pattern"" for scikit-learn text Vectorizer";"<p>I'm using sklearn to do some NLP vectorizing with a tf-idf Vectorizer object. This object can be constructed with a keyword, ""token_pattern"".</p>

<p>I want to avoid hashtags (#foobar), numerics (and strings that begin with a number(s), i.e. 10mg), any line that begin with 'RT' (retweet), or the line ""Deleted tweet"".</p>

<p>In addition, I want to ignore unicode.</p>

<p>I want to keep, the URL's (not the 'http://') and have them tokenized into any words ( [A-Za-z]+ only ) that may exist in them.</p>

<p>I have some experience with Regex, but have not needed more complex patterns until now.</p>

<p>Below is my stab for everything...it's obviously not the best way to investigate, but it does sum up how I currently think about the Regex rules.</p>

<p>NOTE: the skearn doc <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow"">here</a> shows the default ""token_pattern"" using the unicode flag on the string and I don't understand why...separate question perhaps. </p>

<pre><code>pat2 = r""(?im)([A-Z]+)(?&lt;!^@)([A-Z]+)(?&lt;!^#)([A-Z]+)(?&lt;!^(RT))([A-Z]+)(?&lt;!^Deleted)(?&lt;=^(http://))([A-Z]+)""
</code></pre>

<p>My break down:</p>

<pre><code>(?im)  #Are flags for 'multi-line' and 'case insensitive'

([A-Z]+)(?&lt;!^@) #A negative look back, match [A-Z]+ only if not preceded by 'starts with @'.

(?&lt;=^(http://))([A-Z]+) #A positive look forward, match [A-Z]+ only if 'starts with ""http://""' is present.
</code></pre>

<p>I get the feeling this is not an elegant solution even if it is tweaked into working...</p>

<p>TIA </p>

<p>UPDATE:
RAW DATA EXAMPLE:</p>

<p>If it helps to know, I'm using a pandas data frame to load the data. I'm new to pandas and maybe missing some pandas based solution.</p>

<p>From this raw data, I'd want only words taken from the text and the URL's.
This example sucks...please comment further to help me get it better defined... thx!</p>

<p>raw:</p>

<pre><code>http://foxsportswisconsin.ning.com/profiles/blogs/simvastatin-20-mg-pas-cher-sur-internet-acheter-du-simvastatin-20
</code></pre>

<p>tokenized: </p>

<pre><code>[foxsportswisconsin, ning, com, profiles, blogs, simvastatin, mg, pas, cher, sur, internet, acheter, du, simvastatin]
</code></pre>
";28130338;1287834;19655;Slater Victoroff;9;28130338;"<p>tl;dr: if you ever write a <code>regex</code> over 20 characters you're doing something wrong, but it might be an acceptable hack. If you write a <code>regex</code> over 50 characters you need to stop immediately.</p>

<p>Let me just start off by saying that this should, in no way, shape, or form be solved by a regex.</p>

<p>Most of the steps that you describe should be handle in pre-processing or post-processing. You shouldn't try to come up with a <code>regex</code> that filters something that starts with <code>Deleted tweet</code> or <code>RT</code>, you should ignore these lines in pre-processing.</p>

<p>Ignore <code>unicode</code>? Then probably worth getting off the internet since literally everything on the internet, and everything outside of notepad is unicode. If you want to remove all unicode characters that can't be represented in ascii (which is what I assume you meant?), then the encoding step is the place to fix this:</p>

<pre><code>&lt;string&gt;.encode('ascii', 'ignore')
</code></pre>

<p>As far as ignoring <code>http</code> goes, you should just set <code>http</code> as a stopword. This can be passed in as another argument to the vectorizer you're using.</p>

<p>Once that's done, the token regex you use (probably still not a case for regex, but that is the interface that sklearn offers), is actually very simple:</p>

<pre><code>'\b[a-zA-Z]\w+\b'
</code></pre>

<p>Where the only change to be implemented here is the ignoring of numerics like <code>10mg</code> mentioned above.</p>

<p>It's worth noting that this heavy level of token removal is going to negatively affect pretty much any analysis you're trying to do. If you have a decent-sized corpus you shouldn't remove any tokens, if it's small removing stop words and using a stemmer or a lemmatizer is a good way to go, but this kind of token removal is poor practice and will lead to overfitting.</p>
"
1156707;15912;Louis Thibault;<python><machine-learning><scikit-learn>;28130615;1;"What is the nature of the `data` field in the ""Labelled Faces in the Wild"" dataset (scikit learn)?";"<p>I'm trying to train a simple HOG face detector using the data fetched from <code>sklearn.datasets.fetch_lfw_people</code>.  After fetching the dataset, I find the following keys:</p>

<pre><code>In [1]:  lfw_people.keys()
Out[1]:  ['images', 'data', 'target_names', 'DESCR', 'target']
</code></pre>

<ul>
<li><code>images</code> contains the cropped faces</li>
<li><code>target_names</code> contains the human-readable names</li>
<li><code>target</code> contains a numerical ID number for the person in a image</li>
<li><code>DESCR</code> identifies the record as belonging to the LFW dataset.</li>
</ul>

<p>... but what on <strong>earth</strong> is <code>data</code>?</p>

<p>In my case it's a <code>(13233 x 1850)</code> numpy array of floats, which is to say one row of 1850 floats per image.</p>

<p><strong>What is the nature of this <code>data</code> field?</strong></p>
";28130641;1156707;15912;Louis Thibault;0;28130641;"<p><code>(lfw_people.images[0].ravel() == lfw_people.data[0]).all()</code> evaluates to <code>True</code>, so it would appear that the <code>data</code> field is simply the image flattened into a vector.</p>

<p>Mystery solved, but this kind of thing really should be indicated upfront :/</p>
"
1698426;1035;John Schmitt;<compilation><machine-learning><gpu>;28134658;0;How can I build GPUMlib on Fedora 20?;"<p><a href=""http://gpumlib.sourceforge.net/"" rel=""nofollow"">GPUMlib</a> comes with a filed called <code>CMakeLists.txt</code> instead of Makefiles or configure scripts.  I do not see any build instructions and I do not see a list of packages that need to be installed in order to build GPUMlib even in the <a href=""http://sourceforge.net/p/gpumlib/code/ci/default/tree/"" rel=""nofollow"">sources</a>.</p>
";28642373;1698426;1035;John Schmitt;0;28642373;"<p>This answer assumes that cmake and gcc et al are already installed.</p>

<p>First install <a href=""http://www.culatools.com/downloads/dense/"" rel=""nofollow"">the CULA tools</a>.  <a href=""http://www.culatools.com/cula_dense_programmers_guide/"" rel=""nofollow"">Their documentation</a> doesn't seem mention it, but their installer (which is barely mentioned in their <a href=""http://www.culatools.com/dense/faq/"" rel=""nofollow"">FAQ</a>) worked for me.</p>

<p>Once the installer finishes, it'll report some important shell variables, namely <code>CULA_LIB_PATH_32</code>, <code>CULA_LIB_PATH_64</code>, and <code>CULA_INC_PATH</code>.  Take note of their values.</p>

<p>Invoke cmake with the shell variables reported by the CULA installer.  Using me as an example, I installed my CULA tools at <code>/home/john/projects/gpumlib/cula</code> and cloned the gpumlib code into a directory called <code>gpumlib-code</code> so then my cmake command looked like this:</p>

<pre><code>CULA_LIB_PATH_32=/home/john/projects/gpumlib/cula/lib CULA_LIB_PATH_64=/home/john/projects/gpumlib/cula/lib64 CULA_INC_PATH=/home/john/projects/gpumlib/cula/include cmake gpumlib-code
make -j 8
</code></pre>

<p>I wrote a bash script to do this for me which looks like this:</p>

<pre><code>#!/bin/bash

export CULA_LIB_PATH_32=/home/john/projects/gpumlib/cula/lib
export CULA_LIB_PATH_64=/home/john/projects/gpumlib/cula/lib64
export CULA_INC_PATH=/home/john/projects/gpumlib/cula/include

cmake gpumlib-code

make -j 8
</code></pre>

<p>Thanks to Noel de Jesus MendonÃ§a Lopes (the author of gpumlib) for helping me.</p>
"
1960266;2801;Little;<matlab><machine-learning><neural-network>;28137980;0;neural network for handwritten recognition?;"<p>I have been following the course of Andrew Ng about Machine Learning, and I currently have some doubts about the implementation of a handwritten recognition tool.</p>

<p>-First he says that he uses a subset of the MNIST dataset, which contaings 5000 training examples and each training example is an image in a 20x20 gray scale format. With that he says that we have a vector of 400 elements of length that is the ""unrolled"" of the data previously described. Does it mean that the train set has something like the following format?</p>

<pre><code>Training example 1 v[1,2,...,400]
Training example 2 v[1,2,...,400]
...
Training example 5000 v[1,2,...,400]
</code></pre>

<p>For the coding part the author gives the following complete code in Matlab:</p>

<pre><code>%% Machine Learning Online Class - Exercise 3 | Part 2: Neural Networks

%  Instructions
%  ------------
% 
%  This file contains code that helps you get started on the
%  linear exercise. You will need to complete the following functions 
%  in this exericse:
%
%     lrCostFunction.m (logistic regression cost function)
%     oneVsAll.m
%     predictOneVsAll.m
%     predict.m
%
%  For this exercise, you will not need to change any code in this file,
%  or any other files other than those mentioned above.
%

%% Initialization
clear ; close all; clc

%% Setup the parameters you will use for this exercise
input_layer_size  = 400;  % 20x20 Input Images of Digits
hidden_layer_size = 25;   % 25 hidden units
num_labels = 10;          % 10 labels, from 1 to 10   
                          % (note that we have mapped ""0"" to label 10)

%% =========== Part 1: Loading and Visualizing Data =============
%  We start the exercise by first loading and visualizing the dataset. 
%  You will be working with a dataset that contains handwritten digits.
%

% Load Training Data
fprintf('Loading and Visualizing Data ...\n')

load('ex3data1.mat');
m = size(X, 1);

% Randomly select 100 data points to display
sel = randperm(size(X, 1));
sel = sel(1:100);

displayData(X(sel, :));

fprintf('Program paused. Press enter to continue.\n');
pause;

%% ================ Part 2: Loading Pameters ================
% In this part of the exercise, we load some pre-initialized 
% neural network parameters.

fprintf('\nLoading Saved Neural Network Parameters ...\n')

% Load the weights into variables Theta1 and Theta2
load('ex3weights.mat');

%% ================= Part 3: Implement Predict =================
%  After training the neural network, we would like to use it to predict
%  the labels. You will now implement the ""predict"" function to use the
%  neural network to predict the labels of the training set. This lets
%  you compute the training set accuracy.

pred = predict(Theta1, Theta2, X);

fprintf('\nTraining Set Accuracy: %f\n', mean(double(pred == y)) * 100);

fprintf('Program paused. Press enter to continue.\n');
pause;

%  To give you an idea of the network's output, you can also run
%  through the examples one at the a time to see what it is predicting.

%  Randomly permute examples
rp = randperm(m);

for i = 1:m
    % Display 
    fprintf('\nDisplaying Example Image\n');
    displayData(X(rp(i), :));

    pred = predict(Theta1, Theta2, X(rp(i),:));
    fprintf('\nNeural Network Prediction: %d (digit %d)\n', pred, mod(pred, 10));

    % Pause
    fprintf('Program paused. Press enter to continue.\n');
    pause;
end
</code></pre>

<p>and the predict function should be complete by the students, I have done the following:</p>

<pre><code>function p = predict(Theta1, Theta2, X)
%PREDICT Predict the label of an input given a trained neural network
%   p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the
%   trained weights of a neural network (Theta1, Theta2)

% Useful values
m = size(X, 1);
num_labels = size(Theta2, 1);

% You need to return the following variables correctly 
p = zeros(size(X, 1), 1);
X = [ones(m , 1) X];
% ====================== YOUR CODE HERE ======================
% Instructions: Complete the following code to make predictions using
%               your learned neural network. You should set p to a 
%               vector containing labels between 1 to num_labels.
%
% Hint: The max function might come in useful. In particular, the max
%       function can also return the index of the max element, for more
%       information see 'help max'. If your examples are in rows, then, you
%       can use max(A, [], 2) to obtain the max for each row.
%

a1 = X;
a2 = sigmoid(a1*Theta1');
a2 = [ones(m , 1) a2];
a3 = sigmoid(a2*Theta2');
[M , p] = max(a3 , [] , 2);
</code></pre>

<p>Even thought it runs I am not completely aware of how it really works (I have just followed the step by step instructions that is on the author's website). I have doubts in the following:</p>

<ul>
<li><p>The author considers that X(input) is an array of 5000 x 400 elements, or it has 400 neurons as input, with 10 neurons as output and a hidden layer. Does it mean this 5000 x 400 values are the training set?</p></li>
<li><p>The author gives us the values of theta 1 and theta 2, which I believe serve as weights for the calculations on the inner layer, but how does values are obtained? Why does he uses 25 neurons of hidden layer and not 24 or 30?</p></li>
</ul>

<p>Any help will be apreciated.
Thanks</p>
";28157949;3994824;6224;Thomas Pinetz;2;28138053;"<p>I did the same course some time ago.</p>

<p>X is the input data. Therefore X is the matrix consisting of the 5 000 vectors of 400 elements each. There is no training set, because the network is pre trained.</p>

<p>Normally the values for theta 1 and 2 are trained. How this is done is a subject for the next few lectures. (Backpropagation algorithm)</p>

<p>I'm not entirely sure, why he used 25 neurons as hidden layer. However my guess is, that this number of neurons simply works, without making the training step take forever.</p>
"
1960266;2801;Little;<matlab><machine-learning><neural-network>;28137980;0;neural network for handwritten recognition?;"<p>I have been following the course of Andrew Ng about Machine Learning, and I currently have some doubts about the implementation of a handwritten recognition tool.</p>

<p>-First he says that he uses a subset of the MNIST dataset, which contaings 5000 training examples and each training example is an image in a 20x20 gray scale format. With that he says that we have a vector of 400 elements of length that is the ""unrolled"" of the data previously described. Does it mean that the train set has something like the following format?</p>

<pre><code>Training example 1 v[1,2,...,400]
Training example 2 v[1,2,...,400]
...
Training example 5000 v[1,2,...,400]
</code></pre>

<p>For the coding part the author gives the following complete code in Matlab:</p>

<pre><code>%% Machine Learning Online Class - Exercise 3 | Part 2: Neural Networks

%  Instructions
%  ------------
% 
%  This file contains code that helps you get started on the
%  linear exercise. You will need to complete the following functions 
%  in this exericse:
%
%     lrCostFunction.m (logistic regression cost function)
%     oneVsAll.m
%     predictOneVsAll.m
%     predict.m
%
%  For this exercise, you will not need to change any code in this file,
%  or any other files other than those mentioned above.
%

%% Initialization
clear ; close all; clc

%% Setup the parameters you will use for this exercise
input_layer_size  = 400;  % 20x20 Input Images of Digits
hidden_layer_size = 25;   % 25 hidden units
num_labels = 10;          % 10 labels, from 1 to 10   
                          % (note that we have mapped ""0"" to label 10)

%% =========== Part 1: Loading and Visualizing Data =============
%  We start the exercise by first loading and visualizing the dataset. 
%  You will be working with a dataset that contains handwritten digits.
%

% Load Training Data
fprintf('Loading and Visualizing Data ...\n')

load('ex3data1.mat');
m = size(X, 1);

% Randomly select 100 data points to display
sel = randperm(size(X, 1));
sel = sel(1:100);

displayData(X(sel, :));

fprintf('Program paused. Press enter to continue.\n');
pause;

%% ================ Part 2: Loading Pameters ================
% In this part of the exercise, we load some pre-initialized 
% neural network parameters.

fprintf('\nLoading Saved Neural Network Parameters ...\n')

% Load the weights into variables Theta1 and Theta2
load('ex3weights.mat');

%% ================= Part 3: Implement Predict =================
%  After training the neural network, we would like to use it to predict
%  the labels. You will now implement the ""predict"" function to use the
%  neural network to predict the labels of the training set. This lets
%  you compute the training set accuracy.

pred = predict(Theta1, Theta2, X);

fprintf('\nTraining Set Accuracy: %f\n', mean(double(pred == y)) * 100);

fprintf('Program paused. Press enter to continue.\n');
pause;

%  To give you an idea of the network's output, you can also run
%  through the examples one at the a time to see what it is predicting.

%  Randomly permute examples
rp = randperm(m);

for i = 1:m
    % Display 
    fprintf('\nDisplaying Example Image\n');
    displayData(X(rp(i), :));

    pred = predict(Theta1, Theta2, X(rp(i),:));
    fprintf('\nNeural Network Prediction: %d (digit %d)\n', pred, mod(pred, 10));

    % Pause
    fprintf('Program paused. Press enter to continue.\n');
    pause;
end
</code></pre>

<p>and the predict function should be complete by the students, I have done the following:</p>

<pre><code>function p = predict(Theta1, Theta2, X)
%PREDICT Predict the label of an input given a trained neural network
%   p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the
%   trained weights of a neural network (Theta1, Theta2)

% Useful values
m = size(X, 1);
num_labels = size(Theta2, 1);

% You need to return the following variables correctly 
p = zeros(size(X, 1), 1);
X = [ones(m , 1) X];
% ====================== YOUR CODE HERE ======================
% Instructions: Complete the following code to make predictions using
%               your learned neural network. You should set p to a 
%               vector containing labels between 1 to num_labels.
%
% Hint: The max function might come in useful. In particular, the max
%       function can also return the index of the max element, for more
%       information see 'help max'. If your examples are in rows, then, you
%       can use max(A, [], 2) to obtain the max for each row.
%

a1 = X;
a2 = sigmoid(a1*Theta1');
a2 = [ones(m , 1) a2];
a3 = sigmoid(a2*Theta2');
[M , p] = max(a3 , [] , 2);
</code></pre>

<p>Even thought it runs I am not completely aware of how it really works (I have just followed the step by step instructions that is on the author's website). I have doubts in the following:</p>

<ul>
<li><p>The author considers that X(input) is an array of 5000 x 400 elements, or it has 400 neurons as input, with 10 neurons as output and a hidden layer. Does it mean this 5000 x 400 values are the training set?</p></li>
<li><p>The author gives us the values of theta 1 and theta 2, which I believe serve as weights for the calculations on the inner layer, but how does values are obtained? Why does he uses 25 neurons of hidden layer and not 24 or 30?</p></li>
</ul>

<p>Any help will be apreciated.
Thanks</p>
";28157949;534969;2530;Saul Berardo;2;28157949;"<p>Let's break your question in parts:</p>

<blockquote>
  <p>First he says that he uses a subset of the MNIST dataset, which
  contaings 5000 training examples and each training example is an image
  in a 20x20 gray scale format. With that he says that we have a vector
  of 400 elements of length that is the ""unrolled"" of the data
  previously described. Does it mean that the train set has something
  like the following format? (...)</p>
</blockquote>

<p>You're on the right track. Each training example is a 20x20 image. The simplest neural network model, introduced in the course, treats each image just as a simple 1x400 vector (the ""unrolled"" means exactly this transformation). The dataset is stored in a matrix because this way you can perform computations faster exploiting the efficient linear algebra libraries which are used by Octave/Matlab. You don't need necessarily to store all training examples as a 5000x400 matrix, but this way your code will run faster.</p>

<blockquote>
  <p>The author considers that X(input) is an array of 5000 x 400 elements,
  or it has 400 neurons as input, with 10 neurons as output and a hidden
  layer. Does it mean this 5000 x 400 values are the training set?</p>
</blockquote>

<p>The ""input layer"" is nothing but the very input image. You can think of it as neurons whose output values were already calculated or as the values were coming from outside the network (think about your retina. It is like the input layer of you visual system). Thus this network has 400 input units (the ""unrolled"" 20x20 image). But of course, your training set doesn't consist of a single image, thus you put all your 5000 images together in a single 5000x400 matrix to form your training set.</p>

<blockquote>
  <p>The author gives us the values of theta 1 and theta 2, which I believe
  serve as weights for the calculations on the inner layer, but how does
  values are obtained?</p>
</blockquote>

<p>These theta values were found using a algorithm called backpropagation. If you didn't have to implement it in the course yet, just be patient. It might be in the exercises soon! Btw, yes they are the weights.</p>

<blockquote>
  <p>Why does he uses 25 neurons of hidden layer and not 24 or 30?</p>
</blockquote>

<p>He probably chose an arbitrarily value that doesn't run too slow, neither has too poor performance. You probably can find much better values for this hyper-parameters. But if you increase it too much, the training procedure will take probably much longer. Also since you are just using a small portion of the hole training set (the original MNIST has 60000 training examples and 28x28 images), you need to use a ""small"" number of hidden units to prevent over fitting. If you use too many units your neurons will ""learn by heart"" the training examples and will not be able to generalize to new unseen data. Finding the hyper parameters, such as the number of the hidden units, is a kind of art that you will master with experience (and maybe with Bayesian optimization and more advanced method, but that's another story xD).</p>
"
4114372;1915;john doe;<python><python-2.7><machine-learning><nlp><scikit-learn>;28144006;2;Understanding UndefinedMetricWarning in classification report with scikit-learn?;"<p>I have a text classification task with 5 categories the problem is that I am getting bad precision and this warning, probably as a result from unbalaced data(Im not sure):</p>

<pre><code>/usr/local/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
</code></pre>

<p>I guess this warning was produced since the data is clustered in the 5 label. <strong>How can I fix this warning and how can I increase the results of the classification report?</strong>. I also tried a Grid search with the following hyper-parameters:</p>

<pre><code>Best parameters set:
    clf__C: 0.1
    vect__max_df: 0.25
    vect__ngram_range: (1, 1)
    vect__use_idf: True

Accuracy:
0.456923076923
</code></pre>

<p>But still getting bad results, could anybody help me to increse this results with SVC or another model?. </p>
";;676634;23217;Andreas Mueller;1;28305889;"<p>You can use a pipeline and then also grid-search the parameters of the TfidifVectorizer together with the C of the SVC, like n-gram range (1, 1), (1, 2) or (2, 2), maybe set a different max_df, compare against CountVectorizer, maybe try character n-grams (with a higher n-gram range), too.</p>
"
3214477;143;Explorer;<machine-learning><classification><metrics><evaluation><summarization>;28152683;1;What are some good and widely used evaluation metrics to test the accuracy of extractive text summarization methods?;"<p>I am using a classification technique for multi document extractive text summarization. I have calculated f-measure, recall, precision and accuracy. What will be the ideal metric for my purpose here to evaluate the summaries generated by this method?</p>
";;4569096;613;jksnw;2;29270151;"<p>ROUGE calculates Recall, Precision and F-measure for a variety of metrics: ROUGE-N, ROUGE-L, ROUGE-W, ROUGE-S. <a href=""http://anthology.aclweb.org/W/W04/W04-1013.pdf"" rel=""nofollow"">Here</a> is the paper for ROUGE.</p>

<p>ROUGE-N is the number of matching <a href=""http://en.wikipedia.org/wiki/N-gram"" rel=""nofollow"">n-grams</a> divided by the total number of n-grams.</p>

<p>ROUGE-L looks at the longest common subsequences of two texts, a subsequence can contain gaps so that <code>1,3,5</code> is a subsequence of <code>1,2,3,4,5</code>.</p>

<p>ROUGE-W also uses longest common subsequence as a score but gives a higher weight to subsequences with less gaps.</p>

<p>ROUGE-S uses skip-bigrams, a skip-bigram is 2-gram that can contain any 2 words as long as they are in sentence order i.e do not have to be consecutive.</p>
"
200317;14774;add-semi-colons;<python><machine-learning><information-retrieval><evaluation><precision-recall>;28154321;1;Calculating Precision and Recall in Click Data;"<p>I am trying to build a graph of precision and recall using click data. I have two data sources. </p>

<ol>
<li>First data source has all the user clicked item_ids based on a given query_id.</li>
<li>Second data source has all the relevant item_ids for given query_id.</li>
</ol>

<p>I used python and put these in two data sources into dictionaries as follows:</p>

<pre><code>&gt;&gt;&gt; print clicked_data
{101: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 103: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]}

&gt;&gt;&gt; print all_relevant_data
{101: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], 103: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]}
</code></pre>

<p>I was reading the article in scikit-learn web site (<a href=""http://scikit-learn.org/stable/auto_examples/plot_precision_recall.html"" rel=""nofollow"">http://scikit-learn.org/stable/auto_examples/plot_precision_recall.html</a>) and tried to follow the formula but got really confuse really setting up False Positive and False Negative.</p>

<p>following the equations in scikit-learn: According to above example prevision for item <code>101</code></p>

<pre><code>P = T_positive/ (T_positive + F_positive)

&gt;&gt;&gt; float(len(clicked_data[101]))/float(len(all_relevant_data[101]))
0.5555555555555556
</code></pre>

<p>But as I try to figure out Recall I am having trouble in getting False Negative items of click data. In theory False Negative means incorrectly marked. All I have is user clicked data for a given id and all the items that are relevant to that id.</p>

<pre><code>R = T_positive / (T_positive + F_negative)
</code></pre>

<p>How can I calculate precision and recall correctly so I can build a graph. </p>

<p>On a different note if this is not a good metric to evaluate the results, considering the fact that I only have above mention data what would be the good metric? </p>
";40834758;1056563;45835;StephenBoesch;0;28189328;"<p>With just the clicked (TP) and relevant data (TP + FN) you do have the means to calculate the Recall - but not the Precision .  You have no dataset that tells you the FP values.</p>
"
200317;14774;add-semi-colons;<python><machine-learning><information-retrieval><evaluation><precision-recall>;28154321;1;Calculating Precision and Recall in Click Data;"<p>I am trying to build a graph of precision and recall using click data. I have two data sources. </p>

<ol>
<li>First data source has all the user clicked item_ids based on a given query_id.</li>
<li>Second data source has all the relevant item_ids for given query_id.</li>
</ol>

<p>I used python and put these in two data sources into dictionaries as follows:</p>

<pre><code>&gt;&gt;&gt; print clicked_data
{101: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 103: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]}

&gt;&gt;&gt; print all_relevant_data
{101: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], 103: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]}
</code></pre>

<p>I was reading the article in scikit-learn web site (<a href=""http://scikit-learn.org/stable/auto_examples/plot_precision_recall.html"" rel=""nofollow"">http://scikit-learn.org/stable/auto_examples/plot_precision_recall.html</a>) and tried to follow the formula but got really confuse really setting up False Positive and False Negative.</p>

<p>following the equations in scikit-learn: According to above example prevision for item <code>101</code></p>

<pre><code>P = T_positive/ (T_positive + F_positive)

&gt;&gt;&gt; float(len(clicked_data[101]))/float(len(all_relevant_data[101]))
0.5555555555555556
</code></pre>

<p>But as I try to figure out Recall I am having trouble in getting False Negative items of click data. In theory False Negative means incorrectly marked. All I have is user clicked data for a given id and all the items that are relevant to that id.</p>

<pre><code>R = T_positive / (T_positive + F_negative)
</code></pre>

<p>How can I calculate precision and recall correctly so I can build a graph. </p>

<p>On a different note if this is not a good metric to evaluate the results, considering the fact that I only have above mention data what would be the good metric? </p>
";40834758;5352399;26827;Wasi Ahmad;3;40834758;"<p>You can compute precision@k, recall@k based on your dataset. But you need a ranking of the documents to compute them. </p>

<p><strong>Dataset</strong></p>

<p>A well known dataset is <a href=""http://www.researchpipeline.com/mediawiki/index.php?title=AOL_Search_Query_Logs"" rel=""nofollow noreferrer"">AOL Search Query Logs</a> which you can use to build a retrieval-based system (you just need a dataset and a retrieval function) to compute precision, recall, average precision and mean average precision. I am briefly explaining the terms mentioned above.</p>

<p><strong>Document Ranking / Retrieval Function</strong></p>

<p><strong>Okapi BM25</strong> (BM stands for Best Matching) is a ranking function used by search engines to rank matching documents according to their relevance to a given search query. It is based on the probabilistic retrieval framework. BM25 is a <a href=""https://en.wikipedia.org/wiki/Bag-of-words_model"" rel=""nofollow noreferrer"">bag-of-words</a> retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of the inter-relationship between the query terms within a document (e.g., their relative proximity). See the <a href=""https://en.wikipedia.org/wiki/Okapi_BM25"" rel=""nofollow noreferrer"">Wikipedia</a> page for more details.</p>

<p><strong>Precision and Recall</strong></p>

<p>Precision measures ""of all the documents we retrieved as relevant how many are actually relevant?"".</p>

<pre><code>Precision = No. of relevant documents retrieved / No. of total documents retrieved
</code></pre>

<p>Recall measures ""Of all the actual relevant documents how many did we retrieve as relevant?"".</p>

<pre><code>Recall = No. of relevant documents retrieved / No. of total relevant documents
</code></pre>

<p>Suppose, when a query ""q"" is submitted to an information retrieval system (ex., search engine) having 100 relevant documents w.r.t. the query ""q"", the system retrieves 68 documents out of total collection of 600 documents. Out of 68 retrieved documents, 40 documents were relevant. So, in this case: </p>

<p><code>Precision = 40 / 68 = 58.8%</code> and <code>Recall = 40 / 100 = 40%</code></p>

<p>F-Score / F-measure is the weighted harmonic mean of precision and recall. The traditional F-measure or balanced F-score is:</p>

<pre><code>F-Score = 2 * Precision * Recall / Precision + Recall
</code></pre>

<p><strong>Average Precision</strong></p>

<p>You can think of it this way: you type something in <code>Google</code> and it shows you 10 results. Itâ€™s probably best if all of them were relevant. If only some are relevant, say five of them, then itâ€™s much better if the relevant ones are shown first. It would be bad if first five were irrelevant and good ones only started from sixth, wouldnâ€™t it? AP score reflects this.</p>

<p>Giving an example below:</p>

<p><a href=""https://i.stack.imgur.com/GPtPx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GPtPx.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p>AvgPrec of the two rankings:</p>
</blockquote>

<p>Ranking#1: <code>(1.0 + 0.67 + 0.75 + 0.8 + 0.83 + 0.6) / 6 = 0.78</code></p>

<p>Ranking#2: <code>(0.5 + 0.4 + 0.5 + 0.57 + 0.56 + 0.6) / 6 = 0.52</code></p>

<p><strong>Mean Average Precision (MAP)</strong></p>

<p>MAP is mean of average precision across multiple queries/rankings. Giving an example for illustration.</p>

<p><a href=""https://i.stack.imgur.com/XLpof.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XLpof.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p>Mean average precision for the two queries:</p>
</blockquote>

<p>For query 1, <code>AvgPrec: (1.0+0.67+0.5+0.44+0.5) / 5 = 0.62</code></p>

<p>For query 2, <code>AvgPrec: (0.5+0.4+0.43) / 3 = 0.44</code></p>

<p>So, MAP = <code>(0.62 + 0.44) / 2 = 0.53</code></p>

<p>Sometimes, people use <code>precision@k</code>, <code>recall@k</code> as performance measure of a retrieval system. You should build a retrieval system for such testings. If you want to write your program in Java, you should consider <a href=""https://www.tutorialspoint.com/lucene/lucene_first_application.htm"" rel=""nofollow noreferrer""><strong>Apache Lucene</strong></a> to build your index.</p>
"
3307729;85;Adel;<r><machine-learning><lda><topic-modeling><unsupervised-learning>;28154789;7;How can I speed up a topic model in R?;"<p><strong>Background</strong>
I am trying to fit a topic model with the following data and specification documents=140 000, words = 3000, and topics = 15. I am using the package <code>topicmodels</code> in R (3.1.2) on a Windows 7 machine (ram 24 GB, 8 cores). My problem is that the computation only goes on and on without any â€œconvergenceâ€ being produced. </p>

<p>I am using the default options in <code>LDA()</code> function in <code>topicmodels</code>:</p>

<p><em>Run model</em></p>

<pre><code>dtm2.sparse_TM &lt;- LDA(dtm2.sparse, 15)
</code></pre>

<p>The model has been running for about 72 hours â€“ and still is as I am writing.</p>

<p><strong>Question</strong>
 So, my questions are (a) if this is normal behaviour; (b) if not to the first question, do you have any suggestion on what do; (c) if yes to the first question, how can I substantially improve the speed of the computation? </p>

<p><strong>Additional information</strong>: The original data contains not 3000 words but about 3.7 million. When I ran that (on the same machine) it did not converge, not even after a couple of weeks. So I ran it with 300 words and only 500 documents (randomly selected) and not all worked fine. I used the same nr of topics and default values as before for all models. </p>

<p>So for my current model (see my question) I removed sparse terms with the help of the tm package.</p>

<p><em>Remove sparse terms</em></p>

<pre><code>dtm2.sparse &lt;- removeSparseTerms(dtm2, 0.9)
</code></pre>

<p>Thanks for the input in advance
Adel </p>
";;2838606;8970;Amir;2;34583758;"<p>You need to use <em>online</em> variational Bayes which can easily handle the training such number of documents. In online variational Bayes you train the model using mini-batches of your training samples which increases the convergence speed amazingly (refer to SGD link below).</p>

<p>For R, you can use this <a href=""http://www.jstatsoft.org/article/view/v040i13"" rel=""nofollow"">package</a>. <a href=""http://www.jstatsoft.org/index.php/jss/article/view/v040i13/v40i13.pdf"" rel=""nofollow"">Here</a> you can read more about it and how to use it too. Also look at <a href=""http://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf"" rel=""nofollow"">this</a> paper since that R package implements the method used in that paper. If possible, import their Python code uploaded <a href=""https://github.com/Blei-Lab/onlineldavb"" rel=""nofollow"">here</a> in R. I highly recommend the Python code since I had such a great experience with it for a project I recently worked on. When the model is learned, you can save the topic distributions for future use and use the input it to <code>onlineldavb.py</code> along with your test samples to integrate over the topic distributions given those unseen documents. With online variational Bayesian methods I trained an LDA with 500000 documents and 5400 words in the vocabulary data set in less than 15 hours.</p>

<p><strong>Sources</strong></p>

<blockquote>
  <ul>
  <li><a href=""https://en.wikipedia.org/wiki/Variational_Bayesian_methods"" rel=""nofollow"">Variational Bayesian Methods</a></li>
  <li><a href=""https://en.wikipedia.org/wiki/Stochastic_gradient_descent"" rel=""nofollow"">Stochastic Gradient Descent (SGD)</a></li>
  </ul>
</blockquote>
"
3633250;4320;Maksim Khaitovich;<python><pandas><machine-learning><classification><pybrain>;28154887;1;pybrain - ClassificationDataSet - how to understand the output when using SoftmaxLayer;"<p>I am trying to build first classifier with Pybrain neural network along with specialized ClassificationDataSet and I am not sure I fully understand it works.</p>

<p>So I have a pandas dataframe with 6 feature columns and 1 column for class label (Survived, just 0 or 1).</p>

<p>I build a dataset out of it:</p>

<pre><code>ds = ClassificationDataSet(6, 1, nb_classes=2)
for i in df[['Gender', 'Pclass', 'AgeFill', 'FamilySize', 'FarePerPerson', 'Deck','Survived']].values:
    ds.addSample(tuple(i[:-1]), i[-1])
ds._convertToOneOfMany()
return ds
</code></pre>

<p>Ok, I check how dataset looks like:</p>

<pre><code>for i, m in ds:
    i, m


(array([ 1.,  3.,  2.,  2.,  1.,  8.]), array([1, 0]))
(array([ 0.,  1.,  1.,  2.,  0.,  2.]), array([0, 1]))
</code></pre>

<p>And I already have a problem. What means [1,0] or [0,1]? Is it just '0' or '1' of original 'survived' column? How to get back to original values?</p>

<p>Later, when I finish with training of my network:</p>

<pre><code>net = buildNetwork(6, 6, 2, hiddenclass=TanhLayer, bias=True,  outclass=SoftmaxLayer)
trainer = BackpropTrainer(net, ds)
trainer.trainEpochs(10)
</code></pre>

<p>I will try to activate it on my another dataset (for which I want to do actual classification) and I will get a pairs of activation results for each of 2 output neurons, but how to understand which output neuron corresponds to which original class? Probably this is something obvious, but I am not able to understand it from the docs, unfortunately.</p>
";28177274;3633250;4320;Maksim Khaitovich;2;28177274;"<p>Ok, looks like pybrain uses position to determine which class it means by (0,1) or (1,0).</p>

<p>To go back to original 0 or 1 mark you need to use argmax() function. So for example if I already have a trained network and I want to validate it on the same data as I used for training I could do this:</p>

<pre><code>for inProp, num in ds:
    out = net.activate(inProp).argmax()
    if out == num.argmax():
        true+=1
    total+=1
res = true/total
</code></pre>

<p>inProp will look like a tuple of my input values for activation, num - a tuple of expected two-neuron output (either (0,1) or (1,0)) and num.argmax() will translate it into just 0 or 1 - real output. </p>

<p>I might be wrong since this is a pure heuristic, but it works in my example.</p>
"
4496286;21;metalloids;<python><machine-learning><neural-network><pybrain>;28158745;1;PyBrain: Removing individual connections;"<p>How do you remove connections in Pybrain? I'm using the one-neuron-per-module technique to customize connectivity between an input layer, a single hidden layer, and an output layer. But now I'd like to manipulate that connectivity during training, and one such manipulation is to remove a connection/edge. </p>

<p>In the Network class, I see addConnection but no removeConnection function. Is there any nice way to do this?</p>

<p>Thanks in advance.</p>
";;1615070;800;404pio;0;28165764;"<p>You have to modify Connection, replace it with IdentityConnection - <a href=""https://github.com/pybrain/pybrain/blob/master/pybrain/structure/connections/identity.py"" rel=""nofollow"">https://github.com/pybrain/pybrain/blob/master/pybrain/structure/connections/identity.py</a> - I'm not sure 100%.</p>

<p>You may look also at another connection classes.</p>
"
4401046;143;Madhan Gokul;<machine-learning><feature-selection>;28162817;0;detecting consecutive repeated letters from tweets;"<p>I am doing feature selection in machine learning, where i would like to detect words like <strong>happyyyyyyyyy</strong>,<strong>gooood</strong>,<strong>loooooooove</strong> and replace it as happy,good,love. I tried using regex to replace consecutive repeated letters with one of the same but if I do that, Works fine with <strong>looooooooove</strong> -> love and fails in <strong>goooooood</strong> -> god. I collected a list of English words like <strong>book</strong>,<strong>cool</strong>,<strong>chilling</strong>,<strong>breeze</strong>,etc but this list is not sufficient for my dataset.  I need reference to continue , as collecting a list of words is really time consuming. Thanks for your replies.</p>
";;2788862;434;Drake Sobania;0;28183999;"<p>To get your reference, use the regex <code>(.)\1+</code>
 with something like <code>grep</code> to match words from a word list (take a look at <a href=""https://stackoverflow.com/questions/4456446/dictionary-text-file"">Dictionary text file</a> for a good place to start).</p>

<p>You should end up with a list of words that have consecutive letters, so you will have your reference.</p>
"
4498098;31;Wasnuga;<matlab><machine-learning><classification><svm><cross-validation>;28167652;3;Selecting SVM parameters using cross validation and F1-scores;"<p>I need to keep track of the F1-scores while tuning C &amp; Sigma in SVM,
For example the following code keeps track of the Accuracy, I need to change it to F1-Score but I was not able to do thatâ€¦â€¦.</p>

<pre><code>%# read some training data
[labels,data] = libsvmread('./heart_scale');

%# grid of parameters
folds = 5;
[C,gamma] = meshgrid(-5:2:15, -15:2:3);

%# grid search, and cross-validation
cv_acc = zeros(numel(C),1);
    for i=1:numel(C)
cv_acc(i) = svmtrain(labels, data, ...
                sprintf('-c %f -g %f -v %d', 2^C(i), 2^gamma(i), folds));
end

%# pair (C,gamma) with best accuracy
[~,idx] = max(cv_acc);

%# now you can train you model using best_C and best_gamma
best_C = 2^C(idx);
best_gamma = 2^gamma(idx);
%# ...
</code></pre>

<p>I have seen the following two links </p>

<p><a href=""https://stackoverflow.com/questions/9047459/retraining-after-cross-validation-with-libsvm/9049225#9049225"">Retraining after Cross Validation with libsvm</a> </p>

<p><a href=""https://stackoverflow.com/questions/14024740/10-fold-cross-validation-in-one-against-all-svm-using-libsvm"">10 fold cross-validation in one-against-all SVM (using LibSVM)</a></p>

<p>I do understand that I have to first find the best C and gamma/sigma parameters over the training data, then use these two values to do a LEAVE-ONE-OUT crossvalidation classification experiment,
So what I want now is to first do a grid-search for tuning C &amp; sigma.
Please I would prefer to use MATLAB-SVM and not LIBSVM. 
Below is my code for  LEAVE-ONE-OUT crossvalidation classification.</p>

<pre><code>... clc
 clear all
close all
a = load('V1.csv');
X = double(a(:,1:12));
Y = double(a(:,13));
% train data
datall=[X,Y];
A=datall;
n = 40;
ordering = randperm(n);
B = A(ordering, :);  
good=B; 
input=good(:,1:12);
target=good(:,13);
CVO = cvpartition(target,'leaveout',1);  
cp = classperf(target);                      %# init performance tracker
svmModel=[];
for i = 1:CVO.NumTestSets                                %# for each fold
trIdx = CVO.training(i);              
teIdx = CVO.test(i);                   
%# train an SVM model over training instances

svmModel = svmtrain(input(trIdx,:), target(trIdx), ...
       'Autoscale',true, 'Showplot',false, 'Method','ls', ...
      'BoxConstraint',0.1, 'Kernel_Function','rbf', 'RBF_Sigma',0.1);
%# test using test instances
pred = svmclassify(svmModel, input(teIdx,:), 'Showplot',false);
%# evaluate and update performance object
cp = classperf(cp, pred, teIdx); 
end
%# get accuracy
accuracy=cp.CorrectRate*100
sensitivity=cp.Sensitivity*100
specificity=cp.Specificity*100
PPV=cp.PositivePredictiveValue*100
NPV=cp.NegativePredictiveValue*100
%# get confusion matrix
%# columns:actual, rows:predicted, last-row: unclassified instances
cp.CountingMatrix
recallP = sensitivity;
recallN = specificity;
precisionP = PPV;
precisionN = NPV;
f1P = 2*((precisionP*recallP)/(precisionP + recallP));
f1N = 2*((precisionN*recallN)/(precisionN + recallN));
aF1 = ((f1P+f1N)/2);
</code></pre>

<p>i have changed the code
but i making some mistakes and i am getting errors, </p>

<pre><code>a = load('V1.csv');
X = double(a(:,1:12));
Y = double(a(:,13));
% train data
datall=[X,Y];
A=datall;
n = 40;
ordering = randperm(n);
B = A(ordering, :);  
good=B; 
inpt=good(:,1:12);
target=good(:,13);
k=10;
cvFolds = crossvalind('Kfold', target, k);   %# get indices of 10-fold CV
cp = classperf(target);                      %# init performance tracker
svmModel=[];
for i = 1:k 
    testIdx = (cvFolds == i);    %# get indices of test    instances
trainIdx = ~testIdx;   
C = 0.1:0.1:1; 
S = 0.1:0.1:1; 
fscores = zeros(numel(C), numel(S)); %// Pre-allocation
for c = 1:numel(C)   
for s = 1:numel(S)
    vals = crossval(@(XTRAIN, YTRAIN, XVAL, YVAL)(fun(XTRAIN, YTRAIN, XVAL, YVAL, C(c), S(c))),inpt(trainIdx,:),target(trainIdx));
    fscores(c,s) = mean(vals);
end
end
 end

[cbest, sbest] = find(fscores == max(fscores(:)));
C_final = C(cbest);
S_final = S(sbest);    
</code></pre>

<p>.......</p>

<p>and the function.....</p>

<pre><code>.....
function fscore = fun(XTRAIN, YTRAIN, XVAL, YVAL, C, S)
svmModel = svmtrain(XTRAIN, YTRAIN, ...
   'Autoscale',true, 'Showplot',false, 'Method','ls', ...
  'BoxConstraint', C, 'Kernel_Function','rbf', 'RBF_Sigma', S);

   pred = svmclassify(svmModel, XVAL, 'Showplot',false);

   cp = classperf(YVAL, pred)
   %# get accuracy
    accuracy=cp.CorrectRate*100
    sensitivity=cp.Sensitivity*100
    specificity=cp.Specificity*100
    PPV=cp.PositivePredictiveValue*100
    NPV=cp.NegativePredictiveValue*100
    %# get confusion matrix
    %# columns:actual, rows:predicted, last-row: unclassified instances
    cp.CountingMatrix
    recallP = sensitivity;
    recallN = specificity;
    precisionP = PPV;
    precisionN = NPV;
    f1P = 2*((precisionP*recallP)/(precisionP + recallP));
    f1N = 2*((precisionN*recallN)/(precisionN + recallN));
    fscore = ((f1P+f1N)/2);

    end
</code></pre>
";;1011724;43433;Dan;1;28168462;"<p>So basically you want to take this line of yours:</p>

<pre><code>svmModel = svmtrain(input(trIdx,:), target(trIdx), ...
       'Autoscale',true, 'Showplot',false, 'Method','ls', ...
      'BoxConstraint',0.1, 'Kernel_Function','rbf', 'RBF_Sigma',0.1);
</code></pre>

<p>put it in a loop that varies your <code>'BoxConstraint'</code> and <code>'RBF_Sigma'</code> parameters and then uses <a href=""http://www.mathworks.com/help/stats/crossval.html"" rel=""nofollow""><code>crossval</code></a> to output the f1-score for that iterations combination of parameters.</p>

<p>You can use a single for-loop exactly like in your libsvm code example (i.e. using <code>meshgrid</code> and <code>1:numel()</code>, this is probably faster) or a nested for-loop. I'll use a nested loop so that you have both approaches:</p>

<pre><code>C = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300] %// you must choose your own set of values for the parameters that you want to test. You can either do it this way by explicitly typing out a list
S = 0:0.1:1 %// or you can do it this way using the : operator
fscores = zeros(numel(C), numel(S)); %// Pre-allocation
for c = 1:numel(C)   
    for s = 1:numel(S)
        vals = crossval(@(XTRAIN, YTRAIN, XVAL, YVAL)(fun(XTRAIN, YTRAIN, XVAL, YVAL, C(c), S(c)),input(trIdx,:),target(trIdx));
        fscores(c,s) = mean(vals);
    end
end

%// Then establish the C and S that gave you the bet f-score. Don't forget that c and s are just indexes though!
[cbest, sbest] = find(fscores == max(fscores(:)));
C_final = C(cbest);
S_final = S(sbest);
</code></pre>

<p>Now we just have to define the function <code>fun</code>. The docs have this to say about <code>fun</code>:</p>

<blockquote>
  <p>fun is a function handle to a function with two inputs, the training
  subset of X, XTRAIN, and the test subset of X, XTEST, as follows:</p>
  
  <p>testval = fun(XTRAIN,XTEST) Each time it is called, fun should use
  XTRAIN to fit a model, then return some criterion testval computed on
  XTEST using that fitted model.</p>
</blockquote>

<p>So <code>fun</code> needs to:</p>

<ul>
<li>output a single f-score</li>
<li>take as input a training and testing set for X and Y. <em>Note that these are both subsets of your actual training set! Think of them more like a training and validation SUBSET of your training set. Also note that crossval will split these sets up for you!</em></li>
<li>Train a classifier on the training subset (using your current <code>C</code> and <code>S</code> parameters from your loop)</li>
<li>RUN your new classifier on the test (or validation rather) subset</li>
<li>Compute and output a performance metric (in your case you want the f1-score)</li>
</ul>

<p>You'll notice that <code>fun</code> can't take any extra parameters which is why I've wrapped it in an anonymous function so that we can pass the current <code>C</code> and <code>S</code> values in. (i.e. all that <code>@(...)(fun(...))</code> stuff above. That's just a trick to ""convert"" our six parameter <code>fun</code> into the 4 parameter one required by <code>crossval</code>.</p>

<pre><code>function fscore = fun(XTRAIN, YTRAIN, XVAL, YVAL, C, S)

   svmModel = svmtrain(XTRAIN, YTRAIN, ...
       'Autoscale',true, 'Showplot',false, 'Method','ls', ...
      'BoxConstraint', C, 'Kernel_Function','rbf', 'RBF_Sigma', S);

   pred = svmclassify(svmModel, XVAL, 'Showplot',false);

   CP = classperf(YVAL, pred)

   fscore = ... %// You can do this bit the same way you did earlier
end
</code></pre>
"
4498098;31;Wasnuga;<matlab><machine-learning><classification><svm><cross-validation>;28167652;3;Selecting SVM parameters using cross validation and F1-scores;"<p>I need to keep track of the F1-scores while tuning C &amp; Sigma in SVM,
For example the following code keeps track of the Accuracy, I need to change it to F1-Score but I was not able to do thatâ€¦â€¦.</p>

<pre><code>%# read some training data
[labels,data] = libsvmread('./heart_scale');

%# grid of parameters
folds = 5;
[C,gamma] = meshgrid(-5:2:15, -15:2:3);

%# grid search, and cross-validation
cv_acc = zeros(numel(C),1);
    for i=1:numel(C)
cv_acc(i) = svmtrain(labels, data, ...
                sprintf('-c %f -g %f -v %d', 2^C(i), 2^gamma(i), folds));
end

%# pair (C,gamma) with best accuracy
[~,idx] = max(cv_acc);

%# now you can train you model using best_C and best_gamma
best_C = 2^C(idx);
best_gamma = 2^gamma(idx);
%# ...
</code></pre>

<p>I have seen the following two links </p>

<p><a href=""https://stackoverflow.com/questions/9047459/retraining-after-cross-validation-with-libsvm/9049225#9049225"">Retraining after Cross Validation with libsvm</a> </p>

<p><a href=""https://stackoverflow.com/questions/14024740/10-fold-cross-validation-in-one-against-all-svm-using-libsvm"">10 fold cross-validation in one-against-all SVM (using LibSVM)</a></p>

<p>I do understand that I have to first find the best C and gamma/sigma parameters over the training data, then use these two values to do a LEAVE-ONE-OUT crossvalidation classification experiment,
So what I want now is to first do a grid-search for tuning C &amp; sigma.
Please I would prefer to use MATLAB-SVM and not LIBSVM. 
Below is my code for  LEAVE-ONE-OUT crossvalidation classification.</p>

<pre><code>... clc
 clear all
close all
a = load('V1.csv');
X = double(a(:,1:12));
Y = double(a(:,13));
% train data
datall=[X,Y];
A=datall;
n = 40;
ordering = randperm(n);
B = A(ordering, :);  
good=B; 
input=good(:,1:12);
target=good(:,13);
CVO = cvpartition(target,'leaveout',1);  
cp = classperf(target);                      %# init performance tracker
svmModel=[];
for i = 1:CVO.NumTestSets                                %# for each fold
trIdx = CVO.training(i);              
teIdx = CVO.test(i);                   
%# train an SVM model over training instances

svmModel = svmtrain(input(trIdx,:), target(trIdx), ...
       'Autoscale',true, 'Showplot',false, 'Method','ls', ...
      'BoxConstraint',0.1, 'Kernel_Function','rbf', 'RBF_Sigma',0.1);
%# test using test instances
pred = svmclassify(svmModel, input(teIdx,:), 'Showplot',false);
%# evaluate and update performance object
cp = classperf(cp, pred, teIdx); 
end
%# get accuracy
accuracy=cp.CorrectRate*100
sensitivity=cp.Sensitivity*100
specificity=cp.Specificity*100
PPV=cp.PositivePredictiveValue*100
NPV=cp.NegativePredictiveValue*100
%# get confusion matrix
%# columns:actual, rows:predicted, last-row: unclassified instances
cp.CountingMatrix
recallP = sensitivity;
recallN = specificity;
precisionP = PPV;
precisionN = NPV;
f1P = 2*((precisionP*recallP)/(precisionP + recallP));
f1N = 2*((precisionN*recallN)/(precisionN + recallN));
aF1 = ((f1P+f1N)/2);
</code></pre>

<p>i have changed the code
but i making some mistakes and i am getting errors, </p>

<pre><code>a = load('V1.csv');
X = double(a(:,1:12));
Y = double(a(:,13));
% train data
datall=[X,Y];
A=datall;
n = 40;
ordering = randperm(n);
B = A(ordering, :);  
good=B; 
inpt=good(:,1:12);
target=good(:,13);
k=10;
cvFolds = crossvalind('Kfold', target, k);   %# get indices of 10-fold CV
cp = classperf(target);                      %# init performance tracker
svmModel=[];
for i = 1:k 
    testIdx = (cvFolds == i);    %# get indices of test    instances
trainIdx = ~testIdx;   
C = 0.1:0.1:1; 
S = 0.1:0.1:1; 
fscores = zeros(numel(C), numel(S)); %// Pre-allocation
for c = 1:numel(C)   
for s = 1:numel(S)
    vals = crossval(@(XTRAIN, YTRAIN, XVAL, YVAL)(fun(XTRAIN, YTRAIN, XVAL, YVAL, C(c), S(c))),inpt(trainIdx,:),target(trainIdx));
    fscores(c,s) = mean(vals);
end
end
 end

[cbest, sbest] = find(fscores == max(fscores(:)));
C_final = C(cbest);
S_final = S(sbest);    
</code></pre>

<p>.......</p>

<p>and the function.....</p>

<pre><code>.....
function fscore = fun(XTRAIN, YTRAIN, XVAL, YVAL, C, S)
svmModel = svmtrain(XTRAIN, YTRAIN, ...
   'Autoscale',true, 'Showplot',false, 'Method','ls', ...
  'BoxConstraint', C, 'Kernel_Function','rbf', 'RBF_Sigma', S);

   pred = svmclassify(svmModel, XVAL, 'Showplot',false);

   cp = classperf(YVAL, pred)
   %# get accuracy
    accuracy=cp.CorrectRate*100
    sensitivity=cp.Sensitivity*100
    specificity=cp.Specificity*100
    PPV=cp.PositivePredictiveValue*100
    NPV=cp.NegativePredictiveValue*100
    %# get confusion matrix
    %# columns:actual, rows:predicted, last-row: unclassified instances
    cp.CountingMatrix
    recallP = sensitivity;
    recallN = specificity;
    precisionP = PPV;
    precisionN = NPV;
    f1P = 2*((precisionP*recallP)/(precisionP + recallP));
    f1N = 2*((precisionN*recallN)/(precisionN + recallN));
    fscore = ((f1P+f1N)/2);

    end
</code></pre>
";;4983582;1;ayaz;0;30694827;"<p>I found the only problem with <code>target(trainIdx)</code>. It's a row vector so I just replaced <code>target(trainIdx)</code> with <code>target(trainIdx)</code> which is a column vector.</p>
"
775755;1251;user77005;<java><algorithm><machine-learning>;28168965;1;LinUCB exploration-exploitation algorithm does not improve results over time;"<p>I am trying to implement the Algorithm 1 given in the following paper. <a href=""http://www.research.rutgers.edu/~lihong/pub/Li10Contextual.pdf"" rel=""nofollow"">http://www.research.rutgers.edu/~lihong/pub/Li10Contextual.pdf</a></p>

<p>It is a typical exploration-exploitation algorithm. I have used the formula payoff=mean+ contant*standard deviation</p>

<p>First I ran the algorithm for a set of data I have and then I input one record from the dataset as the new input to see if it can predict the correct output. But it gave a wrong output so I gave a 0 reward and recalculated the mean and the standard deviation for that arm and continued with the algorithm. But each time it always returns the same arm. The mean does does not change as well.</p>

<p>Can someone explain to me how the mean and variation changes in this algorithm, when negative feedback is given? What must be the reason for me to alway get the same values? </p>

<p>I have used java to program. The code is as follows.</p>

<pre><code>public void LINUCB(double[] newFeature, Arm arm) {
    LOGGER.log(Level.INFO, ""LINUCB"");
    LOGGER.log(Level.INFO, ""Arm number "" + arm.getArmID());
    if (arm.isNew()) {
        arm.setFeatureMatrix(getIdentityMatrix(ConstantValues.FEATURE_DIMENSION));
        arm.setResponseVector(new double[ConstantValues.FEATURE_DIMENSION]);
    }

    double[][] invertedFeatureMatrix = invert(arm.getFeatureMatrix());
    /**The response vector is [D*M][M].  it is the multiplication of tranpose of design matrix with the user feedback provided to each trial M*/
    //TODO use gradient descent here.
    double[] theta = getSquareMatrixColumnVectorMultiplication(invertedFeatureMatrix, arm.getResponseVector());        
    double meanPayOff = getRowVectorColumnVectorMultiplication(theta, newFeature);
    System.out.print("" meanPayOff "" + meanPayOff);
    double standardDeviation = calculateUCB(newFeature, arm.getFeatureMatrix());
    System.out.print("" standardDeviation "" + standardDeviation);
    double payOffForArm = meanPayOff + standardDeviation;
    System.out.print("" payOffForArm "" + payOffForArm);
    if (payOffForArm &gt; maxPayOff) {
        maxPayOff = payOffForArm;
        //armWithMaxPayOff = arm;
        //indexOfArmWithMaxPayOff = armArrayList.indexOf(arm);
        maxPayOffArmID = arm.getArmID();
    }
    System.out.println("" "");
}

private double calculateUCB(double[] newFeature, double[][] featureMatrix) {
    double[] tmpColumVector = getSquareMatrixColumnVectorMultiplication(featureMatrix, newFeature);
    double tmpUCB = Math.sqrt(getRowVectorColumnVectorMultiplication(tmpColumVector, newFeature));
    double UCB = ConstantValues.ALPHA * tmpUCB;
    return UCB;
}
</code></pre>

<p>alpha is set to 0.3.</p>
";;4400558;172;nbui;1;35147677;"<p>For each round, LinUCB should be updating the Upper Confidence Bound of each arm based on its feature vector. I think you've implemented the algorithm incorrectly.</p>

<ul>
<li>You are passing a feature <em>and</em> an arm. You should be inspecting the features <em>of the</em> arm. Your newFeature argument is possibly independent of the arm and in which case, any correlation with the context (in this case, the feature vector) and the rewards is thrown out the window.</li>
<li>Theta is used to compute an expected payoff assuming it is linear in the arm's feature vector with the coefficient vector theta. You're multiplying it with the newFeature vector when it should be with the arm's features.</li>
<li>Unless it is a static variable, maxPayOff does not track the maximum UCB of all arms. It looks like a local variable within the conditional statement and thus, is not able to capture the maximum value of all arms.</li>
<li>Lastly, check that your rewards are bounded between 0 and 1. It is a very common mistake and can cause weird behaviors.</li>
</ul>
"
3601684;147;mah.aziz;<machine-learning><classification><weka><data-mining>;28174651;3;Products category detection using weka;"<p>I'm working on an ecommerce application. Most of the products i have contains the category attribute, but some do not (about 70-30%). I was trying to use <a href=""http://www.cs.waikato.ac.nz/ml/weka/"" rel=""nofollow"">Weka</a> to detect the category, but the attributes i have are strings <code>(name, brand, price, description, category)</code> so all classifiers are not working as it need the attributes to be numeric, nominal, or binary. </p>

<p>Did any one faced such problem before ?  </p>
";;1118582;92;Aviator;0;28348012;"<p>just make discrete continuous attributes and then it will work, because some of the algorithms does not work with continues values.  </p>
"
3601684;147;mah.aziz;<machine-learning><classification><weka><data-mining>;28174651;3;Products category detection using weka;"<p>I'm working on an ecommerce application. Most of the products i have contains the category attribute, but some do not (about 70-30%). I was trying to use <a href=""http://www.cs.waikato.ac.nz/ml/weka/"" rel=""nofollow"">Weka</a> to detect the category, but the attributes i have are strings <code>(name, brand, price, description, category)</code> so all classifiers are not working as it need the attributes to be numeric, nominal, or binary. </p>

<p>Did any one faced such problem before ?  </p>
";;5580331;139;Arif;0;34494084;"<p>Use ""StringToWordVector"" filter that will convert your string attribute(s) to numeric attributes.</p>
"
3736362;1540;justanothercoder;<c++><machine-learning><neural-network>;28175820;0;Backpropagation algorithm in neural network;"<p>I have some troubles implementing backpropagation in neural network. This implementation is using ideas from slides of Andrew Ng's course on machine learning from Coursera (here is the link <a href=""https://www.coursera.org/course/ml"" rel=""nofollow"">https://www.coursera.org/course/ml</a>). I think that I have understood the algorithm, but there is some subtle error in the code.</p>

<p>I'm using a network with 1 input layer, 1 hidden layer and 1 output layer. They have 2 + 1, 2 + 1, 1 neurons respectively (+1 is for bias).
When I tried to implement logical AND and logical OR everything worked great, and network learned to give correct values. But then I tried to implement XNOR (a XNOR b = NOT (a XOR b)).</p>

<p>I used 4 examples:</p>

<ul>
<li>0 0 1</li>
<li>0 1 0</li>
<li>1 0 0</li>
<li>1 1 1</li>
</ul>

<p>But suddenly, on this function gradient doesn't go anywhere. In the beginning I initialize weights with random small numbers (from -0.01 to 0.01). The output is near 0.5. Then I am doing gradient descent. Output is still always near 0.5 on any input.</p>

<p>I want to know how to fix this problem.</p>

<p>Here's the code:</p>

<pre><code>#include &lt;iostream&gt;
#include &lt;fstream&gt;
#include &lt;vector&gt;
#include &lt;algorithm&gt;

// contains matrix and Vector classes.
// Vector is just like std::valarray, but is compatible with my matrix.
#include ""matrix.hpp"" 

size_t L;
std::vector&lt; Vector&lt;double&gt; &gt; layers;
std::vector&lt; matrix&lt;double&gt; &gt; theta;

struct Example
{
    Vector&lt;double&gt; x;
    Vector&lt;double&gt; y;
};

using TrainingSet = std::vector&lt;Example&gt;;

TrainingSet examples;

double g(double x)
{
    return 1 / (1 + exp(-x));
}

void forwardPropagate(Vector&lt;double&gt; x)
{
    for ( size_t i = 1; i &lt; layers[0].size(); ++i )
        layers[0][i] = x[i - 1];

    for ( size_t i = 0; i &lt; L - 1; ++i )
    {
        auto z = theta[i] * layers[i];
        for ( size_t j = 1; j &lt; layers[i + 1].size(); ++j )
            layers[i + 1][j] = g(z[j - 1]);
    }
}

void backwardPropagate(Vector&lt;double&gt; y, std::vector&lt; matrix&lt;double&gt; &gt;&amp; delta)
{
    auto err = layers.back().slice(1) - y;

    for ( int i = L - 2; i &gt;= 0; --i )
    {   
        delta[i] += asMatrix(err) * asMatrix(layers[i]).transpose();

        auto gdz = layers[i] * (Vector&lt;double&gt;(layers[i].size(), 1.0) - layers[i]);
        auto tmp = theta[i].transpose() * err * gdz;
        err = tmp.slice(1);
    }
}

double costFunction(const TrainingSet&amp; examples)
{
    double result = 0.0;

    for ( const auto&amp; example : examples )
    {
        std::cout &lt;&lt; layers.back()[1] &lt;&lt; '\n';

        forwardPropagate(example.x);
        for ( size_t k = 1; k &lt; layers.back().size(); ++k )
        {
            auto h = layers.back()[k];
            auto y = example.y[k - 1];
            result += y * log(h) + (1 - y) * log(1 - h);
        }
    }

    return (-result) / examples.size();
}

void computeGradient(std::vector&lt; matrix&lt;double&gt; &gt;&amp; delta, const TrainingSet&amp; examples)
{
    for ( auto&amp; m : delta )
        m.fillWith(0);

    for ( auto example : examples )
    {
        forwardPropagate(example.x);
        backwardPropagate(example.y, delta);
    }

    for ( auto&amp; m : delta )
        m /= examples.size();
}

void gradientDescentStep(const std::vector&lt; matrix&lt;double&gt; &gt;&amp; gradient)
{
    const double alpha = 0.01;

    for ( size_t i = 0; i &lt; L - 1; ++i )
        theta[i] -= alpha / examples.size() * gradient[i];
}

double gradientDescent(const TrainingSet&amp; examples)
{
    const double eps = 0.0000001;

    double prev, cur;
    cur = costFunction(examples);

    size_t iterations = 0;
    const size_t max_iterations = 200000000;

    std::vector&lt; matrix&lt;double&gt; &gt; delta;
    delta.reserve(L - 1);
    for ( size_t i = 0; i &lt; L - 1; ++i )
        delta.emplace_back(theta[i].rows(), theta[i].cols());

    do
    {
        prev = cur;
        computeGradient(delta, examples);
        gradientDescentStep(delta);
        cur = costFunction(examples);

    } while ( fabs(cur - prev) &gt;= eps &amp;&amp; iterations++ &lt; max_iterations );

    std::cout &lt;&lt; ""Made "" &lt;&lt; iterations &lt;&lt; "" iterations\n"";

    return cur;
}

int main()
{
    std::ifstream fin(""input.txt"");    
    std::istream&amp; in = fin;    

    std::cout.sync_with_stdio(false);

    in &gt;&gt; L;
    std::vector&lt;size_t&gt; architecture(L);

    for ( size_t i = 0; i &lt; L; ++i )
        in &gt;&gt; architecture[i];

    layers.reserve(L);
    for ( size_t i = 0; i &lt; L; ++i )
    {
        layers.emplace_back(1 + architecture[i]);
        layers.back()[0] = 1;
    }

    const double eps = 0.01;    

    theta.reserve(L - 1);
    for ( size_t i = 0; i &lt; L - 1; ++i )
    {
        theta.emplace_back(layers[i + 1].size() - 1, layers[i].size());
        theta[i].randomInitialize(eps);
    }

    size_t number_of_examples;
    in &gt;&gt; number_of_examples;

    examples.reserve(number_of_examples);
    for ( size_t i = 0; i &lt; number_of_examples; ++i )
    {
        auto x = Vector&lt;double&gt;(architecture.front());
        auto y = Vector&lt;double&gt;(architecture.back());

        for ( size_t j = 0; j &lt; architecture.front(); ++j )
            in &gt;&gt; x[j];

        for ( size_t j = 0; j &lt; architecture.back(); ++j )
            in &gt;&gt; y[j];

        examples.emplace_back(Example{x, y});
    }

    for ( auto example : examples )
    {
        forwardPropagate(example.x);
        std::cout &lt;&lt; layers.back()[1] &lt;&lt; '\n';
    }

    for ( size_t i = 0; i &lt; theta.size(); ++i )
        std::cout &lt;&lt; ""Î¸["" &lt;&lt; i &lt;&lt; ""] = "" &lt;&lt; theta[i];

    gradientDescent(examples);

    for ( size_t i = 0; i &lt; theta.size(); ++i )
        std::cout &lt;&lt; ""Î¸["" &lt;&lt; i &lt;&lt; ""] = "" &lt;&lt; theta[i];

    std::cout &lt;&lt; ""\n\n\n"";

    for ( auto example : examples )
    {
        forwardPropagate(example.x);
        std::cout &lt;&lt; layers.back()[1] &lt;&lt; '\n';
    }

    return 0;
}
</code></pre>
";28251364;3736362;1540;justanothercoder;1;28251364;"<p>Finally I have realized what's wrong. The problem is not in the code itself. The thing is that cost function on such network configuration with XOR has local minimum. So, I came there and was stuck.</p>

<p>Solution is to make a step in random direction until you made it out of local minimum. It allows you to get to the global minimum quite fast.</p>
"
3736362;1540;justanothercoder;<c++><machine-learning><neural-network>;28175820;0;Backpropagation algorithm in neural network;"<p>I have some troubles implementing backpropagation in neural network. This implementation is using ideas from slides of Andrew Ng's course on machine learning from Coursera (here is the link <a href=""https://www.coursera.org/course/ml"" rel=""nofollow"">https://www.coursera.org/course/ml</a>). I think that I have understood the algorithm, but there is some subtle error in the code.</p>

<p>I'm using a network with 1 input layer, 1 hidden layer and 1 output layer. They have 2 + 1, 2 + 1, 1 neurons respectively (+1 is for bias).
When I tried to implement logical AND and logical OR everything worked great, and network learned to give correct values. But then I tried to implement XNOR (a XNOR b = NOT (a XOR b)).</p>

<p>I used 4 examples:</p>

<ul>
<li>0 0 1</li>
<li>0 1 0</li>
<li>1 0 0</li>
<li>1 1 1</li>
</ul>

<p>But suddenly, on this function gradient doesn't go anywhere. In the beginning I initialize weights with random small numbers (from -0.01 to 0.01). The output is near 0.5. Then I am doing gradient descent. Output is still always near 0.5 on any input.</p>

<p>I want to know how to fix this problem.</p>

<p>Here's the code:</p>

<pre><code>#include &lt;iostream&gt;
#include &lt;fstream&gt;
#include &lt;vector&gt;
#include &lt;algorithm&gt;

// contains matrix and Vector classes.
// Vector is just like std::valarray, but is compatible with my matrix.
#include ""matrix.hpp"" 

size_t L;
std::vector&lt; Vector&lt;double&gt; &gt; layers;
std::vector&lt; matrix&lt;double&gt; &gt; theta;

struct Example
{
    Vector&lt;double&gt; x;
    Vector&lt;double&gt; y;
};

using TrainingSet = std::vector&lt;Example&gt;;

TrainingSet examples;

double g(double x)
{
    return 1 / (1 + exp(-x));
}

void forwardPropagate(Vector&lt;double&gt; x)
{
    for ( size_t i = 1; i &lt; layers[0].size(); ++i )
        layers[0][i] = x[i - 1];

    for ( size_t i = 0; i &lt; L - 1; ++i )
    {
        auto z = theta[i] * layers[i];
        for ( size_t j = 1; j &lt; layers[i + 1].size(); ++j )
            layers[i + 1][j] = g(z[j - 1]);
    }
}

void backwardPropagate(Vector&lt;double&gt; y, std::vector&lt; matrix&lt;double&gt; &gt;&amp; delta)
{
    auto err = layers.back().slice(1) - y;

    for ( int i = L - 2; i &gt;= 0; --i )
    {   
        delta[i] += asMatrix(err) * asMatrix(layers[i]).transpose();

        auto gdz = layers[i] * (Vector&lt;double&gt;(layers[i].size(), 1.0) - layers[i]);
        auto tmp = theta[i].transpose() * err * gdz;
        err = tmp.slice(1);
    }
}

double costFunction(const TrainingSet&amp; examples)
{
    double result = 0.0;

    for ( const auto&amp; example : examples )
    {
        std::cout &lt;&lt; layers.back()[1] &lt;&lt; '\n';

        forwardPropagate(example.x);
        for ( size_t k = 1; k &lt; layers.back().size(); ++k )
        {
            auto h = layers.back()[k];
            auto y = example.y[k - 1];
            result += y * log(h) + (1 - y) * log(1 - h);
        }
    }

    return (-result) / examples.size();
}

void computeGradient(std::vector&lt; matrix&lt;double&gt; &gt;&amp; delta, const TrainingSet&amp; examples)
{
    for ( auto&amp; m : delta )
        m.fillWith(0);

    for ( auto example : examples )
    {
        forwardPropagate(example.x);
        backwardPropagate(example.y, delta);
    }

    for ( auto&amp; m : delta )
        m /= examples.size();
}

void gradientDescentStep(const std::vector&lt; matrix&lt;double&gt; &gt;&amp; gradient)
{
    const double alpha = 0.01;

    for ( size_t i = 0; i &lt; L - 1; ++i )
        theta[i] -= alpha / examples.size() * gradient[i];
}

double gradientDescent(const TrainingSet&amp; examples)
{
    const double eps = 0.0000001;

    double prev, cur;
    cur = costFunction(examples);

    size_t iterations = 0;
    const size_t max_iterations = 200000000;

    std::vector&lt; matrix&lt;double&gt; &gt; delta;
    delta.reserve(L - 1);
    for ( size_t i = 0; i &lt; L - 1; ++i )
        delta.emplace_back(theta[i].rows(), theta[i].cols());

    do
    {
        prev = cur;
        computeGradient(delta, examples);
        gradientDescentStep(delta);
        cur = costFunction(examples);

    } while ( fabs(cur - prev) &gt;= eps &amp;&amp; iterations++ &lt; max_iterations );

    std::cout &lt;&lt; ""Made "" &lt;&lt; iterations &lt;&lt; "" iterations\n"";

    return cur;
}

int main()
{
    std::ifstream fin(""input.txt"");    
    std::istream&amp; in = fin;    

    std::cout.sync_with_stdio(false);

    in &gt;&gt; L;
    std::vector&lt;size_t&gt; architecture(L);

    for ( size_t i = 0; i &lt; L; ++i )
        in &gt;&gt; architecture[i];

    layers.reserve(L);
    for ( size_t i = 0; i &lt; L; ++i )
    {
        layers.emplace_back(1 + architecture[i]);
        layers.back()[0] = 1;
    }

    const double eps = 0.01;    

    theta.reserve(L - 1);
    for ( size_t i = 0; i &lt; L - 1; ++i )
    {
        theta.emplace_back(layers[i + 1].size() - 1, layers[i].size());
        theta[i].randomInitialize(eps);
    }

    size_t number_of_examples;
    in &gt;&gt; number_of_examples;

    examples.reserve(number_of_examples);
    for ( size_t i = 0; i &lt; number_of_examples; ++i )
    {
        auto x = Vector&lt;double&gt;(architecture.front());
        auto y = Vector&lt;double&gt;(architecture.back());

        for ( size_t j = 0; j &lt; architecture.front(); ++j )
            in &gt;&gt; x[j];

        for ( size_t j = 0; j &lt; architecture.back(); ++j )
            in &gt;&gt; y[j];

        examples.emplace_back(Example{x, y});
    }

    for ( auto example : examples )
    {
        forwardPropagate(example.x);
        std::cout &lt;&lt; layers.back()[1] &lt;&lt; '\n';
    }

    for ( size_t i = 0; i &lt; theta.size(); ++i )
        std::cout &lt;&lt; ""Î¸["" &lt;&lt; i &lt;&lt; ""] = "" &lt;&lt; theta[i];

    gradientDescent(examples);

    for ( size_t i = 0; i &lt; theta.size(); ++i )
        std::cout &lt;&lt; ""Î¸["" &lt;&lt; i &lt;&lt; ""] = "" &lt;&lt; theta[i];

    std::cout &lt;&lt; ""\n\n\n"";

    for ( auto example : examples )
    {
        forwardPropagate(example.x);
        std::cout &lt;&lt; layers.back()[1] &lt;&lt; '\n';
    }

    return 0;
}
</code></pre>
";28251364;6501886;176;Jjoseph;1;44398354;"<p>Generally for random initialization weights are in the range of ~[-3, 3]. The initial greater amounts of error (by having a larger weights) helps ""jump"" the weights into converging to the proper areas. Yes the XOR problem has local minima but your network should easily be able to converge to the proper answer with only a few hidden nodes. </p>

<p>You shouldn't ""need"" to take steps out of local minima, it should easily converge to the correct optima. XOR problem with the network structure being [2,2,1] has such a small amount of weights that you could essentially ""random initialization"" your weights to the correct optima relatively quickly. (Because the search space is small)</p>

<p><em>note / edit</em>  if your network only has 2 hidden nodes there is a good change it will get stuck at local minima. a network even the size of ~[2,7,1] should be able to converge without the random steps. </p>
"
712997;13241;lollercoaster;<python><numpy><machine-learning><scikit-learn>;28178763;15;sklearn: use Pipeline in a RandomizedSearchCV?;"<p>I'd like to be able to use pipelines in the RandomizedSearchCV construct in sklearn. However right now I believe that only estimators are supported. Here's an example of what I'd like to be able to do:</p>

<pre><code>import numpy as np

from sklearn.grid_search import RandomizedSearchCV
from sklearn.datasets import load_digits
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler    
from sklearn.pipeline import Pipeline

# get some data
iris = load_digits()
X, y = iris.data, iris.target

# specify parameters and distributions to sample from
param_dist = {'C': [1, 10, 100, 1000], 
          'gamma': [0.001, 0.0001], 
          'kernel': ['rbf', 'linear'],}

# create pipeline with a scaler 
steps = [('scaler', StandardScaler()), ('rbf_svm', SVC())]
pipeline = Pipeline(steps)

# do search
search = RandomizedSearchCV(pipeline, 
param_distributions=param_dist, n_iter=50)
search.fit(X, y)

print search.grid_scores_
</code></pre>

<p>If you just run like this, you'll get the following error:</p>

<pre><code>ValueError: Invalid parameter kernel for estimator Pipeline
</code></pre>

<p>Is there a good way to do this in sklearn?</p>
";28179991;1190430;5186;Artem Sobolev;21;28179991;"<p><code>RandomizedSearchCV</code>, as well as <code>GridSearchCV</code>, <strong>do</strong> support pipelines (in fact, they're independent of their implementation, and pipelines are designed to be equivalent to usual classifiers).</p>

<p>The key to the issue is pretty straightforward if you think, what parameters should search be done over. Since pipeline consists of many objects (several transformers + a classifier), one may want to find optimal parameters both for the classifier and transformers. Thus, you need to somehow distinguish where to get / set properties from / to.</p>

<p>So what you need to do is to say that you want to find a value for, say, not just some abstract <code>gamma</code> (which pipeline doesn't have at all), but <code>gamma</code> of pipeline's classifier, which is called in your case <code>rbf_svm</code> (that also justifies the need for names). This can be achieved using double underscore syntax, widely used in sklearn for nested models:</p>

<pre><code>param_dist = {
          'rbf_svm__C': [1, 10, 100, 1000], 
          'rbf_svm__gamma': [0.001, 0.0001], 
          'rbf_svm__kernel': ['rbf', 'linear'],
}
</code></pre>
"
712997;13241;lollercoaster;<python><numpy><machine-learning><scikit-learn>;28178763;15;sklearn: use Pipeline in a RandomizedSearchCV?;"<p>I'd like to be able to use pipelines in the RandomizedSearchCV construct in sklearn. However right now I believe that only estimators are supported. Here's an example of what I'd like to be able to do:</p>

<pre><code>import numpy as np

from sklearn.grid_search import RandomizedSearchCV
from sklearn.datasets import load_digits
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler    
from sklearn.pipeline import Pipeline

# get some data
iris = load_digits()
X, y = iris.data, iris.target

# specify parameters and distributions to sample from
param_dist = {'C': [1, 10, 100, 1000], 
          'gamma': [0.001, 0.0001], 
          'kernel': ['rbf', 'linear'],}

# create pipeline with a scaler 
steps = [('scaler', StandardScaler()), ('rbf_svm', SVC())]
pipeline = Pipeline(steps)

# do search
search = RandomizedSearchCV(pipeline, 
param_distributions=param_dist, n_iter=50)
search.fit(X, y)

print search.grid_scores_
</code></pre>

<p>If you just run like this, you'll get the following error:</p>

<pre><code>ValueError: Invalid parameter kernel for estimator Pipeline
</code></pre>

<p>Is there a good way to do this in sklearn?</p>
";28179991;7554627;131;dzenilee;5;59325876;"<p>I think <a href=""https://medium.com/@dzenilee/randomized-or-grid-search-with-pipeline-cheatsheet-719c72eda68?source=friends_link&amp;sk=c04ad319393084c2c7da23c5b5abcdf3"" rel=""noreferrer"">this</a> is what you need (section 3). </p>

<p><code>pipeline.get_params().keys()</code> -> make sure your param grid keys match those returned by this.</p>
"
2183302;191;Rich Gray;<python><machine-learning><scikit-learn>;28181197;0;How to convert data in to a format suitable to be used for a multi-class classification task, using scikit-learn?;"<p>I am trying to use scikit-learn to make predictions on data that I have obtained from the Ebay API. I want to know the best way to go about converting this data in to a format which can be used for a multi-class classification problem with scikit-learn. The only way it explains of importing external data on the scikit-learn website seems to be to load from an svmlight file, shown here:</p>

<p><a href=""http://scikit-learn.org/stable/datasets/"" rel=""nofollow"">http://scikit-learn.org/stable/datasets/</a></p>

<p>I want to use the data on a number of algorithms and not just an SVM. I have the data in a dict object using the following code:</p>

<pre><code>from ebaysdk.finding import Connection as Finding
from requests.exceptions import ConnectionError

try:
    api = Finding(appid=""my_app_id"")
    api_request = {'keywords': 'Samsung Galaxy S5 G900R4 16GB',     'itemFilter': [{'name': 'SoldItemsOnly',
 'value': 'true'}], 'outputSelector': 'SellerInfo', 'GLOBAL-ID': 'EBAY-      GB'}
    response = api.execute('findCompletedItems', api_request)
    #print(response.dict())
except ConnectionError as e:
    print(e)
    print(e.response.dict())
</code></pre>

<p>I have searched online for tutorials or an explanation of how to go about doing this, but I can't find anything useful, which goes in to enough detail on how to convert the data to the format I require and what exactly that format needs to be.</p>

<p>Please can someone give me some guidance on if I should use an svmlight file to achieve this and how to go about doing it, or if there are any other suggestions for importing my data in. I am new to Machine Learning, as well as Python and scikit-learn, so any help is appreciated.</p>

<p>Here is an example of the format that the raw data is in:</p>

<pre><code>{'autoPay': 'true',
  'condition': {'conditionDisplayName': 'Used', 'conditionId': '3000'},
  'country': 'US',
  'galleryURL': 'http://thumbs4.ebaystatic.com/m/mO-HwGeodkgYX6sRbPyFsrg/140.jpg',
  'globalId': 'EBAY-US',
  'isMultiVariationListing': 'false',
  'itemId': '201265198351',
  'listingInfo': {'bestOfferEnabled': 'false',
   'buyItNowAvailable': 'false',
   'endTime': '2015-01-16T00:23:19.000Z',
   'gift': 'false',
   'listingType': 'StoreInventory',
   'startTime': '2015-01-15T15:47:38.000Z'},
  'location': 'Dandridge,TN,USA',
  'paymentMethod': 'PayPal',
  'postalCode': '37725',
  'primaryCategory': {'categoryId': '9355',
   'categoryName': 'Cell Phones &amp; Smartphones'},
  'productId': {'_type': 'ReferenceID', 'value': '182557948'},
  'returnsAccepted': 'true',
  'sellerInfo': {'feedbackRatingStar': 'Turquoise',
   'feedbackScore': '445',
   'positiveFeedbackPercent': '100.0',
   'sellerUserName': 'dadscritter15'},
  'sellingStatus': {'convertedCurrentPrice': {'_currencyId': 'USD',
    'value': '279.99'},
   'currentPrice': {'_currencyId': 'USD', 'value': '279.99'},
   'sellingState': 'Ended'},
  'shippingInfo': {'expeditedShipping': 'true',
   'handlingTime': '2',
   'oneDayShippingAvailable': 'false',
   'shipToLocations': ['US',
    'CA',
    'GB',
    'AU',
    'AT',
    'BE',
    'FR',
    'DE',
    'IT',
    'JP',
    'ES',
    'CH',
    'NL',
    'CN',
    'HK',
    'MX'],
   'shippingServiceCost': {'_currencyId': 'USD', 'value': '0.0'},
   'shippingType': 'FlatDomesticCalculatedInternational'},
  'title': 'Samsung Galaxy S5 SM-G900R4 (Latest Model) 16GB White (U.S. Cellular) Verify ESN',
  'topRatedListing': 'false',
  'viewItemURL': 'http://www.ebay.com/itm/Samsung-Galaxy-S5-SM-G900R4-Latest-Model-16GB-White-U-S-Cellular-Verify-ESN-/201265198351?pt=LH_DefaultDomain_0'},
 {'autoPay': 'true',
  'condition': {'conditionDisplayName': 'Used', 'conditionId': '3000'},
  'country': 'US',
  'galleryURL': 'http://thumbs4.ebaystatic.com/m/mO-HwGeodkgYX6sRbPyFsrg/140.jpg',
  'globalId': 'EBAY-US',
  'isMultiVariationListing': 'false',
  'itemId': '201265198351',
  'listingInfo': {'bestOfferEnabled': 'false',
   'buyItNowAvailable': 'false',
   'endTime': '2015-01-16T00:23:19.000Z',
   'gift': 'false',
   'listingType': 'StoreInventory',
   'startTime': '2015-01-15T15:47:38.000Z'},
  'location': 'Dandridge,TN,USA',
  'paymentMethod': 'PayPal',
  'postalCode': '37725',
  'primaryCategory': {'categoryId': '9355',
   'categoryName': 'Cell Phones &amp; Smartphones'},
  'productId': {'_type': 'ReferenceID', 'value': '182557948'},
  'returnsAccepted': 'true',
  'sellerInfo': {'feedbackRatingStar': 'Turquoise',
   'feedbackScore': '445',
   'positiveFeedbackPercent': '100.0',
   'sellerUserName': 'dadscritter15'},
  'sellingStatus': {'convertedCurrentPrice': {'_currencyId': 'USD',
    'value': '279.99'},
   'currentPrice': {'_currencyId': 'USD', 'value': '279.99'},
   'sellingState': 'Ended'},
  'shippingInfo': {'expeditedShipping': 'true',
   'handlingTime': '2',
   'oneDayShippingAvailable': 'false',
   'shipToLocations': ['US',
    'CA',
    'GB',
    'AU',
    'AT',
    'BE',
    'FR',
    'DE',
    'IT',
    'JP',
    'ES',
    'CH',
    'NL',
    'CN',
    'HK',
    'MX'],
   'shippingServiceCost': {'_currencyId': 'USD', 'value': '0.0'},
   'shippingType': 'FlatDomesticCalculatedInternational'},
  'title': 'Samsung Galaxy S5 SM-G900R4 (Latest Model) 16GB White (U.S. Cellular) Verify ESN',
  'topRatedListing': 'false',
  'viewItemURL': 'http://www.ebay.com/itm/Samsung-Galaxy-S5-SM-G900R4-Latest-Model-16GB-White-U-S-Cellular-Verify-ESN-/201265198351?pt=LH_DefaultDomain_0'}
</code></pre>
";28181530;1330293;33731;elyase;1;28181530;"<p>If you have a list of dicts, with one json/dict representing a product you can do:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([dict1, dict2])
&gt;&gt;&gt; df
autoPay condition   country galleryURL  globalId    isMultiVariationListing itemId  listingInfo location    paymentMethod   postalCode  primaryCategory productId   returnsAccepted sellerInfo  sellingStatus   shippingInfo    title   topRatedListing viewItemURL
0   true    {u'conditionId': u'3000', u'conditionDisplayNa...   US  http://thumbs4.ebaystatic.com/m/mO-HwGeodkgYX6...   EBAY-US false   201265198351    {u'listingType': u'StoreInventory', u'gift': u...   Dandridge,TN,USA    PayPal  37725   {u'categoryId': u'9355', u'categoryName': u'Ce...   {u'_type': u'ReferenceID', u'value': u'1825579...   true    {u'feedbackRatingStar': u'Turquoise', u'positi...   {u'currentPrice': {u'_currencyId': u'USD', u'v...   {u'expeditedShipping': u'true', u'shipToLocati...   Samsung Galaxy S5 SM-G900R4 (Latest Model) 16G...   false   http://www.ebay.com/itm/Samsung-Galaxy-S5-SM-G...
1   false   {u'conditionId': u'3000', u'conditionDisplayNa...   US  http://thumbs4.ebaystatic.com/m/mO-HwGeodkgYX6...   EBAY-US false   201265198351    {u'listingType': u'StoreInventory', u'gift': u...   Dandridge,TN,USA    PayPal  37725   {u'categoryId': u'9355', u'categoryName': u'Ce...   {u'_type': u'ReferenceID', u'value': u'1825579...   true    {u'feedbackRatingStar': u'Turquoise', u'positi...   {u'currentPrice': {u'_currencyId': u'USD', u'v...   {u'expeditedShipping': u'true', u'shipToLocati...   Samsung Galaxy S5 SM-G900R4 (Latest Model) 16G...   false   http://www.ebay.com/itm/Samsung-Galaxy-S5-SM-G...
</code></pre>

<p>Then you can use the columns of this df as input to your model. You probably want to extract the data in the nested dicts.</p>
"
4501155;154;Tung;<python-2.7><machine-learning>;28183868;0;How to distribute values into group in python;"<p>I have a dataset of actions doing over time, an attribute 'Hour' ( contains values from 0 ->23 ). Now I want to create another attribute, say 'PartOfDay', which group 24 hours into 4 parts. For tuples have 'Hour' value of 0 to 5, then the 'PartOfDay' value should be 1; if 'Hour' value in [6,11], then the 'PartOfDay' value should be 2;...How can I do?</p>

<p>The codes would do this:</p>

<pre><code>train['PartOfDay']=1
train.loc[(train.Hour&gt;=6) &amp; (train.hour&lt;=11),'PartOfDay']=2
train.loc[(train.Hour&gt;=12) &amp; (train.hour&lt;=17),'PartOfDay']=3
train.loc[(train.Hour&gt;=18) &amp; (train.hour&lt;=23),'PartOfDay']=4
</code></pre>

<p>but it seems not so beautiful, I would like to know a more decent one if possible</p>

<p>Thank you for all your supports!!</p>
";28188982;1056563;45835;StephenBoesch;0;28188982;"<p>While it is not clear what train.loc represents, a general approach to your problem is to use modulus function to set the RHS:</p>

<pre><code> 1 + int(train.Hour / 6)
</code></pre>
"
4366295;1;Kensuke Koshijima;<r><optimization><machine-learning><nlp>;28191830;-1;Regularized Latent Semantic Indexing in R;"<p>I am trying to implement the Regularized Latent Semantic Indexing (RLSI) algorithm on R.<br>
The original paper can be found here:<br>
<a href=""http://research.microsoft.com/en-us/people/hangli/sigirfp372-wang.pdf"" rel=""nofollow"">http://research.microsoft.com/en-us/people/hangli/sigirfp372-wang.pdf</a></p>

<p>Below is my code.</p>

<p>Here, I generate a matrix D from two matrices U and V.  Each column of U correspond to a topic vector, and it is made to be sparse.  After that, I apply RLSI on the D matrix to see if I can factorize it into two matrices, one of which has sparse vectors like U.  </p>

<p>However, the resulting U is far from being sparse.  Actually, every element of it is filled with numbers.</p>

<p>Is there something wrong with my code?</p>

<p>Thank you very much in advance.</p>

<pre><code>library(magrittr)

# functions
updateU &lt;- function(D,U,V){
    S &lt;- V %*% t(V)
    R &lt;- D %*% t(V)

    for(m in 1:M){
        u_m &lt;- rep(0, K)

        u_previous &lt;- u_m
        diff_u &lt;- 100
        while(diff_u &gt; 0.1){
            for(k in 1:K){
                w_mk &lt;- R[m,k] - S[k,-k] %*% U[m,-k]
                in_hinge &lt;- (abs(w_mk) - 0.5 * lambda_1)
                u_m[k] &lt;- (ifelse(in_hinge &gt; 0, in_hinge, 0) * ifelse(w_mk &gt;= 0, 1, -1)) / S[k,k]
            }
            diff_u &lt;- sum(u_m - u_previous)
            u_previous &lt;- u_m
        }
        U[m,] &lt;- u_m
    }
    return(U)
}

updateV &lt;- function(D,U,V){
    Sigma &lt;- solve(t(U) %*% U + lambda_2 * diag(K))
    Phi &lt;- t(U) %*% D
    V &lt;- Sigma %*% Phi
    return(V)
}


# Set constants
M &lt;- 5000
N &lt;- 1000
K &lt;- 30
lambda_1 &lt;- 1
lambda_2 &lt;- 0.5


# Create D 
originalU &lt;- c(rpois(50000, lambda = 10), rep(0, 100000)) %&gt;% sample(., 150000) %&gt;% matrix(., M, K)
originalV &lt;- rpois(30000, lambda = 5) %&gt;% sample(., 30000) %&gt;% matrix(., K, N)
D &lt;- originalU %*% originalV


# Initialize U and V
V &lt;- matrix(rpois(30000, lambda = 5), K, N)
U &lt;- matrix(0, M, K)


# Run RLSI (iterate 100 times for now)
for(t in 1:100){
    cat(t,"":"")
    U &lt;- updateU(D,U,V)
    V &lt;- updateV(D,U,V)
    loss &lt;- sum((D - U %*% V) ^ 2)
    cat(loss, ""\n"")
}
</code></pre>
";;4366295;1;Kensuke Koshijima;0;28205985;"<p>I've got it. Each row in U has to be set to a zero vector each time updateU function is run.</p>
"
4503550;71;Supersquirrel;<python><python-3.x><machine-learning><nltk><wordnet>;28196103;4;Find Python NLTK Wordnet Synsets for a each item of a list;"<p>I've been learning basic python, but I am new to NLTK. I want to use nltk to extract hyponyms for a given list of words. It works fine when I enter every term manually, but it does not seem to work when I try to iterate through items of a list.</p>

<p>This works: </p>

<pre><code>from nltk.corpus import wordnet as wn

syn_sets = wn.synsets(""car"")

for syn_set in syn_sets:
    print(syn_set, syn_set.lemma_names())
    print(syn_set.hyponyms())
</code></pre>

<p>But how do I get Wordnet methods to work with a list of items like</p>

<pre><code>token = [""cat"", ""dog"", ""car""]
syn_sets = wn.synsets((*get each item from the list*))
</code></pre>

<p>in a loop? </p>

<p>Thank you!</p>
";;841830;24209;Darren Cook;1;28203493;"<p>I believe you have no choice but to loop through your words. I modified your code to have an outer loop, and it seems to work:</p>

<pre><code>from nltk.corpus import wordnet as wn

tokens = [""cat"", ""dog"", ""car""]

for token in tokens:
    syn_sets = wn.synsets(token)
    for syn_set in syn_sets:
        print(syn_set, syn_set.lemma_names())
        print(syn_set.hyponyms())
</code></pre>

<p>Here is the output:</p>

<pre><code>(Synset('cat.n.01'), [u'cat', u'true_cat'])
[Synset('domestic_cat.n.01'), Synset('wildcat.n.03')]
(Synset('guy.n.01'), [u'guy', u'cat', u'hombre', u'bozo'])
[Synset('sod.n.04')]
...
(Synset('cable_car.n.01'), [u'cable_car', u'car'])
[]
</code></pre>
"
4503550;71;Supersquirrel;<python><python-3.x><machine-learning><nltk><wordnet>;28196103;4;Find Python NLTK Wordnet Synsets for a each item of a list;"<p>I've been learning basic python, but I am new to NLTK. I want to use nltk to extract hyponyms for a given list of words. It works fine when I enter every term manually, but it does not seem to work when I try to iterate through items of a list.</p>

<p>This works: </p>

<pre><code>from nltk.corpus import wordnet as wn

syn_sets = wn.synsets(""car"")

for syn_set in syn_sets:
    print(syn_set, syn_set.lemma_names())
    print(syn_set.hyponyms())
</code></pre>

<p>But how do I get Wordnet methods to work with a list of items like</p>

<pre><code>token = [""cat"", ""dog"", ""car""]
syn_sets = wn.synsets((*get each item from the list*))
</code></pre>

<p>in a loop? </p>

<p>Thank you!</p>
";;1287834;19655;Slater Victoroff;6;28203686;"<p>List comprehensions to the rescue!</p>

<p>Totally possible, even using very similar syntax to what you had before. Python has a construct known as a <code>[list comprehension][1]</code> made exactly for this application. Basically, it's a functional syntax for inline for loops, but tend to be cleaner, more robust implementations with slightly lower overhead.</p>

<p>Example:</p>

<pre><code>tokens = [""cat"", ""dog"", ""car""]
syn_sets = [wn.synsets(token) for token in tokens]
</code></pre>

<p>This will even scale to slightly more complex data structures pretty easily, for instance:</p>

<pre><code>split_syn_sets = [(syn_set.lemma_names(), syn_set.hyponyms()) for syn_set in syn_sets]
</code></pre>

<p>Not sure if that's exactly what you're looking for, but it should generalize to whatever you are looking to do similar to this.</p>

<p>If it's useful I asked a question about grabbing all related synsets <a href=""https://stackoverflow.com/questions/11005529/general-synonym-and-part-of-speech-processing-using-nltk"">here</a> a while ago.</p>
"
2267681;261;crscardellino;<machine-learning><weka>;28196553;2;How to evaluate an updateable classifier in Weka;"<p>I'm using the Weka API to implement an incremental classifier following the steps in <a href=""http://weka.wikispaces.com/Use+Weka+in+your+Java+code"" rel=""nofollow"">http://weka.wikispaces.com/Use+Weka+in+your+Java+code</a></p>

<p>However, I do not find any option to evaluate the classifier (using a test set) since the only documentation is using small datasets (which I don't have, that's why I'm using an updateable classifier). Are there any updateable evaluation available? Or the only way is doing it manually?</p>

<p>Thanks for the help!</p>
";28198964;1473653;1857;ahjohnston25;3;28198964;"<p>Any classifier in WEKA can be tested using a <code>Evaluation</code> object like so:</p>

<pre><code>Evaluation eTest = new Evaluation(testInstances);
eTest.evaluateModel(yourUpdatableModelHere, testInstances);
//Print the results
System.out.println(eTest.toSummaryString());
//Get the confusion matrix
double[][] confMatrix = eTest.confusionMatrix();
</code></pre>

<p>For more, see the JavaDoc on <code>Evaluation</code> <a href=""http://weka.sourceforge.net/doc.dev/weka/classifiers/Evaluation.html#evaluateModel(weka.classifiers.Classifier,%20weka.core.Instances,%20java.lang.Object...)"" rel=""nofollow"">here</a>.</p>
"
4503904;222;rajatsen91;<python><machine-learning><scikit-learn>;28197444;1;Python predict_proba class identification;"<p>Suppose my labeled data has two classes 1 and 0. When I run predict_proba on the test set it returns an array with two columns. Which column corresponds to which class ?  </p>
";28198700;1190430;5186;Artem Sobolev;3;28198700;"<p>Column 0 corresponds to the class 0, column 1 corresponds to the class 1.</p>
"
4503904;222;rajatsen91;<python><machine-learning><scikit-learn>;28197444;1;Python predict_proba class identification;"<p>Suppose my labeled data has two classes 1 and 0. When I run predict_proba on the test set it returns an array with two columns. Which column corresponds to which class ?  </p>
";28198700;11422266;11;Sel;0;56207791;"<p>You can check that by printing the classes with <code>print(estimator.classes_)</code>. The array will have the same order like the output.</p>
"
979663;1518;carboncomputed;<machine-learning><artificial-intelligence><neural-network><convolution>;28199972;1;How does a convolutional neural network connect to the multi-layered perceptron?;"<p>Which operation takes place to produce the output from say a 9x9 filter and pass that output as the input to MLP.</p>
";28211025;1688185;13856;deltheil;3;28211025;"<p>After the last convolutional layer, you have <code>N</code> feature maps, with <code>WxH</code> resolution. This can be seen as a feature vector <code>X</code> of size <code>NxWxH</code> if you concatenate all the values.</p>

<p>This is how you <em>connect</em> it to an MLP: i.e <code>X</code> acts as an input of a linear transformation with nb. rows = MLP output and nb. columns = <code>NxWxH</code>.</p>

<p>Example: a simple convnet with 2 convolutional layers (x) for <a href=""http://computer-vision-tjpn.googlecode.com/svn/trunk/documentation/reference_papers/2-sermanet-ijcnn-11-mscnn.pdf"" rel=""nofollow"">traffic sign recognition</a> gives:</p>

<ul>
<li>input: 3 channels, width=32, height=32</li>
<li>layer 1: 108 feature maps, width=14, height=14</li>
<li>layer 2: 200 feature maps, width=5, height=5</li>
<li>2-layer classifier with 100 hidden units, and 43 output classes</li>
</ul>

<p>So to connect it to the final MLP you <em>reshape</em> the outputs of layer 2 into a vector of 200x5x5=5000 elements.</p>

<p>This vector becomes the input for a linear transform of size 100 (rows) x 5000 (columns).</p>

<p>(x) convolution kernel size = 5, spatial pooling size = 2.</p>
"
4178757;167;Denny Dharmawan;<machine-learning><computer-vision><neural-network>;28201617;4;rotational equivariance in Convolutional Neural Network?;"<p>I would like to know whether basic architecture of CNN has rotational equivariance property? I only know the translational equivariance but not sure about the rotational. </p>

<p>From my search, the rotational equivariance can be achieved by rotating the input image for training. Do I really need to do that? How big is the rotation degree? To put more contex, For example, I have a CNN that can detect/read text in a landscape mode. If I rotate the image 90 degree/make it portrait, will it give the same result/perform the same as the original one?  </p>
";;860196;8365;runDOSrun;2;28217522;"<p>You have scale and rotational invariance only to some degree - how much exactly might be dependent on your setup. You have it since the pools containing the features are potentially overlapping.</p>

<p>What you propose is certainly possible. You can always modify your training data adding noise, rotation, different scales etc. to achieve that goal. However, your model will still not be fully rotation-invariant. It's also possible to modify the network itself to achieve the task ""properly"". I'm sure that you stumbled upon <a href=""http://ai.stanford.edu/~ang/papers/nips10-TiledConvolutionalNeuralNetworks.pdf"" rel=""nofollow"">Tiled CNNs</a> during your research (if not, you should definitely read that paper). They use TICA to pretrain, finding invariant features in the process.</p>

<p>To your last question with 90Â° rotation: I'd suggest testing this out yourself. If the cases where rotation occurs is known (e.g. on mobile devices), I'd personally see if manually rotating the picture back to 0Â° (before giving it to the network) is a satisfying solution for the given constraints. It's the simplest approach.</p>
"
4505159;1;6a_b9;<matlab><machine-learning><liblinear>;28204148;0;Using LIBLINEAR to train 2D features;"<p>This is my first machine learning exercise, and I want to use LIBLINEAR to train on some data. The training data that I have worked is stored as a multidimensional matrix of size <em>m</em>x<em>m</em>x<em>n</em>, where there are n instances of <em>m</em>x<em>m</em> matrices, with each cell in the <em>m</em>x<em>m</em> matrix containing a value between 1-255 to indicate pixel values. </p>

<p>Included with this data is a <em>n</em>x1 vector of labels. However, trying to use the LIBLINEAR function train(training_labels, sparse(training_data)) yields an error. I think its because training_data is a 3d matrix. What steps am I missing or not understanding? I assumed that since the training_labels is <em>n</em>x1, each value in the training_labels vector maps to a <em>m</em>x<em>m</em> matrix of data. Is that not the case?</p>

<p>Thanks!</p>
";;2056067;7911;A. Donda;0;28228040;"<p>Yes, one m x m array is one feature vector, but it has the form of a matrix. liblinear probably can't deal with this. As written in the comment, you have to change the data format. If A is your 3d array, then</p>

<pre><code>reshape(A, m * m, n)
</code></pre>

<p>gives you a 2d matrix with mÂ² rows and n columns.</p>
"
4505324;1;Augmu;<machine-learning><classification>;28204959;0;Can Machine Learning help classify data;"<p>I have a data set as below,</p>

<p><strong>Code  | Description</strong></p>

<p>AB123   |   Cell Phone</p>

<p>B467A   |   Mobile Phone</p>

<p>12345   |   Telephone</p>

<p>WP9876  |   Wireless Phone</p>

<p>SP7654  |   Satellite Phone</p>

<p>SV7608  |   Sedan Vehicle</p>

<p>CC6543  |   Car Coupe</p>

<p>Need to create a automated grouping based on the Code and Description. Lets assume I have so many such data already classified into 0-99 groups. Whenever a new data comes in with a Code and Description, the Machine Learning algorithm needs to automatically classify this based on the previously available data.</p>

<p><strong>Code  | Description</strong>  | <strong>Group</strong> </p>

<p>AB123   |   Cell Phone   |     1</p>

<p>B467A   |   Mobile Phone |     1</p>

<p>12345   |   Telephone    |     1</p>

<p>WP9876  |   Wireless Phone |     1</p>

<p>SP7654  |   Satellite Phone |     1</p>

<p>SV7608  |   Sedan Vehicle |     2</p>

<p>CC6543  |   Car Coupe |     3</p>

<p>Can this be achieved to some level of accuracy? Currently this process is so manual. Any such ideas or references are there, please help with that.</p>
";;1056563;45835;StephenBoesch;0;28205841;"<p>Try reading up on Supervised Learning.  You need to provide labels for your training data so that the algorithms know what are the correct answers - and are able to generate appropriate models for you.</p>

<p>Then you can ""predict"" the output classes for your new incoming data using the generated model(s).</p>

<p>Finally, you may wish to circle back to check the accuracy of the predicted results. If you then enter the labels for the newly received and predicted data then those data can then be used for further training on your model(s).</p>
"
4505324;1;Augmu;<machine-learning><classification>;28204959;0;Can Machine Learning help classify data;"<p>I have a data set as below,</p>

<p><strong>Code  | Description</strong></p>

<p>AB123   |   Cell Phone</p>

<p>B467A   |   Mobile Phone</p>

<p>12345   |   Telephone</p>

<p>WP9876  |   Wireless Phone</p>

<p>SP7654  |   Satellite Phone</p>

<p>SV7608  |   Sedan Vehicle</p>

<p>CC6543  |   Car Coupe</p>

<p>Need to create a automated grouping based on the Code and Description. Lets assume I have so many such data already classified into 0-99 groups. Whenever a new data comes in with a Code and Description, the Machine Learning algorithm needs to automatically classify this based on the previously available data.</p>

<p><strong>Code  | Description</strong>  | <strong>Group</strong> </p>

<p>AB123   |   Cell Phone   |     1</p>

<p>B467A   |   Mobile Phone |     1</p>

<p>12345   |   Telephone    |     1</p>

<p>WP9876  |   Wireless Phone |     1</p>

<p>SP7654  |   Satellite Phone |     1</p>

<p>SV7608  |   Sedan Vehicle |     2</p>

<p>CC6543  |   Car Coupe |     3</p>

<p>Can this be achieved to some level of accuracy? Currently this process is so manual. Any such ideas or references are there, please help with that.</p>
";;860196;8365;runDOSrun;0;28217056;"<p>Yes, it's possible with supervised learning. You pick yourself a model which you ""train"" with the data you already have. The model/algorithm then ""generalizes"" to previously unseen data from the known data. </p>

<p>What you specify as a group would be called class or ""label"" which needs to be predicted based on 2 input features (code/description). Whether you input these features directly or preprocess them into more abstract features which suits the algorithm better, depends on which algorithm you choose. </p>

<p>If you have no experience with Machine Learning, you might start with learning some basics while testing already implemented algorithms in tools such as RapidMiner, Weka or Orange.</p>
"
4505324;1;Augmu;<machine-learning><classification>;28204959;0;Can Machine Learning help classify data;"<p>I have a data set as below,</p>

<p><strong>Code  | Description</strong></p>

<p>AB123   |   Cell Phone</p>

<p>B467A   |   Mobile Phone</p>

<p>12345   |   Telephone</p>

<p>WP9876  |   Wireless Phone</p>

<p>SP7654  |   Satellite Phone</p>

<p>SV7608  |   Sedan Vehicle</p>

<p>CC6543  |   Car Coupe</p>

<p>Need to create a automated grouping based on the Code and Description. Lets assume I have so many such data already classified into 0-99 groups. Whenever a new data comes in with a Code and Description, the Machine Learning algorithm needs to automatically classify this based on the previously available data.</p>

<p><strong>Code  | Description</strong>  | <strong>Group</strong> </p>

<p>AB123   |   Cell Phone   |     1</p>

<p>B467A   |   Mobile Phone |     1</p>

<p>12345   |   Telephone    |     1</p>

<p>WP9876  |   Wireless Phone |     1</p>

<p>SP7654  |   Satellite Phone |     1</p>

<p>SV7608  |   Sedan Vehicle |     2</p>

<p>CC6543  |   Car Coupe |     3</p>

<p>Can this be achieved to some level of accuracy? Currently this process is so manual. Any such ideas or references are there, please help with that.</p>
";;3235916;456;user3235916;0;28264272;"<p>I don't think machine learning methods are the most appropriate for the solution of the problem, because text based machine learning algorithms tend to be quite complicated. From the examples you provided I'm not sure how</p>

<p>I think the simplest way of solving, or attempting to solve this problem is the following, which can be implemented in many free programming languages, such as python. Each description can be stored as a string. What you could do is to store all the substrings of all the strings (ie Phone is your string, the substrings will be 'P','h',Ph',..,'e') that belong in a particular group in a list (see this question for how to implement it in python... <a href=""https://stackoverflow.com/questions/12945029/substrings-of-a-string"">Substrings of a string using Python</a>). Then you want to for each substring and all substrings stored, see which ones are unique to a certain group. Then select strings over a certain length (say 3 characters long, to get rid of random letter concatenations) as your classification criteria. Then when you get new data, check whether the description is unique to a certain group. With this for instance, you would be able to classify all objects that are in group 1 based on whether their description contains the word phone.</p>

<p>Its hard to provide concrete code to solves your problem without knowing what languages you are familiar with/are feasible to use. I hope this helps anyway. Yves</p>
"
1899010;4835;HHH;<azure><mahout><azure-machine-learning-studio>;28205136;1;Does Microsoft Azure Machine Learning use Hadoop as its underlying layer?;"<p>I'm going to use Microsoft Azure ML for some text analysis purposes such as keyword extraction and as the size of my input is big I want to know whether ML package actually uses the Hadoop (HDP) as its underlying layer or not? If not, how can I use the ML in combination with Hadoop?</p>

<p>Does Mahout have some text analysis tools?</p>
";;1341806;6166;Dan Ciborowski - MSFT;4;28208172;"<p><a href=""https://studio.azureml.net/"" rel=""nofollow"">Microsoft Azure ML</a> does not use hadoop. It uses a custom back end that runs each module of an experiment independently(and in parallel when the DAG allows).</p>

<p>Azure ML is not a package, but is a design studio for creating and operationalizing ML solutions.</p>

<p>What is the size of your dataset?</p>

<p>Azure ML currently supports about 6gigs of data for training. </p>

<p>It is recommended you use hdinsight if you need preprocessing of your data. This is also a good place to extract your specific features. Using the feature extraction module on a sample of training data can help determine key columns. </p>

<p>Having to much data is never a bad thing. I recommend down sampling your data to small chucks of maybe about 512-1 gigs. Determine your accuracy with that data size, then scale up 2x or 3x up to 6 gigs and see how much accuracy you gain.</p>
"
789965;1419;tan;<machine-learning><grouping><cluster-analysis>;28219275;1;What is a good way to classify or cluster free form text entries?;"<p>I have a set of ratings entered by users for N-items, along with reasons as to why they select that rating for that item. The ratings are in an ordinal scale (-2, -1, 0, +1, +2).
I would like to come up with meaningful groupings of these reasons. For example, say users are rating movies and reasons behind the ratings might fall under 3 broad groups: 1). 'They are huge fan of the actor', 2). 'Amazing Story line', 4). 'Lacks originality'. This is just a dummy example.</p>

<p>More concretely, given a set of free form textual entries, can one come up with such groupings. I know that topic modeling is one way of doing this. I can specify the number of topics K, and then feed data into my topic model (LDA etc.), the model will output K topics where each topic is a list of most probable words in that topic. So with respect to this dummy example, topic 1 may contain words and phrases like - 'fan', 'actor', 'great acting'.</p>

<p>Is there other ways to do this clustering? Do I need to consider the ordinal rating scale while clustering? How can I take that into account?</p>
";;1149913;4354;user1149913;1;28223477;"<p>Word embeddings might be useful. <a href=""http://nlp.stanford.edu/projects/glove/"" rel=""nofollow"">Here</a> is a recent, relevant Stanford project.</p>
"
789965;1419;tan;<machine-learning><grouping><cluster-analysis>;28219275;1;What is a good way to classify or cluster free form text entries?;"<p>I have a set of ratings entered by users for N-items, along with reasons as to why they select that rating for that item. The ratings are in an ordinal scale (-2, -1, 0, +1, +2).
I would like to come up with meaningful groupings of these reasons. For example, say users are rating movies and reasons behind the ratings might fall under 3 broad groups: 1). 'They are huge fan of the actor', 2). 'Amazing Story line', 4). 'Lacks originality'. This is just a dummy example.</p>

<p>More concretely, given a set of free form textual entries, can one come up with such groupings. I know that topic modeling is one way of doing this. I can specify the number of topics K, and then feed data into my topic model (LDA etc.), the model will output K topics where each topic is a list of most probable words in that topic. So with respect to this dummy example, topic 1 may contain words and phrases like - 'fan', 'actor', 'great acting'.</p>

<p>Is there other ways to do this clustering? Do I need to consider the ordinal rating scale while clustering? How can I take that into account?</p>
";;1056563;45835;StephenBoesch;1;28224496;"<p>It depends on how sophisticated you wish the handling of the text to be. If just matching single words (1-grams) were sufficient then:</p>

<ul>
<li>remove stop words</li>
<li>possibly do stemming or other text preprocessing</li>
<li>apply a naive bayes classification algorithm  Options are here: <a href=""http://en.wikipedia.org/wiki/Naive_Bayes_classifier"" rel=""nofollow"">http://en.wikipedia.org/wiki/Naive_Bayes_classifier</a></li>
</ul>

<p>However you may also wish to do a better job with phrases / related words. In that case there is plenty of research - and implementations - to help you. Ngrams is a relatively simple approach, but more advanced methods that understand the semantics of the language have better statistical performance.</p>
"
2004667;31;DmitryV;<machine-learning><ssas><classification><decision-tree>;28221159;0;Decision Tree English Rules and Dependency Network in MS SSAS;"<p>I created a Decision Tree model in Microsoft Analysis Services (SSAS, Visual Studio 2010). There are two tabs in the Mining Model Viewer tab: (1) Decision Tree that shows a tree itself, and (2) Dependency Network that shows the chart of most important variables.</p>

<p><img src=""https://i.stack.imgur.com/Kh0Gu.png"" alt=""enter image description here""></p>

<p>On the Decision Tree tab I can click on each individual leaf and see the English Rule for that leaf. Is there a way to get ALL the rules at once as a list with the case numbers?</p>

<p>The Dependency Network tab has a slider that you can move to see which variables influence the decision tree most. Is it possible to get ALL important variables as a list with their ""importance"" number?</p>
";;2004667;31;DmitryV;0;28349081;"<p>I have found a way to get the <strong>list of ALL English Rules</strong> from the Mining Structure.</p>

<pre><code>SELECT * FROM [MyModel].CONTENT WHERE [CHILDREN_CARDINALITY] = 0
</code></pre>

<p><strong>[CHILDREN_CARDINALITY] = 0</strong> defines the DT leaves only.</p>

<p>The following links may be helpful: <a href=""https://msdn.microsoft.com/en-us/library/cc645903.aspx"" rel=""nofollow"">Decision Trees Model Query Examples</a>, <a href=""https://msdn.microsoft.com/en-us/library/cc645758.aspx"" rel=""nofollow"">Mining Model Content for Decision Tree Models</a></p>

<p><strong>""FLATTENED""</strong> keyword will split [NODE_DISTRIBUTION] outcomes into separate rows.</p>

<pre><code>SELECT FLATTENED [MODEL_CATALOG],[MODEL_NAME],[NODE_CAPTION],[CHILDREN_CARDINALITY],[NODE_DESCRIPTION],[NODE_PROBABILITY],[NODE_DISTRIBUTION],[NODE_SUPPORT] FROM [MyModel].CONTENT WHERE [CHILDREN_CARDINALITY] = 0 ORDER BY [NODE_SUPPORT] DESC
</code></pre>
"
823859;6395;Adam_G;<java><machine-learning><weka><arff>;28223979;0;Creating Compatible Train and Test Instances in Weka;"<p>I am trying to create test and train <code>Instances</code> objects. There are some attributes in train that are not in test. I'm having trouble with the proper filtering methodology, though. I have tried two filters. Below is the code with the errors they produce.</p>

<pre><code>Instances rawTraining = new Instances(arffFile);
Instances rawTesting = new Instances(arffFile);
System.out.println(""Raw Training Attributes: ""+rawTraining.numAttributes());
//Raw Training Attributes: 2446
System.out.println(""Raw Testing Attributes: ""+rawTesting.numAttributes());
//Raw Testing Attributes: 2381
rawTraining.setClassIndex(rawTraining.numAttributes()-1);
</code></pre>

<p><strong>NumericToNominal Filter</strong></p>

<pre><code>NumericToNominal filter = new NumericToNominal();
filter.setAttributeIndicesArray(new int[] {rawTraining.classAttribute().index()});
filter.setInputFormat(rawTraining);
Instances finalTraining = Filter.useFilter(rawTraining, filter);
Instances finalTesting = Filter.useFilter(rawTesting, filter);
</code></pre>

<p>Produces the error:</p>

<pre><code>java.lang.IllegalArgumentException: Src and Dest differ in # of attributes: 2381 != 2446
    at weka.core.RelationalLocator.copyRelationalValues(RelationalLocator.java:87)
    at weka.filters.Filter.copyValues(Filter.java:371)
    at weka.filters.Filter.bufferInput(Filter.java:313)
    at weka.filters.SimpleBatchFilter.input(SimpleBatchFilter.java:199)
    at weka.filters.Filter.useFilter(Filter.java:680)
</code></pre>

<p><strong>Standardize Filter</strong></p>

<pre><code>Standardize filter = new Standardize();
filter.setInputFormat(rawTraining);
Instances finalTraining = Filter.useFilter(rawTraining, filter);
Instances finalTesting = Filter.useFilter(rawTesting, filter);
</code></pre>

<p>Produces the error:</p>

<pre><code>java.lang.IndexOutOfBoundsException: Index: 2381, Size: 2381
    at java.util.ArrayList.rangeCheck(ArrayList.java:653)
    at java.util.ArrayList.get(ArrayList.java:429)
    at weka.core.Instances.attribute(Instances.java:341)
    at weka.core.AbstractInstance.attribute(AbstractInstance.java:72)
    at weka.filters.unsupervised.attribute.Standardize.convertInstance(Standardize.java:240)
    at weka.filters.unsupervised.attribute.Standardize.input(Standardize.java:142)
    at weka.filters.Filter.useFilter(Filter.java:680)
</code></pre>

<p>How can I make these two Instances` compatible?</p>
";28225063;4500741;1366;NBartley;1;28225063;"<p>The answer provided here will help address some of your concerns: <a href=""https://stackoverflow.com/questions/10947015/does-test-file-in-weka-requires-same-or-less-number-of-features-as-train/10947334#10947334"">Does test file in WEKA require or less number of features as train</a>. </p>

<p>In short, you first need to make sure you have the same attributes for your training and testing instances (you should be able to insert '?' into any class attributes). The code snippets you provided look fine, so I would handle this first and then see what happens.</p>
"
1170652;541;Eric;<machine-learning><neural-network><computer-vision><vgg-net>;28232235;43;How to calculate the number of parameters of convolutional neural networks?;"<p>I can't give the correct number of parameters of <a href=""http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf"">AlexNet</a> or <a href=""http://arxiv.org/abs/1409.1556"">VGG Net</a>.</p>

<p>For example, to calculate the number of parameters of a <code>conv3-256</code> layer of VGG Net, the answer is 0.59M = (3*3)*(256*256), that is (kernel size) * (product of both number of channels in the joint layers), however in that way, I can't get the <code>138M</code> parameters.</p>

<p>So could you please show me where is wrong with my calculation, or show me the right calculation procedure?</p>
";28242883;1688185;13856;deltheil;64;28242883;"<p>If you refer to VGG Net with 16-layer (table 1, column D) then <code>138M</code> refers to the <strong>total number of parameters</strong> of this network, i.e including all convolutional layers, but also the fully connected ones.</p>

<p>Looking at the 3rd convolutional stage composed of 3 x <code>conv3-256</code> layers:</p>

<ul>
<li>the first one has N=128 input planes and F=256 output planes,</li>
<li>the two other ones have N=256 input planes and F=256 output planes.</li>
</ul>

<p>The convolution kernel is 3x3 for each of these layers. In terms of parameters this gives:</p>

<ul>
<li>128x3x3x256 (weights) + 256 (biases) = 295,168 parameters for the 1st one,</li>
<li>256x3x3x256 (weights) + 256 (biases) = 590,080 parameters for the two other ones.</li>
</ul>

<p><em>As explained above you have to do that for all layers, but also the fully-connected ones, and sum these values to obtain the final 138M number.</em></p>

<p>-</p>

<p><strong>UPDATE</strong>: the breakdown among layers give:</p>

<pre><code>conv3-64  x 2       : 38,720
conv3-128 x 2       : 221,440
conv3-256 x 3       : 1,475,328
conv3-512 x 3       : 5,899,776
conv3-512 x 3       : 7,079,424
fc1                 : 102,764,544
fc2                 : 16,781,312
fc3                 : 4,097,000
TOTAL               : 138,357,544
</code></pre>

<p>In particular for the fully-connected layers (fc):</p>

<pre><code> fc1 (x): (512x7x7)x4,096 (weights) + 4,096 (biases)
 fc2    : 4,096x4,096     (weights) + 4,096 (biases)
 fc3    : 4,096x1,000     (weights) + 1,000 (biases)
</code></pre>

<p>(x) see section 3.2 of the article: <em>the fully-connected layers are first converted to convolutional layers (the first FC layer to a 7 Ã— 7 conv. layer, the last two FC layers to 1 Ã— 1 conv. layers).</em></p>

<p><strong>Details about <code>fc1</code></strong></p>

<p>As precised above the spatial resolution right before feeding the fully-connected layers is 7x7 pixels. This is because this VGG Net uses <em>spatial padding</em> before convolutions, as detailed within section 2.1 of the paper:</p>

<p><em>[...] the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3Ã—3 conv. layers.</em></p>

<p>With such a padding, and working with a 224x224 pixels input image, the resolution decreases as follow along the layers: 112x112, 56x56, 28x28, 14x14 and 7x7 after the last convolution/pooling stage which has 512 feature maps.</p>

<p>This gives a feature vector passed to <code>fc1</code> with dimension: 512x7x7.</p>
"
1170652;541;Eric;<machine-learning><neural-network><computer-vision><vgg-net>;28232235;43;How to calculate the number of parameters of convolutional neural networks?;"<p>I can't give the correct number of parameters of <a href=""http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf"">AlexNet</a> or <a href=""http://arxiv.org/abs/1409.1556"">VGG Net</a>.</p>

<p>For example, to calculate the number of parameters of a <code>conv3-256</code> layer of VGG Net, the answer is 0.59M = (3*3)*(256*256), that is (kernel size) * (product of both number of channels in the joint layers), however in that way, I can't get the <code>138M</code> parameters.</p>

<p>So could you please show me where is wrong with my calculation, or show me the right calculation procedure?</p>
";28242883;2255305;2362;Ray;46;36976958;"<p>A great breakdown of the calculation for VGG-16 network is also given in <a href=""http://cs231n.github.io/convolutional-networks/"" rel=""noreferrer"">CS231n</a> lecture notes.</p>

<pre><code>INPUT:     [224x224x3]    memory:  224*224*3=150K   weights: 0
CONV3-64:  [224x224x64]   memory:  224*224*64=3.2M  weights: (3*3*3)*64 = 1,728
CONV3-64:  [224x224x64]   memory:  224*224*64=3.2M  weights: (3*3*64)*64 = 36,864
POOL2:     [112x112x64]   memory:  112*112*64=800K  weights: 0
CONV3-128: [112x112x128]  memory:  112*112*128=1.6M weights: (3*3*64)*128 = 73,728
CONV3-128: [112x112x128]  memory:  112*112*128=1.6M weights: (3*3*128)*128 = 147,456
POOL2:     [56x56x128]    memory:  56*56*128=400K   weights: 0
CONV3-256: [56x56x256]    memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912
CONV3-256: [56x56x256]    memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824
CONV3-256: [56x56x256]    memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824
POOL2:     [28x28x256]    memory:  28*28*256=200K   weights: 0
CONV3-512: [28x28x512]    memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648
CONV3-512: [28x28x512]    memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [28x28x512]    memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
POOL2:     [14x14x512]    memory:  14*14*512=100K   weights: 0
CONV3-512: [14x14x512]    memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]    memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]    memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
POOL2:     [7x7x512]      memory:  7*7*512=25K      weights: 0
FC:        [1x1x4096]     memory:  4096             weights: 7*7*512*4096 = 102,760,448
FC:        [1x1x4096]     memory:  4096             weights: 4096*4096 = 16,777,216
FC:        [1x1x1000]     memory:  1000             weights: 4096*1000 = 4,096,000

TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)
TOTAL params: 138M parameters
</code></pre>
"
1170652;541;Eric;<machine-learning><neural-network><computer-vision><vgg-net>;28232235;43;How to calculate the number of parameters of convolutional neural networks?;"<p>I can't give the correct number of parameters of <a href=""http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf"">AlexNet</a> or <a href=""http://arxiv.org/abs/1409.1556"">VGG Net</a>.</p>

<p>For example, to calculate the number of parameters of a <code>conv3-256</code> layer of VGG Net, the answer is 0.59M = (3*3)*(256*256), that is (kernel size) * (product of both number of channels in the joint layers), however in that way, I can't get the <code>138M</code> parameters.</p>

<p>So could you please show me where is wrong with my calculation, or show me the right calculation procedure?</p>
";28242883;3455895;19;rav;1;44924956;"<p>I know this is a old post nevertheless, I think the accepted answer by @deltheil contains a mistake. If not, I would be happy to be corrected. The convolution layer should not have bias.
i.e. 
    128x3x3x256 (weights) + 256 (biases) = 295,168 
should be 
    128x3x3x256 (weights)  = 294,9112 </p>

<p>Thanks</p>
"
1170652;541;Eric;<machine-learning><neural-network><computer-vision><vgg-net>;28232235;43;How to calculate the number of parameters of convolutional neural networks?;"<p>I can't give the correct number of parameters of <a href=""http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf"">AlexNet</a> or <a href=""http://arxiv.org/abs/1409.1556"">VGG Net</a>.</p>

<p>For example, to calculate the number of parameters of a <code>conv3-256</code> layer of VGG Net, the answer is 0.59M = (3*3)*(256*256), that is (kernel size) * (product of both number of channels in the joint layers), however in that way, I can't get the <code>138M</code> parameters.</p>

<p>So could you please show me where is wrong with my calculation, or show me the right calculation procedure?</p>
";28242883;10594060;43;saunter;2;53219217;"<p>Here is how to compute the number of parameters in each cnn layer:<br>
  some definitions<br>
  n--width of filter<br>
  m--height of filter<br>
  k--number of input feature maps<br>
  L--number of output feature maps<br>
  Then number of paramters  #= (n*m *k+1)*L in which the first contribution is from 
  weights and the second is from bias. </p>
"
1170652;541;Eric;<machine-learning><neural-network><computer-vision><vgg-net>;28232235;43;How to calculate the number of parameters of convolutional neural networks?;"<p>I can't give the correct number of parameters of <a href=""http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf"">AlexNet</a> or <a href=""http://arxiv.org/abs/1409.1556"">VGG Net</a>.</p>

<p>For example, to calculate the number of parameters of a <code>conv3-256</code> layer of VGG Net, the answer is 0.59M = (3*3)*(256*256), that is (kernel size) * (product of both number of channels in the joint layers), however in that way, I can't get the <code>138M</code> parameters.</p>

<p>So could you please show me where is wrong with my calculation, or show me the right calculation procedure?</p>
";28242883;6484358;1989;Anu;3;54295849;"<p>The below VGG-16 architechture is in the <a href=""https://arxiv.org/pdf/1409.1556.pdf"" rel=""nofollow noreferrer"">original paper as highlighted by @deltheil in  (table 1, column D)  </a>, and I quote from there</p>

<blockquote>
  <p>2.1 ARCHITECTURE</p>
  
  <p>During training, the input to our ConvNets is a fixed-size 224 Ã— 224
  RGB images. The only preprocessing we do is subtracting the mean RGB
  value, computed on the training set, from each pixel.</p>
  
  <p>The image is passed through a stack of convolutional (conv.) layers,
  where we use filters with a very small receptive field: 3 Ã— 3 (which
  is the smallest size to capture the notion of left/right, up/down,
  center). The convolution stride is fixed to 1 pixel; the spatial
  padding of conv. layer input is such that the spatial resolution is
  preserved after convolution, i.e. the padding is 1 pixel for 3 Ã— 3
  conv. layers.  Spatial pooling is carried out by five max-pooling
  layers, which follow some of the conv. layers (not all the conv.
  layers are followed by max-pooling). Max-pooling is performed over a 2
  Ã— 2 pixel window, with stride 2.</p>
  
  <p>A stack of convolutional layers (which has a different depth in
  different architectures) is followed by three Fully-Connected (FC)
  layers: the first two have 4096 channels each, the third performs
  1000-way ILSVRC classification and thus contains 1000 channels (one
  for each class). </p>
  
  <p>The final layer is the soft-max layer.</p>
</blockquote>

<p>Using the above, and </p>

<ul>
<li>A formula to find activation shape of a layer!</li>
</ul>

<p><img src=""https://i.stack.imgur.com/oucTkt.jpg"" width=""200"" /></p>

<ul>
<li>A formula to calculate the weights corresponding to every layer:</li>
</ul>

<p><img src=""https://i.stack.imgur.com/R8F7a.png"" width=""200"" /></p>

<p><strong>Note:</strong></p>

<ul>
<li><p>you can simply multiply respective activation shape column to get the activation size</p></li>
<li><p>CONV3: means a filter of 3*3 will convolve on the input!</p></li>
<li><p>MAXPOOL3-2: means, 3rd pooling layer, with 2*2 filter, stride=2, padding=0(pretty standard in pooling layers)</p></li>
<li><p>Stage-3 : means it has multiple CONV layer stacked! with same padding=1, , stride=1, and filter 3*3</p></li>
<li><p>Cin : means the depth a.k.a channel coming from the input layer!</p></li>
<li><p>Cout: means the depth a.k.a channel outgoing (you configure it differently- to learn more complex features!),</p></li>
</ul>

<p>Cin and Cout are the number of filters that you stack together to learn multiple features at different scales such as in the first layer you might want to learn vertical edges, and horizontal edges and edges at say 45degree, blah blah!, 64 possible different filters each of different kind of edges!! </p>

<ul>
<li><p>n: input dimension without depth such n=224 in case of INPUT-image!</p></li>
<li><p>p: padding for each layer</p></li>
<li><p>s: stride used for each layer</p></li>
<li><p>f: filter size i.e 3*3 for CONV and 2*2 for MAXPOOL layers!  </p></li>
<li><p>After MAXPOOL5-2, you simply flatten the volume and interface it with the first FC layer.!</p></li>
</ul>

<p><strong><em>We get the table:</em></strong>
<a href=""https://i.stack.imgur.com/zCwzX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zCwzX.jpg"" alt=""enter image description here""></a></p>

<p><strong><em>Finally, if you add all the weights calculated in the last column, you end up with 138,357,544(138 million) parameters to train for VGG-15!</em></strong></p>
"
3966933;1610;iec2011007;<machine-learning><computer-vision>;28234279;0;Content based Image retrieval?;"<p>Well I googled about <em>Content Based Image Retrieval</em>(CBIR) and actually the wiki definition is quite clear but there is not much material nor books related to subject. Can anyone explain what are the components that comprise <strong>Content Based Image Retrieval </strong>and any resources ?</p>
";;762517;163;minhduc;0;36014194;"<p>The task of Content-based image retrieval (CBIR) can be described like: ""given a query image, find the similar images in your database"".</p>

<p>As far as I know, CBIR has three basic steps
(1) Feature extraction: extract useful features to describe the images (for both images in the database and query image)
(2) Matching: compute distances between extracted features of the query again database images and produce a ranking of database images with regards to distance to the query
(3) Refinement: refine the matches (re-rank them)</p>

<p>a) Most efforts so far is put in the first step, feature extraction: </p>

<ul>
<li>Traditionally, hand-crafted local features (SIFT, SURF, etc.) are most commonly used. </li>
<li>Recently, researchers proposed to use encoding methods such as Bag-of-word, VLAD, Fisher vector, etc. to produce compact descriptors from raw local features. The benefits are two fold: (i) compact descriptors are more reliable than raw local features; (ii) compact descriptors have smaller foot-prints than raw features, thus more easily scalable and suitable for large-scale retrieval</li>
<li>Most recently, with the great success of deep learning in computer vision, people are switching to use features learned from Convolutional neural networks (CNN codes) to replace local hand-craft features. CNN codes may be used as is, or combined with some encoding methods mentioned above. In general, CNN codes have better performances than hand-crafted features (confirmed on some standard benchmarks such as Holiday dataset, Oxford5K, Oxford100K, Paris, UKB)</li>
</ul>

<p>b) For the second step: some simple distance metrics might do the job (Euclidean distance, cosine distance, etc.)</p>

<p>c) The last step (re-ranking) can be performed using RANSAC or prior-knowledge. (I actually don't know much about this step)</p>

<p>With some keywords from the texts above, you can find useful resources via Google.</p>
"
3966933;1610;iec2011007;<machine-learning><computer-vision>;28234279;0;Content based Image retrieval?;"<p>Well I googled about <em>Content Based Image Retrieval</em>(CBIR) and actually the wiki definition is quite clear but there is not much material nor books related to subject. Can anyone explain what are the components that comprise <strong>Content Based Image Retrieval </strong>and any resources ?</p>
";;1222783;619;Nothing More;0;39442645;"<p>Content based image retrieval (CBIR) can be simply regarded as ""given a query images, get a rank list that are most similar to the query image, based on the content of the query image. Traditional methods include vocabulary-tree approach. You can checkout this library <a href=""https://github.com/hlzz/libvot"" rel=""nofollow"">libvot</a> for details.</p>

<p>And there are also emerging methods, brought by the recent success of deep learning. Deep learning methods usually don't depend on local features, but global images descriptions. That is another huge topic.</p>
"
4152453;15;Prem;<amazon-web-services><apache-spark><machine-learning><svm><apache-spark-mllib>;28236483;1;How to configure kernel selection and loss function for Support Vector Machines in Spark MLLib;"<p>I have installed spark on AWS Elastic Map Reduce(EMR) and have been running SVM using the packages in MLLib. But there are no options to choose parameters for building the model like kernel selection and cost of misclassification (Like in e1071 package of R). Can someone please tell me how to set these parameters while building the model?</p>
";28246925;1056563;45835;StephenBoesch;1;28246925;"<p><strong>Summary / TL;DR:</strong></p>

<p>The hardcoded methods for SVMWithSGD are:</p>

<blockquote>
  <p>private val gradient = new <strong>HingeGradient</strong>()<br>
    private val updater new <strong>SquaredL2Updater</strong>()</p>
</blockquote>

<p>Since these are hard-coded - you can not configure them the way you are used to in R.</p>

<p><strong>Details:</strong></p>

<p>At the ""bare metal"" level the mllib SVMWithSGD supports the following parameters:</p>

<ul>
<li>Weights computed for every feature.</li>
<li>Intercept computed for this model.</li>
<li>Threshold between positive/negative predictions (defaults to 0.0)</li>
</ul>

<p>There are other convenience methods that allow you to define:</p>

<ul>
<li>regularization type (L1 vs L2)</li>
<li>regularization parameter (lambda)</li>
<li>what fraction of the input data to use for each training batch</li>
<li>initial step size (for the gradient descent)</li>
</ul>

<p>You will notice that the two items you mention:</p>

<ul>
<li>kernel selection</li>
<li>cost of misclassification</li>
</ul>

<p>are not included in those configurable parameters</p>

<p>Under the covers these are defined by the invocation of <em>GradientDescent</em> class as follows:</p>

<pre><code>* @param gradient Gradient function to be used.
* @param updater Updater to be used to update weights after every iteration.
GradientDescent(gradient: Gradient, private var updater: Updater)
</code></pre>

<p>with the following settings</p>
"
4152453;15;Prem;<amazon-web-services><apache-spark><machine-learning><svm><apache-spark-mllib>;28236483;1;How to configure kernel selection and loss function for Support Vector Machines in Spark MLLib;"<p>I have installed spark on AWS Elastic Map Reduce(EMR) and have been running SVM using the packages in MLLib. But there are no options to choose parameters for building the model like kernel selection and cost of misclassification (Like in e1071 package of R). Can someone please tell me how to set these parameters while building the model?</p>
";28246925;307848;156;Kyle.;0;28333775;"<p>MLLib's implementation of SVM is limited to linear kernels, so you're not going to find anything related to kernels. There is some work related to this happening, though, for example <a href=""http://www.ee.oulu.fi/research/imag/courses/Vedaldi/ShalevSiSr07.pdf"" rel=""nofollow"">Pegasos</a>.</p>
"
4512170;91;airjordan707;<machine-learning><regression><cross-validation>;28239980;9;Log transform dependent variable for regression tree;"<p>I have a dataset where I find that the dependent (target) variable has a skewed distribution - i.e. there are a few very large values and a long tail. </p>

<p>When I run the regression tree, one end-node is created for the large-valued observations and one end-node is created for majority of the other observations.</p>

<p>Would it be ok to log transform the dependent (target) variable and use it for regression tree analysis ? When I tried this, I get a different set of nodes and splits that seem to have a more even distribution of observations in each bucket. With log transformation, the Rsquare value for Predicted vs. Observed is also quite good. In other words, I seem to get better testing and validation performance with log transformation.
Just want to make sure log transformation is an accepted way to run regression tree when the dependent variable has a skewed distribution.</p>

<p>Thanks !</p>
";;925105;496;Sandeep;11;33893173;"<p>Yes. It is completely fine to apply log transformation on target variable when it has skewed distribution. That being said, you need to apply inverse function on top of the predicted values to get the actual predicted target value.</p>

<p>Moreover you have tested that by transforming you are getting better estimates on Rsquare error. I am assuming you have computed RSquare after inverting the log using exponent function.</p>

<p>For more details please refer, <a href=""https://en.wikipedia.org/wiki/Data_transformation_%28statistics%29"" rel=""noreferrer"">wiki link</a> on data transformation. </p>

<p>Note that if your training data contains any negative target values, log transformation cannot be applied directly. You might have to apply some other functions which can accept negative values.</p>
"
823859;6395;Adam_G;<java><machine-learning><weka>;28247240;0;Using InputMappedClassifier From Command Line;"<p>I'm trying to run a classifier with the InputMappedClassifier, since I know the test arff is missing some attributes in the training arff. However, when I run:</p>

<pre><code>java -cp ./weka.jar weka.classifiers.misc.InputMappedClassifier -t aa/lang-train.arff \
-T aa/lang-test.arff -W weka.classifiers.trees.J48 -classifications \ 
weka.classifiers.evaluation.output.prediction.PlainText
</code></pre>

<p>It generates the exception:</p>

<pre><code>java.lang.IllegalArgumentException: Invalid class index: 2466
    at weka.core.Instances.setClassIndex(Instances.java:1293)
    at weka.core.converters.ConverterUtils$DataSource.getStructure(ConverterUtils.java:346)
    at weka.classifiers.evaluation.output.prediction.AbstractOutput.printClassifications(AbstractOutput.java:625)
    at weka.classifiers.evaluation.output.prediction.AbstractOutput.print(AbstractOutput.java:702)
    at weka.classifiers.evaluation.Evaluation.evaluateModel(Evaluation.java:1572)
    at weka.classifiers.Evaluation.evaluateModel(Evaluation.java:649)
    at weka.classifiers.AbstractClassifier.runClassifier(AbstractClassifier.java:297)
    at weka.classifiers.misc.InputMappedClassifier.main(InputMappedClassifier.java:943)
</code></pre>

<p>If I run it without <code>-classifications</code>, it works. How can I get the classifications?</p>
";29844636;798008;302;Osaka;2;29844636;"<p>You're giving the InputMappedClassifier the wrong options. It's complaining that you are giving it the training (-t) and test (-T) data. It supports the following: </p>

<pre><code>Options specific to weka.classifiers.misc.InputMappedClassifier:

-I
    Ignore case when matching attribute names and nominal values.
-M
    Suppress the output of the mapping report.
-trim
    Trim white space from either end of names before matching.
-L &lt;path to model to load&gt;
    Path to a model to load. If set, this model
    will be used for prediction and any base classifier
    specification will be ignored. Environment variables
    may be used in the path (e.g. ${HOME}/myModel.model)
-W
    Full name of base classifier.
    (default: weka.classifiers.rules.ZeroR)
-output-debug-info
    If set, classifier is run in debug mode and
    may output additional info to the console
-do-not-check-capabilities
    If set, classifier capabilities are not checked before classifier is built
    (use with caution).
</code></pre>

<p>So your command should look like this:</p>

<pre><code>java -cp ./weka.jar weka.classifiers.misc.InputMappedClassifier -W weka.classifiers.trees.J48 \
-t aa/lang-train.arff \
-T aa/lang-test.arff \
-classifications weka.classifiers.evaluation.output.prediction.PlainText
</code></pre>
"
857963;796;bfaskiplar;<java><machine-learning><apache-spark>;28253834;1;Processing JavaDStream at fixed intervals;"<p>I am building a Stream Learning application using Apache Spark. My application needs to process the streamed data at regular intervals but before or after these intervals, it can let it pass without any processing. So, I am not interested in all the stream items but some of them that show up at some specific intervals (at least for training part) I could not figure out how to process only some windows in the stream while ignoring the others.</p>
";28256679;764040;35826;maasg;1;28256679;"<p>If the messages contain a timestamp, a way to approach this is to filter the 'interesting' messages matching the intended interval and pass those to processing by Spark.</p>

<pre><code>val dstream = ???
val targetStream = dstream.filter(msg =&gt; withinInterval(timestamp(msg))}
targetStream.forEachRDD(rdd =&gt; // do something with the filtered elements)
</code></pre>
"
2396228;303;vinayakshukl;<python><machine-learning><scikit-learn><feature-extraction><tf-idf>;28254824;6;Feature space reduction for tag prediction;"<p>I am writing a ML module (python) to predict tags for a stackoverflow question (tag + body). My corpus is of around 5 million questions with title, body and tags for each. I'm splitting this 3:2 for training and testing. I'm  plagued by the <a href=""http://www.google.co.in/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=5&amp;cad=rja&amp;uact=8&amp;sqi=2&amp;ved=0CDEQFjAE&amp;url=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FCurse_of_dimensionality&amp;ei=3hfNVPP1KeTCmQWn94GgAg&amp;usg=AFQjCNFtGwiNd-uw_EvWQNyY1BXEAdXsvg&amp;sig2=4i1YZnLefahwMXWJau9EGg&amp;bvm=bv.85076809,d.dGY"" rel=""noreferrer"">curse of dimensionality</a>.</p>

<hr>

<h2>Work Done</h2>

<ol>
<li><strong>Pre-processing:</strong> markup removal, stopword removal, special character removal and a few bits and pieces. Store into MySQL. This almost halves the size of the test data.</li>
<li><p><strong>ngram association:</strong> for <em>each</em> unigram and bigram in the title and the body of <em>each</em> question, I maintain a list of the associated tags. Store into redis. This results in about a million unique unigrams and 20 million unique bigrams, each with a corresponding list of tag frequencies. Ex.</p>

<pre><code>""continuous integration"": {""ci"":42, ""jenkins"":15, ""windows"":1, ""django"":1, ....}
</code></pre></li>
</ol>

<p><strong>Note:</strong> There are 2 problems here: <em>a)</em> Not all unigrams and bigrams are important and, <em>b)</em> not all tags associated with a ngram are important, although this doesn't mean that tags with frequency 1 are all equivalent or can be haphazardly removed. The number of tags associated with a given ngram easily runs into the thousands - most of them unrelated and irrelevant. </p>

<ol start=""3"">
<li><p><strong>tfidf:</strong> to aid in selecting which ngrams to keep, I calculated the tfidf score for the entire corpus for each unigram and bigram and stored the corresponding idf values with associated tags. Ex.</p>

<pre><code>""continuous integration"": {""ci"":42, ""jenkins"":15, ...., ""__idf__"":7.2123}
</code></pre>

<p>The tfidf scores are stored in a <code>documentxfeature</code> <a href=""http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html"" rel=""noreferrer"">sparse.csr_matrix</a>, and I'm not sure how I can leverage that at the moment. (it is generated by <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.fit_transform"" rel=""noreferrer"">fit_transform()</a>)</p></li>
</ol>

<hr>

<h2>Questions</h2>

<ol>
<li>How can I use this processed data to reduce the size of my feature set? I've read about SVD and PCA but the examples always talk about a set of documents and a vocabulary. I'm not sure where the <em>tags</em> from my set can come in. Also, the way my data is stored (redis + sparse matrix), it is difficult to use an already implemented module (sklearn, nltk etc) for this task.</li>
<li><p>Once the feature set is reduced, the way I have planned to use it is as follows:</p>

<ul>
<li>Preprocess the test data.</li>
<li>Find the unigrams and bigrams.</li>
<li>For the ones stored in redis, find the corresponding best-k tags</li>
<li>Apply some kind of weight for the title and body text</li>
<li>Apart from this I might also search for exact known tag matches in the document. Ex, if ""ruby-on-rails"" occurs in the title/body then its a high probability that it's also a relevant tag.</li>
<li>Also, for tags predicted with a high probability, I might leverage a tag graph (a undirected graph with tags frequently occurring together having weighted edges between them) to predict more tags.</li>
</ul>

<p>Are there any suggestions on how to improve upon this? Can a classifier come in handy?</p></li>
</ol>

<hr>

<h2>Footnote</h2>

<p>I've a 16-core, 16GB RAM machine. The redis-server (which I'll move to a different machine) is stored in RAM and is ~10GB. All the tasks mentioned above (apart from tfidf) are done in parallel using ipython clusters.  </p>
";;867695;390;romanows;0;28598834;"<p>A baseline statistical approach would treat this as a classification problem.  Features are bags-of-words processed by a maximum entropy classifier like Mallet <a href=""http://mallet.cs.umass.edu/classification.php"" rel=""nofollow"">http://mallet.cs.umass.edu/classification.php</a>.  Maxent (aka logistic regression) is good at handling large feature spaces.  Take the probability associated with each each tag (i.e., the class labels) and choose some decision threshold that gives you a precision/recall tradeoff that works for your project.  Some of the Mallet documentation even mentions topic classification, which is very similar to what you are trying to do.</p>

<p>The open questions are how well Mallet handles the size of your data (which isn't that big) and whether this particular tool is a non-starter with the technology stack you mentioned.  You might be able to train offline (dump the reddis database to a text file in Mallet's feature format) and run the Mallet-learned model in Python.  Evaluating a maxent model is simple.  If you want to stay in Python and have this be more automated, there are Python-based maxent implementations in NLTK and probably in scikit-learn.  This approach is not at all state-of-the-art, but it'll work okay and be a decent baseline with which to compare more complicated methods.</p>
"
2396228;303;vinayakshukl;<python><machine-learning><scikit-learn><feature-extraction><tf-idf>;28254824;6;Feature space reduction for tag prediction;"<p>I am writing a ML module (python) to predict tags for a stackoverflow question (tag + body). My corpus is of around 5 million questions with title, body and tags for each. I'm splitting this 3:2 for training and testing. I'm  plagued by the <a href=""http://www.google.co.in/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=5&amp;cad=rja&amp;uact=8&amp;sqi=2&amp;ved=0CDEQFjAE&amp;url=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FCurse_of_dimensionality&amp;ei=3hfNVPP1KeTCmQWn94GgAg&amp;usg=AFQjCNFtGwiNd-uw_EvWQNyY1BXEAdXsvg&amp;sig2=4i1YZnLefahwMXWJau9EGg&amp;bvm=bv.85076809,d.dGY"" rel=""noreferrer"">curse of dimensionality</a>.</p>

<hr>

<h2>Work Done</h2>

<ol>
<li><strong>Pre-processing:</strong> markup removal, stopword removal, special character removal and a few bits and pieces. Store into MySQL. This almost halves the size of the test data.</li>
<li><p><strong>ngram association:</strong> for <em>each</em> unigram and bigram in the title and the body of <em>each</em> question, I maintain a list of the associated tags. Store into redis. This results in about a million unique unigrams and 20 million unique bigrams, each with a corresponding list of tag frequencies. Ex.</p>

<pre><code>""continuous integration"": {""ci"":42, ""jenkins"":15, ""windows"":1, ""django"":1, ....}
</code></pre></li>
</ol>

<p><strong>Note:</strong> There are 2 problems here: <em>a)</em> Not all unigrams and bigrams are important and, <em>b)</em> not all tags associated with a ngram are important, although this doesn't mean that tags with frequency 1 are all equivalent or can be haphazardly removed. The number of tags associated with a given ngram easily runs into the thousands - most of them unrelated and irrelevant. </p>

<ol start=""3"">
<li><p><strong>tfidf:</strong> to aid in selecting which ngrams to keep, I calculated the tfidf score for the entire corpus for each unigram and bigram and stored the corresponding idf values with associated tags. Ex.</p>

<pre><code>""continuous integration"": {""ci"":42, ""jenkins"":15, ...., ""__idf__"":7.2123}
</code></pre>

<p>The tfidf scores are stored in a <code>documentxfeature</code> <a href=""http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html"" rel=""noreferrer"">sparse.csr_matrix</a>, and I'm not sure how I can leverage that at the moment. (it is generated by <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.fit_transform"" rel=""noreferrer"">fit_transform()</a>)</p></li>
</ol>

<hr>

<h2>Questions</h2>

<ol>
<li>How can I use this processed data to reduce the size of my feature set? I've read about SVD and PCA but the examples always talk about a set of documents and a vocabulary. I'm not sure where the <em>tags</em> from my set can come in. Also, the way my data is stored (redis + sparse matrix), it is difficult to use an already implemented module (sklearn, nltk etc) for this task.</li>
<li><p>Once the feature set is reduced, the way I have planned to use it is as follows:</p>

<ul>
<li>Preprocess the test data.</li>
<li>Find the unigrams and bigrams.</li>
<li>For the ones stored in redis, find the corresponding best-k tags</li>
<li>Apply some kind of weight for the title and body text</li>
<li>Apart from this I might also search for exact known tag matches in the document. Ex, if ""ruby-on-rails"" occurs in the title/body then its a high probability that it's also a relevant tag.</li>
<li>Also, for tags predicted with a high probability, I might leverage a tag graph (a undirected graph with tags frequently occurring together having weighted edges between them) to predict more tags.</li>
</ul>

<p>Are there any suggestions on how to improve upon this? Can a classifier come in handy?</p></li>
</ol>

<hr>

<h2>Footnote</h2>

<p>I've a 16-core, 16GB RAM machine. The redis-server (which I'll move to a different machine) is stored in RAM and is ~10GB. All the tasks mentioned above (apart from tfidf) are done in parallel using ipython clusters.  </p>
";;3289963;179;Antonio Ercole De Luca;1;31485352;"<p>Use the public Api of <a href=""https://dandelion.eu/"" rel=""nofollow"">Dandelion</a>, this is a <a href=""https://dandelion.eu/semantic-text/entity-extraction-demo/"" rel=""nofollow"">demo</a>.
<br>
It extracts concepts from a text, so, in order to reduce dimentionality, you could use those concepts, instead of the bag-of-word paradigm.</p>
"
3494525;87;Sarah;<algorithm><machine-learning><artificial-intelligence><neural-network><perceptron>;28256441;3;Understanding Perceptrons;"<p>I just started a Machine learning class and we went over Perceptrons. For homework we are supposed to:
""Choose appropriate training and test data sets of two dimensions (plane). Use 10 data points for training and 5 for testing. "" Then we are supposed to write a program that will use a perceptron algorithm and output:</p>

<ul>
<li>a comment on whether the training data points are linearly
separable    </li>
<li>a comment on whether the test points are linearly separable</li>
<li>your initial choice of the weights and constants</li>
<li>the final solution equation (decision boundary)</li>
<li>the total number of weight updates that your algorithm made</li>
<li>the total number of iterations made over the training set</li>
<li>the final misclassification error, if any, on the training data and
also on the test data</li>
</ul>

<p>I have read the first chapter of my book several times and I am still having trouble fully understanding perceptrons.</p>

<p>I understand that you change the weights if a point is misclassified until none are misclassified anymore, I guess what I'm having trouble understanding is</p>

<ol>
<li>What do I use the test data for and how does that relate to the
training data?</li>
<li>How do I know if a point is misclassified?</li>
<li>How do I go about choosing test points, training points, threshold or a bias?</li>
</ol>

<p>It's really hard for me to know how to make up one of these without my book providing good examples. As you can tell I am pretty lost, any help would be so much appreciated.</p>
";28263357;2822762;1565;Dave;0;28256485;"<blockquote>
  <p>What do I use the test data for and how does that relate to the training data?</p>
</blockquote>

<p>Usually, to asses how well a particular algorithm performs, one first trains it and then uses <em>different</em> data to test how well it does on data it has never seen before.</p>

<blockquote>
  <p>How do I know if a point is misclassified?</p>
</blockquote>

<p>Your training data has labels, which means that for each point in the training set, you know what class it belongs to.</p>

<blockquote>
  <p>How do I go about choosing test points, training points, threshold or a bias?</p>
</blockquote>

<p>For simple problems, you usually take all the training data and split it around 80/20. You train on the 80% and test against the remaining 20%.</p>
"
3494525;87;Sarah;<algorithm><machine-learning><artificial-intelligence><neural-network><perceptron>;28256441;3;Understanding Perceptrons;"<p>I just started a Machine learning class and we went over Perceptrons. For homework we are supposed to:
""Choose appropriate training and test data sets of two dimensions (plane). Use 10 data points for training and 5 for testing. "" Then we are supposed to write a program that will use a perceptron algorithm and output:</p>

<ul>
<li>a comment on whether the training data points are linearly
separable    </li>
<li>a comment on whether the test points are linearly separable</li>
<li>your initial choice of the weights and constants</li>
<li>the final solution equation (decision boundary)</li>
<li>the total number of weight updates that your algorithm made</li>
<li>the total number of iterations made over the training set</li>
<li>the final misclassification error, if any, on the training data and
also on the test data</li>
</ul>

<p>I have read the first chapter of my book several times and I am still having trouble fully understanding perceptrons.</p>

<p>I understand that you change the weights if a point is misclassified until none are misclassified anymore, I guess what I'm having trouble understanding is</p>

<ol>
<li>What do I use the test data for and how does that relate to the
training data?</li>
<li>How do I know if a point is misclassified?</li>
<li>How do I go about choosing test points, training points, threshold or a bias?</li>
</ol>

<p>It's really hard for me to know how to make up one of these without my book providing good examples. As you can tell I am pretty lost, any help would be so much appreciated.</p>
";28263357;860196;8365;runDOSrun;12;28263357;"<blockquote>
  <p>What do I use the test data for and how does that relate to the
  training data?</p>
</blockquote>

<p>Think about a Perceptron as young child. You want to teach a child how to distinguish apples from oranges. You show it 5 different apples (all red/yellow) and 5 oranges (of different shape) while telling it what it sees at every turn (""this is a an apple. this is an orange). Assuming the child has perfect memory, it will learn to understand what makes an apple an apple and an orange an orange if you show him enough examples. He will eventually start to use meta-<strong>features</strong> (like shapes) without you actually telling him. This is what a Perceptron does. After you showed him all examples, you start at the beginning, this is called a new <strong>epoch</strong>.</p>

<p>What happens when you want to test the child's knowledge? You show it <strong>something new</strong>. A green apple (not just yellow/red), a grapefruit, maybe a watermelon. Why not show the child the exact same data as before during training? Because the child has perfect memory, it will only tell you what you told him. You won't see how good it <strong>generalizes</strong> from known to <strong>unseen</strong> data unless you have different <strong>training data</strong> that you never showed him during training. If the child has a horrible performance on the test data but a 100% performance on the training data, you will know that he has <strong>learned nothing</strong> - it's simply repeating what he has been told during training - you trained him too long, he only <strong>memorized</strong> your examples without understanding what makes an apple an apple because you gave him too many details - this is called <strong>overfitting</strong>. To prevent your Perceptron from only (!) recognizing training data you'll have to stop training at a reasonable time and find a good balance between the size of the training and testing set.</p>

<blockquote>
  <p>How do I know if a point is misclassified?</p>
</blockquote>

<p>If it's different from what it should be. Let's say an apple has class 0 and an orange has 1 (here you should start reading into Single/MultiLayer Perceptrons and how Neural Networks of multiple Perceptrons work). The network will take your input. How it's coded is irrelevant for this, let's say input is a string ""apple"". Your training set then is {(apple1,0), (apple2,0), (apple3,0), (orange1,1), (orange2,1).....}. Since you know the class beforehand, the network will either output 1 or 0 for the input ""apple1"". If it outputs 1, you perform (targetValue-actualValue) = (1-0) = 1. 1 in this case means that the network gives a wrong output. Compare this to the delta rule and you will understand that this small equation is part of the larger update equation. In case you get a 1 you will perform a weight update. If target and actual value are the same, you will always get a 0 and you know that the network didn't misclassify.</p>

<blockquote>
  <p>How do I go about choosing test points, training points, threshold or
  a bias?</p>
</blockquote>

<p>Practically the bias and threshold isn't ""chosen"" per se. The bias is trained like any other unit using a simple ""trick"", namely using the bias as an additional input unit with value 1 - this means the actual bias value is encoded in this additional unit's weight and the algorithm we use will make sure it learns the bias for us automatically. </p>

<p>Depending on your activation function, the threshold is predetermined. For a simple perceptron, the classification will occur as follows:</p>

<p><img src=""https://i.stack.imgur.com/yPxg8.png"" alt=""Perceptron""></p>

<p>Since we use a binary output (between 0 and 1), it's a good start to put the threshold at 0.5 since that's exactly the middle of the range [0,1].</p>

<p>Now to your last question about choosing training and test points: This is quite difficult, you do that by experience. Where you're at, you start off by implementing simple logical functions like AND, OR, XOR etc. There's it's trivial. You put everything in your training set and test with the same values as your training set (since for x XOR y etc. there are only 4 possible inputs 00, 10, 01, 11). For complex data like images, audio etc. you'll have to try and tweak your data and features until you feel like the network can work with it as good as you want it to.</p>
"
2769956;391;Palash Kumar;<matlab><machine-learning><classification><svm><cvx>;28263214;0;How To Do Linearly Separable Binary Classification?;"<p>I want to solve following optimization problem - </p>

<blockquote>
  <p><strong>Cost Function:</strong> 1/2 ||W||^2</p>
  
  <p><strong>Subject to   :</strong> Y_i(w.X_i - b) >= 1</p>
</blockquote>

<p>Where <code>X</code> is a 700x3 matrix, <code>Y</code> is a vector stores the label of classes for those instances (valued as 1/-1) and <code>w.X_i</code> is the dot product of <code>w</code> and <code>X_i</code>.</p>

<p>I am using CVX -</p>

<pre><code>cvx_begin
    variable W(3);
    variable B;
    minimize (0.5*W'*W)
    subject to 
        Y'*(X*W - B) &gt;= 1;
cvx_end
</code></pre>

<p>then, I am plotting, <code>w1.x1 + w2.x2 - b</code>
which does not seem to be separating hyper-plane?</p>

<p>Whats wrong am I doing?</p>
";;2216369;3192;Alan;0;28263586;"<p><strong>In short:</strong> 
when you are doing <code>w1.x1 + w2.x2 - b</code> you are trying to specify a hyperplane at a particular location, which is also the same as specifying a particular point on a vector.  To do either in a 3D space you need to use all three dimensions, so: <code>w1.x1 + w2.x2 +w3.x3 - b</code></p>

<p><strong>In longer:</strong>
When performing a linear classification such as this, the task can be viewed in two ways:</p>

<ol>
<li><p>Finding a separating hyperplane such that all samples of one class are on one side, and all samples of the other class are on the other side.</p></li>
<li><p>Finding a projection of the multidimensional space which the samples are in, into a single dimensional line, such that there is a point on the line which clearly separates them.</p></li>
</ol>

<p>These are identical tasks, since the single dimension in 2 is essentially how far each sample is from the separating hyperplane (and which side said sample is on).  I find it helps to bear both of these viewpoints in mind, particularly since the separating hyperplane is the plane orthogonal to the single dimensional vector.</p>

<p>So, in the case you are dealing with, the weight vector <code>w</code> provided by the model is used to project the samples in matrix <code>X</code> onto a single dimensional line and the offset <code>b</code> indicates at which point along this vector the separating hyperplane occurs. By subtracting <code>b</code> from the projected values they are shifted such that this hyperplane is the one orthogonal to the line at point 0 which makes for simple thresholding.</p>
"
2163392;1975;mad;<python><machine-learning><scikit-learn><random-forest>;28263685;0;Value Error in Scikit-learn Random forest fit method;"<p>I am trying to train (fit) a Random forest classifier using python and scikit-learn for a set of data stored as feature vectors. I can read the data, but I can't run the training of the classifier because of Value Erros. The source code that I am using is the following:</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier
from numpy import genfromtxt

 my_training_data = genfromtxt('csv-data.txt', delimiter=',')

 X_train = my_training_data[:,0]
 Y_train = my_training_data[:,1:my_training_data.shape[1]]

 clf = RandomForestClassifier(n_estimators=50)
 clf = clf.fit(X_train.tolist(), Y_train.tolist())
</code></pre>

<p>The error returned to me is the following:</p>

<pre><code>Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""/usr/lib/python2.7/dist-packages/sklearn/ensemble/forest.py"",  line 260, in fit
n_samples, self.n_features_ = X.shape
ValueError: need more than 1 value to unpack
</code></pre>

<p>The csv-data.txt is a comma separated values file, containing 3996 vectors for training of the classifier. I use the f
irst dimension to label the vector and the rest are float values. These are the dimensions of the feature vectors used in the classifier.</p>

<p>Did I miss some conversion here?  </p>
";28264168;1430829;3051;Matt Hancock;3;28264168;"<p>The training examples are stored by row in <code>""csv-data.txt""</code> with the first number of each row containing the class label. Therefore you should have:</p>

<pre><code>X_train = my_training_data[:,1:]
Y_train = my_training_data[:,0]
</code></pre>

<p>Note that in the second index in <code>X_train</code>, you can leave off the end index, and the indices will automatically run to the end (of course you can be explicit for clarity, but this is just FYI.</p>

<p>Also, there is no need to call <code>tolist()</code> in your call to <code>fit()</code> since these are already <code>numpy</code> <code>ndarray</code>, and the <code>fit()</code> function will convert them back to <code>numpy</code> <code>ndarray</code> if the argument is a list.</p>

<pre><code>clf.fit(X_train.tolist(), Y_train.tolist())
</code></pre>
"
2656211;77;Ivica Obadic;<machine-learning><nlp>;28265758;0;Categorize social events;"<p>I am having name and description of event and i want to find out about the categories of the event(for example is it entertainment event, politic event or something else).
 I was searching on the web and i looked at some natural language processing  techniques such as Latent Dirichlet Allocation but i can not see a way to use it in my situation. 
Is it a good idea to try to categorize by having predefined keywords for each category, and then to query the text and decide by the amount of keywords from each category?
Can someone give me a clue about my problem ? Many thanks</p>
";28511966;2887031;1518;Eric D. Brown;0;28511966;"<p>One approach you could take is to start simple and use a bayesian classifier to analyze/classify your data. </p>

<p>I would approach this problem by taking your dataset and splitting it into a training dataset and a non-training dataset. Then, manually review each event and categorize it as a type of event. Using this training dataset to run your classifier against the remainder of your data. </p>

<p>This may not be ideal for a large amount of event types but it might be a way for you to get started addressing the problem.</p>
"
4257646;73;kim;<opencv><image-processing><machine-learning><computer-vision>;28270373;1;How to remove text region in a document image?;"<p>Similar to this <a href=""https://stackoverflow.com/questions/26955513/how-to-detect-text-region-from-a-document-image"">post</a>, I want to ask the same question and see if there are other suggestions and ideas. </p>

<p>Given an document image (i.e. newspaper), how to extract photos in it or remove text region?</p>

<p>I think traditional OCR methods may not be suitable here, as I don't need to recognize the text, and OCR is not accurate and slow. I believe text region (i.e. text blocks) and image region should be distinguishable by some threshold based methods in image processing. Any suggestions or example codes in OpenCV will be appreciated. Thanksï¼</p>

<p><strong>BTW</strong>, what if the background color is not white, or the background color of certain blocks are not white?</p>

<p>Example image:</p>

<p><img src=""https://i.stack.imgur.com/7qrQP.jpg"" alt=""enter image description here""></p>
";;457687;4223;Vlad;0;28385925;"<p>Ocr doesnt take newspapers as input. It takes text regions that have to be found first. So it is indeed irrelevent. To find text regions one tipically uses adaptive thresholding (<a href=""http://docs.opencv.org/trunk/doc/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html"" rel=""nofollow"">tutorial</a>) to binarize an image and then a stroke width transform with connected components to find cosistent text regions. See this paper, <a href=""http://www.google.com/url?sa=t&amp;source=web&amp;cd=1&amp;ved=0CB0QFjAA&amp;url=http%3A%2F%2Fwww.math.tau.ac.il%2F~turkel%2Fimagepapers%2Ftext_detection.pdf&amp;rct=j&amp;q=stroke%20width%20transform&amp;ei=olrWVLiaPM6TNrCnhNAM&amp;usg=AFQjCNHc4piIXiX1AYV3fwvr3Vc1UXJInA&amp;sig2=5Eij2mtTfUaeAZf4nH3iyA"" rel=""nofollow"">swt</a>.
Trying to find images seems to be harder an endeavor though they too have some distinct features such as less amount of high spatial frequencies compared to text. </p>
"
835461;476;Jideobi Benedine Ofomah;<scala><matrix><machine-learning><apache-spark>;28273651;0;Spark MLib Matrix Multiplication;"<p>I tried the following code in Spark console</p>

<pre><code>import org.apache.spark.mllib.linalg.{Matrix, Matrices, DenseMatrix}

val dm: DenseMatrix = new DenseMatrix(3, 2, Array(1.0, 3.0, 5.0, 2.0, 4.0, 6.0))

val md: DenseMatrix = new DenseMatrix(2, 3, Array(1.0, 3.0, 5.0, 2.0, 4.0, 6.0))

dm.multiply(md) //this line throws an error
</code></pre>

<p>The error message that i get is:</p>

<pre><code>error: value multiply is not a member of org.apache.spark.mllib.linalg.Matrix
</code></pre>

<p>meanwhile i can call <code>dm.toString</code>, <code>dm.numRows</code> and <code>dm.numCols</code> i get results but when i call <code>dm.multiply</code>, it throws an error.</p>

<p>what am i doing wrong??</p>
";28275069;835461;476;Jideobi Benedine Ofomah;2;28275069;"<p>i figured out that multiply() is not supported on v1.1.0</p>

<p><a href=""http://spark.apache.org/docs/1.1.0/api/scala/index.html#org.apache.spark.mllib.linalg.DenseMatrix"" rel=""nofollow"">Spark v1.1.0</a></p>

<p>fixed this by upgrading to v1.2.0</p>
"
3804489;61;Chong Zheng;<machine-learning><cluster-analysis>;28274280;-2;Machine Learning: How to avoid over-fitting in for small data sets?;"<p>If we want to apply machine learning job on a small data set with many attributes, it may be easy to get over-fitting problem. 
How to avoid this?</p>
";;2040628;1181;TNM;0;28317583;"<p>The simplest answer is to use over-fitting resistant methods such as random forest or boosting with stump as the weak learner.</p>
"
4481550;142;MickaÃ«l B.;<opencv><machine-learning>;28274974;2;Using OpenCV cascade of classifier with traffic sign;"<p>I'm trying to detect some traffic signs like that: <a href=""http://i.stack.imgur.com/yOTZb.jpg"" rel=""nofollow"">http://i.stack.imgur.com/yOTZb.jpg</a></p>

<p>I managed to isolate the traffic sign (which is, for the moment, always round): <a href=""http://image.noelshack.com/fichiers/2015/06/1422869734-3.jpg"" rel=""nofollow"">http://image.noelshack.com/fichiers/2015/06/1422869734-3.jpg</a></p>

<p>With bad results with SURF for real-time, and tips of a preceding post, I want to use a cascade of a classifier for the detection of the different traffic sign (30,50,70...).</p>

<p>1) Is traffic signs will be nicely recognized to cascade of classifier like faces are? I mean by that, is a traffic sign is susceptible to haar features detection ?</p>

<p>2) Do I need to train one cascade of classifier (one .xml) for each sign? Do I need to put images of 30,70 traffic signs in the false positive of the 50 (
and conversely) ?</p>

<p>3) If I have understood, the positive images have to have the same size, and i need to delete the background ?</p>

<p>Sorry for my english, thanks :)</p>
";28276879;1348388;4898;Kornel;1;28276879;"<p>I'm of the opinion that you need to train some Haar detector per traffic sign's shape (one for triangular warning signs, an other one for circular signs, etc.). As a result of detection you will have some candidates for further processing and should be decided whether a candidate is true positive or not.<br />
If it is true positive: additional classification needs to recognize the type of a known shape. This classification can be an ANN algorithm or SVMs.
<br /><br />
Answers to your questions:<br /></p>

<ol>
<li>It's strongly depends on the positive/negative database, the features used for training (Haar, LBP, HoG), but I think this cascade structure can be useful for your purposes.</li>
<li>Partly answered above. For the negatives: you should use a very different set of images. E.g. landscapes, animals, etc. It's important to collect a large database because most of the negatives will be rejected during the first steps of the training.</li>
<li>You need to use same scale (for positives) during the training and it is recommended to use some global transformation for reducing the effect of different lighting conditions. But you don't need to remove the background, just crop the images along the border of signs.</li>
</ol>
"
1565344;924;Alex;<python><text><machine-learning><pybrain><supervised-learning>;28281218;1;Text conversion in python;"<p>I'm constructing a recurrent neural network with PyBrain for text classification problems. After numerous attempts I still can't sort out how to convert a list of strings into an array that can be used as a dataset. What I did: </p>

<pre><code>import collections,re
from pybrain.datasets import SupervisedDataSet

#create the supervised dataset variable with 5 inputs and 1 output
windowSize=5
main_ds = SupervisedDataSet(windowSize,1)

with open('ltest5lg_d1.fr','r') as train_1:
        import_data_train=train_1.readlines()

train_data = []

for lines in import_data_train:
    s = lines.split()        
    for words in s:
        train_data.append(words)

bagsofwords = [collections.Counter(re.findall(r'\w+', txt)) for txt in train_data]

sumbags = sum(bagsofwords, collections.Counter())
</code></pre>

<p>So I got the frequency table for the training data, but I can't sort out how to convert the data itself to some format that could be used as an input in main_ds variable. </p>
";;98975;6872;viksit;1;28289373;"<p>The standard way of representing words in the context of learning is the word embeddings model. </p>

<p>What you want (and this is with only a cursory glance at PyBrain's dataset page [1]) is to build a dataset by converting the text into their vector representations. </p>

<p>For an example of how to do it yourself, see glove-python [2]. If you'd like to use an existing package to do this, see Google's word2vec [3] or Stanford's GloVe [4], of which the python version is a naive implementation.</p>

<p>You could then use this representation to train your NN.</p>

<pre><code>[1] http://pybrain.org/docs/quickstart/dataset.html
[2] https://github.com/maciejkula/glove-python
[3] https://code.google.com/p/word2vec/
[4] http://www-nlp.stanford.edu/projects/glove/
</code></pre>
"
3322273;9310;SomethingSomething;<machine-learning><neural-network>;28288489;6;Neural Networks: Does the input layer consist of neurons?;"<p>I currently study the Neural Networks theory and I see that everywhere it is written that it consists of the following layers:</p>

<ul>
<li>Input Layer</li>
<li>Hidden Layer(s)</li>
<li>Output Layer</li>
</ul>

<p>I see some graphical descriptions that show the input layer as real nodes in the net, while others show this layer as just a vector of values <strong>[x1, x2, ... xn]</strong></p>

<p>What is the correct structure?</p>

<p>Is the ""input layer"" a real layer of neurons? Or is this just abstractly named as layer, while it really is just the input vector?</p>

<p>Adding contradicting and confusing photos I found in the web:</p>

<p><strong>Here it looks like the input layer consists of neurons:</strong>
<img src=""https://i.imgur.com/mIsGh53.jpg"" alt=""Screenshot""></p>

<p><strong>Here it looks like the input layer is just an input vector:</strong>
<img src=""https://c.mql5.com/18/20/NN1__1.gif"" alt=""Screenshot""></p>
";28289293;98975;6872;viksit;2;28289293;"<p>Let me answer your question with some mathematical notations that will make it easier to understand than just random images. First, remember the Perceptron.</p>

<p>The task of the Perceptron is to find a decision function that will classify some points in a given set into n classes. So, for a function </p>

<pre><code>f : R^n -&gt; R , f(X) = &lt;W, X&gt; + b
</code></pre>

<p>where W is a vector of weights, and X is the vector of points. As an example, if you have a line defined by the equation <code>3x + y = 0</code> then W is <code>(3,1)</code> and X is <code>(x,y)</code>.</p>

<p>A Neural Network can be thought of as a graph where each vertex of the graph is a simple perceptron - that is, each node in the network is nothing but a function that takes in some value and outputs a new one, which could then be used for the next node. In your second image, this would be the two hidden layers.</p>

<p>What then do these nodes need as input? A set of W and Xs - weight and point vectors. Which in your image is expressed by <code>x0, x1, .. xn</code> and <code>w0, w1, .. wn</code>.</p>

<p>Ultimately, we can conclude that what a Neural Network needs to function is a set of input vectors of weights and points. </p>

<p><em>My overall advice to you would be to pick one source for your learning and stick to that rather than going over the internet with conflicting images.</em></p>
"
3322273;9310;SomethingSomething;<machine-learning><neural-network>;28288489;6;Neural Networks: Does the input layer consist of neurons?;"<p>I currently study the Neural Networks theory and I see that everywhere it is written that it consists of the following layers:</p>

<ul>
<li>Input Layer</li>
<li>Hidden Layer(s)</li>
<li>Output Layer</li>
</ul>

<p>I see some graphical descriptions that show the input layer as real nodes in the net, while others show this layer as just a vector of values <strong>[x1, x2, ... xn]</strong></p>

<p>What is the correct structure?</p>

<p>Is the ""input layer"" a real layer of neurons? Or is this just abstractly named as layer, while it really is just the input vector?</p>

<p>Adding contradicting and confusing photos I found in the web:</p>

<p><strong>Here it looks like the input layer consists of neurons:</strong>
<img src=""https://i.imgur.com/mIsGh53.jpg"" alt=""Screenshot""></p>

<p><strong>Here it looks like the input layer is just an input vector:</strong>
<img src=""https://c.mql5.com/18/20/NN1__1.gif"" alt=""Screenshot""></p>
";28289293;860196;8365;runDOSrun;4;28289385;"<blockquote>
  <p>Is the ""input layer"" a real layer of neurons? Or is this just abstractly named as layer, while it really is just the input vector?</p>
</blockquote>

<p>Yes, it's both - depending on the abstraction. On paper the network has input neurons. On implementation level you have to organize this data (usually using arrays/vectors) which is why you speak of an input vector: </p>

<p><em>An input vector holds the input neuron values (representing the input layer).</em></p>

<p>If you're familiar with basics of graph theory or image processing - it's the same principle. For example, you can call an image a matrix (technical view) or a field of pixels (more abstract view).</p>
"
3347188;2421;Sami;<java><php><mysql><machine-learning><datumbox>;28291045;2;How to Connect a Java-based Machine Learning Framework to a MySQL Database Using PHP?;"<p>I want to connect the <a href=""https://github.com/datumbox/datumbox-framework/"" rel=""nofollow noreferrer"">Datumbox machine learning framework</a> to a MySQL database using PHP.<br>
Datumbox is an open-source Machine Learning Framework written in Java. Unfortunately I could not find any example to complete this task. I would appreciate any help to complete this task.</p>
";;3688538;1222;stepozer;1;28297303;"<p>I think you can use datumbox api:</p>

<ul>
<li>Common API URL: <a href=""http://www.datumbox.com/machine-learning-api/"" rel=""nofollow"">http://www.datumbox.com/machine-learning-api/</a></li>
<li>API doc PDF: <a href=""http://www.datumbox.com/files/API-Documentation-1.0v.pdf"" rel=""nofollow"">http://www.datumbox.com/files/API-Documentation-1.0v.pdf</a></li>
</ul>

<p>As you can see it used REST <code>JSON</code> API, so i think it will be not hard to used it.</p>

<p><strong>EDIT:</strong></p>

<p>Also for this API exits PHP client:</p>

<p><a href=""http://www.datumbox.com/files/Datumbox_APIclient_PHP_1.0.zip"" rel=""nofollow"">http://www.datumbox.com/files/Datumbox_APIclient_PHP_1.0.zip</a></p>
"
4257646;73;kim;<opencv><image-processing><machine-learning><computer-vision><object-detection>;28294847;1;How do I deal with occlusion, warping, or perspective distortion for object detection?;"<p>I am using SIFT features for detecting logos in clothes. SIFT is quite powerful, as it is scale invariant, but how do I deal with the problem of occlusion and warping?</p>

<p>More specifically, the logos in clothes might be partially occluded, and can also be warped when the clothes are stretched or squeezed.</p>

<p>Any suggestion and directions? Thanks in advance!</p>

<p>Some example are showed here:</p>

<p>Logo on leg: </p>

<p><img src=""https://i.stack.imgur.com/DcV1e.jpg"" alt=""enter image description here""></p>

<p><img src=""https://i.stack.imgur.com/AzoLD.jpg"" alt=""enter image description here""></p>

<p><img src=""https://i.stack.imgur.com/ctMJH.jpg"" alt=""enter image description here""></p>
";;457687;4223;Vlad;0;28334469;"<p><a href=""http://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision"" rel=""nofollow"">bag of words</a> is your answer. It skips an affine transform confirming a proper alignment of your SIFT points and uses only their descriptors to confirm their identities. The price you pay - you will need more points to confirm a logo. </p>

<p>On top of that you can write your own spatial consistency validation routine since even when affine transform is a bad fit to your distorted images the basic ordering constraints (whatâ€™s on the right, left, bottom and top) should still be valid. </p>
"
4524784;57;Legie;<c++><opencv><machine-learning><libsvm><multilabel-classification>;28303334;2;Multi-class image classification with probability estimation;"<p>my goal is to do multi-class image classification with probability estimation. </p>

<p>So far the 'one-label'-classification is working nicely out-of-the-box with all the great functionalities the OpenCV C++ libraries provide. Currently I am using a BoW descriptor with local Sift descriptors and SVM classification. So far so good. But now I need probability estimates for the images. So instead of ""image A is class X"", <strong>I need the output ""image A is with 50% likelihood class X, with 10% class Y, 30% class Z""</strong>, etc. with estimations for all classes.</p>

<p>Unfortunately I am not that competent in machine learning. I started to investigate the problem and now my brain hurts. My noob-questions for you:</p>

<ul>
<li>Is the libsvm option <code>-b probability_estimates</code> what I am looking for?</li>
<li>Is there a way to do this solely with OpenCV? (If <a href=""https://stackoverflow.com/questions/19810228/get-svm-classification-score-in-multiclass-classification-with-opencv"">Get SVM classification score in multiclass classification with OpenCV</a> is the only way, could someone explain the output to me?)</li>
<li>Do you have any other suggestions on how to achieve my goal? Papers to read, libraries to use?</li>
</ul>

<p>Any tips are appreciated. Thanks!</p>

<p>P.S.: I know there are a bunch of similar questions answered here before but to me none of them really captured my point.</p>
";28313972;540259;9022;lightalchemist;3;28313972;"<p>Some implementations of the SVM algorithm do provide probability estimates. However, the SVM does <em>not</em> inherently provide probability estimates. It is a function that is ""tacked on"" after the algorithm was created. These probability estimates are not ""trustworthy"", and if I remember correctly, the ability to compute probability estimates was removed from the Scikit-Learn library a few releases ago for this reason. Still, if you insist on using SVM, look at <a href=""http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf"" rel=""nofollow"" title=""A practical guide to support vector classification"">A Practical Guide to Support Vector Classification</a> from LibSVM. It is the library that OpenCV calls. You can skip the math to get to the tips. The outputs of LibSVM, and hence OpenCV's SVM, are explained in the document. Alternatively, you can choose to use LibSVM instead. This will allow you to get to the probability estimates without recompiling OpenCV (as suggested in your link), but the downside is you will have to pass your data to the appropriate form for LibSVM (i.e., OpenCV's Mat is unlikely to work directly with LibSVM).</p>

<p>If you are using a Linear SVM, i.e., SVM with the linear kernel, then you can try replacing it with a Logistic Regression classifier as empirically they behave similarly (both are linear classifiers, just that one uses hinge loss and the other, logistic loss). The probability estimates from Logistic Regression would work.</p>

<p>Alternatively, consider using a Random Forest (or its variant, Extremely Randomized Trees) classifier. They also provide probability estimates as the proportion of training samples in a given leaf node reached by your test sample. Having said that, these two classifiers are not based on principled mathematics (although researchers are working on figuring out how they work theoretically), although they have been known to work superbly in many real-world settings (Kinect pose estimation is an example).</p>

<p>The thing is coming up with probability estimates is a very hard if your classifier is not designed to do that from the beginning, i.e., not one of those you find from a standard statistical machine learning textbook. It is like pulling numbers out of one's ass. Most algorithm that perform classification simply compute a ""score"" for each category/label for each test sample and go with the one with the ""best"" score. That is much easier to do. For the SVM, it tries to ""translate"" this score to a ""probability"", but it is not ""calibrated"", which effectively makes it useless.</p>

<p>You can take a look at this paper: <a href=""http://machinelearning.org/proceedings/icml2005/papers/079_GoodProbabilities_NiculescuMizilCaruana.pdf"" rel=""nofollow"">Predicting Good Probabilities With Supervised Learning</a> for more details on how the probabilities are computed for some of these classifiers, and why they need to be calibrated.</p>

<p>In general, I would advice taking probability estimates returned by classifier with a grain of salt. If you want them, go with a statistical classifier, e.g., Logistic Regression, not SVM.</p>

<p>As for libraries, while OpenCV do provide some machine learning algorithms, they are very limited. Try a proper ML library. I'm assuming you are using C++, so I will recommend taking a look at the free <a href=""http://www.shogun-toolbox.org/"" rel=""nofollow"" title=""Shogun Machine Learning Library"">Shogun Machine Learning Library</a>.</p>

<p>If you are using Python, or just wish to take a look at tutorials on how to use machine learning algorithms, then check out the excellent <a href=""http://scikit-learn.org/"" rel=""nofollow"">Scikit-Learn library</a>.</p>

<p>Some general advice on applying machine learning algorithms to industry problems (slides): <a href=""http://kdd2012.sigkdd.org/sites/images/ipe/IPE-Lin.pdf"" rel=""nofollow"">Experiences and Lessons in Developing Industry-Strength Machine Learning and Data Mining Software</a>.</p>
"
3246234;234;shanky_thebearer;<java><python><machine-learning><nlp>;28303344;1;Automated Textual feedback analysis in Java/Python;"<p>I need to develop an automated learning based Textual Feedback analysis system for a set of online courses, pretty much like the usual MOOCs such as Coursera, EdX, etc. What Java/Python tools/services can be used. The system should scan the sentence understand it's implication. For eg. </p>

<p><strong>Input:</strong> ""This course is very informative."" </p>

<p><strong>Output</strong>: <strong>Set</strong> positive-feedback for course observed flag. </p>

<p><strong>Input</strong>: ""The instructor's language skills were poor."" </p>

<p><strong>Output</strong>: <strong>Set</strong> positive-feedback for instructor observed flag.. </p>

<p><strong>Input</strong>: ""The course material was not adequate."" </p>

<p><strong>Output</strong>: <strong>Set</strong> negative-feedback for course observed flag.</p>
";28303728;2362381;10431;JAB;1;28303728;"<p>This question is very general. Here are some link to start with.
<a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/corenlp.shtml</a> does sentiment scoring out of the box using Socher et al's sentiment model.</p>

<p>Python's NLTK is also a good place to start. Here is a blog post on how to use it to build a sentiment analyizer:
<a href=""http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/"" rel=""nofollow"">http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/</a></p>

<p>Also TextBlob (also python) has a pretrained sentiment classifier:
<a href=""http://textblob.readthedocs.org/en/latest/quickstart.html"" rel=""nofollow"">http://textblob.readthedocs.org/en/latest/quickstart.html</a></p>
"
2523851;632;Dman2;<python><machine-learning><scikit-learn><cross-validation>;28303522;1;Custom scorer in SciKit-Learn - allow grid search optimisation for a particular class;"<p>I would like to create a custom scorer in SciKit-Learn that I can pass to GridSearchCV, which evaluates model performance based upon the accuracy of predictions for a particular class.</p>

<p>Suppose that my training data consists of data-points belonging to one of three classes: </p>

<blockquote>
  <p>'dog', 'cat', 'mouse'</p>
</blockquote>

<pre><code># Create a classifier:
clf = ensemble.RandomForestClassifier()

# Set up some parameters to explore:
param_dist =    {
                 'n_estimators':[500, 1000, 2000, 4000],
                 ""criterion"": [""gini"", ""entropy""],
                 'bootstrap':[True, False]
                }

# Construct grid search
search = GridSearchCV(clf,\
                      param_grid=param_dist,\
                      cv=StratifiedKFold(y, n_folds=10),\
                      scoring=my_scoring_function)


# Perform search
X = training_data
y = ground_truths
search.fit(X, y)
</code></pre>

<p>Is there a way to construct my_scoring_function, such that only the accuracy of predictions for the 'dog' class is returned? The <a href=""http://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules"" rel=""nofollow"">make_scorer function</a> seems to be limited in that it only deals with the ground truth and the predicted class for each data-point.</p>

<p>Many thanks in advance of your help!</p>
";28371596;2523851;632;Dman2;1;28371596;"<p>I missed a section in the sklearn documentation.</p>

<p>You can create a function that requires the following inputs; model, x_test, y_test, and outputs a value between 0 and 1 (where 1 is best), that can be used as the optimisation function.</p>

<p>Simply create the function, apply model.predict(x_test), and then analyse the results using a metric such as accuracy.</p>
"
4096526;64;Brett Evanson;<php><machine-learning><svm><feature-extraction>;28310111;0;PHP SVM - Feature engineering;"<p>We have data for people going to school. Things like previous education level. Interests. Gender. Military status.</p>

<p>How do we convert these to integer/float values so we can throw them into an SVM model? Am I going about this the right way?</p>

<p>Side note, is the PHP SVM module an ok thing for just doing some simple modeling? Or is it not worth using?</p>
";;4096526;64;Brett Evanson;0;28351250;"<p>I found the answer. The answer is to make matrices out of the options. So gender would become male and female as 2 separate columns, each with their own 0/1 value.</p>
"
4114372;1915;john doe;<python><python-2.7><machine-learning><nlp><scikit-learn>;28312268;2;How to increase the presicion of text classification with the RBM?;"<p>I am learning about text classification and I classify with my own corpus with linnear regression as follows:</p>

<pre><code>from sklearn.linear_model.logistic import LogisticRegression
classifier = LogisticRegression(penalty='l2', C=7)
classifier.fit(training_matrix, y_train)
prediction = classifier.predict(testing_matrix)
</code></pre>

<p>I would like to increase the classification report with a Restricted Boltzman Machine that scikit-learn provide, from the <a href=""http://scikit-learn.org/stable/auto_examples/plot_rbm_logistic_classification.html"" rel=""nofollow"">documentation</a> I read that this could be use to increase the classification recall, f1-score, accuracy, etc. Could anybody help me to increase this is what I tried so far, thanks in advance:</p>

<pre><code>vectorizer = TfidfVectorizer(max_df=0.5,
                             max_features=None,
                             ngram_range=(1, 1),
                             norm='l2',
                             use_idf=True)


X_train = vectorizer.fit_transform(X_train_r)
X_test = vectorizer.transform(X_test_r)


from sklearn.pipeline import Pipeline
from sklearn.neural_network import BernoulliRBM
logistic = LogisticRegression()
rbm= BernoulliRBM(random_state=0, verbose=True)
classifier = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])
classifier.fit(X_train, y_train)
</code></pre>
";;2040628;1181;TNM;2;28317500;"<p>First, you have to understand the concepts here. RBM can be seen as a powerful clustering algorithm and clustering algorithms are unsupervised, i.e., they don't need labels.
 Perhaps, the best way to use RBM in your problem is, first to train an RBM (which only needs data without labels) and then use the RBM weights to initialize a Neural network. To get a logistic regression in the output, you have to add an output layer with logistic reg. cost function to this neural net and train this neural network. This setting may result in performance improvement.  </p>
"
4114372;1915;john doe;<python><python-2.7><machine-learning><nlp><scikit-learn>;28312268;2;How to increase the presicion of text classification with the RBM?;"<p>I am learning about text classification and I classify with my own corpus with linnear regression as follows:</p>

<pre><code>from sklearn.linear_model.logistic import LogisticRegression
classifier = LogisticRegression(penalty='l2', C=7)
classifier.fit(training_matrix, y_train)
prediction = classifier.predict(testing_matrix)
</code></pre>

<p>I would like to increase the classification report with a Restricted Boltzman Machine that scikit-learn provide, from the <a href=""http://scikit-learn.org/stable/auto_examples/plot_rbm_logistic_classification.html"" rel=""nofollow"">documentation</a> I read that this could be use to increase the classification recall, f1-score, accuracy, etc. Could anybody help me to increase this is what I tried so far, thanks in advance:</p>

<pre><code>vectorizer = TfidfVectorizer(max_df=0.5,
                             max_features=None,
                             ngram_range=(1, 1),
                             norm='l2',
                             use_idf=True)


X_train = vectorizer.fit_transform(X_train_r)
X_test = vectorizer.transform(X_test_r)


from sklearn.pipeline import Pipeline
from sklearn.neural_network import BernoulliRBM
logistic = LogisticRegression()
rbm= BernoulliRBM(random_state=0, verbose=True)
classifier = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])
classifier.fit(X_train, y_train)
</code></pre>
";;270287;41194;IVlad;2;28320865;"<p>There are a couple of things that could be wrong.</p>

<p><strong>1. You haven't properly calibrated the RBM</strong></p>

<p>Look at the example on the scikit-learn site: <a href=""http://scikit-learn.org/stable/auto_examples/plot_rbm_logistic_classification.html"" rel=""nofollow"">http://scikit-learn.org/stable/auto_examples/plot_rbm_logistic_classification.html</a></p>

<p>In particular, these lines:</p>

<pre><code>rbm.learning_rate = 0.06
rbm.n_iter = 20
# More components tend to give better prediction performance, but larger
# fitting time
rbm.n_components = 100
</code></pre>

<p>You don't set these anywhere. In the example, these are obtained through cross validation using a grid search. You should do the same and try to obtain (close to) optimal parameters for your own problem.</p>

<p>Additionally, you might want to try using cross validation to determine other parameters as well, such as the ngram range (using higher level ngrams as well usually helps, if you can afford the memory and execution time. For some problems, character level ngrams do better than word level) and logistic regression parameters.</p>

<p><strong>2. You are just unlucky</strong></p>

<p>There is nothing that says using an RBM in an intermediate step will definitely improve any performance measure. It can, but it's not a rule, it may very well do nothing or very little for your problem. You have to be prepared for this. </p>

<p>It's worth trying because it shouldn't take long to implement, but be prepare to have to look elsewhere.</p>

<p>Also look at the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"" rel=""nofollow"">SGDClassifier</a> and the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveClassifier.html"" rel=""nofollow"">PassiveAggressiveClassifier</a>. These <em>might</em> improve performance.</p>
"
4140027;3985;tumbleweed;<python><numpy><machine-learning><nlp><scikit-learn>;28314337;10;"TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0] while using RF classifier?";"<p>I am learning about random forests in scikit learn and as an example I would like to use Random forest classifier for text classification, with my own dataset. So first I vectorized the text with tfidf and for classification:</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier
classifier=RandomForestClassifier(n_estimators=10) 
classifier.fit(X_train, y_train)           
prediction = classifier.predict(X_test)
</code></pre>

<p>When I run the classification I got this:</p>

<pre><code>TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.
</code></pre>

<p>then I used the <code>.toarray()</code> for <code>X_train</code> and I got the following:</p>

<pre><code>TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]
</code></pre>

<p>From a previous <a href=""https://stackoverflow.com/questions/21689141/classifying-text-documents-with-random-forests"">question</a> as I understood I need to reduce the dimensionality of the numpy array so I do the same:</p>

<pre><code>from sklearn.decomposition.truncated_svd import TruncatedSVD        
pca = TruncatedSVD(n_components=300)                                
X_reduced_train = pca.fit_transform(X_train)               

from sklearn.ensemble import RandomForestClassifier                 
classifier=RandomForestClassifier(n_estimators=10)                  
classifier.fit(X_reduced_train, y_train)                            
prediction = classifier.predict(X_testing) 
</code></pre>

<p>Then I got this exception:</p>

<pre><code>  File ""/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.py"", line 419, in predict
    n_samples = len(X)
  File ""/usr/local/lib/python2.7/site-packages/scipy/sparse/base.py"", line 192, in __len__
    raise TypeError(""sparse matrix length is ambiguous; use getnnz()""
TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]
</code></pre>

<p>The I tried the following:</p>

<pre><code>prediction = classifier.predict(X_train.getnnz()) 
</code></pre>

<p>And got this:</p>

<pre><code>  File ""/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.py"", line 419, in predict
    n_samples = len(X)
TypeError: object of type 'int' has no len()
</code></pre>

<p>Two questions were raised from this: How can I use Random forests to classify correctly? and what's happening with <code>X_train</code>?. </p>

<p>Then I tried the following:</p>

<pre><code>df = pd.read_csv('/path/file.csv',
header=0, sep=',', names=['id', 'text', 'label'])



X = tfidf_vect.fit_transform(df['text'].values)
y = df['label'].values



from sklearn.decomposition.truncated_svd import TruncatedSVD
pca = TruncatedSVD(n_components=2)
X = pca.fit_transform(X)

a_train, a_test, b_train, b_test = train_test_split(X, y, test_size=0.33, random_state=42)

from sklearn.ensemble import RandomForestClassifier

classifier=RandomForestClassifier(n_estimators=10)
classifier.fit(a_train, b_train)
prediction = classifier.predict(a_test)

from sklearn.metrics.metrics import precision_score, recall_score, confusion_matrix, classification_report
print '\nscore:', classifier.score(a_train, b_test)
print '\nprecision:', precision_score(b_test, prediction)
print '\nrecall:', recall_score(b_test, prediction)
print '\n confussion matrix:\n',confusion_matrix(b_test, prediction)
print '\n clasification report:\n', classification_report(b_test, prediction)
</code></pre>
";28314568;2362381;10431;JAB;3;28314568;"<p>It is a bit unclear if you are passing the same data structure (type and shape) to the <code>fit</code> method and <code>predict</code> method of the classifier. Random forests will take a long time to run with a large number of features, hence the suggestion to reduce the dimensionality in the post you link to.</p>

<p>You should apply the SVD to both the training and test data so the classifier in trained on the same shaped input as the data you wish to predict for. Check the input to the fit, and the input to the predict method have the same number of features, and are both arrays rather than sparse matrices. </p>

<p><strong>updated with example:</strong>
<strong>updated to use dataframe</strong></p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vect= TfidfVectorizer(  use_idf=True, smooth_idf=True, sublinear_tf=False)
from sklearn.cross_validation import train_test_split

df= pd.DataFrame({'text':['cat on the','angel eyes has','blue red angel','one two blue','blue whales eat','hot tin roof','angel eyes has','have a cat']\
              ,'class': [0,0,0,1,1,1,0,3]})



X = tfidf_vect.fit_transform(df['text'].values)
y = df['class'].values

from sklearn.decomposition.truncated_svd import TruncatedSVD        
pca = TruncatedSVD(n_components=2)                                
X_reduced_train = pca.fit_transform(X)  

a_train, a_test, b_train, b_test = train_test_split(X, y, test_size=0.33, random_state=42)

from sklearn.ensemble import RandomForestClassifier 

classifier=RandomForestClassifier(n_estimators=10)                  
classifier.fit(a_train.toarray(), b_train)                            
prediction = classifier.predict(a_test.toarray()) 
</code></pre>

<p>Note the SVD happens before the split into training and test sets, so that the array passed to the predictor has the same <code>n</code> as the array the <code>fit</code> method is called on.</p>
"
4140027;3985;tumbleweed;<python><numpy><machine-learning><nlp><scikit-learn>;28314337;10;"TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0] while using RF classifier?";"<p>I am learning about random forests in scikit learn and as an example I would like to use Random forest classifier for text classification, with my own dataset. So first I vectorized the text with tfidf and for classification:</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier
classifier=RandomForestClassifier(n_estimators=10) 
classifier.fit(X_train, y_train)           
prediction = classifier.predict(X_test)
</code></pre>

<p>When I run the classification I got this:</p>

<pre><code>TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.
</code></pre>

<p>then I used the <code>.toarray()</code> for <code>X_train</code> and I got the following:</p>

<pre><code>TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]
</code></pre>

<p>From a previous <a href=""https://stackoverflow.com/questions/21689141/classifying-text-documents-with-random-forests"">question</a> as I understood I need to reduce the dimensionality of the numpy array so I do the same:</p>

<pre><code>from sklearn.decomposition.truncated_svd import TruncatedSVD        
pca = TruncatedSVD(n_components=300)                                
X_reduced_train = pca.fit_transform(X_train)               

from sklearn.ensemble import RandomForestClassifier                 
classifier=RandomForestClassifier(n_estimators=10)                  
classifier.fit(X_reduced_train, y_train)                            
prediction = classifier.predict(X_testing) 
</code></pre>

<p>Then I got this exception:</p>

<pre><code>  File ""/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.py"", line 419, in predict
    n_samples = len(X)
  File ""/usr/local/lib/python2.7/site-packages/scipy/sparse/base.py"", line 192, in __len__
    raise TypeError(""sparse matrix length is ambiguous; use getnnz()""
TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]
</code></pre>

<p>The I tried the following:</p>

<pre><code>prediction = classifier.predict(X_train.getnnz()) 
</code></pre>

<p>And got this:</p>

<pre><code>  File ""/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.py"", line 419, in predict
    n_samples = len(X)
TypeError: object of type 'int' has no len()
</code></pre>

<p>Two questions were raised from this: How can I use Random forests to classify correctly? and what's happening with <code>X_train</code>?. </p>

<p>Then I tried the following:</p>

<pre><code>df = pd.read_csv('/path/file.csv',
header=0, sep=',', names=['id', 'text', 'label'])



X = tfidf_vect.fit_transform(df['text'].values)
y = df['label'].values



from sklearn.decomposition.truncated_svd import TruncatedSVD
pca = TruncatedSVD(n_components=2)
X = pca.fit_transform(X)

a_train, a_test, b_train, b_test = train_test_split(X, y, test_size=0.33, random_state=42)

from sklearn.ensemble import RandomForestClassifier

classifier=RandomForestClassifier(n_estimators=10)
classifier.fit(a_train, b_train)
prediction = classifier.predict(a_test)

from sklearn.metrics.metrics import precision_score, recall_score, confusion_matrix, classification_report
print '\nscore:', classifier.score(a_train, b_test)
print '\nprecision:', precision_score(b_test, prediction)
print '\nrecall:', recall_score(b_test, prediction)
print '\n confussion matrix:\n',confusion_matrix(b_test, prediction)
print '\n clasification report:\n', classification_report(b_test, prediction)
</code></pre>
";28314568;901925;173987;hpaulj;13;28315424;"<p>I don't know much about <code>sklearn</code>, though I vaguely recall some earlier issue triggered by a switch to using sparse matricies.  Internally some of the matrices had to replaced by <code>m.toarray()</code> or <code>m.todense()</code>.</p>

<p>But to give you an idea of what the error message was about, consider</p>

<pre><code>In [907]: A=np.array([[0,1],[3,4]])
In [908]: M=sparse.coo_matrix(A)
In [909]: len(A)
Out[909]: 2
In [910]: len(M)
...
TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]

In [911]: A.shape[0]
Out[911]: 2
In [912]: M.shape[0]
Out[912]: 2
</code></pre>

<p><code>len()</code> usually is used in Python to count the number of 1st level terms of a list.  When applied to a 2d array, it is the number of rows.  But <code>A.shape[0]</code> is a better way of counting the rows. And <code>M.shape[0]</code> is the same.  In this case you aren't interested in <code>.getnnz</code>, which is the number of nonzero terms of a sparse matrix.  <code>A</code> doesn't have this method, though can be derived from <code>A.nonzero()</code>.</p>
"
4470717;61;Andyrey;<opencv><machine-learning><computer-vision><haar-classifier><anpr>;28324582;-1;"How to differentiate 2 classes: digits and ""other letters and noise"" on an image?";"<p>I develop an image recognition algorithm that helps to find characters on dirty pannels from the real world. Actually the image is a car registering plate containing letters, digits and a mud.</p>

<p>The algorithm must classify characters into two classes: alphabet characters and digits. Is it possible to train LBP or Haar cascade to discriminate between the two classes, will be training result stable due to digits shape variety?</p>

<p>Could you explain briefly or recommend better method, please?</p>
";;457687;4223;Vlad;0;28334117;"<p>""The algorithm must classify characters into two classes: alphabet characters and digits.â€ - you forgot mud and background though technically you can add them to a broad category â€œotherâ€. Haars cascades are used for something like face detection since they typically approximate wavelets on the middle spatial scale where faces have characteristic features. Your problem is different.You need to first understand your problem structure, read the literature and only then try to use a sheer force of learning algorithms. This <a href=""http://web4.cs.ucl.ac.uk/staff/s.prince/book/book.pdf"" rel=""nofollow"">book</a> actually talks a bit about people starting to think about method first instead of analyzing the problem which is not always a good idea.</p>

<p>Technically you first need to find the text in the image which can be more challenging than recognizing it given the current state of art OCR that is typically used as a library rather than created from scratch. To find text in the image I suggest first do adaptive thresholding to create a binary map (1-foreground that is letters and numbers and 0 is background), then perform connected components on the foreground coupled with SWT (stroke width transform) <a href=""http://research.microsoft.com/pubs/149305/1509.pdf"" rel=""nofollow"">http://research.microsoft.com/pubs/149305/1509.pdf</a></p>
"
3322273;9310;SomethingSomething;<machine-learning><neural-network>;28326776;22;Liquid State Machine: How it works and how to use it?;"<p>I am now learning about LSM (Liquid State Machines), and I try to understand how they are used for learning.</p>

<p>I am pretty confused from what I read over the web.</p>

<p>I'll write what I understood -> It may be incorrect and I'll be glad if you can correct me and explain what is true:</p>

<ol>
<li><p>LSMs are not trained at all: They are just initialized with many ""temporal neurons"" (e.g. Leaky Integrate &amp; Fire neurons), while their thresholds are selected randomly, and so the connections between them (i.e. not each neuron has to have a common edge with each of the other neurons).</p></li>
<li><p>If you want to ""learn"" that <strong>x</strong> time-units after inputting <strong>I</strong>, the occurrence <strong>Y</strong> occurs, you need to ""wait"" <strong>x</strong> time-units with the LIF ""detectors"", and see which neurons fired at this specific moment. Then, you can train a classifier (e.g. FeedForward Network), that this specific subset of firing neurons means that the occurrence <strong>Y</strong> happened.</p></li>
<li><p>You may use many ""temporal neurons"" in your ""liquid"", so you may have many possible different subsets of firing neurons, so a specific subset of firing neurons becomes almost unique for the moment after you waited <strong>x</strong> time-units, after inputting your input <strong>I</strong></p></li>
</ol>

<p>I don't know whether what I wrote above is true, or whether it is a total garbage.</p>

<p>Please tell me if this is the correct usage and targets of LIF.</p>
";28343829;2059318;356;Hananel;23;28343829;"<p>From your questions, it seems that you are on the right track. Anyhow, the Liquid State Machine and Echo State machine are complex topics that deal with computational neuroscience and physics, topics like chaos, dynamic action system, and feedback system and machine learning. So, itâ€™s ok if you feel like itâ€™s hard to wrap your head around it.</p>

<p>To answer your questions: </p>

<ol>
<li>Most implementations of Liquid State Machines using the reservoir of neurons untrained. There have been some attempts to train the reservoir but they haven't had the dramatic success that justifies the computational power that is needed for this aim.
(See: <a href=""http://www.sciencedirect.com/science/article/pii/S1574013709000173"" rel=""nofollow noreferrer"">Reservoir Computing Approaches to Recurrent Neural Network Training</a>) or (<a href=""http://www.igi.tugraz.at/psfiles/pdelta-journal.pdf"" rel=""nofollow noreferrer"">The p-Delta Learning Rule for Parallel Perceptrons</a> )<br><br>
My opinion is that if you want to use the Liquid as classifier in terms of separability or generalization of pattern, you can gain much more from the way the neurons connect between each other (see <a href=""http://dx.doi.org/10.1016/j.eswa.2011.06.052"" rel=""nofollow noreferrer"">Hazan, H. and Manevitz, L., Topological constraints and robustness in liquid state machines, Expert Systems with Applications, Volume 39, Issue 2, Pages 1597-1606, February 2012</a>.) or (<a href=""http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5178822"" rel=""nofollow noreferrer"">Which Model to Use for the Liquid State Machine?</a>)
The biological approach (in my opinion the most interesting one) (<a href=""http://www.mitpressjournals.org/doi/abs/10.1162/0899766054796888"" rel=""nofollow noreferrer"">What Can a Neuron Learn with Spike-Timing-Dependent Plasticity?</a> ) </li>
<li>You are right, you need to wait at least until you finish giving the input, otherwise you risk in detect your input, and not the activity that occurs as a result from your input as it should be.</li>
<li>Yes, you can imagine that your liquid complexity is a kernel in SVM that try to project the data points to some hyperspace and the detector in the liquid as the part that try to separate between the classes in the dataset. As a rule of the thumb, the number of neurons and the way they connect between each other determine the degree of complexity of the liquid.
<br></li>
</ol>

<p>Regarding LIF (Leaky Integrate & Fire neurons), as I see it (I could be wrong) the big difference between the two approaches is the individual unit. In liquid state machine uses biological like neurons, and in the Echo state uses more analog units. So, in terms of â€œvery short term memoryâ€ the Liquid State approach each individual neuron remembers its own history, where in the Echo state approach each individual neuron reacts based only on the current state, and therefore the memory stored in the activity between the units.</p>
"
3322273;9310;SomethingSomething;<machine-learning><neural-network>;28326776;22;Liquid State Machine: How it works and how to use it?;"<p>I am now learning about LSM (Liquid State Machines), and I try to understand how they are used for learning.</p>

<p>I am pretty confused from what I read over the web.</p>

<p>I'll write what I understood -> It may be incorrect and I'll be glad if you can correct me and explain what is true:</p>

<ol>
<li><p>LSMs are not trained at all: They are just initialized with many ""temporal neurons"" (e.g. Leaky Integrate &amp; Fire neurons), while their thresholds are selected randomly, and so the connections between them (i.e. not each neuron has to have a common edge with each of the other neurons).</p></li>
<li><p>If you want to ""learn"" that <strong>x</strong> time-units after inputting <strong>I</strong>, the occurrence <strong>Y</strong> occurs, you need to ""wait"" <strong>x</strong> time-units with the LIF ""detectors"", and see which neurons fired at this specific moment. Then, you can train a classifier (e.g. FeedForward Network), that this specific subset of firing neurons means that the occurrence <strong>Y</strong> happened.</p></li>
<li><p>You may use many ""temporal neurons"" in your ""liquid"", so you may have many possible different subsets of firing neurons, so a specific subset of firing neurons becomes almost unique for the moment after you waited <strong>x</strong> time-units, after inputting your input <strong>I</strong></p></li>
</ol>

<p>I don't know whether what I wrote above is true, or whether it is a total garbage.</p>

<p>Please tell me if this is the correct usage and targets of LIF.</p>
";28343829;3759231;974;maniac;5;42212074;"<p>To understand LSMs you have to understand the comparision with Liquid. Regard the following image:</p>

<ul>
<li>You are randomly throwing stones into water.  Depending of what kind
of stones you have throwed into the water, there's another wave
pattern after x timesteps. </li>
<li>Regarding this wave pattern you can have
conclusions about the features of the different stones</li>
<li>Out of this pattern you can tell what kind of stones you threw in.</li>
</ul>

<p>The LSM models this behavior we have:</p>

<ul>
<li>A input layer which is randomly connected to the reservoir of neurons. Take it as the stones you throw into the water</li>
<li><p>A reservoir of randomly connected neurons. Those represent your Water which interacts with your stones in a specific way.</p>

<ul>
<li>In terms of LSM we have special Neurons (they try to model real neurons). They add activations over the timesteps and only fire if a certain amount of activation is reached, a cooldown factor representing the natrium kalium pumps in the brain is applied in addition.</li>
<li>After x timesteps you'll have a pattern of spiking neurons at that time.</li>
</ul></li>
<li><p>A output layer which interprets that pattern, and uses it for classification.</p></li>
</ul>
"
3322273;9310;SomethingSomething;<machine-learning><neural-network>;28326776;22;Liquid State Machine: How it works and how to use it?;"<p>I am now learning about LSM (Liquid State Machines), and I try to understand how they are used for learning.</p>

<p>I am pretty confused from what I read over the web.</p>

<p>I'll write what I understood -> It may be incorrect and I'll be glad if you can correct me and explain what is true:</p>

<ol>
<li><p>LSMs are not trained at all: They are just initialized with many ""temporal neurons"" (e.g. Leaky Integrate &amp; Fire neurons), while their thresholds are selected randomly, and so the connections between them (i.e. not each neuron has to have a common edge with each of the other neurons).</p></li>
<li><p>If you want to ""learn"" that <strong>x</strong> time-units after inputting <strong>I</strong>, the occurrence <strong>Y</strong> occurs, you need to ""wait"" <strong>x</strong> time-units with the LIF ""detectors"", and see which neurons fired at this specific moment. Then, you can train a classifier (e.g. FeedForward Network), that this specific subset of firing neurons means that the occurrence <strong>Y</strong> happened.</p></li>
<li><p>You may use many ""temporal neurons"" in your ""liquid"", so you may have many possible different subsets of firing neurons, so a specific subset of firing neurons becomes almost unique for the moment after you waited <strong>x</strong> time-units, after inputting your input <strong>I</strong></p></li>
</ol>

<p>I don't know whether what I wrote above is true, or whether it is a total garbage.</p>

<p>Please tell me if this is the correct usage and targets of LIF.</p>
";28343829;9307033;31;Jojker;2;48834535;"<p>I just want to add 2 additional points for other readers. First, that ""natrium-kalium"" pumps are sodium-potassium pumps in English. Second is the relationship between liquid state machines (LSM) and finite state machines (FSM) (since some readers may come with an understanding of finite state machines already).</p>

<p>The relationship between LSM and FSM is mostly a mere analogy. However, the units (neurons) of an LSM can be individually modeled as FSM with regards to whether or not they fire action potentials (change state). A difficulty with this is that the timing of the state changes of each unit and its neighbors is not fixed. So when we consider the states of all the units and how they change in time then we get an infinite transition table, and that puts the LSM in the class of a transition system, not an FSM (maybe this is a little obvious). However, we then add the linear discriminator...  This is a simple deterministic readout layer which is trained to pick out patterns in the LSM corresponding to desired computations. The readout system monitors a subset of units, and usually has well defined temporal rules. In other words, it ignores many state transitions and is sensitive to only a few. This makes it somewhat like an FSM.</p>

<p>You may read that combinations of units in the LSM can form FSM such that the readout identifies FSM ""virtually contained within it"". This comes from a writer who is thinking about the LSM as a computer model first and foremost (when in principle you might elucidate the units and connections comprising a ""virtual FSM"" and construct an actual analogous FSM). Such a statement can be confusing for anyone thinking about LSM as a biological system, where it is better to think about the readout as an element which selects and combines features of the LSM in a manner that ignores the high-dimensional variability and produces a reliable low dimensional FSM like result.</p>
"
492372;24872;London guy;<python><machine-learning><nlp><scikit-learn><tf-idf>;28328372;1;Why isn't the token_pattern parameter in Tfidfvectorizer working with scikit learn?;"<p>I have this text:</p>

<pre><code>data = ['Hi, this is XYZ and XYZABC is $$running']
</code></pre>

<p>I am using the following tfidfvectorizer:</p>

<pre><code>vectorizer = TfidfVectorizer(
            stop_words='english',
            use_idf=False, 
            norm=None,
            min_df=1,
            tokenizer = tokenize,
            ngram_range=(1, 1),
            token_pattern=u'\w{4,}')
</code></pre>

<p>I am fitting the data as follows:</p>

<pre><code>tdm =vectorizer.fit_transform(data)
</code></pre>

<p>Now, when I print </p>

<pre><code>vectorizer.get_feature_names()
</code></pre>

<p>I get this:</p>

<pre><code>[u'hi', u'run', u'thi', u'xyz', u'xyzabc']
</code></pre>

<p>My question is why am I getting 'hi' and 'xyz' even thought I mentioned that I want it to capture only words that have at least 4 characters? - token_pattern=u'\w{4,}'</p>
";28334552;2362381;10431;JAB;2;28334552;"<p>I was able to recreate the behavior of the passing a tokenizer function over-rides the <code>token_pattern</code> pattern.</p>

<p>Here is a tokenizer that excludes tokens less than 4 characters:</p>

<pre><code>from nltk import word_tokenize
def tokenizer(x):
    return ( w for w in word_tokenize(x) if len(w) &gt;3)
</code></pre>

<p>The good news is passing your own tokenizer doesn't override the ngram parameter.</p>
"
492372;24872;London guy;<python><machine-learning><nlp><scikit-learn><tf-idf>;28328372;1;Why isn't the token_pattern parameter in Tfidfvectorizer working with scikit learn?;"<p>I have this text:</p>

<pre><code>data = ['Hi, this is XYZ and XYZABC is $$running']
</code></pre>

<p>I am using the following tfidfvectorizer:</p>

<pre><code>vectorizer = TfidfVectorizer(
            stop_words='english',
            use_idf=False, 
            norm=None,
            min_df=1,
            tokenizer = tokenize,
            ngram_range=(1, 1),
            token_pattern=u'\w{4,}')
</code></pre>

<p>I am fitting the data as follows:</p>

<pre><code>tdm =vectorizer.fit_transform(data)
</code></pre>

<p>Now, when I print </p>

<pre><code>vectorizer.get_feature_names()
</code></pre>

<p>I get this:</p>

<pre><code>[u'hi', u'run', u'thi', u'xyz', u'xyzabc']
</code></pre>

<p>My question is why am I getting 'hi' and 'xyz' even thought I mentioned that I want it to capture only words that have at least 4 characters? - token_pattern=u'\w{4,}'</p>
";28334552;4879270;5478;Windsooon;0;57620319;"<p>token pattern will not work if you are using your own tokenizer, from the <a href=""https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py#L319"" rel=""nofollow noreferrer"">code</a> here.</p>

<pre><code>def build_tokenizer(self):
    """"""Return a function that splits a string into a sequence of tokens""""""
    if self.tokenizer is not None:
        return self.tokenizer
    token_pattern = re.compile(self.token_pattern)
    return token_pattern.findall
</code></pre>
"
3356506;75;alexfrag;<opencv><image-processing><machine-learning><computer-vision><pattern-recognition>;28330888;0;Computing ROC curve to verify the quality of segmentation algorithm;"<p>1) Let's say I have a set of 100 images, from which I manually extracted the foreground and the background (ground truth).</p>

<p>2) Now I have a segmentation algorithm algorithm which I apply to the previous dataset. The results depend on several algorithm parameters (let's assume I do color segmentation, so the results depend on different color space thresholds in use). The result of the segmentation is a binary mask.</p>

<p>3) So for each of the 100 images I have a ground truth and one prediction. I can easily compute the confusion matrix per image or the final confusion matrix for all the images.</p>

<p>My question: How can I compute the ROC curve?</p>

<p>Ideas:</p>

<p>a) I am thinking to have one ROC curve for each parameter of the algorithm. So I keep N-1 parameters constant  and I vary the other in a normalized scale [0,1]. So for every change in [0,1] I need to re-run the segmentation for all images and compute the TPR and FPR which is a point in the ROC curve. But in this approach I could never evaluate the classifier for all parameters at the same time.</p>

<p>b) Maybe the Roc curve is not the ideal evaluation metric for this case and I have to use something else eg F1 score.</p>

<p>This is a well known <a href=""http://www.hpl.hp.com/techreports/2003/HPL-2003-4.pdf"" rel=""nofollow"">tutorial on ROC curves</a> but in my case I can't compute those scores shown in Figure 3 (they seem to be a metric that shows how confident you are for your classification result, if I understood correctly).</p>
";;1536976;9273;Trilarion;0;28363157;"<p>First you need to define a similarity measure between different segmantations. I think, this cannot be defined in general but strongly depends on what is important to you (size, shape, area, number of segments, ..). But you opted for the confusion and that is a good compromise of all I think.</p>

<p>Second there are alternatives to ROC (receiver operating characteristic) curves like the precision-recall curve or same ratios but I think they all want to display the type I and type II errors. I think ROC is fine too.</p>

<p>Third there are multiple parameters in your segmentation algorithm. So what to do about them?</p>

<p>I propose you compute all possible combinations and for each false positive rate (FPR) you then display the maximal true positive rate (TPR) that you obtain for any parameter set. In short: just take the best (and memorize the parameters).</p>

<p>Then when you display this ROC curve, for each FPR, you know which parameter set was the best and you also obtain a dependency of these parameter sets on the FPR. This means there is a corresponding curve through the parameter space were the TPR is maximal for each FPR. </p>
"
1512848;469;Soroosh;<machine-learning><random-forest>;28333113;0;Training Random forest with different datasets gives totally different result! Why?;"<p>I am working with a dataset which contains 12 attributes including the timestamp and one attribute as the output. Also it has about 4000 rows. Besides there is no duplication in the records. I am trying to train a random forest to predict the output. For this purpose I created two different datasets:</p>

<ol>
<li><strong>ONE:</strong> Randomly chose 80% of data for the training and the other 20% for the testing.</li>
<li><strong>TWO:</strong> Sort the dataset based on timestamp and then the first 80% for the training and the last 20% for the testing.</li>
</ol>

<p>Then I removed the timestamp attribute from the both dataset and used the other 11 attributes for the training and the testing (I am sure the timestamp should not be part of the training).</p>

<p><strong>RESULT:</strong> I am getting totally different result for these two datasets. For the first one AUC(Area under the curve) is 85%-90% (I did the experiment several times) and for the second one is 45%-50%.</p>

<p>I do appreciate if someone can help me to know </p>

<ol>
<li>why I have this huge difference.</li>
<li>Also I need to have the test dataset with the latest timestamps (same as the dataset in the second experiment). Is there anyway to select data from the rest of the dataset for the training to improve the
training.</li>
</ol>

<p>PS: I already test the random selection from the first 80% of the timestamp and it doesn't improved the performance.</p>
";28333534;270287;41194;IVlad;3;28333534;"<p>First of all, it is not clear how exactly you're testing. Second, either way, <strong>you are doing the testing wrong</strong>.</p>

<blockquote>
  <blockquote>
    <p>RESULT: I am getting totally different result for these two datasets. For the first one AUC(Area under the curve) is 85%-90% (I did the experiment several times) and for the second one is 45%-50%.</p>
  </blockquote>
</blockquote>

<p>Is this for the training set or the test set? If the test set, that means you have poor generalization.</p>

<p>You are doing it wrong because you are not allowed to tweak your model so that it performs well on the same test set, because it might lead you to a model that does just that, but that generalizes badly. </p>

<p>You should do one of two things:</p>

<p><strong>1. A training-validation-test split</strong></p>

<p>Keep 60% of the data for training, 20% for validation and 20% for testing in a <strong>random</strong> manner. Train your model so that it performs well on the validation set using your training set. Make sure you don't overfit: the performance on the training set should be close to that on the validation set, if it's very far, you've overfit your training set. <strong>Do not use the test set at all at this stage</strong>.</p>

<p>Once you're happy, train your selected model on the training set + validation set and test it on the test set you've held out. You should get acceptable performance. You are not allowed to tweak your model further based on the results you get on this test set, if you're not happy, you have to start from scratch.</p>

<p><strong>2. Use <a href=""http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29"" rel=""nofollow"">cross validation</a></strong></p>

<p>A popular form is 10-fold cross validation: shuffle your data and split it into 10 groups of equal or almost equal size. For each of the 10 groups, train on the other 9 and test on the remaining one. Average your results on the test groups.</p>

<p>You are allowed to make changes on your model to improve that average score, just run cross validation again after each change (make sure to reshuffle).</p>

<p>Personally I prefer cross validation.</p>

<p>I am guessing what happens is that by sorting based on timestamp, you make your algorithm generalize poorly. Maybe the 20% you keep for testing differ significantly somehow, and your algorithm is not given a chance to capture this difference? In general, your data should be sorted randomly in order to avoid such issues.</p>

<p>Of course, you might also have a buggy implementation.</p>

<p>I would suggest you try cross validation and see what results you get then.</p>
"
1172265;1590;Batuhan B;<python><numpy><pandas><machine-learning>;28334091;5;Turning a Pandas Dataframe to an array and evaluate Multiple Linear Regression Model;"<p>I am trying to evaluate a multiple linear regression model. I have a data set like this : </p>

<p><img src=""https://i.stack.imgur.com/DCzmo.png"" alt=""enter image description here""></p>

<p>This data set has 157 rows * 54 columns.</p>

<p>I need to predict ground_truth value from articles. I will add my multiple linear model 7 articles between <strong>en_Amantadine</strong> with <strong>en_Common</strong>.</p>

<p>I have code for multiple linear regression : </p>

<pre><code>from sklearn.linear_model import LinearRegression
X = [[6, 2], [8, 1], [10, 0], [14, 2], [18, 0]] // need to modify for my problem
y = [[7],[9],[13],[17.5], [18]] // need to modify
model = LinearRegression()
model.fit(X, y)
</code></pre>

<p>My problem is, I cannot extract data from my DataFrame for <strong>X</strong> and <strong>y</strong> variables. In my code X should be:</p>

<pre><code>X = [[4984, 94, 2837, 857, 356, 1678, 29901],
     [4428, 101, 4245, 906, 477, 2313, 34176],
      ....
     ]
y = [[3.135999], [2.53356] ....]
</code></pre>

<p>I cannot convert DataFrame to this type of structure. 
How can i do this ? </p>

<p>Any help is appreciated.</p>
";28334204;2362381;10431;JAB;11;28334204;"<p>You can turn the dataframe into a matrix using the method <code>as_matrix</code> directly on the dataframe object. You might need to specify the columns which you are interested in <code>X=df[['x1','x2','X3']].as_matrix()</code> where the different x's are the column names.</p>

<p>For the y variables you can use <code>y = df['ground_truth'].values</code> to get an array.</p>

<p>Here is an example with some randomly generated data:</p>

<pre><code>import numpy as np
#create a 5X5 dataframe
df = pd.DataFrame(np.random.random_integers(0, 100, (5, 5)), columns = ['X1','X2','X3','X4','y'])
</code></pre>

<p>calling <code>as_matrix()</code> on <code>df</code> returns a <code>numpy.ndarray</code> object</p>

<pre><code>X = df[['X1','X2','X3','X4']].as_matrix()
</code></pre>

<p>Calling <code>values</code> returns a <code>numpy.ndarray</code> from a pandas <code>series</code></p>

<pre><code>y =df['y'].values
</code></pre>

<p>Notice: You might get a warning saying:<code>FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.</code></p>

<p>To fix it use <code>values</code> instead of <code>as_matrix</code> as shown below</p>

<pre><code>X = df[['X1','X2','X3','X4']].values
</code></pre>
"
1172265;1590;Batuhan B;<python><numpy><pandas><machine-learning>;28334091;5;Turning a Pandas Dataframe to an array and evaluate Multiple Linear Regression Model;"<p>I am trying to evaluate a multiple linear regression model. I have a data set like this : </p>

<p><img src=""https://i.stack.imgur.com/DCzmo.png"" alt=""enter image description here""></p>

<p>This data set has 157 rows * 54 columns.</p>

<p>I need to predict ground_truth value from articles. I will add my multiple linear model 7 articles between <strong>en_Amantadine</strong> with <strong>en_Common</strong>.</p>

<p>I have code for multiple linear regression : </p>

<pre><code>from sklearn.linear_model import LinearRegression
X = [[6, 2], [8, 1], [10, 0], [14, 2], [18, 0]] // need to modify for my problem
y = [[7],[9],[13],[17.5], [18]] // need to modify
model = LinearRegression()
model.fit(X, y)
</code></pre>

<p>My problem is, I cannot extract data from my DataFrame for <strong>X</strong> and <strong>y</strong> variables. In my code X should be:</p>

<pre><code>X = [[4984, 94, 2837, 857, 356, 1678, 29901],
     [4428, 101, 4245, 906, 477, 2313, 34176],
      ....
     ]
y = [[3.135999], [2.53356] ....]
</code></pre>

<p>I cannot convert DataFrame to this type of structure. 
How can i do this ? </p>

<p>Any help is appreciated.</p>
";28334204;6638903;454;Tanmoy;0;45071745;"<pre><code>y = broken_df.ground_truth.values
X = broken_df.drop('ground_truth', axis=1).values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
linreg = LinearRegression()
linreg.fit(X_train, y_train)
y_pred = linreg.predict(X_test)
print(linreg.score(X_test, y_test)
print(classification_report(y_test, y_pred))
</code></pre>
"
4256839;194;zazu;<r><machine-learning>;28337963;0;Length mismatch with model from Machine Learning MDA package;"<p>Can someone help me even phrase what I am trying to do? (I am new to this.)</p>

<p>I am trying out Machine Learning in R now that I have it nailed in Matlab. R is just a passion of mine at the moment.</p>

<p>Data:</p>

<pre><code>&gt; head(newzap1209, n=5)
  buoy_douglas  avgtopsum avgstdwin1 stddiff2
1            3 -12.097720   410.4747 410.6323
2            2 -10.462240   260.7213 263.2085
3            2 -11.539432   357.1802 362.3258
4            2  -9.524074   234.8285 234.8571
5            3 -11.498597   356.4736 359.4485
</code></pre>

<p>Code:</p>

<pre><code>library(mda)
fit&lt;-mda(buoy_douglas~.,data=newzap1209)
summary(fit)
predictions&lt;-predict(fit,newzap1209[,2:4])
table(predictions,newzap1209$buoy_douglas)
</code></pre>

<p>Error message:</p>

<blockquote>
  <p>Error in table(predictions, newzap1209$buoy_douglas) : all arguments must have the same length<br/></p>
</blockquote>

<p>Everything works except the table!</p>

<p>Same goes for the confusion matrix.</p>
";28338580;202229;25726;smci;0;28338580;"<p>The error is saying that <code>predictions</code> and <code>newzap1209</code> have mismatching lengths (nrows). Which should be impossible since you generated <code>fit</code> from <code>newzap1209[,2:4]</code>.</p>

<p>Check the length of each and debug why they mismatch.</p>
"
4531587;179;vij555;<python><machine-learning><scikit-learn><cluster-analysis><k-means>;28344660;10;How to identify Cluster labels in kmeans scikit learn;"<p>I am learning python scikit.
The example given here 
displays the top occurring words in each Cluster and not Cluster name.</p>

<p><a href=""http://scikit-learn.org/stable/auto_examples/document_clustering.html"" rel=""noreferrer"">http://scikit-learn.org/stable/auto_examples/document_clustering.html</a></p>

<p>I found that the km object has ""km.label"" which lists the centroid id, which is the number.</p>

<p>I have two question</p>

<pre><code>1. How do I generate the cluster labels?
2. How to identify the members of the clusters for further processing.
</code></pre>

<p>I have working knowledge of k-means and aware of tf-ids concepts. </p>
";;270287;41194;IVlad;6;28346077;"<blockquote>
  <ol>
  <li>How do I generate the cluster labels?</li>
  </ol>
</blockquote>

<p>I'm not sure what you mean by this. You have no cluster labels other than cluster 1, cluster 2, ..., cluster <code>n</code>. That is why it's called unsupervised learning, because there are no labels.</p>

<p>Do you mean you actually have labels and you want to see if the clustering algorithm happened to cluster the data according to your labels?</p>

<p>In that case, the documentation you linked to provides an example:</p>

<pre><code>print(""Homogeneity: %0.3f"" % metrics.homogeneity_score(labels, km.labels_))
print(""Completeness: %0.3f"" % metrics.completeness_score(labels, km.labels_))
print(""V-measure: %0.3f"" % metrics.v_measure_score(labels, km.labels_))
</code></pre>

<blockquote>
  <ol start=""2"">
  <li>How to identify the members of the clusters for further processing.</li>
  </ol>
</blockquote>

<p>See the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans"" rel=""noreferrer"">documentation for KMeans</a>. In particular, the <code>predict</code> method:</p>

<blockquote>
  <blockquote>
    <p>predict(X)</p>
    
    <p>Parameters: 
    X : {array-like, sparse matrix}, shape = [n_samples, n_features] New data to predict.</p>
    
    <p>Returns:<br>
    labels : array, shape [n_samples,]
    Index of the cluster each sample belongs to.</p>
  </blockquote>
</blockquote>

<p>If you don't want to predict something new, <code>km.labels_</code> should do that for the training data.</p>
"
4531587;179;vij555;<python><machine-learning><scikit-learn><cluster-analysis><k-means>;28344660;10;How to identify Cluster labels in kmeans scikit learn;"<p>I am learning python scikit.
The example given here 
displays the top occurring words in each Cluster and not Cluster name.</p>

<p><a href=""http://scikit-learn.org/stable/auto_examples/document_clustering.html"" rel=""noreferrer"">http://scikit-learn.org/stable/auto_examples/document_clustering.html</a></p>

<p>I found that the km object has ""km.label"" which lists the centroid id, which is the number.</p>

<p>I have two question</p>

<pre><code>1. How do I generate the cluster labels?
2. How to identify the members of the clusters for further processing.
</code></pre>

<p>I have working knowledge of k-means and aware of tf-ids concepts. </p>
";;6618417;729;ArmandduPlessis;3;61087620;"<p>Oh that's easy</p>

<p>My environment:
scikit-learn version '0.20.0'</p>

<p>Just use the attribute <code>.labels_</code> as in the docs: <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html</a></p>

<pre><code>from sklearn.cluster import KMeans
import numpy as np
</code></pre>

<p>Working example:</p>

<pre><code>x1 = [[1],[1],[2],[2],[2],[3],[3],[7],[7],[7]]
x2 = [[1],[1],[2],[2],[2],[3],[3],[7],[7],[7]]

X_2D = np.concatenate((x1,x2),axis=1)

kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=300, n_init=10, random_state=0)
labels = kmeans.fit(X_2D)

print(labels.labels_)
</code></pre>

<p>Output:</p>

<p><code>[2 2 3 3 3 0 0 1 1 1]</code></p>

<p>So as you can see, we have 4 clusters, and each data example in the X_2D array is assigned a label accordingly.</p>
"
2632736;132;Nithin.P;<python><machine-learning>;28347831;0;stylometry using machine learning in python;"<p>We are trying to implement a system that takes input as novels and can extract the author's styles(word length,sentence length,usage of comma etc) so that it can predict the authorship of document; when we give an anonymous document of any trained author.
<p>
our trained system have to learn author's style by using machine learning algorithm.
is there any problem of using Naive Bayes algorithm to train our system to extract each author's style. Otherwise, can you please give me a better alternative??
<p>
We are trying to get implement it in python.
Can you tell me where to start? Thanks in advance..</p>
";28348578;270287;41194;IVlad;1;28348578;"<p>I would start by looking at <a href=""http://scikit-learn.org/stable/index.html"" rel=""nofollow"">scikit-learn</a>: a machine learning library with a lot of implemented algorithms.</p>

<p>For text classification, Naive Bayes does not usually achieve the best results. Look at Support Vector Machines and algorithms based on ideas from them, such as (you can search for these on the scikit website and go from there): SGDClassifier, PassiveAggressiveClassifier and LinearSVC.</p>

<p>Scikit also implements Naive Bayes classifiers, so have a look at those as well.</p>

<p>I wouldn't start by manually deciding what is relevant to an author's style. Look into CountVectorizer (bag of words model) and TfidfVectorizer (tf-idf weighting for the bag of words model), which should build decent features for you to start with.</p>
"
2632736;132;Nithin.P;<python><machine-learning>;28347831;0;stylometry using machine learning in python;"<p>We are trying to implement a system that takes input as novels and can extract the author's styles(word length,sentence length,usage of comma etc) so that it can predict the authorship of document; when we give an anonymous document of any trained author.
<p>
our trained system have to learn author's style by using machine learning algorithm.
is there any problem of using Naive Bayes algorithm to train our system to extract each author's style. Otherwise, can you please give me a better alternative??
<p>
We are trying to get implement it in python.
Can you tell me where to start? Thanks in advance..</p>
";28348578;4075155;1233;Lucas Azevedo;0;55418444;"<p>There are many python libraries that could be used to measure different linguistic aspects of your input text and then using this values as features would probably enhance the quality of your model. I'll name a few here that can help you measuring aspects like Subjectivity, Complexity, Informality, Specificity etc:</p>

<p><a href=""https://textblob.readthedocs.io/en/dev/"" rel=""nofollow noreferrer"">TextBlob</a></p>

<p><a href=""https://github.com/wjko2/Domain-Agnostic-Sentence-Specificity-Prediction"" rel=""nofollow noreferrer"">Domain-Agnostic-Sentence-Specificity-Prediction</a></p>

<p><a href=""https://pypi.org/project/readability/"" rel=""nofollow noreferrer"">Readability</a></p>

<p><a href=""https://pypi.org/project/textstat/"" rel=""nofollow noreferrer"">TextStat</a></p>

<p><a href=""https://github.com/afel-project/pySemanticComplexity"" rel=""nofollow noreferrer"">Python Semantic Complexity Analyser</a></p>

<p><a href=""https://github.com/meyersbs/uncertainty"" rel=""nofollow noreferrer"">Uncertainty</a></p>

<p><a href=""https://pypi.org/project/lexical-diversity/"" rel=""nofollow noreferrer"">Lexical Diversity</a></p>

<p><a href=""https://pypi.org/project/lexicalrichness/"" rel=""nofollow noreferrer"">Lexical Richness</a></p>

<p>I hope that helps!</p>
"
557022;3277;picmate æ¶…;<algorithm><machine-learning><nlp><cluster-analysis>;28353888;1;Clustering Strings Based on Similar Word Sequences;"<p>I am looking for an efficient way to cluster about 10 million strings into  clusters based on the appearance of similar word sequences.</p>

<p>Consider a list of strings like:</p>

<pre><code>the fruit hut number one
the ice cre  am shop number one
jim's taco
ice cream shop in the corner
the ice cream shop
the fruit hut
jim's taco outlet number one
jim's t  aco in the corner
the fruit hut in the corner
</code></pre>

<p>After the algorithm runs on them I want them clustered as follows:</p>

<pre><code>the ice cre  am shop number one
ice cream shop in the corner
the ice cream shop

jim's taco
jim's taco outlet number one
jim's t  aco in the corner

the fruit hut
fruit hut number one
the fruit hut in the corner
</code></pre>

<p>As it is obvious, the sequences that differentiate the clusters are:</p>

<pre><code>ice cream shop, jim's taco and fruit hut
</code></pre>
";;572670;166096;amit;6;28354750;"<p>I think you are looking for <strong>Near Duplicates Detection</strong>, with some unknown threshold you will use to cluster not only ""near duplicates"" - but also similar enough documents together.</p>

<p>One of the known solutions to it is to use <strong><a href=""http://en.wikipedia.org/wiki/Jaccard_index"" rel=""nofollow noreferrer"">Jaccard-Similarity</a></strong> for getting the difference between two documents.</p>

<p>Jaccard Similarity is basically - get sets of words from each document, let these sets be <code>s1</code> and <code>s2</code> - and the jaccard similarity is <code>|s1 [intersection] s2|/|s1 [union] s2|</code>.</p>

<p>Usually when facing near duplicates - the order of words has some importance however. In order to deal with it - when generating the sets <code>s1</code> and <code>s2</code> - you actually generate sets of k-shinglings (or k-grams), instead sets of only words.
<br>In your example, with <code>k=2</code>, the sets will be:
ice cream shop in the corner</p>

<pre><code>s2 = { the ice, ice cre, cre am, am shop, shop number, number one }
s4 = {ice cream, cream shop, shop in, in the, the corner }
s5 = { the ice, ice cream, cream shop }

s4 [union] s5 = { ice cream, cream shop, shop in, in the, the corner, the ice } 
s4 [intersection] s5 = { ice cream, cream shop }
</code></pre>

<p>In the above, the jaccard-similarity will be <code>2/6</code>. 
<br>In your case maybe the ordinary k-shinglings will perform worse than using a single word (1-shingling), but you will have to test these approaches.</p>

<p>This procedure can be scaled nicely to deal very efficiently with huge collections, without checking all pairs and creating huge numbers of sets. More details could be found in <a href=""http://webcourse.cs.technion.ac.il/236375/Winter2013-2014/ho/WCFiles/tutorial_8_near_duplicates_detection.pdf"" rel=""nofollow noreferrer"">these lecture notes</a> (I gave this lecture ~2 years ago, based on the author's notes).</p>

<p>After you are done with this procedure, you basically have a measure <code>d(s1,s2)</code> that measures the distance between every two sentences, and you can use any known <a href=""http://en.wikipedia.org/wiki/Cluster_analysis"" rel=""nofollow noreferrer"">clustering</a> algorithm to cluster them.</p>

<hr>

<p>Disclaimer: used my answer from <a href=""https://stackoverflow.com/a/23053827/572670"">this thread</a> as basis for this after realizing near duplicates might fit here.</p>
"
557022;3277;picmate æ¶…;<algorithm><machine-learning><nlp><cluster-analysis>;28353888;1;Clustering Strings Based on Similar Word Sequences;"<p>I am looking for an efficient way to cluster about 10 million strings into  clusters based on the appearance of similar word sequences.</p>

<p>Consider a list of strings like:</p>

<pre><code>the fruit hut number one
the ice cre  am shop number one
jim's taco
ice cream shop in the corner
the ice cream shop
the fruit hut
jim's taco outlet number one
jim's t  aco in the corner
the fruit hut in the corner
</code></pre>

<p>After the algorithm runs on them I want them clustered as follows:</p>

<pre><code>the ice cre  am shop number one
ice cream shop in the corner
the ice cream shop

jim's taco
jim's taco outlet number one
jim's t  aco in the corner

the fruit hut
fruit hut number one
the fruit hut in the corner
</code></pre>

<p>As it is obvious, the sequences that differentiate the clusters are:</p>

<pre><code>ice cream shop, jim's taco and fruit hut
</code></pre>
";;1060350;70512;Has QUIT--Anony-Mousse;2;28360798;"<p>Clustering is the wrong tool for you.</p>

<p>For any unsupervised algorithm, the following partitioning will be as good:</p>

<pre><code>the fruit hut number one
the ice cre am shop number one
jim's taco outlet number one

the ice cream shop
the fruit hut
jim's taco

ice cream shop in the corner
jim's t aco in the corner
the fruit hut in the corner
</code></pre>

<p>Because to a clustering algorithm ""number one"" and ""in the corner"" are also shared phrases. The second cluster are the leftovers.</p>

<p>Use something <em>supervised</em> instead.</p>
"
4126221;231;Achekroud;<r><machine-learning><classification><r-caret><calibration>;28357390;3;Calculate model calibration during cross-validation in caret?;"<p>first time poster here, so apologies for rookie errors</p>

<p>I am using the caret package in R for classification. I am fitting some models (GBM, linear SVM, NB, LDA) using repeated 10-fold cross validation over a training set. Using a custom trainControl, caret even gives me a whole range of model performance metrics like ROC, Spec/sens, Kappa, Accuracy over the test folds. That really is fantastic. There is just one more metric I would love to have: some measure of model calibration.</p>

<p>I noticed that there is a <a href=""http://www.inside-r.org/packages/cran/caret/docs/xyplot.calibration"" rel=""nofollow"">function</a> within caret that can create a calibration plot to estimate the consistency of model performance across portions of your data. Is it possible to have caret compute this for each test-fold during the cross-validated model building? Or can it only be applied to some new held out data that we are making predictions on?</p>

<p>For some context, at the moment I have something like this:</p>

<pre><code>fitControl &lt;- trainControl(method = ""repeatedcv"", repeats=2, number = 10, classProbs = TRUE, summaryFunction = custom.summary)
gbmGrid &lt;-  expand.grid(.interaction.depth = c(1,2,3),.n.trees = seq(100,800,by=100),.shrinkage = c(0.01))
gbmModel &lt;- train(y= train_target, x = data.frame(t_train_predictors),
              method = ""gbm"",
              trControl = fitControl,
              tuneGrid = gbmGrid,
              verbose = FALSE)
</code></pre>

<p>If it helps, I am using ~25 numeric predictors and N=2,200, predicting a two-class factor.</p>

<p>Many thanks in advance for any help/advice.
Adam</p>
";28532103;1078601;11384;topepo;10;28532103;"<p>The <code>calibration</code> function takes whatever data that you give it. You can get the resampled values from the <code>train</code> sub-object <code>pred</code>:</p>

<pre><code>&gt; set.seed(1)
&gt; dat &lt;- twoClassSim(2000)
&gt; 
&gt; set.seed(2)
&gt; mod &lt;- train(Class ~ ., data = dat, 
+              method = ""lda"",
+              trControl = trainControl(savePredictions = TRUE,
+                                       classProbs = TRUE))
&gt; 
&gt; str(mod$pred)
'data.frame':   18413 obs. of  7 variables:
 $ pred     : Factor w/ 2 levels ""Class1"",""Class2"": 1 2 2 1 1 2 1 1 2 1 ...
 $ obs      : Factor w/ 2 levels ""Class1"",""Class2"": 1 2 2 1 1 2 1 1 2 2 ...
 $ Class1   : num  0.631 0.018 0.138 0.686 0.926 ...
 $ Class2   : num  0.369 0.982 0.8616 0.3139 0.0744 ...
 $ rowIndex : int  1 3 4 10 12 13 18 22 25 27 ...
 $ parameter: Factor w/ 1 level ""none"": 1 1 1 1 1 1 1 1 1 1 ...
 $ Resample : chr  ""Resample01"" ""Resample01"" ""Resample01"" ""Resample01"" ...
</code></pre>

<p>Then you could use:</p>

<pre><code>&gt; cal &lt;- calibration(obs ~ Class1, data = mod$pred)
&gt; xyplot(cal)
</code></pre>

<p>Just keep in mind that, with many resampling methods, a single training set instance will be held-out multiple times:</p>

<pre><code>&gt; table(table(mod$pred$rowIndex))

  2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17 
  2  11  30  77 135 209 332 314 307 231 185  93  48  16   6   4 
</code></pre>

<p>You could average the class probabilities per <code>rowIndex</code> if you like.</p>

<p>Max</p>
"
2224925;61;InfoUser;<c#><machine-learning><classification><document-classification><mallet>;28362098;2;Applying Mallet in document classification as binary classifier;"<p>I have implemented a document classification tool using Mallet which classifies each page of a document to certain categories. I have tried Weka too but Mallet is smarter than Weka on this aspect. My approach is as below:</p>

<ol>
<li>Train pages of a document to known category </li>
<li>Test few sample documents whether Mallet identifies pages of a certain category or not. Here Mallet matches from the test set with Known categories.</li>
<li>if test is successful and satisfactory then run on huge document repository using classifier and mallet file. </li>
</ol>

<p>This part is already implemented with good success rate.</p>

<p>For Text documents which I have not trained and different from known categories should be returned as NO Match, Mallet is trying to find match from training set for documents which are not known to Mallet.</p>

<p>For example I have 4 pages in a document. Page 1 belongs to class A, page 3 belongs to class B. Pages 2 and 4 do not belong to any classes. How to mark, pages 2 and 4 as 'NON Match' through Mallet?</p>

<p>Please help me to achieve this. Let me know if I am doing anything wrong or any other tool which can give me desired output.</p>
";;1930402;1175;pnv;2;28365063;"<p>Two quick thoughts:</p>

<ol>
<li><p>You can give some threshold for the confidence value you want. For example, mallet is saying that Page 1 belongs to Class A with 90% confidence, accept it. If it is saying that Page 2 belongs to Class C, with 60% confidence, and that is the best value, may be, reject that suggestion. You can get the scores of classification through the function-getClassificationScores  (<a href=""http://mallet.cs.umass.edu/api/cc/mallet/classify/MaxEnt.html#getClassificationScores(cc.mallet.types.Instance,%20double[])"" rel=""nofollow"">documentation: </a><a href=""http://mallet.cs.umass.edu/api/cc/mallet/classify/MaxEnt.html#getClassificationScores(cc.mallet.types.Instance"" rel=""nofollow"">http://mallet.cs.umass.edu/api/cc/mallet/classify/MaxEnt.html#getClassificationScores(cc.mallet.types.Instance</a>, double[])</p></li>
<li><p>You can you scikit-learn in python. I have heard that if it doesn't know which class your page belongs to, it will tell <code>NA</code>. </p></li>
</ol>
"
1813291;331;Max;<matlab><machine-learning><time-series><euclidean-distance>;28367743;2;Subsequent time-series matching;"<p>I've been stuck with subsequent matching of time-series in MATLAB (I'm new to it).</p>

<p>I have two time-series: A (of length a) and B (of length b). Assume that a is much larger than b. The task is to find the closest window from A to B (according to Euclidian metric). </p>

<p>In order to do that I construct additional matrix C that stores all subsequences of the length b from A and then use pdist2(C, B). Obviously it works slowly and requires too much memory.</p>

<p>So I have a couple of questions:</p>

<ol>
<li><p>How to obtain C without loops (actually to reshape A)?</p></li>
<li><p>What are the common ways to solve this problem? (preferably in MATLAB but other environments are also possible)</p></li>
</ol>

<p>Thanks for your help!</p>
";28368382;1149913;4354;user1149913;1;28367980;"<p>For part 2 of your question, a typical way of comparing sequences is through <a href=""http://en.wikipedia.org/wiki/Dynamic_time_warping"" rel=""nofollow"">Dynamic Time Warping</a> (DTW). You should almost certainly be able to Google for a Matlab implementation.</p>

<p>The basic version of the DTW algorithm has complexity O(nm), but approximate versions that typically have comparable performance have complexity closer to O(max(n, m)). </p>
"
1813291;331;Max;<matlab><machine-learning><time-series><euclidean-distance>;28367743;2;Subsequent time-series matching;"<p>I've been stuck with subsequent matching of time-series in MATLAB (I'm new to it).</p>

<p>I have two time-series: A (of length a) and B (of length b). Assume that a is much larger than b. The task is to find the closest window from A to B (according to Euclidian metric). </p>

<p>In order to do that I construct additional matrix C that stores all subsequences of the length b from A and then use pdist2(C, B). Obviously it works slowly and requires too much memory.</p>

<p>So I have a couple of questions:</p>

<ol>
<li><p>How to obtain C without loops (actually to reshape A)?</p></li>
<li><p>What are the common ways to solve this problem? (preferably in MATLAB but other environments are also possible)</p></li>
</ol>

<p>Thanks for your help!</p>
";28368382;4295405;403;jolo;1;28368382;"<p>For the first question you could try</p>

<pre><code>tmp = repmat(A,1,b);
C = reshape([tmp zeros(1,b)],a,b);
C = C(1:(a-b+1),:);
</code></pre>

<p>Besides, <code>pdist2</code> is very slow in comparison to this very nice solution: <a href=""https://stackoverflow.com/questions/23911670/efficiently-compute-pairwise-squared-euclidean-distance-in-matlab/23911671#23911671"">Efficiently compute pairwise squared Euclidean distance in Matlab</a> </p>


"
1813291;331;Max;<matlab><machine-learning><time-series><euclidean-distance>;28367743;2;Subsequent time-series matching;"<p>I've been stuck with subsequent matching of time-series in MATLAB (I'm new to it).</p>

<p>I have two time-series: A (of length a) and B (of length b). Assume that a is much larger than b. The task is to find the closest window from A to B (according to Euclidian metric). </p>

<p>In order to do that I construct additional matrix C that stores all subsequences of the length b from A and then use pdist2(C, B). Obviously it works slowly and requires too much memory.</p>

<p>So I have a couple of questions:</p>

<ol>
<li><p>How to obtain C without loops (actually to reshape A)?</p></li>
<li><p>What are the common ways to solve this problem? (preferably in MATLAB but other environments are also possible)</p></li>
</ol>

<p>Thanks for your help!</p>
";28368382;2861669;1720;mbschenkel;0;28380722;"<p>I would like to suggest <a href=""http://en.wikipedia.org/wiki/Cross-correlation"" rel=""nofollow"">cross-correlation</a> (<a href=""http://ch.mathworks.com/help/signal/ref/xcorr.html"" rel=""nofollow""><code>xcorr</code></a>) as an approach to this problem. On how cross-correlation and euclidian distance are related, refer for instance to <a href=""http://scribblethink.org/Work/nvisionInterface/nip.html"" rel=""nofollow"">the introduction of this article</a>. It is not invariant to scaling in time or amplitude and may be sensitive to noise, but the question does not imply any such distortions.</p>

<p>An advantage of cross-correlation is its efficient implementation in the transform domain. Unfortunately I have only an old Matlab version without <code>pdist2</code> at hands, so I can not time it. But consider</p>

<pre><code>%// Parameters
a = 1e4;
b = 1e2;
noise = 0.1;

%// Create sample signals with some distortion
A = rand(1, a);
Offset_actual = 321
B = A(Offset_actual + [1:b]) + noise*rand(1, b);

%// Computation
CC = xcorr(A, B);
[m, i] = max(CC);
Offset_estimated = i - a
plot(CC)
</code></pre>

<p>which should recover <code>Offset_estimated == Offset_actual</code>. </p>
"
3070617;240;Bit Manipulator;<machine-learning><kernel><svm><ranking>;28371321;0;How to tune parameters of SVM Rank?;"<p>I am using <a href=""http://svmlight.joachims.org/"" rel=""nofollow"">SVM Rank</a>, which has multiple parameters, changing whom I am getting a variety of results. Is there some mechanism to tune and get the best parameters, as tuned according to the best results on validation set?</p>

<p>Below are the different parameters:</p>

<pre><code>Learning Options:
     -c float    -&gt; C: trade-off between training error
                    and margin (default 0.01)
     -p [1,2]    -&gt; L-norm to use for slack variables. Use 1 for L1-norm,
                    use 2 for squared slacks. (default 1)
     -o [1,2]    -&gt; Rescaling method to use for loss.
                    1: slack rescaling
                    2: margin rescaling
                    (default 2)
     -l [0..]    -&gt; Loss function to use.
                    0: zero/one loss
                    ?: see below in application specific options
                    (default 1)
Optimization Options (see [2][5]):
     -w [0,..,9] -&gt; choice of structural learning algorithm (default 3):
                    0: n-slack algorithm described in [2]
                    1: n-slack algorithm with shrinking heuristic
                    2: 1-slack algorithm (primal) described in [5]
                    3: 1-slack algorithm (dual) described in [5]
                    4: 1-slack algorithm (dual) with constraint cache [5]
                    9: custom algorithm in svm_struct_learn_custom.c
     -e float    -&gt; epsilon: allow that tolerance for termination
                    criterion (default 0.001000)
     -k [1..]    -&gt; number of new constraints to accumulate before
                    recomputing the QP solution (default 100)
                    (-w 0 and 1 only)
     -f [5..]    -&gt; number of constraints to cache for each example
                    (default 5) (used with -w 4)
     -b [1..100] -&gt; percentage of training set for which to refresh cache
                    when no epsilon violated constraint can be constructed
                    from current cache (default 100%) (used with -w 4)
SVM-light Options for Solving QP Subproblems (see [3]):
     -n [2..q]   -&gt; number of new variables entering the working set
                    in each svm-light iteration (default n = q).
                    Set n &lt; q to prevent zig-zagging.
     -m [5..]    -&gt; size of svm-light cache for kernel evaluations in MB
                    (default 40) (used only for -w 1 with kernels)
     -h [5..]    -&gt; number of svm-light iterations a variable needs to be
                    optimal before considered for shrinking (default 100)
     -# int      -&gt; terminate svm-light QP subproblem optimization, if no
                    progress after this number of iterations.
                    (default 100000)
Kernel Options:
     -t int      -&gt; type of kernel function:
                    0: linear (default)
                    1: polynomial (s a*b+c)^d
                    2: radial basis function exp(-gamma ||a-b||^2)
                    3: sigmoid tanh(s a*b + c)
                    4: user defined kernel from kernel.h
     -d int      -&gt; parameter d in polynomial kernel
     -g float    -&gt; parameter gamma in rbf kernel
     -s float    -&gt; parameter s in sigmoid/poly kernel
     -r float    -&gt; parameter c in sigmoid/poly kernel
     -u string   -&gt; parameter of user defined kernel
</code></pre>
";28376540;270287;41194;IVlad;2;28376540;"<p>This is known as <a href=""http://en.wikipedia.org/wiki/Hyperparameter_optimization"" rel=""nofollow"">grid search</a>. I don't know if you're familiar with python and scikit-learn, but either way, I think <a href=""http://scikit-learn.org/stable/modules/grid_search.html"" rel=""nofollow"">their description and examples</a> are very good and language agnostic.</p>

<p>Basically, you specify some values you're interested in for each parameter (or an interval from which to take random samples, see the randomized search) and then for each combination of settings, cross validation (usually <a href=""http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29"" rel=""nofollow"">k fold cross validation</a>) is used to compute how well the model does with those settings. The best performing combination is returned (scikit-learn can actually return a ranking of the combinations).</p>

<p>Note that this can take a long time. You should be rather certain of some parameters yourself, based on your problem. For example, for text classification, you should just pick the linear kernel, for other problems you'll probably want <code>rbf</code> etc. Don't just throw everything at the grid search, decide for as many parameters as you can using your knowledge of the algorithm and the problem at hand.</p>
"
2245718;440;IrishDog;<apache-spark><machine-learning><mahout><k-means><apache-spark-mllib>;28383753;1;Train Spark k-means with Mahout vectors;"<p>I have some Mahout vectors in my hdfs in sequence file format. Is it possible to use the same vectors in some way to train a KMeans model in Spark? I could just convert the existing Mahout vectors into Spark vectors (mllib) but I'd like to avoid that.</p>
";28389514;1056563;45835;StephenBoesch;1;28389514;"<p>Mahout vectors are not directly supported by Spark. You would - along the lines of your concern - need to convert them to Spark Vectors.</p>

<pre><code>val sc = new SparkContext(""local[2]"", ""MahoutTest"")
val sfData = sc.sequenceFile[NullWritable, MVector](dir)
val xformedVectors = sfData.map { case (label, vect) =&gt;
  import collection.JavaConversions._
  (label, Vectors.dense(vect.all.iterator.map{ e =&gt; e.get}.toArray))
}
</code></pre>
"
3471177;733;RockOnRockOut;<java><machine-learning><weka>;28389142;0;How do you copy a dataset's class values in weka?;"<p>I have two <code>Instances</code> datasets: <code>data</code> is the original and <code>stumpyInsts</code> is the one I am trying to copy the class values to. Here is my code:</p>

<pre><code>FastVector attributes = new FastVector();
ArrayList&lt;Instance&gt; instances = new ArrayList&lt;Instance&gt;();
for(int i = 0; i &lt;= 100; i++){
    Attribute newAttr = new Attribute(""Stump"" + i, i);
    attributes.addElement(newAttr);
}
//make new instances
Instances stumpyInsts = new Instances(""Stumps"", attributes, data.numInstances());

stumpyInsts.setClassIndex(stumpyInsts.numAttributes() - 1);
Enumeration instEnum = stumpyInsts.enumerateInstances();
Enumeration somethingElseLOL = data.enumerateInstances();
while (instEnum.hasMoreElements()) {
    Instance instance = (Instance) instEnum.nextElement();
    Instance other = (Instance) somethingElseLOL.nextElement();
    String s = other.stringValue(other.classIndex());
    instance.setValue(instance.classIndex(), s);
}
</code></pre>

<p>I keep getting this when I try to set the value: </p>

<pre><code>`java.lang.IllegalArgumentException: Attribute neither nominal nor string!
at weka.core.Instance.setValue(Instance.java:687)`
</code></pre>

<p>Does anyone know why it happens? I also get the error if I try to <code>setClassValue</code> before adding the instance to the dataset, or if I make new strings with those values. To me it doesn't really make sense, since <code>stringValue</code> is clearly returning a string.</p>
";28389648;391338;3966;Sentry;1;28389648;"<p>The error</p>

<pre><code>java.lang.IllegalArgumentException: Attribute neither nominal nor string!
  at weka.core.Instance.setValue(Instance.java:687)
</code></pre>

<p>does not refer to <code>stringValue</code>, but to the class attribute of <code>instance</code>. When you do</p>

<pre><code>stumpyInsts.setClassIndex(stumpyInsts.numAttributes() - 1);
</code></pre>

<p>you tell <code>stumpyInsts</code> what index its class is, but not that is should be a nominal or String attribute. According to <a href=""https://stackoverflow.com/questions/6639802/creating-a-string-attribute-in-weka-java-api"">this answer</a> you would have to do something like</p>

<pre><code>FastVector classAttr = new FastVector();
classAttr .addElement(new Attribute(""class"", (FastVector) null));
</code></pre>

<p>to create a (class) attribute that has string or nominal values.</p>
"
4542766;11;Cio;<python><csv><machine-learning><scikit-learn>;28393460;1;Scikit-learn DictVectorizer for categoricals variables;"<p>I have a .csv file which entries look like this:</p>

<p>b0002 ,0,>0.00 ,3,&lt;=0.644 ,&lt;=0.472 ,&lt;=0.690 ,&lt;=0.069672 ,>15.00 ,>21.00 ,>16.00 ,>6.00 ,>16.00 ,>21.00 ,>9.00 ,>11.00 ,>20.00 ,>7.00 ,>4.00 ,>9.00 ,>9.00 ,>13.00 ,>8.00 ,>14.00 ,>3.00 ,""(1.00, 8.00] "",>10.00 ,>9.00 ,>183.00 ,1</p>

<p>I want to use the GaussianNB() to classify this. So far I managed to do that using another csv with numerical data, now I wanted to use this but I'm stuck.</p>

<p>What's the best way to transform categorical data for a classifier?</p>

<p>This:</p>

<pre><code>p = read_csv(""C:path to\\file.csv"")

trainSet = p.iloc[1:20,2:5] //first 20 rows and just 3 attributes
dic = trainSet.transpose().to_dict()

vec = DictVectorizer()
vec.fit_transform(dic)
</code></pre>

<p>give this error:</p>

<pre><code>Traceback (most recent call last):
  File ""\prova.py"", line 23, in &lt;module&gt;
vec.fit_transform(dic)
File ""\dict_vectorizer.py"", line 142, in fit_transform
return self.transform(X)
File ""\\dict_vectorizer.py"", line 230, in transform
values.append(dtype(v))
TypeError: float() argument must be a string or a number
</code></pre>

<p>What's the best way to transform categorical data for a classifier?</p>
";;2362381;10431;JAB;3;28398046;"<p>The issue is with the transposed 'dataframe' returns a nested <code>dict</code> when <code>.to_dict()</code> is called on it.</p>

<pre><code>#create a dummy frame
df = pd.DataFrame({'factor':['a','a','a','b','c','c','c'], 'factor1':['d','a','d','b','c','d','c'], 'num':range(1,8)})

#transpose the dataframe and get the inner dict from to_dict()
feats =df.T().to_dict().values()

from sklearn.feature_extraction import DictVectorizer
Dvec = DictVectorizer()
Dvec.fit_transform(feats).toarray()
</code></pre>

<p>The solution is to call <code>.values()</code> on the <code>dict</code> to get the inner <code>dict</code></p>

<p>Get new feature names from <code>Dvec</code>:</p>

<pre><code>Dvec.get_feature_names()
</code></pre>
"
823859;6395;Adam_G;<machine-learning><weka>;28395561;0;Use a balanced training set in weka;"<p>Is there a way to make weka use a balanced training set, where it only takes an equal number of instances for each class?</p>
";28438015;823859;6395;Adam_G;1;28438015;"<p>From the Classify panel choose metalearner ""FilteredClassifier"", and make its filter parameter ""weka.filter.supervised.instance.ClassBalancer"". Then select which classifier to use, i.e. J48, AdaBoost, etc.</p>
"
4544677;21;frylock405;<python><numpy><machine-learning>;28402167;2;Calculating mean vector for multivariate normal distribution python;"<p>I am having trouble fitting a multivariate gaussian distribution to my dataset, more specifically, finding a mean vector (or multiple mean vectors). My dataset is an N x 8 matrix and currently I am using this code:</p>

<p><code>muVector = np.mean(Xtrain, axis=0)</code>    where Xtrain is my training data set. </p>

<p>For the covariance I am building it using a arbitrary variance value (.5) and doing:</p>

<p><code>covariance = np.dot(.5, np.eye(N,N)</code>    where N is the number of observations. </p>

<p>But when I construct my Phi matrix, I am getting all zeros. Here is my code:</p>

<pre><code>muVector = np.mean(Xtrain, axis=0)
# get covariance matrix from Xtrain
cov = np.dot(var, np.eye(N,N))
cov = np.linalg.inv(cov)  

# build Xtrain Phi
Phi = np.ones((N,M))
for row in range(N):
  temp = Xtrain[row,:] - muVector
  temp.shape = (1,M)
  temp = np.dot((-.5), temp)
  temp = np.dot(temp, cov)
  temp = np.dot(temp, (Xtrain[row,:] - muVector))
  Phi[row,:] = np.exp(temp)
</code></pre>

<p>Any help is appreciated. I think I might have to use np.random.multivariate_normal()? But I do not know how to use it in this case. </p>
";;3925914;104;bjbschmitt;2;28416571;"<p>By ""Phi"" I believe that you mean the probability density function (pdf) that you want to estimate. In this case, the covariance matrix should be MxM and the output Phi will be Nx1:</p>

<pre><code># -*- coding: utf-8 -*-

import numpy as np

N = 1024
M = 8
var = 0.5

# Creating a Xtrain NxM observation matrix.
# Its muVector is [0, 1, 2, 3, 4, 5, 6, 7] and the variance for all
# independent random variables is 0.5.
Xtrain = np.random.multivariate_normal(np.arange(8), np.eye(8,8)*var, N)

# Estimating the mean vector.
muVector = np.mean(Xtrain, axis=0)

# Creating the estimated covariance matrix and its inverse.
cov = np.eye(M,M)*var
inv_cov = np.linalg.inv(cov)  

# Normalization factor from the pdf.
norm_factor = 1/np.sqrt((2*np.pi)**M * np.linalg.det(cov))

# Estimating the pdf.
Phi = np.ones((N,1))
for row in range(N):
    temp = Xtrain[row,:] - muVector
    temp.shape = (1,M)
    temp = np.dot(-0.5*temp, inv_cov)
    temp = np.dot(temp, (Xtrain[row,:] - muVector))
    Phi[row] = norm_factor*np.exp(temp)
</code></pre>

<p>Alternatively, you can use the <code>pdf</code> method from <code>scipy.stats.multivariate_normal</code>:</p>

<pre><code># -*- coding: utf-8 -*-

import numpy as np
from scipy.stats import multivariate_normal

N = 1024
M = 8
var = 0.5

# Creating a Xtrain NxM observation matrix.
# Its muVector is [0, 1, 2, 3, 4, 5, 6, 7] and the variance for all
# independent random variables is 0.5.
Xtrain = np.random.multivariate_normal(np.arange(8), np.eye(8,8)*var, N)

# Estimating the mean vector.
muVector = np.mean(Xtrain, axis=0)

# Creating the estimated covariance matrix.
cov = np.eye(M,M)*var

Phi2 = multivariate_normal.pdf(Xtrain, mean=muVector, cov=cov)
</code></pre>

<p>Both <code>Phi</code> and <code>Phi2</code> output arrays will be equal. </p>
"
4544800;157;sergulaydore;<python><machine-learning><theano><perceptron><deep-learning>;28402666;0;How to access to a theano symbolic variable's value inside a class?;"<p>I want to access to the value of my_classifier.y_binary. My goal is to compute my_classifier.error.</p>

<p>I know how to get the value of my_classifier.y_hat using eval but I don't know how to use it when the input is a self parameter.</p>

<p>Thanks</p>

<pre class=""lang-py prettyprint-override""><code># imports
import theano
import theano.tensor as T
import numpy as np
import matplotlib.pyplot as plt
import os, subprocess

class Perceptron(object):
    """"""Perceptron for the last layer
    """"""
    def __init__(self, input, targets, n_features):
        """""" Initialize parameters for Perceptron

        :type input:theano.tensor.TensorType
        :param input:symbolic variable that describes the 
                     input of the architecture

        :type targets:theano.tensor.TensorType
        :param targets:symbolic variable that describes the 
                       targets of the architecture

        :type n_features:int
        :param n_features:number of features 
                          (including ""1"" for bias term)   

        """"""

        # initilialize with 0 the weights W as a matrix of shape 
        # n_features x n_targets

        self.w = theano.shared( value=np.zeros((n_features), dtype=theano.config.floatX),
                                name='w',
                                borrow=True
                                )  

        self.y_hat = T.nnet.sigmoid(T.dot(input,self.w))  
        self.y_binary = self.y_hat&gt;0.5
        self.binary_crossentropy = T.mean(T.nnet.binary_crossentropy(self.y_hat,targets))  
        self.error= T.mean(T.neq(self.y_binary, targets))      

# create training data
features = np.array([[1., 0., 0],[1., 0., 1.], [1.,1.,0.], [1., 1., 1.]])
targets = np.array([0., 1., 1., 1])
n_targets = features.shape[0]
n_features = features.shape[1]

# Symbolic variable initialization
X = T.matrix(""X"")
y = T.vector(""y"")   

my_classifier = Perceptron(input=X, targets=y,n_features=n_features)  
cost = my_classifier.binary_crossentropy 
error = my_classifier.error  
gradient = T.grad(cost=cost, wrt=my_classifier.w)
updates = [[my_classifier.w, my_classifier.w-gradient*0.05]]  
# compiling to a theano function
train = theano.function(inputs = [X,y], outputs=cost, updates=updates, allow_input_downcast=True)

# iterate through data
# Iterate through data
l = np.linspace(-1.1,1.1)
cost_list = []
for idx in range(500):
    cost = train(features, targets)
    if my_classifier.error==0:
        break
</code></pre>
";;2805751;410;user2805751;0;28612148;"<p>If you want the value of a node in the graph you'll need to compile a function to get it.  I think something like </p>

<pre><code>y_binary = theano.function(inputs = [X,], outputs=my_classifier.y_binary, allow_input_downcast=True)
</code></pre>

<p>should give you the function <code>y_binary()</code> and calling <code>y_binary(features)</code> should forward propagate the network and yield the binarized output.</p>
"
4544800;157;sergulaydore;<python><machine-learning><theano><perceptron><deep-learning>;28402666;0;How to access to a theano symbolic variable's value inside a class?;"<p>I want to access to the value of my_classifier.y_binary. My goal is to compute my_classifier.error.</p>

<p>I know how to get the value of my_classifier.y_hat using eval but I don't know how to use it when the input is a self parameter.</p>

<p>Thanks</p>

<pre class=""lang-py prettyprint-override""><code># imports
import theano
import theano.tensor as T
import numpy as np
import matplotlib.pyplot as plt
import os, subprocess

class Perceptron(object):
    """"""Perceptron for the last layer
    """"""
    def __init__(self, input, targets, n_features):
        """""" Initialize parameters for Perceptron

        :type input:theano.tensor.TensorType
        :param input:symbolic variable that describes the 
                     input of the architecture

        :type targets:theano.tensor.TensorType
        :param targets:symbolic variable that describes the 
                       targets of the architecture

        :type n_features:int
        :param n_features:number of features 
                          (including ""1"" for bias term)   

        """"""

        # initilialize with 0 the weights W as a matrix of shape 
        # n_features x n_targets

        self.w = theano.shared( value=np.zeros((n_features), dtype=theano.config.floatX),
                                name='w',
                                borrow=True
                                )  

        self.y_hat = T.nnet.sigmoid(T.dot(input,self.w))  
        self.y_binary = self.y_hat&gt;0.5
        self.binary_crossentropy = T.mean(T.nnet.binary_crossentropy(self.y_hat,targets))  
        self.error= T.mean(T.neq(self.y_binary, targets))      

# create training data
features = np.array([[1., 0., 0],[1., 0., 1.], [1.,1.,0.], [1., 1., 1.]])
targets = np.array([0., 1., 1., 1])
n_targets = features.shape[0]
n_features = features.shape[1]

# Symbolic variable initialization
X = T.matrix(""X"")
y = T.vector(""y"")   

my_classifier = Perceptron(input=X, targets=y,n_features=n_features)  
cost = my_classifier.binary_crossentropy 
error = my_classifier.error  
gradient = T.grad(cost=cost, wrt=my_classifier.w)
updates = [[my_classifier.w, my_classifier.w-gradient*0.05]]  
# compiling to a theano function
train = theano.function(inputs = [X,y], outputs=cost, updates=updates, allow_input_downcast=True)

# iterate through data
# Iterate through data
l = np.linspace(-1.1,1.1)
cost_list = []
for idx in range(500):
    cost = train(features, targets)
    if my_classifier.error==0:
        break
</code></pre>
";;2498151;6163;Linda MacPhee-Cobb;0;37125559;"<p>A compiled function is a much better choice, but while you're setting stuff up a quick and dirty way is like this:</p>

<p>like this:</p>

<pre><code>while (epoch &lt; n_epochs):    
        epoch = epoch + 1    
        for minibatch_index in range(n_train_batches):
            minibatch_avg_cost = train_model(minibatch_index)
            iter = (epoch - 1) * n_train_batches + minibatch_index
            print(""**********************************"")
            print(classifier.hiddenLayer.W.get_value()) 
</code></pre>

<p>full code here: <a href=""https://github.com/timestocome/MiscDeepLearning/blob/master/MLP_iris2.py"" rel=""nofollow"">https://github.com/timestocome/MiscDeepLearning/blob/master/MLP_iris2.py</a></p>

<p>I think in your example you'd use 'my_classifier.w.get_value()'</p>
"
2028043;5367;USB;<machine-learning><neural-network><classification><backpropagation>;28403782;35;What is the difference between back-propagation and feed-forward Neural Network?;"<p>What is the difference between back-propagation and feed-forward neural networks?</p>

<p>By googling and reading, I found that in feed-forward there is only forward direction, but in back-propagation once we need to do a forward-propagation and then back-propagation. I referred to <a href=""http://www.nnwj.de/backpropagation.html"" rel=""noreferrer"">this link</a></p>

<ol>
<li>Any other difference other than the direction of flow? What about the weight calculation? The outcome? </li>
<li>Say I am implementing back-propagation, i.e. it contains forward and backward flow. So is back-propagation enough for showing feed-forward?</li>
</ol>
";28407788;4545181;151;user4545181;15;28404272;"<p>There is no pure backpropagation or pure feed-forward neural network.</p>

<p>Backpropagation is algorithm to train (adjust weight) of neural network.
Input for backpropagation is output_vector, target_output_vector,
output is adjusted_weight_vector.</p>

<p>Feed-forward is algorithm to calculate output vector from input vector.
Input for feed-forward is input_vector,
output is output_vector.</p>

<p>When you are training neural network, you need to use both algorithms.</p>

<p>When you are using neural network (which have been trained), you are using only feed-forward.</p>

<p>Basic type of neural network is multi-layer perceptron, which is Feed-forward backpropagation neural network.</p>

<p>There are also more advanced types of neural networks, using modified algorithms.</p>

<p>Also good source to study : <a href=""ftp://ftp.sas.com/pub/neural/FAQ.html"" rel=""noreferrer"">ftp://ftp.sas.com/pub/neural/FAQ.html</a>
Best to understand principle is to program it (tutorial in this video) <a href=""https://www.youtube.com/watch?v=KkwX7FkLfug"" rel=""noreferrer"">https://www.youtube.com/watch?v=KkwX7FkLfug</a></p>
"
2028043;5367;USB;<machine-learning><neural-network><classification><backpropagation>;28403782;35;What is the difference between back-propagation and feed-forward Neural Network?;"<p>What is the difference between back-propagation and feed-forward neural networks?</p>

<p>By googling and reading, I found that in feed-forward there is only forward direction, but in back-propagation once we need to do a forward-propagation and then back-propagation. I referred to <a href=""http://www.nnwj.de/backpropagation.html"" rel=""noreferrer"">this link</a></p>

<ol>
<li>Any other difference other than the direction of flow? What about the weight calculation? The outcome? </li>
<li>Say I am implementing back-propagation, i.e. it contains forward and backward flow. So is back-propagation enough for showing feed-forward?</li>
</ol>
";28407788;860196;8365;runDOSrun;62;28407788;"<ul>
<li><p>A <a href=""http://en.wikipedia.org/wiki/Feedforward_neural_network"" rel=""noreferrer"">Feed-Forward Neural Network</a> is a type of Neural Network <strong>architecture</strong> where the connections are ""fed forward"", i.e. do not form cycles (like in recurrent nets).</p></li>
<li><p>The term ""Feed forward"" is also used when you input something at the input layer and it <em>travels</em> from input to hidden and from hidden to output layer. <br><strong>The values are ""fed forward""</strong>.</p></li>
</ul>

<p>Both of these uses of the phrase ""feed forward"" are in a context that has nothing to do with training per se.</p>

<ul>
<li>Backpropagation is a <strong>training algorithm</strong> consisting of 2 steps: 1) <strong>Feed forward</strong> the values 2) calculate the error and <strong>propagate it back</strong> to the earlier layers. So to be precise, forward-propagation is part of the backpropagation <strong>algorithm</strong> but comes before back-propagating.</li>
</ul>
"
2028043;5367;USB;<machine-learning><neural-network><classification><backpropagation>;28403782;35;What is the difference between back-propagation and feed-forward Neural Network?;"<p>What is the difference between back-propagation and feed-forward neural networks?</p>

<p>By googling and reading, I found that in feed-forward there is only forward direction, but in back-propagation once we need to do a forward-propagation and then back-propagation. I referred to <a href=""http://www.nnwj.de/backpropagation.html"" rel=""noreferrer"">this link</a></p>

<ol>
<li>Any other difference other than the direction of flow? What about the weight calculation? The outcome? </li>
<li>Say I am implementing back-propagation, i.e. it contains forward and backward flow. So is back-propagation enough for showing feed-forward?</li>
</ol>
";28407788;8202132;1875;Ananth;0;53344507;"<p>Neural Networks can have different architectures. The connections between their neurons decide direction of flow of information. Depending on network connections, they are categorised as - Feed-Forward and Recurrent (back-propagating).</p>

<p><strong>Feed Forward Neural Networks</strong></p>

<p>In these types of neural networks information flows in only one direction i.e. from input layer to output layer. When the weights are once decided, they are not usually changed. One either explicitly decides weights or uses functions like Radial Basis Function to decide weights. The nodes here do their job without being aware whether results produced are accurate or not(i.e. they don't re-adjust according to result produced). There is no communication back from the layers ahead.</p>

<p><strong>Recurrent Neural Networks (Back-Propagating)</strong></p>

<p>Information passes from input layer to output layer to produce result. Error in result is then communicated back to previous layers now. Nodes get to know how much they contributed in the answer being wrong. Weights are re-adjusted. Neural network is improved. It learns. There is bi-directional flow of information. This basically has both algorithms implemented, feed-forward and back-propagation.</p>
"
2028043;5367;USB;<machine-learning><neural-network><classification><backpropagation>;28403782;35;What is the difference between back-propagation and feed-forward Neural Network?;"<p>What is the difference between back-propagation and feed-forward neural networks?</p>

<p>By googling and reading, I found that in feed-forward there is only forward direction, but in back-propagation once we need to do a forward-propagation and then back-propagation. I referred to <a href=""http://www.nnwj.de/backpropagation.html"" rel=""noreferrer"">this link</a></p>

<ol>
<li>Any other difference other than the direction of flow? What about the weight calculation? The outcome? </li>
<li>Say I am implementing back-propagation, i.e. it contains forward and backward flow. So is back-propagation enough for showing feed-forward?</li>
</ol>
";28407788;13126782;21;S Z;1;61019871;"<p>To be simple: </p>

<p>Feed-foward is an architecture. The contrary one is Recurrent Neural Networks. </p>

<p>Back Propagation (BP) is a solving method. BP can solve both feed-foward and Recurrent Neural Networks.</p>
"
4114372;1915;john doe;<python><numpy><machine-learning><scipy><scikit-learn>;28404352;2;How to binarize textual data with scikit learn?;"<p>I am vectorizing some textual data with scikit's tfidf. By the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow"">documentation</a> I read that that you can set idf and normalization to False to get 0/1 outputs(binary feature vector?). So I tried the following:</p>

<pre><code>tfidf_vect= TfidfVectorizer(use_idf=False,
                            smooth_idf=True,
                            sublinear_tf=False,
                            ngram_range=(2,2),
                            norm=False)
</code></pre>

<p>With this the feature vector will be binary?. By the other hand in the <a href=""http://scikit-learn.org/stable/modules/preprocessing.html#binarization"" rel=""nofollow"">documentation</a> there is another module that can do this task, Preprocessing. Any idea of how to aproach this?. </p>
";28415849;2362381;10431;JAB;1;28415849;"<p><code>TfidfVectorizer</code> take a parameter called <code>binary</code>:</p>

<pre><code>tfidf_vect= TfidfVectorizer(use_idf=False,binary=True, norm=False, ngram_range=(2, 2))
</code></pre>

<p>This will make the features binary</p>
"
2202420;129;user2202420;<machine-learning><neural-network><deep-learning><dbn><conv-neural-network>;28415426;2;Convolutional Deep Belief Networks (CDBN) vs. Convolutional Neural Networks (CNN);"<p>Lastly, I started to learn neural networks and I would like know the difference between Convolutional Deep Belief Networks and Convolutional Networks. In <a href=""https://stackoverflow.com/questions/24545725/deep-belief-networks-vs-convolutional-neural-networks"">here</a>, there is a similar question but there is no exact answer for it. We know that Convolutional Deep Belief Networks are CNNs + DBNs. So, I am going to do an object recognition. I want to know which one is much better than other or their complexity. I searched but I couldn't find anything maybe doing something wrong.</p>
";29446411;3415562;22;Redirectk;-2;29446411;"<p>I don't know if you still need an answer but anyway I hope you will find this useful.</p>

<p>A CDBN adds the complexity of a DBN, but if you already have some background it's not that much.<br>
If you are worried about computational complexity instead, it really depends on how you use the DBN part. The role of DBN usually is to initialize the weights of the network for faster convergence. In this scenario, the DBN appears only during pre-training.<br>
You can also use the whole DBN like a discriminative network (keeping the generative power) but the weight initialization provided by it is enough for discriminative tasks. So during an hypothetical real-time utilization, the two system are equal performance-wise.<br></p>

<p>Also the weight-initialization provided by the first model anyway really helps for difficult task like object recognition (even a good Convolutional Neural network alone doesn't reach good success rate, at least compared to a human) so it's generally a good choice.</p>
"
4520962;85;Jae ;<matlab><image-processing><machine-learning>;28417456;2;Matlab - Machine Learning?;"<p>In my research progress, I have separated the parasite from the image now. the parasite looks like a worm. I want the MATLAB to read all the input images, look for the worm like dark purple image and if detected, to give out a reply saying detected. I tried using histogram comparison but I think using area or shape is a better option and it deals with machine learning. I don't know how to do that.</p>

<p>(<a href=""http://www.cdc.gov/dpdx/lymphaticFilariasis/gallery.html#microwbancrofti"" rel=""nofollow"">http://www.cdc.gov/dpdx/lymphaticFilariasis/gallery.html#microwbancrofti</a>)</p>
";28422228;1190430;5186;Artem Sobolev;2;28422228;"<p>It's hard to tell what you need to do, since the problem is not formalized, so we can't give an 100%-working algorithm.</p>
<p>After a glance at the pictures, I noticed that the pictures have, roughly, 3 kinds of objects:</p>
<ol>
<li>Background</li>
<li>Little balls in foreground color</li>
<li>Long and curvy worm in foreground color</li>
</ol>
<p>What can you do then:</p>
<ol>
<li><p>Binarize the image (every pixel) into 2 colors: foreground and background. You can use either the pixel only, or its neighbors, too.</p>
</li>
<li><p>Find <a href=""http://en.wikipedia.org/wiki/Connected_component_(graph_theory)"" rel=""nofollow noreferrer"">connectivity components</a> among the foreground pixels. You can represent each pixel of a picture as a node, or do something more sophisticated.</p>
</li>
<li><p>The bigger component would be a worm, if it's presented. You can either analyze pictures you have and come up with a threshold to discriminate worms from balls, or do something more fancy.</p>
<p>For example, these balls have circular shape, so you can try to fit each connectivity component into a circle. All balls on the pictures look pretty distinct and separated, so the component with enormous radius is what you're looking for. Or, you can calculate maximum inter-component distance.</p>
</li>
</ol>
<p>To conclude: as I said earlier, there's no exact solution to your problem. You need to try different approaches, and see what works best.</p>
"
1356645;1213;Karim Tarabishy;<matlab><machine-learning><classification>;28421744;0;1D Gaussian Bayes Calssification using matlab;"<p>If I have 2 classes and one feature and the feature is normally distributes on both class with different mean and variance, something like that
<img src=""https://i.stack.imgur.com/f8rQT.png"" alt=""enter image description here""></p>

<p>Now I want to find the equation of the discriminant surface and draw it on graph, something like that (this may not be the correct surface, it is just an illustration of what I am seeking for)</p>

<p><img src=""https://i.stack.imgur.com/Qe1gM.png"" alt=""enter image description here""></p>

<p>Is there a way to do that with matlab?!</p>
";28422046;1190430;5186;Artem Sobolev;1;28422046;"<p>Obviously, you'd like to classify a point as a point from distribution which has higher density at that point. Thus, the point of separation would be the one where both densities are equal. In general (multivariate) case your problem is known as <a href=""http://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis"" rel=""nofollow noreferrer"">Quadratic discriminant analysis</a>.</p>

<p>For QDA one can find a separating curve (in general it's 2-nd order surface, a generalization of parabola) analytically. Fortunately, your case is 1-dimensional, so 1D parabola is just a point (or two).</p>

<p>The derivation goes as follows</p>

<p><img src=""https://i.stack.imgur.com/iiZ6z.png"" alt=""Derivation of QDA for 1D case""></p>

<p>The last one is a quadratic equation on <code>x</code>, its solution is the point of separation. In some cases there are 2 solutions, that means there are 2 points of intersection of densities.</p>

<p>What you need to do is to complete my derivation (write our formula(s) for <code>x</code>), this <code>x</code> is a function of Gaussians' parameters, which you could calculate in any language you like.</p>
"
4550032;85;Bubba;<machine-learning><neural-network><black-box>;28429971;1;How are the following types of neural network-like techniques called?;"<p>The neural network applications I've seen always learn the weights of their inputs and use fixed ""hidden layers"".</p>

<p>But I'm wondering about the following techniques:</p>

<p>1) fixed inputs, but the hidden layers are no longer fixed, in the sense that the functions of the input they compute can be tweaked (learned)</p>

<p>2) fixed inputs, but the hidden layers are no longer fixed, in the sense that although they have clusters which compute fixed functions (multiplication, addition, etc... just like ALUs in a CPU or GPU) of their inputs, the weights of the connections between them and between them and the input can be learned (this should in some ways be equivalent to 1)  )</p>

<p>These could be used to model systems for which we know the inputs and the output but not how the input is turned into the output (figuring out what is inside a ""black box""). Do such techniques exist and if so, what are they called?</p>
";;2014584;6576;lmjohns3;2;28435961;"<p>For part (1) of your question, there are a couple of relatively recent techniques that come to mind.</p>

<p>The first one is a type of feedforward layer called ""maxout"" which computes a piecewise linear output function of its inputs.</p>

<p>Consider a traditional neural network unit with <code>d</code> inputs and a linear transfer function. We can describe the output of this unit as a function of its input <code>z</code> (a vector with <code>d</code> elements) as <code>g(z) = w z</code>, where <code>w</code> is a vector with <code>d</code> weight values.</p>

<p>In a maxout unit, the output of the unit is described as</p>

<pre><code>g(z) = max_k w_k z
</code></pre>

<p>where <code>w_k</code> is a vector with <code>d</code> weight values, and there are <code>k</code> such weight vectors <code>[w_1 ... w_k]</code> <em>per unit</em>. Each of the weight vectors in the maxout unit computes some linear function of the input, and the <code>max</code> combines all of these linear functions into a single, convex, piecewise linear function. The individual weight vectors can be learned by the network, so that in effect each linear transform learns to model a specific part of the input (<code>z</code>) space.</p>

<p>You can read more about maxout networks at <a href=""http://arxiv.org/abs/1302.4389"" rel=""nofollow"">http://arxiv.org/abs/1302.4389</a>.</p>

<p>The second technique that has recently been developed is the ""parametric relu"" unit. In this type of unit, all neurons in a network layer compute an output <code>g(z) = max(0, w z) + a min(w z, 0)</code>, as compared to the more traditional rectified linear unit, which computes <code>g(z) = max(0, w z)</code>. The parameter <code>a</code> is shared across all neurons in a layer in the network and is learned along with the weight vector <code>w</code>.</p>

<p>The prelu technique is described by <a href=""http://arxiv.org/abs/1502.01852"" rel=""nofollow"">http://arxiv.org/abs/1502.01852</a>.</p>

<p>Maxout units have been shown to work well for a number of image classification tasks, particularly when combined with dropout to prevent overtraining. It's unclear whether the parametric relu units are extremely useful in modeling images, but the prelu paper gets really great results on what has for a while been considered the benchmark task in image classification.</p>
"
4366077;181;Nelson;<r><machine-learning><sparse-matrix>;28430674;4;Create sparse matrix from data frame;"<p>Iâ€™m trying to create a sparse data matrix from a data frame without having to build a dense matrix which causes serious memory issues .</p>

<p>I found a SO the following post where a solution seems to be found: 
<a href=""https://stackoverflow.com/questions/26207850/create-sparse-matrix-from-a-data-frame"">Create Sparse Matrix from a data frame</a></p>

<p>I've tried this solution, but, it doesn't work for me, perhaps because my <code>UserID</code> and <code>MovieID</code> doesn't t start in 1.</p>

<p>Here is my sample code:</p>

<pre><code>library(Matrix)

UserID&lt;-c(10090,10090,10090,10316,10316)
MovieID &lt;-c(63155,63530,63544,63155,63545)
Rating &lt;-c(2,2,1,2,1)
trainingData&lt;-data.frame(UserID,MovieID,Rating)
trainingData

UIMatrix &lt;- sparseMatrix(i = trainingData$UserID,
                         j = trainingData$MovieID,
                         x = trainingData$Rating)

dim(UIMatrix)
</code></pre>

<p>I expected to get a 2 x 3 matrix but the dims corresponds to the maximum user and movie id.</p>

<p>I've tryed the second solutions suggested in the post but it doesn't with may data  work as well.</p>

<p>Can anyone give some advise?</p>
";28431966;1627235;75256;Sven Hohenstein;1;28431966;"<p>You can convert your indices to indices starting at one with <code>as.integer(as.factor(.))</code>.</p>

<pre><code>UIMatrix &lt;- sparseMatrix(i = as.integer(as.factor(trainingData$UserID)),
                         j = as.integer(as.factor(trainingData$MovieID)),
                         x = trainingData$Rating)

dim(UIMatrix)
# [1] 2 4

dimnames(UIMatrix) &lt;- list(sort(unique(trainingData$UserID)),
                           sort(unique(trainingData$MovieID)))

UIMatrix
# 2 x 4 sparse Matrix of class ""dgCMatrix""
#       63155 63530 63544 63545
# 10090     2     2     1     .
# 10316     2     .     .     1
</code></pre>
"
4202221;453;Spu;<python-3.x><machine-learning><scikit-learn><probability><bayesian-networks>;28431350;27;Create Bayesian Network and learn parameters with Python3.x;"<p>I'm searching for the most appropriate tool for python3.x on Windows to create a Bayesian Network, learn its parameters from data and perform the inference.</p>

<p>The network structure I want to define myself as follows:
<img src=""https://i.stack.imgur.com/iscKX.jpg"" alt=""enter image description here""></p>

<p>It is taken from <a href=""http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=6697180&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6697180"" rel=""noreferrer"">this</a> paper. </p>

<p>All the variables are discrete (and can take only 2 possible states) except ""Size"" and ""GraspPose"", which are continuous and should be modeled as Mixture of Gaussians.</p>

<p>Authors use <em>Expectation-Maximization algorithm</em> to learn the parameters for conditional probability tables and <em>Junction-Tree algorithm</em> to compute the exact inference.</p>

<p>As I understand all is realised in MatLab with Bayes Net Toolbox by Murphy.</p>

<p>I tried to search something similar in python and here are my results:</p>

<ol>
<li>Python Bayesian Network Toolbox <a href=""http://sourceforge.net/projects/pbnt.berlios/"" rel=""noreferrer"">http://sourceforge.net/projects/pbnt.berlios/</a> (<a href=""http://pbnt.berlios.de/"" rel=""noreferrer"">http://pbnt.berlios.de/</a>). Web-site doesn't work, project doesn't seem to be supported. </li>
<li>BayesPy <a href=""https://github.com/bayespy/bayespy"" rel=""noreferrer"">https://github.com/bayespy/bayespy</a>
I think this is what I actually need, but I fail to find some examples similar to my case, to understand how to approach construction of the network structure.</li>
<li><p>PyMC seems to be a powerful module, but I have problems with importing it on Windows 64, python 3.3. I get error when I install development version </p>

<p>WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.</p></li>
</ol>

<p>UPDATE:</p>

<ol start=""4"">
<li>libpgm (<a href=""http://pythonhosted.org/libpgm/"" rel=""noreferrer"">http://pythonhosted.org/libpgm/</a>). Exactly what I need, unfortunately not supported by python 3.x</li>
<li>Very interesting actively developing library: PGMPY. Unfortunately continuous variables and learning from data is not supported yet. <a href=""https://github.com/pgmpy/pgmpy/"" rel=""noreferrer"">https://github.com/pgmpy/pgmpy/</a> </li>
</ol>

<p>Any advices and concrete examples will be highly appreciated.</p>
";;938325;3369;James Atwood;12;33241252;"<p>It looks like <a href=""https://github.com/jmschrei/pomegranate"" rel=""noreferrer"">pomegranate</a> was recently updated to include Bayesian Networks.  I haven't tried it myself, but the interface looks nice and sklearn-ish.</p>
"
4202221;453;Spu;<python-3.x><machine-learning><scikit-learn><probability><bayesian-networks>;28431350;27;Create Bayesian Network and learn parameters with Python3.x;"<p>I'm searching for the most appropriate tool for python3.x on Windows to create a Bayesian Network, learn its parameters from data and perform the inference.</p>

<p>The network structure I want to define myself as follows:
<img src=""https://i.stack.imgur.com/iscKX.jpg"" alt=""enter image description here""></p>

<p>It is taken from <a href=""http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=6697180&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6697180"" rel=""noreferrer"">this</a> paper. </p>

<p>All the variables are discrete (and can take only 2 possible states) except ""Size"" and ""GraspPose"", which are continuous and should be modeled as Mixture of Gaussians.</p>

<p>Authors use <em>Expectation-Maximization algorithm</em> to learn the parameters for conditional probability tables and <em>Junction-Tree algorithm</em> to compute the exact inference.</p>

<p>As I understand all is realised in MatLab with Bayes Net Toolbox by Murphy.</p>

<p>I tried to search something similar in python and here are my results:</p>

<ol>
<li>Python Bayesian Network Toolbox <a href=""http://sourceforge.net/projects/pbnt.berlios/"" rel=""noreferrer"">http://sourceforge.net/projects/pbnt.berlios/</a> (<a href=""http://pbnt.berlios.de/"" rel=""noreferrer"">http://pbnt.berlios.de/</a>). Web-site doesn't work, project doesn't seem to be supported. </li>
<li>BayesPy <a href=""https://github.com/bayespy/bayespy"" rel=""noreferrer"">https://github.com/bayespy/bayespy</a>
I think this is what I actually need, but I fail to find some examples similar to my case, to understand how to approach construction of the network structure.</li>
<li><p>PyMC seems to be a powerful module, but I have problems with importing it on Windows 64, python 3.3. I get error when I install development version </p>

<p>WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.</p></li>
</ol>

<p>UPDATE:</p>

<ol start=""4"">
<li>libpgm (<a href=""http://pythonhosted.org/libpgm/"" rel=""noreferrer"">http://pythonhosted.org/libpgm/</a>). Exactly what I need, unfortunately not supported by python 3.x</li>
<li>Very interesting actively developing library: PGMPY. Unfortunately continuous variables and learning from data is not supported yet. <a href=""https://github.com/pgmpy/pgmpy/"" rel=""noreferrer"">https://github.com/pgmpy/pgmpy/</a> </li>
</ol>

<p>Any advices and concrete examples will be highly appreciated.</p>
";;7468700;27;Teng Fu;0;42675579;"<p>For pymc's g++ problem, I highly recommend to get g++ installation done, it would hugely boost the sampling process, otherwise you will have to live with this warning and sit there for 1 hour for a 2000 sampling process.</p>

<p>The way to get the warning fixed is:
1. get g++ installed, download cywing and get g++ install, you can google that. To check this, just go to ""cmd"" and type ""g++"", if it says ""require input file"", great, you got g++ installed.
2. install python package: mingw, libpython
3. install python package: theano</p>

<p>this should get this problem fixed.</p>

<p>I am currently working on the same problem with you, good luck!</p>
"
4202221;453;Spu;<python-3.x><machine-learning><scikit-learn><probability><bayesian-networks>;28431350;27;Create Bayesian Network and learn parameters with Python3.x;"<p>I'm searching for the most appropriate tool for python3.x on Windows to create a Bayesian Network, learn its parameters from data and perform the inference.</p>

<p>The network structure I want to define myself as follows:
<img src=""https://i.stack.imgur.com/iscKX.jpg"" alt=""enter image description here""></p>

<p>It is taken from <a href=""http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=6697180&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6697180"" rel=""noreferrer"">this</a> paper. </p>

<p>All the variables are discrete (and can take only 2 possible states) except ""Size"" and ""GraspPose"", which are continuous and should be modeled as Mixture of Gaussians.</p>

<p>Authors use <em>Expectation-Maximization algorithm</em> to learn the parameters for conditional probability tables and <em>Junction-Tree algorithm</em> to compute the exact inference.</p>

<p>As I understand all is realised in MatLab with Bayes Net Toolbox by Murphy.</p>

<p>I tried to search something similar in python and here are my results:</p>

<ol>
<li>Python Bayesian Network Toolbox <a href=""http://sourceforge.net/projects/pbnt.berlios/"" rel=""noreferrer"">http://sourceforge.net/projects/pbnt.berlios/</a> (<a href=""http://pbnt.berlios.de/"" rel=""noreferrer"">http://pbnt.berlios.de/</a>). Web-site doesn't work, project doesn't seem to be supported. </li>
<li>BayesPy <a href=""https://github.com/bayespy/bayespy"" rel=""noreferrer"">https://github.com/bayespy/bayespy</a>
I think this is what I actually need, but I fail to find some examples similar to my case, to understand how to approach construction of the network structure.</li>
<li><p>PyMC seems to be a powerful module, but I have problems with importing it on Windows 64, python 3.3. I get error when I install development version </p>

<p>WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.</p></li>
</ol>

<p>UPDATE:</p>

<ol start=""4"">
<li>libpgm (<a href=""http://pythonhosted.org/libpgm/"" rel=""noreferrer"">http://pythonhosted.org/libpgm/</a>). Exactly what I need, unfortunately not supported by python 3.x</li>
<li>Very interesting actively developing library: PGMPY. Unfortunately continuous variables and learning from data is not supported yet. <a href=""https://github.com/pgmpy/pgmpy/"" rel=""noreferrer"">https://github.com/pgmpy/pgmpy/</a> </li>
</ol>

<p>Any advices and concrete examples will be highly appreciated.</p>
";;6690508;983;morganics;0;43897371;"<p>Late to the party, as always, but I've wrapped up the BayesServer Java API using JPype; it might not have all the functionality that you need but you would create the above network using something like:</p>

<pre><code>from bayesianpy.network import Builder as builder
import bayesianpy.network

nt = bayesianpy.network.create_network()

# where df is your dataframe
task = builder.create_discrete_variable(nt, df, 'task')

size = builder.create_continuous_variable(nt, 'size')
grasp_pose = builder.create_continuous_variable(nt, 'GraspPose')

builder.create_link(nt, size, grasp_pose)
builder.create_link(nt, task, grasp_pose)

for v in ['fill level', 'object shape', 'side graspable']:
    va = builder.create_discrete_variable(nt, df, v)
    builder.create_link(nt, va, grasp_pose)
    builder.create_link(nt, task, va)

# write df to data store
with bayesianpy.data.DataSet(df, bayesianpy.utils.get_path_to_parent_dir(__file__), logger) as dataset:
    model = bayesianpy.model.NetworkModel(nt, logger)
    model.train(dataset)

    # to query model multi-threaded
    results = model.batch_query(dataset, [bayesianpy.model.QueryModelStatistics()], append_to_df=False)
</code></pre>

<p>I'm not affiliated with Bayes Server - and the Python wrapper is not 'official' (you can use the Java API via Python directly). My wrapper makes some assumptions and places limitations on functions that I don't use very much. The repo is here: <a href=""http://github.com/morganics/bayesianpy"" rel=""nofollow noreferrer"">github.com/morganics/bayesianpy</a></p>
"
4202221;453;Spu;<python-3.x><machine-learning><scikit-learn><probability><bayesian-networks>;28431350;27;Create Bayesian Network and learn parameters with Python3.x;"<p>I'm searching for the most appropriate tool for python3.x on Windows to create a Bayesian Network, learn its parameters from data and perform the inference.</p>

<p>The network structure I want to define myself as follows:
<img src=""https://i.stack.imgur.com/iscKX.jpg"" alt=""enter image description here""></p>

<p>It is taken from <a href=""http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=6697180&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6697180"" rel=""noreferrer"">this</a> paper. </p>

<p>All the variables are discrete (and can take only 2 possible states) except ""Size"" and ""GraspPose"", which are continuous and should be modeled as Mixture of Gaussians.</p>

<p>Authors use <em>Expectation-Maximization algorithm</em> to learn the parameters for conditional probability tables and <em>Junction-Tree algorithm</em> to compute the exact inference.</p>

<p>As I understand all is realised in MatLab with Bayes Net Toolbox by Murphy.</p>

<p>I tried to search something similar in python and here are my results:</p>

<ol>
<li>Python Bayesian Network Toolbox <a href=""http://sourceforge.net/projects/pbnt.berlios/"" rel=""noreferrer"">http://sourceforge.net/projects/pbnt.berlios/</a> (<a href=""http://pbnt.berlios.de/"" rel=""noreferrer"">http://pbnt.berlios.de/</a>). Web-site doesn't work, project doesn't seem to be supported. </li>
<li>BayesPy <a href=""https://github.com/bayespy/bayespy"" rel=""noreferrer"">https://github.com/bayespy/bayespy</a>
I think this is what I actually need, but I fail to find some examples similar to my case, to understand how to approach construction of the network structure.</li>
<li><p>PyMC seems to be a powerful module, but I have problems with importing it on Windows 64, python 3.3. I get error when I install development version </p>

<p>WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.</p></li>
</ol>

<p>UPDATE:</p>

<ol start=""4"">
<li>libpgm (<a href=""http://pythonhosted.org/libpgm/"" rel=""noreferrer"">http://pythonhosted.org/libpgm/</a>). Exactly what I need, unfortunately not supported by python 3.x</li>
<li>Very interesting actively developing library: PGMPY. Unfortunately continuous variables and learning from data is not supported yet. <a href=""https://github.com/pgmpy/pgmpy/"" rel=""noreferrer"">https://github.com/pgmpy/pgmpy/</a> </li>
</ol>

<p>Any advices and concrete examples will be highly appreciated.</p>
";;6182097;465;Shaboti;2;52620403;"<p>I was looking for a similar library, and I found that the <a href=""https://github.com/jmschrei/pomegranate"" rel=""nofollow noreferrer"">pomegranate</a> is  a good one. Thanks <a href=""https://stackoverflow.com/users/938325/james-atwood"">James Atwood</a></p>
<p>Here is an example how to use it.</p>
<pre><code>from pomegranate import *
import numpy as np

mydb=np.array([[1,2,3],[1,2,4],[1,2,5],[1,2,6],[1,3,8],[2,3,8],[1,2,4]])

bnet = BayesianNetwork.from_samples(mydb)

print(bnet.node_count())

print(bnet.probability([[1,2,3]]))
print (bnet.probability([[1,2,8]]))
</code></pre>
"
4202221;453;Spu;<python-3.x><machine-learning><scikit-learn><probability><bayesian-networks>;28431350;27;Create Bayesian Network and learn parameters with Python3.x;"<p>I'm searching for the most appropriate tool for python3.x on Windows to create a Bayesian Network, learn its parameters from data and perform the inference.</p>

<p>The network structure I want to define myself as follows:
<img src=""https://i.stack.imgur.com/iscKX.jpg"" alt=""enter image description here""></p>

<p>It is taken from <a href=""http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=6697180&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6697180"" rel=""noreferrer"">this</a> paper. </p>

<p>All the variables are discrete (and can take only 2 possible states) except ""Size"" and ""GraspPose"", which are continuous and should be modeled as Mixture of Gaussians.</p>

<p>Authors use <em>Expectation-Maximization algorithm</em> to learn the parameters for conditional probability tables and <em>Junction-Tree algorithm</em> to compute the exact inference.</p>

<p>As I understand all is realised in MatLab with Bayes Net Toolbox by Murphy.</p>

<p>I tried to search something similar in python and here are my results:</p>

<ol>
<li>Python Bayesian Network Toolbox <a href=""http://sourceforge.net/projects/pbnt.berlios/"" rel=""noreferrer"">http://sourceforge.net/projects/pbnt.berlios/</a> (<a href=""http://pbnt.berlios.de/"" rel=""noreferrer"">http://pbnt.berlios.de/</a>). Web-site doesn't work, project doesn't seem to be supported. </li>
<li>BayesPy <a href=""https://github.com/bayespy/bayespy"" rel=""noreferrer"">https://github.com/bayespy/bayespy</a>
I think this is what I actually need, but I fail to find some examples similar to my case, to understand how to approach construction of the network structure.</li>
<li><p>PyMC seems to be a powerful module, but I have problems with importing it on Windows 64, python 3.3. I get error when I install development version </p>

<p>WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.</p></li>
</ol>

<p>UPDATE:</p>

<ol start=""4"">
<li>libpgm (<a href=""http://pythonhosted.org/libpgm/"" rel=""noreferrer"">http://pythonhosted.org/libpgm/</a>). Exactly what I need, unfortunately not supported by python 3.x</li>
<li>Very interesting actively developing library: PGMPY. Unfortunately continuous variables and learning from data is not supported yet. <a href=""https://github.com/pgmpy/pgmpy/"" rel=""noreferrer"">https://github.com/pgmpy/pgmpy/</a> </li>
</ol>

<p>Any advices and concrete examples will be highly appreciated.</p>
";;13730780;708;erdogant;4;62452991;"<p>Try the bnlearn library, it contains many function to learn parameters from data and perform the inference.</p>

<pre><code>pip install bnlearn
</code></pre>

<p>Your use-case would be like this:</p>

<pre><code># Import the library
import bnlearn

# Define the network structure
edges = [('task', 'size'),
         ('lat var', 'size'),
         ('task', 'fill level'),
         ('task', 'object shape'),
         ('task', 'side graspable'),
         ('size', 'GrasPose'),
         ('task', 'GrasPose'),
         ('fill level', 'GrasPose'),
         ('object shape', 'GrasPose'),
         ('side graspable', 'GrasPose'),
         ('GrasPose', 'latvar'),
]

# Make the actual Bayesian DAG
DAG = bnlearn.make_DAG(edges)

# DAG is stored in adjacency matrix
print(DAG['adjmat'])

# target           task   size  lat var  ...  side graspable  GrasPose  latvar
# source                                 ...                                  
# task            False   True    False  ...            True      True   False
# size            False  False    False  ...           False      True   False
# lat var         False   True    False  ...           False     False   False
# fill level      False  False    False  ...           False      True   False
# object shape    False  False    False  ...           False      True   False
# side graspable  False  False    False  ...           False      True   False
# GrasPose        False  False    False  ...           False     False    True
# latvar          False  False    False  ...           False     False   False
# 
# [8 rows x 8 columns]

# No CPDs are in the DAG. Lets see what happens if we print it.
bnlearn.print_CPD(DAG)
# &gt;[BNLEARN.print_CPD] No CPDs to print. Use bnlearn.plot(DAG) to make a plot.

# Plot DAG. Note that it can be differently orientated if you re-make the plot.
bnlearn.plot(DAG)
</code></pre>

<p><a href=""https://i.stack.imgur.com/ewkq7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ewkq7.png"" alt=""Pre-defined DAG""></a></p>

<p>Now we need the data to learn its parameters. Suppose these are stored in your <em>df</em>. The variable names in the data-file must be present in the DAG.</p>

<pre><code># Read data
df = pd.read_csv('path_to_your_data.csv')

# Learn the parameters and store CPDs in the DAG. Use the methodtype your desire. Options are maximumlikelihood or bayes.
DAG = bnlearn.parameter_learning.fit(DAG, df, methodtype='maximumlikelihood')
# CPDs are present in the DAG at this point.
bnlearn.print_CPD(DAG)

# Start making inferences now. As an example:
q1 = bnlearn.inference.fit(DAG, variables=['lat var'], evidence={'fill level':1, 'size':0, 'task':1})
</code></pre>

<p>Below is a working example with a demo dataset (sprinkler). You can play around with this.</p>

<pre><code># Import example dataset
df = bnlearn.import_example('sprinkler')
print(df)
#      Cloudy  Sprinkler  Rain  Wet_Grass
# 0         0          0     0          0
# 1         1          0     1          1
# 2         0          1     0          1
# 3         1          1     1          1
# 4         1          1     1          1
# ..      ...        ...   ...        ...
# 995       1          0     1          1
# 996       1          0     1          1
# 997       1          0     1          1
# 998       0          0     0          0
# 999       0          1     1          1

# [1000 rows x 4 columns]


# Define the network structure
edges = [('Cloudy', 'Sprinkler'),
         ('Cloudy', 'Rain'),
         ('Sprinkler', 'Wet_Grass'),
         ('Rain', 'Wet_Grass')]

# Make the actual Bayesian DAG
DAG = bnlearn.make_DAG(edges)
# Print the CPDs
bnlearn.print_CPD(DAG)
# [BNLEARN.print_CPD] No CPDs to print. Use bnlearn.plot(DAG) to make a plot.
# Plot the DAG
bnlearn.plot(DAG)
</code></pre>

<p><a href=""https://i.stack.imgur.com/Ynbvd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ynbvd.png"" alt=""enter image description here""></a></p>

<pre><code># Parameter learning on the user-defined DAG and input data
DAG = bnlearn.parameter_learning.fit(DAG, df)

# Print the learned CPDs
bnlearn.print_CPD(DAG)

# [BNLEARN.print_CPD] Independencies:
# (Cloudy _|_ Wet_Grass | Rain, Sprinkler)
# (Sprinkler _|_ Rain | Cloudy)
# (Rain _|_ Sprinkler | Cloudy)
# (Wet_Grass _|_ Cloudy | Rain, Sprinkler)
# [BNLEARN.print_CPD] Nodes: ['Cloudy', 'Sprinkler', 'Rain', 'Wet_Grass']
# [BNLEARN.print_CPD] Edges: [('Cloudy', 'Sprinkler'), ('Cloudy', 'Rain'), ('Sprinkler', 'Wet_Grass'), ('Rain', 'Wet_Grass')]
# CPD of Cloudy:
# +-----------+-------+
# | Cloudy(0) | 0.494 |
# +-----------+-------+
# | Cloudy(1) | 0.506 |
# +-----------+-------+
# CPD of Sprinkler:
# +--------------+--------------------+--------------------+
# | Cloudy       | Cloudy(0)          | Cloudy(1)          |
# +--------------+--------------------+--------------------+
# | Sprinkler(0) | 0.4807692307692308 | 0.7075098814229249 |
# +--------------+--------------------+--------------------+
# | Sprinkler(1) | 0.5192307692307693 | 0.2924901185770751 |
# +--------------+--------------------+--------------------+
# CPD of Rain:
# +---------+--------------------+---------------------+
# | Cloudy  | Cloudy(0)          | Cloudy(1)           |
# +---------+--------------------+---------------------+
# | Rain(0) | 0.6518218623481782 | 0.33695652173913043 |
# +---------+--------------------+---------------------+
# | Rain(1) | 0.3481781376518219 | 0.6630434782608695  |
# +---------+--------------------+---------------------+
# CPD of Wet_Grass:
# +--------------+--------------------+---------------------+---------------------+---------------------+
# | Rain         | Rain(0)            | Rain(0)             | Rain(1)             | Rain(1)             |
# +--------------+--------------------+---------------------+---------------------+---------------------+
# | Sprinkler    | Sprinkler(0)       | Sprinkler(1)        | Sprinkler(0)        | Sprinkler(1)        |
# +--------------+--------------------+---------------------+---------------------+---------------------+
# | Wet_Grass(0) | 0.7553816046966731 | 0.33755274261603374 | 0.25588235294117645 | 0.37910447761194027 |
# +--------------+--------------------+---------------------+---------------------+---------------------+
# | Wet_Grass(1) | 0.2446183953033268 | 0.6624472573839663  | 0.7441176470588236  | 0.6208955223880597  |
# +--------------+--------------------+---------------------+---------------------+---------------------+

# Make inference
q1 = bnlearn.inference.fit(DAG, variables=['Wet_Grass'], evidence={'Rain':1, 'Sprinkler':0, 'Cloudy':1})

# +--------------+------------------+
# | Wet_Grass    |   phi(Wet_Grass) |
# +==============+==================+
# | Wet_Grass(0) |           0.2559 |
# +--------------+------------------+
# | Wet_Grass(1) |           0.7441 |
# +--------------+------------------+

print(q1.values)
# array([0.25588235, 0.74411765])
</code></pre>

<p>More examples can be found at pages of bnlearn:
<a href=""https://erdogant.github.io/bnlearn"" rel=""nofollow noreferrer"">https://erdogant.github.io/bnlearn</a></p>
"
4550527;11;dreams come true;<java><machine-learning><weka><smo>;28432393;1;Reading Already Trained SMO Model Gives StreamoutofException Error;"<p>I have trained the <code>svm</code> model in the <code>smo</code> and write it to a file using SerializationHelper.</p>

<p>when I try to read the model to classify it gives the following error: </p>

<blockquote>
  <p>Problem found: java.io.StreamCorruptedException: invalid stream header: 47656E65</p>
</blockquote>

<p>Here is the line that gives the error:</p>

<pre><code>classifier = (Classifier) SerializationHelper.read(strModel);
</code></pre>

<p>EDITED:</p>

<pre><code>    Instances inst = Filter.useFilter(data, vectorizer);
    inst.setClassIndex(0);
    SMO svm = new SMO();
    RBFKernel kernal = new RBFKernel();
    svm.setKernel(kernal);
    svm.buildClassifier(filteredDate);
    SerializationHelper.write(""SMO_samp.model"", svm);
</code></pre>
";;968064;2363;Rushdi Shams;0;28463378;"<p>For the error, <a href=""http://list.waikato.ac.nz/pipermail/wekalist/2008-August/040649.html"" rel=""nofollow"">this</a> might be helpful. </p>

<p>This is a completely working example: </p>

<pre><code>import java.io.BufferedReader;
import java.io.FileReader;
import java.util.Random;
import weka.classifiers.Classifier;
import weka.classifiers.Evaluation;
import weka.classifiers.bayes.NaiveBayes;
import weka.classifiers.functions.LibSVM;
import weka.classifiers.functions.SMO;
import weka.core.Instances;
import weka.core.SerializationHelper;


public class CodeTest {

    public static void main(String[] args) throws Exception{

        BufferedReader breader = null;
        breader = new BufferedReader(new FileReader(""iris.arff""));

        Instances train_data = new Instances (breader);
        breader.close();
        train_data.setClassIndex(train_data.numAttributes() -1);

        SMO smo = new SMO();
        smo.buildClassifier(train_data);

        SerializationHelper s = new SerializationHelper();
        s.write(""SMO_samp.model"", smo);
        Classifier c = new SMO();
        c = (Classifier) s.read(""SMO_Samp.model"");
        Evaluation evaluation = null;
        evaluation = new Evaluation (train_data);
        evaluation.crossValidateModel(c, train_data, 10, new Random(1));
        System.out.println(evaluation.toSummaryString());
    }
}
</code></pre>
"
4488922;83;Sriram;<python><machine-learning><nlp><nltk>;28437945;5;Extract Dates and events associated with the date from Text corpus;"<p>I am currently running a pyhton code that runs through every line of the text file and parses the line for Dates. If it does find the date in the line, the line is copied to a new Output file. 
I am repeating this process on 100 documents and at the end i get a output file which contains lines which has Dates Like ""2013, august 2014, 01-11-1987 and so on.""</p>

<p>The problem with this is, it does not give accurate information of the events associated with some Dates. </p>

<p>Is there a more elegant approach to this problem?
Below is the file in which i am trying to extract events for the date December 2010</p>

<blockquote>
  <p>Taipei is the most competitive place among all major cities and counties, according to a study published by a local magazine yesterday.
  Taipei came in first in each of the categories - economy, employment, education, environmental protection, public safety, medical care and local finances - evaluated in the study by Global View Magazine.
  In terms of overall competitiveness, Taipei is therefore number one, followed by Hsinchu City, Chiayi City and New Taipei.
  Taipei, with more than six decades of privileged development heavily funded by the central government, will remain unchallenged in the foreseeable future, Global View commented.
  Taipei and New Taipei are two of the country's five Cabinet-level special municipalities, but the other three - Taichung, Tainan and Kaohsiung - failed to receive good ratings in the study though they have more resources than most other local governments.
  Taichung ranks seventh, Tainan 12th and Kaohsiung 15th of all 19 local governments graded in the study.
  The three special municipalities grew to the present size by merging neighboring counties in December 2010. But Global View said the mergers crippled their competitiveness.
  But all five special municipalities are in the top-10 in terms of economic competitiveness.
  At the bottom is the agricultural Pingtung County. But another agricultural county, Taitung, made it to the top-10, occupying the eighth place mainly because of its low crime rate, the magazine said.</p>
</blockquote>

<p>As you can see when i parse the line containing December 2010 i dont really get any meaningful information
But actually there is one major event which is merging of nneighbouring counties.
This is not captured. Hence i need to know is there any algorithm/library which can help me capture events that has occured on particular date</p>

<p>Thanks</p>
";;3633250;4320;Maksim Khaitovich;8;28441577;"<p>I suggest you to try out the NLTK library for python. You could get it here, also here is some basic manual for it:
<a href=""http://www.nltk.org/book/ch07.html"" rel=""noreferrer"">http://www.nltk.org/book/ch07.html</a></p>

<p>It has tons of algorithms for extraction of meaning from text. Also it has some of modules which allow you to:</p>

<p>1) Extract entities
2) Extract dates
3) Establish relationship between extracted entities and dates.</p>

<p>I suggest you to pay attention to timex.py module in NLTK library:
<a href=""https://github.com/nltk/nltk_contrib/blob/master/nltk_contrib/timex.py"" rel=""noreferrer"">https://github.com/nltk/nltk_contrib/blob/master/nltk_contrib/timex.py</a></p>

<p>It is mainly built to tokenize dates and times in text.</p>

<p>And here is guide to extracting entity relationship:
<a href=""http://www.nltk.org/howto/relextract.html"" rel=""noreferrer"">http://www.nltk.org/howto/relextract.html</a></p>

<p>So I beleive you could extract interesting entities from your text (like the event you mentioned), you could extract dates as another set of entities, and using NLTK you could establish relationship between these extracted entities. As there result you should get what you need - what happened when.</p>
"
4366077;181;Nelson;<r><matrix><machine-learning><sparse-matrix><recommendation-engine>;28439487;2;Convert sparse matrix (dgCMatrix) to realRatingMatrix;"<p>I've converted a data frame to a sparse matrix to avoid memory issues and save space, once the original data doesn't fit in the memory. </p>

<p>Now, I need to convert this sparse matrix to a realratingmatrix so I can build a recommender with recommenderlab, but i got the following error:</p>

<pre><code>Error in as(aux_max, ""realRatingMatrix"") : 
  no method or default for coercing â€œdgCMatrixâ€ to â€œrealRatingMatrixâ€
</code></pre>

<p>My sample code is the following:</p>

<pre><code>library(Matrix)
UserID&lt;-c(10090,10090,10090,10316,10316)
MovieID &lt;-c(63155,63530,63544,63155,63545)
Rating &lt;-c(2,2,1,2,1)
trainingData&lt;-data.frame(UserIDa,MovieID,Rating)

UIMatrix &lt;- sparseMatrix(i = as.integer(as.factor(trainingData$UserID)),
                         j = as.integer(as.factor(trainingData$MovieID)),
                         x = trainingData$Rating
                        )

dimnames(UIMatrix) &lt;- list(sort(unique(trainingData$UserID)),
                           sort(unique(trainingData$MovieID)))

rrm &lt;- as(UIMatrix, ""realRatingMatrix"")
</code></pre>

<p>Can anyone give some advise on how to solve that?</p>
";28439800;4366077;181;Nelson;3;28439800;"<p>Well, I think I got the answer. 
I coerced the  ""dgCMatrix"" to ""matrix"" and then to ""realratingmatrix"".
Seems to work fine.</p>

<pre><code>rrm&lt;- as(  as(UIMatrix, ""matrix"")   , ""realRatingMatrix"")
</code></pre>
"
4366077;181;Nelson;<r><matrix><machine-learning><sparse-matrix><recommendation-engine>;28439487;2;Convert sparse matrix (dgCMatrix) to realRatingMatrix;"<p>I've converted a data frame to a sparse matrix to avoid memory issues and save space, once the original data doesn't fit in the memory. </p>

<p>Now, I need to convert this sparse matrix to a realratingmatrix so I can build a recommender with recommenderlab, but i got the following error:</p>

<pre><code>Error in as(aux_max, ""realRatingMatrix"") : 
  no method or default for coercing â€œdgCMatrixâ€ to â€œrealRatingMatrixâ€
</code></pre>

<p>My sample code is the following:</p>

<pre><code>library(Matrix)
UserID&lt;-c(10090,10090,10090,10316,10316)
MovieID &lt;-c(63155,63530,63544,63155,63545)
Rating &lt;-c(2,2,1,2,1)
trainingData&lt;-data.frame(UserIDa,MovieID,Rating)

UIMatrix &lt;- sparseMatrix(i = as.integer(as.factor(trainingData$UserID)),
                         j = as.integer(as.factor(trainingData$MovieID)),
                         x = trainingData$Rating
                        )

dimnames(UIMatrix) &lt;- list(sort(unique(trainingData$UserID)),
                           sort(unique(trainingData$MovieID)))

rrm &lt;- as(UIMatrix, ""realRatingMatrix"")
</code></pre>

<p>Can anyone give some advise on how to solve that?</p>
";28439800;13125759;1;Scott;0;60859617;"<p>I just received this error (no method or default for coercing â€œmatrixâ€ to â€œrealRatingMatrixâ€), I needed to install library(recommenderlab) package. </p>

<p><a href=""https://i.stack.imgur.com/sbY3r.png"" rel=""nofollow noreferrer"">error message and solution</a></p>
"
2858968;715;BioChemoinformatics;<machine-learning><classification><feature-extraction><feature-selection>;28445441;2;How to combine two (or multiple) kinds of features as one final feature to build classification model?;"<p>Currently, I meeting such question:How to combine two (or multiple) kinds of features as one final feature to build classification model?</p>

<p>For example, I would like to do a classification model to predict the drug-target interaction, here for each drug I can get 500 features, and each targets I can get 800 features (this two kinds of features are independent each other). As a know, a simple method to combine this two kinds of features which just plus two together (that is 500 + 800 = 1300 feature for each drug-target pair).</p>

<p>Does anybody know other methods to do such kind of thing and use the combined feature to build classification models?</p>
";28448052;1060350;70512;Has QUIT--Anony-Mousse;3;28448052;"<p>There are a number of methods which should work just fine even with this many features, such as SVM.</p>

<p>Also, there is a whole bunch of literature on this, including SVD, PCA, MDS, feature selection, feature transformation... you'll have to read up on these, we can't pick your magic bullet out of all that for you without having your data.</p>
"
2858968;715;BioChemoinformatics;<machine-learning><classification><feature-extraction><feature-selection>;28445441;2;How to combine two (or multiple) kinds of features as one final feature to build classification model?;"<p>Currently, I meeting such question:How to combine two (or multiple) kinds of features as one final feature to build classification model?</p>

<p>For example, I would like to do a classification model to predict the drug-target interaction, here for each drug I can get 500 features, and each targets I can get 800 features (this two kinds of features are independent each other). As a know, a simple method to combine this two kinds of features which just plus two together (that is 500 + 800 = 1300 feature for each drug-target pair).</p>

<p>Does anybody know other methods to do such kind of thing and use the combined feature to build classification models?</p>
";28448052;5861039;1;dkohlsdorf;0;35102426;"<p>Random Forest select the best features for your classification task using information gain. The classifier works for multiple feature sources and also types. For example, you can combine continuous attributes and discrete ones.</p>

<p>The training time is a little longer since you have to iterate all features multiple times but the memory performance and the classification speed is 
pretty good. </p>
"
2632736;132;Nithin.P;<machine-learning><naivebayes>;28451051;2;Use naive bayes for numerical attributes;"<p>Can we use naive bayes for classification of numerical data.  We have a system which extract authors styles such as sentence length,word length and use it to predict authorship of anonymous documents. So can we use naive bayes to classify authors based on these numerical data. Else can you suggest a better alternative.</p>
";28451243;270287;41194;IVlad;2;28451243;"<p>Yes you can. In fact, every learning algorithm that I'm aware of works with numerical features: when dealing with other kind of data you have to preprocess it to numerical data first.</p>

<p>I would suggest you also look into <a href=""http://en.wikipedia.org/wiki/Support_vector_machine"" rel=""nofollow"">Support Vector Machines</a>, <a href=""http://webee.technion.ac.il/people/koby/publications/crammer06a.pdf"" rel=""nofollow"">Online Passive Aggressive Classifiers</a> and <a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow"">TF-IDF weighting</a> for this task.</p>
"
1610765;1390;Chiffa;<machine-learning><algorithmic-trading>;28451852;-6;Machine Learning on financial big data;"<p><em>Disclaimer:</em> although I know some things about big data and am currently learning some other things about machine learning, the specific area that I wish to study is vague, or at least appears vague to me now. I'll do my best to describe it, but this question could still be categorised as too vague or not really a question. Hopefully, I'll be able to reword it more precisely once I get a reaction.</p>

<p>So,</p>

<p>I have some experience with Hadoop and the Hadoop stack (gained via using CDH), and I'm reading a book about Mahout, which is a collection of machine learning libraries. I also think I know enough statistics to be able to comprehend the math behind the machine learning algorithms, and I have some experience with R.
My ultimate goal is making a setup that would make trading predictions and deal with financial data in real time.</p>

<p>I wonder if there're any materials that I can further read to help me understand ways of managing that problem; books, video tutorials and exercises with example datasets are all welcome.</p>
";28457498;3633250;4320;Maksim Khaitovich;2;28457498;"<p>Take ML course on coursera. It is a good introductery into ML algorithms which will tell you what ML could do\some general approaches:
<a href=""https://www.coursera.org/course/ml"" rel=""nofollow"">https://www.coursera.org/course/ml</a></p>

<p>Also to get a broader picture I suggest coursera's DataSciense course:
<a href=""https://www.coursera.org/course/datasci"" rel=""nofollow"">https://www.coursera.org/course/datasci</a></p>

<p>Finally a good book is Mahout in action - it is more about solving practical matters with mahout and has lots of examples and case-studies.
I beleive after that you will have a better understanding of what you want to do next.</p>
"
4508464;39;Alex;<python><machine-learning><pybrain>;28453408;3;Pybrain neural network: _convertToOneOfMany error;"<p>I am new to Pybrain and trying to put together a neural network. First, I came across <a href=""https://stackoverflow.com/questions/27887936/attributeerror-using-pybrain-splitwithportion-object-type-changed"">the error described here</a>: </p>

<pre><code>AttributeError: 'SupervisedDataSet' object has no attribute '_convertToOneOfMany'
</code></pre>

<p>I tried the workaround described in the accepted answer of that thread. While it seems it works, it now gives me a new error. These are the relevant chunks of my code:</p>

<p>The part that reads the file into a classification dataset. 3 input attributes, 2 classes, splits the read array, first 3 columns to 'input' and the last one to 'target':</p>

<pre><code>ds = ClassificationDataSet(inp=3, target=1, nb_classes=2)
tf = open('datafile.txt')
a = np.loadtxt(tf) 
a = np.hsplit(a, (3,4))
ds.setField('input', a[0])
ds.setField('target', a[1])
</code></pre>

<p>The part that builds a simple network, pretty standard for pybrain:</p>

<pre><code>inLayer = SigmoidLayer(3)
hiddenLayer = SigmoidLayer(5)
outLayer = SigmoidLayer(2)

fnn.addInputModule(inLayer)
fnn.addModule(hiddenLayer)
fnn.addOutputModule(outLayer)

in_to_hidden = FullConnection(inLayer, hiddenLayer)
hidden_to_out = FullConnection(hiddenLayer, outLayer)

fnn.addConnection(in_to_hidden)
fnn.addConnection(hidden_to_out)

fnn.sortModules()
</code></pre>

<p>This is the workaround, as described above:</p>

<pre><code>tstdata_temp, trndata_temp = ds.splitWithProportion(0.25)

tstdata = ClassificationDataSet(3, target=1, nb_classes=2)
for n in xrange(0, tstdata_temp.getLength()):
     tstdata.addSample( tstdata_temp.getSample(n)[0], tstdata_temp.getSample(n)[1] )

trndata = ClassificationDataSet(3, target=1, nb_classes=2)
for n in xrange(0, trndata_temp.getLength()):
     trndata.addSample( trndata_temp.getSample(n)[0], trndata_temp.getSample(n)[1] )

trndata._convertToOneOfMany()
tstdata._convertToOneOfMany()
</code></pre>

<p>And this is the error I am getting on the first convert line:</p>

<pre><code>IndexError: index 2 is out of bounds for axis 1 with size 2
</code></pre>
";;4654039;11;mkw123;1;28964517;"<p>I don't know what values your 'target' field elements have, but I got the same error with _convertToOneOfMany() as a result of having class labels starting from 1 not 0. </p>

<p>_convertToOneOfMany() converts the 'target' field of a dataset from an array of class labels like 0, 1, 2 of size [n_samples,1] to an array of labels like 100, 010, 001 of size [n_samples,n_classes] (so it does: 0 -> 100, 1->010 and 2->001). Consequently if you have 3 classes labelled as 1, 2 and 3 _convertToOneOfMany() will do 1->010, 2->001, 3-> error!  </p>

<p>The code for this function is here: <a href=""https://github.com/pybrain/pybrain/blob/master/pybrain/datasets/classification.py"" rel=""nofollow"">https://github.com/pybrain/pybrain/blob/master/pybrain/datasets/classification.py</a> and on line 144 the class labels (oldtarg[i]) are used as the column indices for newtarg. </p>
"
2718932;299;Abhishek Chauhan;<python><pandas><machine-learning><scikit-learn><random-forest>;28453486;3;"Python: ""TypeError: Could not operate with block values"" coming when I import RandomForestClassifier";"<p>I am writing a digit recognition program in python. The basic code is as follows:</p>

<pre><code>import pandas as pd
from sklearn.ensemble import RandomForestClassifier
filteredColumns = delete_useless_columns()
train = pd.read_csv('C:\\Users\\abchauhan\\Downloads\\train.csv')
trainData = train.loc[0:24998, filteredColumns]
target = train['label']
targetData = target[0:24999]
rf = RandomForestClassifier(n_estimators=150, min_samples_split=2, n_jobs=-1)
x = trainData/255 #Feature scaling
print('Fitting the data')
rf.fit(x, targetData)
</code></pre>

<p>The feature scaling line gives the error: <code>TypeError: Could not operate 255 with block values</code>. Now if I remove the <code>RandomForestClassifier</code> import statement the feature scaling works fine, but obviously then the program is of no use. Why is the division working without the import statement?</p>

<p>Edit: 
<code>trainData.info()</code> is as follows:</p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 24999 entries, 0 to 24998
Columns: 708 entries, pixel12 to pixel779
dtypes: int64(708)
memory usage: 135.2 MB
None
</code></pre>

<p>Stack Trace is as follows:</p>

<pre><code>Traceback (most recent call last):
 File ""C:\Python34\lib\site-packages\pandas-0.15.2-py3.4-win32.egg\pandas\core\internals.py"", line 965, in eval
result = get_result(other)
 File ""C:\Python34\lib\site-packages\pandas-0.15.2-py3.4-win32.egg\pandas\core\internals.py"", line 949, in get_result
return self._try_coerce_result(func(values, other))
 File ""C:\Python34\lib\site-packages\pandas-0.15.2-py3.4-win32.egg\pandas\core\ops.py"", line 765, in na_op
op, str_rep, x, y, raise_on_error=True, **eval_kwargs)
 File ""C:\Python34\lib\site-packages\pandas-0.15.2-py3.4-win32.egg\pandas\computation\expressions.py"", line 218, in evaluate
**eval_kwargs)
 File ""C:\Python34\lib\site-packages\pandas-0.15.2-py3.4-win32.egg\pandas\computation\expressions.py"", line 71, in _evaluate_standard
return op(a, b)
 MemoryError

 During handling of the above exception, another exception occurred:

 Traceback (most recent call last):
 File        ""C:/Users/abchauhan/PycharmProjects/DigitRecognition/PreProcess/RandomForest.py"", line 20, in &lt;module&gt;
 x = trainData/255
 File ""C:\Python34\lib\site-packages\pandas-0.15.2-py3.4-win32.egg\pandas\core\ops.py"", line 831, in f
return self._combine_const(other, na_op)
 File ""C:\Python34\lib\site-packages\pandas-0.15.2-py3.4-win32.egg\pandas\core\frame.py"", line 3111, in _combine_const
new_data = self._data.eval(func=func, other=other, raise_on_error=raise_on_error)
 File ""C:\Python34\lib\site-packages\pandas-0.15.2-py3.4-win32.egg\pandas\core\internals.py"", line 2478, in eval
return self.apply('eval', **kwargs)
 File ""C:\Python34\lib\site-packages\pandas-0.15.2-py3.4-win32.egg\pandas\core\internals.py"", line 2457, in apply
applied = getattr(b, f)(**kwargs)
 File ""C:\Python34\lib\site-packages\pandas-0.15.2-py3.4-win32.egg\pandas\core\internals.py"", line 972, in eval
result = handle_error()
 File ""C:\Python34\lib\site-packages\pandas-0.15.2-py3.4-win32.egg\pandas\core\internals.py"", line 956, in handle_error
% (repr(other), str(detail)))
 TypeError: Could not operate 255 with block values 

 Process finished with exit code 1
</code></pre>
";;8411122;121;beevor;0;53413386;"<p>This is a memory error, as indicated by the first error in your stack trace.</p>
"
4556978;23;BenyaminH;<matlab><machine-learning><kernel><svm><libsvm>;28466485;0;How can I Use from a variable instead of an amount for nu parameter in nu-SVM for libSVM in Matlab?;"<p>As you know, libSVM command is like this:</p>

<pre><code>model_nu = svmtrain(train_classset, train_dataset,'-s 1 -t 0 -n 0.5'  );
</code></pre>

<p>and when I run it, it's OK.
But I need to use a variable instead of a number for nu. like this:</p>

<pre><code>nu = 0.5;
 model_nu = svmtrain(train_classset, train_dataset,'-s 1 -t 0 -n nu'  );
</code></pre>

<p>When I do this, svmtrain function doesn't work.
So how can I use a variable instead of a number in svmtrain function of libSVM toolbox in Matlab?</p>
";;2732801;35987;Daniel;0;28467225;"<p>You have to convert the number into char:</p>

<pre><code>model_nu = svmtrain(train_classset, train_dataset,sprintf('-s 1 -t 0 -n %f',nu))
</code></pre>
"
2755321;79;CSK;<python><machine-learning><nlp><classification><nltk>;28469476;2;TypeError: 'WordListCorpusReader' object has no attribute '__getitem__' while using nltk.classify.apply_features;"<p>I'm following this tutorial to learn NaiveBayes on <a href=""http://www.nltk.org/book/ch06.html"" rel=""nofollow"">this site</a>. The code I have is:</p>

<pre><code>from nltk.corpus import names
from nltk.classify import apply_features

def gender_features(word):
  return {'last_letter': word[-1]}

labeled_names = ([(name, 'male') for name in names.words('male.txt')] +
[(name, 'female') for name in names.words('female.txt')])

feature_sets = [(gender_features(n), gender) for (n, gender) in labeled_names]

#train_set, test_set = feature_sets[500:], feature_sets[:500]
train_set = apply_features(gender_features, names[500:])
test_set = apply_features(gender_features, names[:500])

classifier = NaiveBayesClassifier.train(train_set)

print classifier.classify(gender_features('Neo'))
</code></pre>

<p>Using train_set with out apply_features works fine. Anybody have any idea how I could solve it? Thank you.</p>
";28488116;610569;93723;alvas;0;28488116;"<p>Firstly, I think there is a typo in the tutorial on <a href=""http://www.nltk.org/book/ch06.html"" rel=""nofollow"">http://www.nltk.org/book/ch06.html</a></p>

<p>The wordlist corpus cannot be access like a list. </p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import names
&gt;&gt;&gt; names[:5]
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
TypeError: 'LazyCorpusLoader' object has no attribute '__getitem__'
&gt;&gt;&gt; names.words()[:5]
[u'Abagael', u'Abagail', u'Abbe', u'Abbey', u'Abbi']
</code></pre>

<p>Next see here on what <code>apply_features</code> does (<a href=""https://github.com/nltk/nltk/blob/develop/nltk/classify/util.py#L28"" rel=""nofollow"">https://github.com/nltk/nltk/blob/develop/nltk/classify/util.py#L28</a>).</p>

<p>Basically, given a list of tuples of <code>[('input_1', 'label_1'), ...('input_N', 'label_N')]</code>, it returns <code>[(feature_func(tok), label) for (tok, label) in toks]</code>. E.g.</p>

<pre><code># To get the input list of tuples for apply_features, we do this:
&gt;&gt;&gt; [(word,'female') for word in names.words('female.txt')[:10]]
[(u'Abagael', 'female'), (u'Abagail', 'female'), (u'Abbe', 'female'), (u'Abbey', 'female'), (u'Abbi', 'female'), (u'Abbie', 'female'), (u'Abby', 'female'), (u'Abigael', 'female'), (u'Abigail', 'female'), (u'Abigale', 'female')]

# Let's get 250 from female and 250 from male names.
&gt;&gt;&gt; train_female = [(word,'female') for word in names.words('female.txt')[:250]] 
&gt;&gt;&gt; train_male = [(word,'male') for word in names.words('male.txt')[:250]]
&gt;&gt;&gt; train_data = train_female + train_male
&gt;&gt;&gt; apply_features(gender_features, train_data)
[({'last_letter': u'l'}, 'female'), ({'last_letter': u'l'}, 'female'), ...]
</code></pre>

<p>The full code to get the Naivebayes to work in NLTK for the names corpus:</p>

<pre><code>from nltk.corpus import names
from nltk.classify import apply_features, NaiveBayesClassifier

def gender_features(word):
    return {'last_letter': word[-1]}


train_female = [(word,'female') for word in names.words('female.txt')[:250]] 
train_male = [(word,'male') for word in names.words('male.txt')[:250]]
train_data = train_female + train_male
train_set = apply_features(gender_features, train_data)

# Do like wise for the test set.
'''
test_female = [(word,'female') for word in names.words('female.txt')[250:]]
test_male = [(word,'male') for word in names.words('male.txt')[250:]] 
test_data = test_female + test_male
test_set = apply_features(gender_features, test_data)
'''

classifier = NaiveBayesClassifier.train(train_set)
print classifier.classify(gender_features('Neo'))
</code></pre>

<p>[out]:</p>

<pre><code>'male'
</code></pre>
"
4140027;3985;tumbleweed;<machine-learning><nlp><artificial-intelligence><scikit-learn><svm>;28470165;5;How to plot SVC classification for an unbalanced dataset with scikit-learn and matplotlib?;"<p>I have a text classification task with 2599 documents and five labels from 1 to 5. The documents are</p>

<pre><code>label | texts
----------
5     |1190
4     |839
3     |239
1     |204
2     |127
</code></pre>

<p>All ready classified this textual data with very low performance, and also get warnings about ill defined metrics:</p>

<pre><code>Accuracy: 0.461057692308

score: 0.461057692308

precision: 0.212574195636

recall: 0.461057692308

  'precision', 'predicted', average, warn_for)
 confussion matrix:
[[  0   0   0   0 153]
  'precision', 'predicted', average, warn_for)
 [  0   0   0   0  94]
 [  0   0   0   0 194]
 [  0   0   0   0 680]
 [  0   0   0   0 959]]

 clasification report:
             precision    recall  f1-score   support

          1       0.00      0.00      0.00       153
          2       0.00      0.00      0.00        94
          3       0.00      0.00      0.00       194
          4       0.00      0.00      0.00       680
          5       0.46      1.00      0.63       959

avg / total       0.21      0.46      0.29      2080
</code></pre>

<p>Clearly this is happening by the fact that I have an unbalanced dataset, so I found this <a href=""http://sci2s.ugr.es/keel/pdf/specific/congreso/akbani_svm_2004.pdf"" rel=""nofollow noreferrer"">paper</a> where the authors propose several aproaches to deal with this issue:</p>

<blockquote>
  <p>The problem is that with imbalanced datasets, the learned boundary is
  too close to the positive instances. We need to bias SVM in a way that
  will push the boundary away from the positive instances. Veropoulos et
  al [14] suggest using different error costs for the positive (C +) and
  negative (C - ) classes</p>
</blockquote>

<p>I know that this could be very complicated but SVC offers several hyper parameters, So my question is: Is there any way to bias SVC in a way that push the boundary away from possitive instances with the hyper parameters that offer SVC classifier?. I know that this could be a difficult problem but any help is welcome, thanks in advance guys.</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
tfidf_vect= TfidfVectorizer(use_idf=True, smooth_idf=True, sublinear_tf=False, ngram_range=(2,2))
from sklearn.cross_validation import train_test_split, cross_val_score

import pandas as pd
df = pd.read_csv('/path/of/the/file.csv',
                     header=0, sep=',', names=['id', 'text', 'label'])



reduced_data = tfidf_vect.fit_transform(df['text'].values)
y = df['label'].values



from sklearn.decomposition.truncated_svd import TruncatedSVD
svd = TruncatedSVD(n_components=5)
reduced_data = svd.fit_transform(reduced_data)

from sklearn import cross_validation
X_train, X_test, y_train, y_test = cross_validation.train_test_split(reduced_data,
                                                    y, test_size=0.33)

#with no weights:

from sklearn.svm import SVC
clf = SVC(kernel='linear', class_weight={1: 10})
clf.fit(reduced_data, y)
prediction = clf.predict(X_test)

w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-5, 5)
yy = a * xx - clf.intercept_[0] / w[1]


# get the separating hyperplane using weighted classes
wclf = SVC(kernel='linear', class_weight={1: 10})
wclf.fit(reduced_data, y)

ww = wclf.coef_[0]
wa = -ww[0] / ww[1]
wyy = wa * xx - wclf.intercept_[0] / ww[1]

# plot separating hyperplanes and samples
import matplotlib.pyplot as plt
h0 = plt.plot(xx, yy, 'k-', label='no weights')
h1 = plt.plot(xx, wyy, 'k--', label='with weights')
plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=y, cmap=plt.cm.Paired)
plt.legend()

plt.axis('tight')
plt.show()
</code></pre>

<p>But I get nothing and I cant understand what happened, this is the plot:</p>

<p><img src=""https://i.stack.imgur.com/9eEqX.png"" alt=""weighted vs normal""></p>

<p>then:</p>

<pre><code>#Let's show some metrics[unweighted]:
from sklearn.metrics.metrics import precision_score, \
    recall_score, confusion_matrix, classification_report, accuracy_score
print '\nAccuracy:', accuracy_score(y_test, prediction)
print '\nscore:', clf.score(X_train, y_train)
print '\nrecall:', recall_score(y_test, prediction)
print '\nprecision:', precision_score(y_test, prediction)
print '\n clasification report:\n', classification_report(y_test, prediction)
print '\n confussion matrix:\n',confusion_matrix(y_test, prediction)

#Let's show some metrics[weighted]:
print 'weighted:\n'

from sklearn.metrics.metrics import precision_score, \
    recall_score, confusion_matrix, classification_report, accuracy_score
print '\nAccuracy:', accuracy_score(y_test, prediction)
print '\nscore:', wclf.score(X_train, y_train)
print '\nrecall:', recall_score(y_test, prediction)
print '\nprecision:', precision_score(y_test, prediction)
print '\n clasification report:\n', classification_report(y_test, prediction)
print '\n confussion matrix:\n',confusion_matrix(y_test, prediction)
</code></pre>

<p>This is the <a href=""http://pastebin.com/0cwiLGG2"" rel=""nofollow noreferrer"">data</a> that Im using. How can I fix this and plot in a right way this problem?. thanks in advance guys!.</p>

<p>From an answer in this question I remove this lines:</p>

<pre><code>#
# from sklearn.decomposition.truncated_svd import TruncatedSVD
# svd = TruncatedSVD(n_components=5)
# reduced_data = svd.fit_transform(reduced_data)


#
# w = clf.coef_[0]
# a = -w[0] / w[1]
# xx = np.linspace(-10, 10)
# yy = a * xx - clf.intercept_[0] / w[1]

# ww = wclf.coef_[0]
# wa = -ww[0] / ww[1]
# wyy = wa * xx - wclf.intercept_[0] / ww[1]
#
# # plot separating hyperplanes and samples
# import matplotlib.pyplot as plt
# h0 = plt.plot(xx, yy, 'k-', label='no weights')
# h1 = plt.plot(xx, wyy, 'k--', label='with weights')
# plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=y, cmap=plt.cm.Paired)
# plt.legend()
#
# plt.axis('tight')
# plt.show()

This where the results:

Accuracy: 0.787878787879

score: 0.779437105112

recall: 0.787878787879

precision: 0.827705441238
</code></pre>

<p>This metrics improved. <strong>How can I plot this results in order to have a nice example like the documentation one. I would like to see the behavior of the two hyper planes?</strong>. Thanks guys!</p>
";;3633250;4320;Maksim Khaitovich;2;28479621;"<p>If I understood your input correctly you have:</p>

<p>1190 of 5 labeled texts
1409 of 1-4 labeled texts</p>

<p>You may try to do a sequental classification. First threat all 5 labels as 1 and all other as 0. Train a classifier for this task</p>

<p>Second, drop out all 5 examples from your dataset. Train classifier to classify 1-4 labels.</p>

<p>Upon classification run first classifier, if it returns 0 - run second classifier to obtain final label.</p>

<p>Though I don't think that this distribution is really skewed and unballanced (it should be smth like 90% of 5, 10% - all rest, to be really skewed, so that it might be interesting to introduce bias to SVC). Thus I think you might want to try some other classification algorithm since looks like your choice is not suitable for this task. Or maybe you need to use different kernel with your SVC (I assume you use linear kernel, try something different - RBF or polynomial maybe).</p>
"
4140027;3985;tumbleweed;<machine-learning><nlp><artificial-intelligence><scikit-learn><svm>;28470165;5;How to plot SVC classification for an unbalanced dataset with scikit-learn and matplotlib?;"<p>I have a text classification task with 2599 documents and five labels from 1 to 5. The documents are</p>

<pre><code>label | texts
----------
5     |1190
4     |839
3     |239
1     |204
2     |127
</code></pre>

<p>All ready classified this textual data with very low performance, and also get warnings about ill defined metrics:</p>

<pre><code>Accuracy: 0.461057692308

score: 0.461057692308

precision: 0.212574195636

recall: 0.461057692308

  'precision', 'predicted', average, warn_for)
 confussion matrix:
[[  0   0   0   0 153]
  'precision', 'predicted', average, warn_for)
 [  0   0   0   0  94]
 [  0   0   0   0 194]
 [  0   0   0   0 680]
 [  0   0   0   0 959]]

 clasification report:
             precision    recall  f1-score   support

          1       0.00      0.00      0.00       153
          2       0.00      0.00      0.00        94
          3       0.00      0.00      0.00       194
          4       0.00      0.00      0.00       680
          5       0.46      1.00      0.63       959

avg / total       0.21      0.46      0.29      2080
</code></pre>

<p>Clearly this is happening by the fact that I have an unbalanced dataset, so I found this <a href=""http://sci2s.ugr.es/keel/pdf/specific/congreso/akbani_svm_2004.pdf"" rel=""nofollow noreferrer"">paper</a> where the authors propose several aproaches to deal with this issue:</p>

<blockquote>
  <p>The problem is that with imbalanced datasets, the learned boundary is
  too close to the positive instances. We need to bias SVM in a way that
  will push the boundary away from the positive instances. Veropoulos et
  al [14] suggest using different error costs for the positive (C +) and
  negative (C - ) classes</p>
</blockquote>

<p>I know that this could be very complicated but SVC offers several hyper parameters, So my question is: Is there any way to bias SVC in a way that push the boundary away from possitive instances with the hyper parameters that offer SVC classifier?. I know that this could be a difficult problem but any help is welcome, thanks in advance guys.</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
tfidf_vect= TfidfVectorizer(use_idf=True, smooth_idf=True, sublinear_tf=False, ngram_range=(2,2))
from sklearn.cross_validation import train_test_split, cross_val_score

import pandas as pd
df = pd.read_csv('/path/of/the/file.csv',
                     header=0, sep=',', names=['id', 'text', 'label'])



reduced_data = tfidf_vect.fit_transform(df['text'].values)
y = df['label'].values



from sklearn.decomposition.truncated_svd import TruncatedSVD
svd = TruncatedSVD(n_components=5)
reduced_data = svd.fit_transform(reduced_data)

from sklearn import cross_validation
X_train, X_test, y_train, y_test = cross_validation.train_test_split(reduced_data,
                                                    y, test_size=0.33)

#with no weights:

from sklearn.svm import SVC
clf = SVC(kernel='linear', class_weight={1: 10})
clf.fit(reduced_data, y)
prediction = clf.predict(X_test)

w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-5, 5)
yy = a * xx - clf.intercept_[0] / w[1]


# get the separating hyperplane using weighted classes
wclf = SVC(kernel='linear', class_weight={1: 10})
wclf.fit(reduced_data, y)

ww = wclf.coef_[0]
wa = -ww[0] / ww[1]
wyy = wa * xx - wclf.intercept_[0] / ww[1]

# plot separating hyperplanes and samples
import matplotlib.pyplot as plt
h0 = plt.plot(xx, yy, 'k-', label='no weights')
h1 = plt.plot(xx, wyy, 'k--', label='with weights')
plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=y, cmap=plt.cm.Paired)
plt.legend()

plt.axis('tight')
plt.show()
</code></pre>

<p>But I get nothing and I cant understand what happened, this is the plot:</p>

<p><img src=""https://i.stack.imgur.com/9eEqX.png"" alt=""weighted vs normal""></p>

<p>then:</p>

<pre><code>#Let's show some metrics[unweighted]:
from sklearn.metrics.metrics import precision_score, \
    recall_score, confusion_matrix, classification_report, accuracy_score
print '\nAccuracy:', accuracy_score(y_test, prediction)
print '\nscore:', clf.score(X_train, y_train)
print '\nrecall:', recall_score(y_test, prediction)
print '\nprecision:', precision_score(y_test, prediction)
print '\n clasification report:\n', classification_report(y_test, prediction)
print '\n confussion matrix:\n',confusion_matrix(y_test, prediction)

#Let's show some metrics[weighted]:
print 'weighted:\n'

from sklearn.metrics.metrics import precision_score, \
    recall_score, confusion_matrix, classification_report, accuracy_score
print '\nAccuracy:', accuracy_score(y_test, prediction)
print '\nscore:', wclf.score(X_train, y_train)
print '\nrecall:', recall_score(y_test, prediction)
print '\nprecision:', precision_score(y_test, prediction)
print '\n clasification report:\n', classification_report(y_test, prediction)
print '\n confussion matrix:\n',confusion_matrix(y_test, prediction)
</code></pre>

<p>This is the <a href=""http://pastebin.com/0cwiLGG2"" rel=""nofollow noreferrer"">data</a> that Im using. How can I fix this and plot in a right way this problem?. thanks in advance guys!.</p>

<p>From an answer in this question I remove this lines:</p>

<pre><code>#
# from sklearn.decomposition.truncated_svd import TruncatedSVD
# svd = TruncatedSVD(n_components=5)
# reduced_data = svd.fit_transform(reduced_data)


#
# w = clf.coef_[0]
# a = -w[0] / w[1]
# xx = np.linspace(-10, 10)
# yy = a * xx - clf.intercept_[0] / w[1]

# ww = wclf.coef_[0]
# wa = -ww[0] / ww[1]
# wyy = wa * xx - wclf.intercept_[0] / ww[1]
#
# # plot separating hyperplanes and samples
# import matplotlib.pyplot as plt
# h0 = plt.plot(xx, yy, 'k-', label='no weights')
# h1 = plt.plot(xx, wyy, 'k--', label='with weights')
# plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=y, cmap=plt.cm.Paired)
# plt.legend()
#
# plt.axis('tight')
# plt.show()

This where the results:

Accuracy: 0.787878787879

score: 0.779437105112

recall: 0.787878787879

precision: 0.827705441238
</code></pre>

<p>This metrics improved. <strong>How can I plot this results in order to have a nice example like the documentation one. I would like to see the behavior of the two hyper planes?</strong>. Thanks guys!</p>
";;4370183;371;klubow;1;28485984;"<p>You may want to check class_weight parameter (<a href=""http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"" rel=""nofollow"">http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html</a>) for SVM classifier 
or 
balance your data (<a href=""https://github.com/fmfn/UnbalancedDataset/blob/master/UnbalancedDataset.py"" rel=""nofollow"">https://github.com/fmfn/UnbalancedDataset/blob/master/UnbalancedDataset.py</a>)</p>
"
4140027;3985;tumbleweed;<machine-learning><nlp><artificial-intelligence><scikit-learn><svm>;28470165;5;How to plot SVC classification for an unbalanced dataset with scikit-learn and matplotlib?;"<p>I have a text classification task with 2599 documents and five labels from 1 to 5. The documents are</p>

<pre><code>label | texts
----------
5     |1190
4     |839
3     |239
1     |204
2     |127
</code></pre>

<p>All ready classified this textual data with very low performance, and also get warnings about ill defined metrics:</p>

<pre><code>Accuracy: 0.461057692308

score: 0.461057692308

precision: 0.212574195636

recall: 0.461057692308

  'precision', 'predicted', average, warn_for)
 confussion matrix:
[[  0   0   0   0 153]
  'precision', 'predicted', average, warn_for)
 [  0   0   0   0  94]
 [  0   0   0   0 194]
 [  0   0   0   0 680]
 [  0   0   0   0 959]]

 clasification report:
             precision    recall  f1-score   support

          1       0.00      0.00      0.00       153
          2       0.00      0.00      0.00        94
          3       0.00      0.00      0.00       194
          4       0.00      0.00      0.00       680
          5       0.46      1.00      0.63       959

avg / total       0.21      0.46      0.29      2080
</code></pre>

<p>Clearly this is happening by the fact that I have an unbalanced dataset, so I found this <a href=""http://sci2s.ugr.es/keel/pdf/specific/congreso/akbani_svm_2004.pdf"" rel=""nofollow noreferrer"">paper</a> where the authors propose several aproaches to deal with this issue:</p>

<blockquote>
  <p>The problem is that with imbalanced datasets, the learned boundary is
  too close to the positive instances. We need to bias SVM in a way that
  will push the boundary away from the positive instances. Veropoulos et
  al [14] suggest using different error costs for the positive (C +) and
  negative (C - ) classes</p>
</blockquote>

<p>I know that this could be very complicated but SVC offers several hyper parameters, So my question is: Is there any way to bias SVC in a way that push the boundary away from possitive instances with the hyper parameters that offer SVC classifier?. I know that this could be a difficult problem but any help is welcome, thanks in advance guys.</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
tfidf_vect= TfidfVectorizer(use_idf=True, smooth_idf=True, sublinear_tf=False, ngram_range=(2,2))
from sklearn.cross_validation import train_test_split, cross_val_score

import pandas as pd
df = pd.read_csv('/path/of/the/file.csv',
                     header=0, sep=',', names=['id', 'text', 'label'])



reduced_data = tfidf_vect.fit_transform(df['text'].values)
y = df['label'].values



from sklearn.decomposition.truncated_svd import TruncatedSVD
svd = TruncatedSVD(n_components=5)
reduced_data = svd.fit_transform(reduced_data)

from sklearn import cross_validation
X_train, X_test, y_train, y_test = cross_validation.train_test_split(reduced_data,
                                                    y, test_size=0.33)

#with no weights:

from sklearn.svm import SVC
clf = SVC(kernel='linear', class_weight={1: 10})
clf.fit(reduced_data, y)
prediction = clf.predict(X_test)

w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-5, 5)
yy = a * xx - clf.intercept_[0] / w[1]


# get the separating hyperplane using weighted classes
wclf = SVC(kernel='linear', class_weight={1: 10})
wclf.fit(reduced_data, y)

ww = wclf.coef_[0]
wa = -ww[0] / ww[1]
wyy = wa * xx - wclf.intercept_[0] / ww[1]

# plot separating hyperplanes and samples
import matplotlib.pyplot as plt
h0 = plt.plot(xx, yy, 'k-', label='no weights')
h1 = plt.plot(xx, wyy, 'k--', label='with weights')
plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=y, cmap=plt.cm.Paired)
plt.legend()

plt.axis('tight')
plt.show()
</code></pre>

<p>But I get nothing and I cant understand what happened, this is the plot:</p>

<p><img src=""https://i.stack.imgur.com/9eEqX.png"" alt=""weighted vs normal""></p>

<p>then:</p>

<pre><code>#Let's show some metrics[unweighted]:
from sklearn.metrics.metrics import precision_score, \
    recall_score, confusion_matrix, classification_report, accuracy_score
print '\nAccuracy:', accuracy_score(y_test, prediction)
print '\nscore:', clf.score(X_train, y_train)
print '\nrecall:', recall_score(y_test, prediction)
print '\nprecision:', precision_score(y_test, prediction)
print '\n clasification report:\n', classification_report(y_test, prediction)
print '\n confussion matrix:\n',confusion_matrix(y_test, prediction)

#Let's show some metrics[weighted]:
print 'weighted:\n'

from sklearn.metrics.metrics import precision_score, \
    recall_score, confusion_matrix, classification_report, accuracy_score
print '\nAccuracy:', accuracy_score(y_test, prediction)
print '\nscore:', wclf.score(X_train, y_train)
print '\nrecall:', recall_score(y_test, prediction)
print '\nprecision:', precision_score(y_test, prediction)
print '\n clasification report:\n', classification_report(y_test, prediction)
print '\n confussion matrix:\n',confusion_matrix(y_test, prediction)
</code></pre>

<p>This is the <a href=""http://pastebin.com/0cwiLGG2"" rel=""nofollow noreferrer"">data</a> that Im using. How can I fix this and plot in a right way this problem?. thanks in advance guys!.</p>

<p>From an answer in this question I remove this lines:</p>

<pre><code>#
# from sklearn.decomposition.truncated_svd import TruncatedSVD
# svd = TruncatedSVD(n_components=5)
# reduced_data = svd.fit_transform(reduced_data)


#
# w = clf.coef_[0]
# a = -w[0] / w[1]
# xx = np.linspace(-10, 10)
# yy = a * xx - clf.intercept_[0] / w[1]

# ww = wclf.coef_[0]
# wa = -ww[0] / ww[1]
# wyy = wa * xx - wclf.intercept_[0] / ww[1]
#
# # plot separating hyperplanes and samples
# import matplotlib.pyplot as plt
# h0 = plt.plot(xx, yy, 'k-', label='no weights')
# h1 = plt.plot(xx, wyy, 'k--', label='with weights')
# plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=y, cmap=plt.cm.Paired)
# plt.legend()
#
# plt.axis('tight')
# plt.show()

This where the results:

Accuracy: 0.787878787879

score: 0.779437105112

recall: 0.787878787879

precision: 0.827705441238
</code></pre>

<p>This metrics improved. <strong>How can I plot this results in order to have a nice example like the documentation one. I would like to see the behavior of the two hyper planes?</strong>. Thanks guys!</p>
";;648896;11152;erogol;2;28502074;"<p>As a simple solution, just multiply instances in the smaller classes and balance the number of instances. This works even it seems stupid and it does not require in the rig configuration.</p>

<p>The idea of using this approach is to mimic the behaviour of scaled learning rate for each class regarding its class size. That is, in a gradient based optimization methods, you should scale the learning rate inversely proportional to the class sizes for each class so that you can prevent the model to overlearn some classes against the others. </p>

<p>If your problem is pretty big and you are using batch updates, then instead of booking all the dataset and counting classes, consider only the mini-batch and tune learning rates dynamically regarding number of instances for each class in the mini-batch. </p>

<p>That means if your master-learning rate is 0.01 and in a batch of instances you have 0.4 of them class A and 0.6 of them class B then for each class you need to tune the final learning rate as master_learning rate for class A (that means keep it same), 2/3*master_learning rate for class B. Hence you step wider for class A and reversely for class B.</p>

<p>My choice to go, especially for large problems and augmenting the data for smaller classes by replicating instances or as a more robust choice, adding some noises and variances to replicated instances. In such way, (depending on your problem) you can also train a model which is more robust to small changes (this is very common for especially image classification problems.).</p>
"
4140027;3985;tumbleweed;<machine-learning><nlp><artificial-intelligence><scikit-learn><svm>;28470165;5;How to plot SVC classification for an unbalanced dataset with scikit-learn and matplotlib?;"<p>I have a text classification task with 2599 documents and five labels from 1 to 5. The documents are</p>

<pre><code>label | texts
----------
5     |1190
4     |839
3     |239
1     |204
2     |127
</code></pre>

<p>All ready classified this textual data with very low performance, and also get warnings about ill defined metrics:</p>

<pre><code>Accuracy: 0.461057692308

score: 0.461057692308

precision: 0.212574195636

recall: 0.461057692308

  'precision', 'predicted', average, warn_for)
 confussion matrix:
[[  0   0   0   0 153]
  'precision', 'predicted', average, warn_for)
 [  0   0   0   0  94]
 [  0   0   0   0 194]
 [  0   0   0   0 680]
 [  0   0   0   0 959]]

 clasification report:
             precision    recall  f1-score   support

          1       0.00      0.00      0.00       153
          2       0.00      0.00      0.00        94
          3       0.00      0.00      0.00       194
          4       0.00      0.00      0.00       680
          5       0.46      1.00      0.63       959

avg / total       0.21      0.46      0.29      2080
</code></pre>

<p>Clearly this is happening by the fact that I have an unbalanced dataset, so I found this <a href=""http://sci2s.ugr.es/keel/pdf/specific/congreso/akbani_svm_2004.pdf"" rel=""nofollow noreferrer"">paper</a> where the authors propose several aproaches to deal with this issue:</p>

<blockquote>
  <p>The problem is that with imbalanced datasets, the learned boundary is
  too close to the positive instances. We need to bias SVM in a way that
  will push the boundary away from the positive instances. Veropoulos et
  al [14] suggest using different error costs for the positive (C +) and
  negative (C - ) classes</p>
</blockquote>

<p>I know that this could be very complicated but SVC offers several hyper parameters, So my question is: Is there any way to bias SVC in a way that push the boundary away from possitive instances with the hyper parameters that offer SVC classifier?. I know that this could be a difficult problem but any help is welcome, thanks in advance guys.</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
tfidf_vect= TfidfVectorizer(use_idf=True, smooth_idf=True, sublinear_tf=False, ngram_range=(2,2))
from sklearn.cross_validation import train_test_split, cross_val_score

import pandas as pd
df = pd.read_csv('/path/of/the/file.csv',
                     header=0, sep=',', names=['id', 'text', 'label'])



reduced_data = tfidf_vect.fit_transform(df['text'].values)
y = df['label'].values



from sklearn.decomposition.truncated_svd import TruncatedSVD
svd = TruncatedSVD(n_components=5)
reduced_data = svd.fit_transform(reduced_data)

from sklearn import cross_validation
X_train, X_test, y_train, y_test = cross_validation.train_test_split(reduced_data,
                                                    y, test_size=0.33)

#with no weights:

from sklearn.svm import SVC
clf = SVC(kernel='linear', class_weight={1: 10})
clf.fit(reduced_data, y)
prediction = clf.predict(X_test)

w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-5, 5)
yy = a * xx - clf.intercept_[0] / w[1]


# get the separating hyperplane using weighted classes
wclf = SVC(kernel='linear', class_weight={1: 10})
wclf.fit(reduced_data, y)

ww = wclf.coef_[0]
wa = -ww[0] / ww[1]
wyy = wa * xx - wclf.intercept_[0] / ww[1]

# plot separating hyperplanes and samples
import matplotlib.pyplot as plt
h0 = plt.plot(xx, yy, 'k-', label='no weights')
h1 = plt.plot(xx, wyy, 'k--', label='with weights')
plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=y, cmap=plt.cm.Paired)
plt.legend()

plt.axis('tight')
plt.show()
</code></pre>

<p>But I get nothing and I cant understand what happened, this is the plot:</p>

<p><img src=""https://i.stack.imgur.com/9eEqX.png"" alt=""weighted vs normal""></p>

<p>then:</p>

<pre><code>#Let's show some metrics[unweighted]:
from sklearn.metrics.metrics import precision_score, \
    recall_score, confusion_matrix, classification_report, accuracy_score
print '\nAccuracy:', accuracy_score(y_test, prediction)
print '\nscore:', clf.score(X_train, y_train)
print '\nrecall:', recall_score(y_test, prediction)
print '\nprecision:', precision_score(y_test, prediction)
print '\n clasification report:\n', classification_report(y_test, prediction)
print '\n confussion matrix:\n',confusion_matrix(y_test, prediction)

#Let's show some metrics[weighted]:
print 'weighted:\n'

from sklearn.metrics.metrics import precision_score, \
    recall_score, confusion_matrix, classification_report, accuracy_score
print '\nAccuracy:', accuracy_score(y_test, prediction)
print '\nscore:', wclf.score(X_train, y_train)
print '\nrecall:', recall_score(y_test, prediction)
print '\nprecision:', precision_score(y_test, prediction)
print '\n clasification report:\n', classification_report(y_test, prediction)
print '\n confussion matrix:\n',confusion_matrix(y_test, prediction)
</code></pre>

<p>This is the <a href=""http://pastebin.com/0cwiLGG2"" rel=""nofollow noreferrer"">data</a> that Im using. How can I fix this and plot in a right way this problem?. thanks in advance guys!.</p>

<p>From an answer in this question I remove this lines:</p>

<pre><code>#
# from sklearn.decomposition.truncated_svd import TruncatedSVD
# svd = TruncatedSVD(n_components=5)
# reduced_data = svd.fit_transform(reduced_data)


#
# w = clf.coef_[0]
# a = -w[0] / w[1]
# xx = np.linspace(-10, 10)
# yy = a * xx - clf.intercept_[0] / w[1]

# ww = wclf.coef_[0]
# wa = -ww[0] / ww[1]
# wyy = wa * xx - wclf.intercept_[0] / ww[1]
#
# # plot separating hyperplanes and samples
# import matplotlib.pyplot as plt
# h0 = plt.plot(xx, yy, 'k-', label='no weights')
# h1 = plt.plot(xx, wyy, 'k--', label='with weights')
# plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=y, cmap=plt.cm.Paired)
# plt.legend()
#
# plt.axis('tight')
# plt.show()

This where the results:

Accuracy: 0.787878787879

score: 0.779437105112

recall: 0.787878787879

precision: 0.827705441238
</code></pre>

<p>This metrics improved. <strong>How can I plot this results in order to have a nice example like the documentation one. I would like to see the behavior of the two hyper planes?</strong>. Thanks guys!</p>
";;270287;41194;IVlad;4;28524806;"<p>By reducing your data to <code>5</code> features with the <code>SVD</code>:</p>

<pre><code>svd = TruncatedSVD(n_components=5)
reduced_data = svd.fit_transform(reduced_data)
</code></pre>

<p>You lose a lot of information. Just by removing those lines I get <code>78%</code> accuracy.</p>

<p>Leaving the <code>class_weight</code> parameter as you set it seems to do better than removing it. I haven't tried giving it other values.</p>

<p>Look into using <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html"" rel=""nofollow"">k-fold cross validation</a> and <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html"" rel=""nofollow"">grid search</a> to tune the parameters of your model. You can also use a <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"" rel=""nofollow"">pipeline</a> if you want to reduce the dimensionality of your data, in order to figure out how much you want to reduce it without affecting performance. <a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#parameter-tuning-using-grid-search"" rel=""nofollow"">Here</a> is an example that shows how to tune your entire pipeline using grid search.</p>

<p>As for plotting, you can only plot 2d or 3d data. After you train using more dimensions, you can reduce your data to 2 or 3 dimensions and plot that. See <a href=""http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html"" rel=""nofollow"">here</a> for a plotting example. The code looks similar to what you're plotting and I got similar results to yours. The problem is that your data has many features and you can only plot things to a 2d or 3d surface. That will usually make it look weird and hard to tell what is going on.</p>

<p>I suggest you don't bother with plotting as it's not going to tell you much for data in high dimensions. Use k-fold cross validation with a grid search in order to get the best parameters and if you want to look into overfitting closer, plot <a href=""http://scikit-learn.org/stable/auto_examples/plot_learning_curve.html"" rel=""nofollow"">learning curves</a> instead.</p>

<p>All this combined will tell you a lot more about the behavior of your model than plotting the hyperplane.</p>
"
4140027;3985;tumbleweed;<machine-learning><nlp><artificial-intelligence><scikit-learn><svm>;28470165;5;How to plot SVC classification for an unbalanced dataset with scikit-learn and matplotlib?;"<p>I have a text classification task with 2599 documents and five labels from 1 to 5. The documents are</p>

<pre><code>label | texts
----------
5     |1190
4     |839
3     |239
1     |204
2     |127
</code></pre>

<p>All ready classified this textual data with very low performance, and also get warnings about ill defined metrics:</p>

<pre><code>Accuracy: 0.461057692308

score: 0.461057692308

precision: 0.212574195636

recall: 0.461057692308

  'precision', 'predicted', average, warn_for)
 confussion matrix:
[[  0   0   0   0 153]
  'precision', 'predicted', average, warn_for)
 [  0   0   0   0  94]
 [  0   0   0   0 194]
 [  0   0   0   0 680]
 [  0   0   0   0 959]]

 clasification report:
             precision    recall  f1-score   support

          1       0.00      0.00      0.00       153
          2       0.00      0.00      0.00        94
          3       0.00      0.00      0.00       194
          4       0.00      0.00      0.00       680
          5       0.46      1.00      0.63       959

avg / total       0.21      0.46      0.29      2080
</code></pre>

<p>Clearly this is happening by the fact that I have an unbalanced dataset, so I found this <a href=""http://sci2s.ugr.es/keel/pdf/specific/congreso/akbani_svm_2004.pdf"" rel=""nofollow noreferrer"">paper</a> where the authors propose several aproaches to deal with this issue:</p>

<blockquote>
  <p>The problem is that with imbalanced datasets, the learned boundary is
  too close to the positive instances. We need to bias SVM in a way that
  will push the boundary away from the positive instances. Veropoulos et
  al [14] suggest using different error costs for the positive (C +) and
  negative (C - ) classes</p>
</blockquote>

<p>I know that this could be very complicated but SVC offers several hyper parameters, So my question is: Is there any way to bias SVC in a way that push the boundary away from possitive instances with the hyper parameters that offer SVC classifier?. I know that this could be a difficult problem but any help is welcome, thanks in advance guys.</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
tfidf_vect= TfidfVectorizer(use_idf=True, smooth_idf=True, sublinear_tf=False, ngram_range=(2,2))
from sklearn.cross_validation import train_test_split, cross_val_score

import pandas as pd
df = pd.read_csv('/path/of/the/file.csv',
                     header=0, sep=',', names=['id', 'text', 'label'])



reduced_data = tfidf_vect.fit_transform(df['text'].values)
y = df['label'].values



from sklearn.decomposition.truncated_svd import TruncatedSVD
svd = TruncatedSVD(n_components=5)
reduced_data = svd.fit_transform(reduced_data)

from sklearn import cross_validation
X_train, X_test, y_train, y_test = cross_validation.train_test_split(reduced_data,
                                                    y, test_size=0.33)

#with no weights:

from sklearn.svm import SVC
clf = SVC(kernel='linear', class_weight={1: 10})
clf.fit(reduced_data, y)
prediction = clf.predict(X_test)

w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-5, 5)
yy = a * xx - clf.intercept_[0] / w[1]


# get the separating hyperplane using weighted classes
wclf = SVC(kernel='linear', class_weight={1: 10})
wclf.fit(reduced_data, y)

ww = wclf.coef_[0]
wa = -ww[0] / ww[1]
wyy = wa * xx - wclf.intercept_[0] / ww[1]

# plot separating hyperplanes and samples
import matplotlib.pyplot as plt
h0 = plt.plot(xx, yy, 'k-', label='no weights')
h1 = plt.plot(xx, wyy, 'k--', label='with weights')
plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=y, cmap=plt.cm.Paired)
plt.legend()

plt.axis('tight')
plt.show()
</code></pre>

<p>But I get nothing and I cant understand what happened, this is the plot:</p>

<p><img src=""https://i.stack.imgur.com/9eEqX.png"" alt=""weighted vs normal""></p>

<p>then:</p>

<pre><code>#Let's show some metrics[unweighted]:
from sklearn.metrics.metrics import precision_score, \
    recall_score, confusion_matrix, classification_report, accuracy_score
print '\nAccuracy:', accuracy_score(y_test, prediction)
print '\nscore:', clf.score(X_train, y_train)
print '\nrecall:', recall_score(y_test, prediction)
print '\nprecision:', precision_score(y_test, prediction)
print '\n clasification report:\n', classification_report(y_test, prediction)
print '\n confussion matrix:\n',confusion_matrix(y_test, prediction)

#Let's show some metrics[weighted]:
print 'weighted:\n'

from sklearn.metrics.metrics import precision_score, \
    recall_score, confusion_matrix, classification_report, accuracy_score
print '\nAccuracy:', accuracy_score(y_test, prediction)
print '\nscore:', wclf.score(X_train, y_train)
print '\nrecall:', recall_score(y_test, prediction)
print '\nprecision:', precision_score(y_test, prediction)
print '\n clasification report:\n', classification_report(y_test, prediction)
print '\n confussion matrix:\n',confusion_matrix(y_test, prediction)
</code></pre>

<p>This is the <a href=""http://pastebin.com/0cwiLGG2"" rel=""nofollow noreferrer"">data</a> that Im using. How can I fix this and plot in a right way this problem?. thanks in advance guys!.</p>

<p>From an answer in this question I remove this lines:</p>

<pre><code>#
# from sklearn.decomposition.truncated_svd import TruncatedSVD
# svd = TruncatedSVD(n_components=5)
# reduced_data = svd.fit_transform(reduced_data)


#
# w = clf.coef_[0]
# a = -w[0] / w[1]
# xx = np.linspace(-10, 10)
# yy = a * xx - clf.intercept_[0] / w[1]

# ww = wclf.coef_[0]
# wa = -ww[0] / ww[1]
# wyy = wa * xx - wclf.intercept_[0] / ww[1]
#
# # plot separating hyperplanes and samples
# import matplotlib.pyplot as plt
# h0 = plt.plot(xx, yy, 'k-', label='no weights')
# h1 = plt.plot(xx, wyy, 'k--', label='with weights')
# plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=y, cmap=plt.cm.Paired)
# plt.legend()
#
# plt.axis('tight')
# plt.show()

This where the results:

Accuracy: 0.787878787879

score: 0.779437105112

recall: 0.787878787879

precision: 0.827705441238
</code></pre>

<p>This metrics improved. <strong>How can I plot this results in order to have a nice example like the documentation one. I would like to see the behavior of the two hyper planes?</strong>. Thanks guys!</p>
";;4588780;4531;Nikita Astrakhantsev;2;28638543;"<p>You probably already tried to set <code>class-weight</code> to <code>auto</code>, but I'd like to check for certain.</p>

<p>Maybe experiments with balancing (<a href=""http://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis"" rel=""nofollow"">oversampling or undersampling</a>) can help, some lib for it is already advised by klubow.</p>
"
3243366;75;user3243366;<machine-learning><svm><libsvm><gate><svmlight>;28470820;0;error loading svm learning configuration file in gate tool;"<p>I am new to machine learning. When I tried learning through gate, it is showing some error. The learning configuration file is given below.</p>

<pre><code>&lt;?xml version=""1.0""?&gt;  
&lt;ML-CONFIG&gt;

&lt;SURROUND value=""false""/&gt; 

 &lt;FILTERING ratio='0.2' dis='far'/&gt;

&lt;EVALUATION method=""holdout"" runs=""2"" ratio=""0.66""/&gt;

&lt;multiClassiï¬cation2Binary method=""one-vs-anothers"" thread-pool-size=""2""/&gt;

 &lt;PARAMETER name=""thresholdProbabilityBoundary"" value=""1.0""/&gt; 

&lt;PARAMETER name=""thresholdProbabilityEntity"" value=""1.0""/&gt; 

&lt;PARAMETER name=""thresholdProbabilityClassification"" value=""0.0""/&gt;

&lt;IS-LABEL-UPDATABLE value=""true""/&gt; 

&lt;IS-NLPFEATURELIST-UPDATABLE value=""true""/&gt; 

 &lt;ENGINE nickname=""SVM"" implementationName=""SVMLibSvmJava"" options = ""-s 0 -t 1 -d 4 -c 5 -tau 1.2""/&gt;


&lt;/ML-CONFIG&gt; 
</code></pre>

<p>Training attributes are inside this file. When I trained without the line containing <em>multiclass</em> xml tag it is working. when this line is added, an error is showing  like below</p>

<pre><code>Caused by: org.jdom.input.JDOMParseException: Error on line 6 of document file:/home/cognicor/vagateplugin/scripts/ML_script/learningsvm: Element type ""multiClassi"" must be followed by either attribute specifications, ""&gt;"" or ""/&gt;"".
</code></pre>

<p>I am not aware of this thing and why it happens and seek for a solution. </p>
";28527062;1857897;6711;dedek;0;28527062;"<p>The problem is in the <code>multiClassiï¬cation2Binary</code> string. There is a single glyph <code>ï¬</code> that contains two joined characters ""fi"" together. You probably copied the text from some pdf... Simply replace <code>ï¬</code> by <code>fi</code> and the error should go away.</p>
"
119624;12870;Rebecca;<c#><regex><machine-learning><fuzzy-search>;28479212;0;Fuzzy pattern matching from emails in C#;"<p>I'm looking for a way to extract bits of data from emails. I'm primarily looking at subject lines and the email body, and extracting customer and order reference numbers. </p>

<p>Imagine I'm a company where customers can email an info@mydomain.com and they might add a specific customer number or order reference in the subject line or body of the email. However, they might not always provide these references in the optimal format. I want to extract the data out, and return a probability of how likely the data is valid.</p>

<p>Is there some kind of technique I can use to attempt to scan an email and return a probable customer number and or order reference with a degree of probability (a bit like Bayesian spam filtering)?</p>

<p>I was considering some kind of regular expression engine, but that seemed too rigid. I was also looking at <a href=""https://github.com/sethjuarez/numl"" rel=""nofollow"">NUML.net</a> and wondering if it could help me, but I'm a little out of my depth, since I'm not entirely sure what I need. I've come across the Levenshtein algorithm, but that seems to be matching two fixed strings, rather than a fixed string and a pattern.</p>

<p>I'm imagining an API that look a little like this:</p>

<pre><code>// emailMessage is a Mandrill inbound object, in case anybody wonders
EmailScanResult results = EmailScanner.Scan(emailMessage, new {ScanType.CustomerNo, ScanType.OrderReference});
foreach (var result in results)
{
    var scanType = result.Type; // I.e. ScanType.CustomerNo
    var score = result.Score; // e.g. 1.2
    var value = result.Value; // CU-233454345-2321
}
</code></pre>

<p>Possible inputs for this are varied; E.g. For the same customer number:</p>

<ul>
<li>DF-232322-AB2323</li>
<li>df-232322-AB2323</li>
<li>232322-ab2323</li>
<li>232322AB2323</li>
</ul>

<p>What kinds of algorithms would be useful for such a task? Are there any recommended .NET libraries for this, and do you know of any appropriate examples?</p>
";;1996872;96;David Belmont;0;28482932;"<p>If I got it right, you could use a regular expression with no problem. For example, with the input samples you gave, you could use a regex like:</p>

<blockquote>
  <p>([A-Z|a-z]{2,2}-){0,1}\d{6,6}-{0,1}\d{4,4}</p>
</blockquote>

<ul>
<li>The first part gets the <em>DF-</em> or <em>df-</em>, which may or may not occur: <strong>([A-Z|a-z]{2,2}-){0,1}</strong></li>
<li>The second part gets the first group of digits: <strong>\d{6,6}</strong></li>
<li>Then, we say that it could have a dash: <strong>\-{0,1}</strong></li>
<li>Finally, we get the last group of digits: <strong>\d{4,4}</strong></li>
</ul>

<p>This would cover the values you provided as sample, but you also could write other expressions to fetch other values.</p>

<p>Or, maybe, you could use something like <a href=""http://lucenenet.apache.org/"" rel=""nofollow"">Lucene.net</a>. From what I know, this could help you too.<br />
<a href=""http://pt.slideshare.net/nitin_stephens/lucene-basics"" rel=""nofollow"">http://pt.slideshare.net/nitin_stephens/lucene-basics</a> <br />
<a href=""http://jsprunger.com/getting-started-with-lucene-net/"" rel=""nofollow"">http://jsprunger.com/getting-started-with-lucene-net/</a></p>
"
3140106;295;user3140106;<python><numpy><machine-learning><scikit-learn><feature-extraction>;28482943;2;How to use feature hasher to convert non-numerical discrete data so that it can be passed to SVM?;"<p>I am trying to use the CRX dataset from the UCI Machine Learning repository. This particular dataset contains some features which are not continuous variables. Therefore I need to convert them into numerical values before they can be passed to an SVM.</p>

<p>I initially looked into using the one-hot decoder, which takes integer values and converts them into matrices (e.g. if a feature has three possible values, 'red' 'blue' and 'green', this would be converted into three binary features: 1,0,0 for 'red', '0,1,0 for 'blue' and 0,0,1 for 'green'. This would be ideal for my needs, except for the fact that it only can deal with integer features.</p>

<pre><code>def get_crx_data(debug=False):

    with open(""/Volumes/LocalDataHD/jt306/crx.data"", ""rU"") as infile:
        features_array = []
        reader = csv.reader(infile,dialect=csv.excel_tab)
        for row in reader:
            features_array.append(str(row).translate(None,""[]'"").split("",""))
        features_array = np.array(features_array)
        print features_array.shape
        print features_array[0]
        labels_array = features_array[:,15]
        features_array = features_array[:,:15]
        print features_array.shape
        print labels_array.shape


        print(""FeatureHasher on frequency dicts"")

        hasher = FeatureHasher(n_features=44)
        X = hasher.fit_transform(line for line in features_array)

        print X.shape



get_crx_data()
</code></pre>

<p>This returns </p>

<pre><code>Reading CRX data from disk
Traceback (most recent call last):
  File""/Volumes/LocalDataHD/PycharmProjects/FeatureSelectionPython278/Crx2.py"", line 38, in &lt;module&gt;

get_crx_data()
  File ""/Volumes/LocalDataHD/PycharmProjects/FeatureSelectionPython278/Crx2.py"", line 32, in get_crx_data

X = hasher.fit_transform(line for line in features_array)

File ""/Volumes/LocalDataHD/anaconda/lib/python2.7/site-packages/sklearn/base.py"", line 426, in fit_transform
    return self.fit(X, **fit_params).transform(X)

File ""/Volumes/LocalDataHD/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py"", line 129, in transform
    _hashing.transform(raw_X, self.n_features, self.dtype)

File ""_hashing.pyx"", line 44, in sklearn.feature_extraction._hashing.transform (sklearn/feature_extraction/_hashing.c:1649)

File ""/Volumes/LocalDataHD/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py"", line 125, in &lt;genexpr&gt;
    raw_X = (_iteritems(d) for d in raw_X)

File ""/Volumes/LocalDataHD/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.py"", line 15, in _iteritems
    return d.iteritems() if hasattr(d, ""iteritems"") else d.items()

AttributeError: 'numpy.ndarray' object has no attribute 'items'

(690, 16)
['0' ' 30.83' ' 0' ' u' ' g' ' w' ' v' ' 1.25' ' 1' ' 1' ' 1' ' 0' ' g'
 ' 202' ' 0' ' +']
(690, 15)
(690,)
FeatureHasher on frequency dicts

Process finished with exit code 1


How can I use feature hashing (or an alternative method) to convert this data from classes (some of which are strings, others are discrete numerical values) into data which can be handled by an SVM? I have also looked into using one-hot coding, but that only takes integers as input.
</code></pre>
";28484684;577088;124140;senderle;6;28484684;"<p>The issue is that the <code>FeatureHasher</code> object expects each row of input to have a particular structure -- or really, one of three different <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html"" rel=""noreferrer"">possible structures</a>. The first possibility is a dictionary of <code>feature_name:value</code> pairs. The second is a list of <code>(feature_name, value)</code> tuples. And the third is a flat list of <code>feature_name</code>s. In the first two cases, the feature names are mapped to columns in the matrix, and given values are stored at those columns for each row. In the last, the presence or absence of a feature in the list is implicitly understood as a <code>True</code> or <code>False</code> value. Here are some simple, concrete examples:</p>

<pre><code>&gt;&gt;&gt; hasher = sklearn.feature_extraction.FeatureHasher(n_features=10,
...                                                   non_negative=True,
...                                                   input_type='dict')
&gt;&gt;&gt; X_new = hasher.fit_transform([{'a':1, 'b':2}, {'a':0, 'c':5}])
&gt;&gt;&gt; X_new.toarray()
array([[ 1.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.,  0.,  0.]])
</code></pre>

<p>This illustrates the default mode -- what the <code>FeatureHasher</code> will expect if you don't pass <code>input_type</code>, as in your original code. As you can see, the expected input is a list of dictionaries, one for each input sample or row of data. Each dictionary contains an arbitrary number of feature names, mapped to values for that row. </p>

<p>The output, <code>X_new</code>, contains a sparse representation of the array; calling <code>toarray()</code> returns a new copy of the data as a vanilla <code>numpy</code> array.</p>

<p>If you want to pass pairs of tuples instead, pass <code>input_type='pairs'</code>. Then you can do this:</p>

<pre><code>&gt;&gt;&gt; hasher = sklearn.feature_extraction.FeatureHasher(n_features=10,
...                                                   non_negative=True,
...                                                   input_type='pair')
&gt;&gt;&gt; X_new = hasher.fit_transform([[('a', 1), ('b', 2)], [('a', 0), ('c', 5)]])
&gt;&gt;&gt; X_new.toarray()
array([[ 1.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.,  0.,  0.]])
</code></pre>

<p>And finally, if you just have boolean values, you don't have to pass values explicitly at all -- the <code>FeatureHasher</code> will simply assume that if a feature name is present, then its value is <code>True</code> (represented here as the floating point value <code>1.0</code>). </p>

<pre><code>&gt;&gt;&gt; hasher = sklearn.feature_extraction.FeatureHasher(n_features=10,
...                                                   non_negative=True,
...                                                   input_type='string')
&gt;&gt;&gt; X_new = hasher.fit_transform([['a', 'b'], ['a', 'c']])
&gt;&gt;&gt; X_new.toarray()
array([[ 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]])
</code></pre>

<p>Unfortunately, your data doesn't seem to consistently be in any one of these formats. However, it shouldn't be <em>too</em> hard to modify what you have to fit the <code>'dict'</code> or <code>'pair'</code> format. Let me know if you need help with that; in that case, please say more about the format of the data you're trying to convert. </p>
"
3952838;1525;Hungry;<java><math><machine-learning><svm><linear-algebra>;28483905;6;Computing Lagrange Multiplers for a simple Support Vector Machine;"<p>Firstly, I am a beginner to Support Vector Machines so I'm sorry if I am going about this problem in the wrong way. I am trying to implement a very simple SVM from scratch which uses the identity kernel function to classify linearly separable data into one of two classes. As an example of the sort of data which I will be using, consider the plot below seen in <a href=""http://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;sqi=2&amp;ved=0CCcQFjAB&amp;url=http%3A%2F%2Faxon.cs.byu.edu%2FDan%2F678%2Fmiscellaneous%2FSVM.example.pdf&amp;ei=HNPcVKKxGait7AaCsYH4Aw&amp;usg=AFQjCNE39TU0YlTK8nVujc0_V1oXfzrgNw&amp;sig2=owXfnucKE8uq3tnQ6oCP4A&amp;bvm=bv.85761416,d.ZGU&amp;cad=rja"" rel=""nofollow noreferrer"">this document</a>: </p>

<p><img src=""https://i.stack.imgur.com/z6xnS.png"" alt=""Plotted linearly separable data""></p>

<p>Using the points (1,0), (3, 1) and (3, -1) as support vectors, we know that the following is true with regards to calculating the decision plane (Screenshotted from the same document): </p>

<p><img src=""https://i.stack.imgur.com/gJWVh.png"" alt=""Lagrange Multipler Formula One"">
Which when fiddled and rearranged a bit gives us Lagrange multipliers of -3.5, 0.75 and 0.75 respectively. </p>

<p>I understand how this algebra works on paper, however I am unsure as to the best approach when it comes to implementation. So my question is as follows: <strong>how are the SVM's  Lagrange Multipliers calculated in practice</strong>? Is there an algorithm which I am missing which will be able to determine these values for <em>arbitrary</em> linearly separable support vectors? Should I use a standard maths library to solve the linear equations (I am implementing the SVM in java)? Would such a maths library be slow for large scale learning? Note that this is a learning exercise so I'm not just looking for a ready made SVM library.</p>

<p>Any other advice would be much appreciated!</p>

<p><strong>EDIT 1</strong>: LutzL made a good point that half the problem is actually determining which points are to be used as the support vectors, so to keep things simple assume for the purpose of this question that they have already been computed.</p>
";;3088138;21098;Lutz Lehmann;2;28484209;"<p>Independent of the kernel function, the determination of the coefficients leads to a quadratic optimization problem with linear positivity constraints. Which has a horrendous complexity if implemented naively testing all boundary components, so you can not avoid advanced optimization algorithms like barrier or trust region methods.</p>

<p>There are also heuristic approaches that try to keep the optimization problem in low dimension by searching for point sets close to the separation line and eliminating points that are most probably far away from it.</p>
"
3746660;63;Xone;<matlab><machine-learning><classification><svm><libsvm>;28487718;-1;Multi-class classification for large database (matlab);"<p>Can you suggest any implementation (matlab) of Multi-class classification algorithm for large database, I tried libsvm it's good except for large database and for the liblinear I can't use it for the multi classification</p>
";28497218;3391524;1834;saurabh agarwal;2;28497218;"<p>If you want to use liblinear for multi class classification, you can use <a href=""http://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest"" rel=""nofollow noreferrer"">one vs all</a> technique. For more information Look at <a href=""https://stackoverflow.com/questions/9041753/multi-class-classification-in-libsvm"">this.</a></p>

<p>But if you have large database then use of SVM is not recommended. As Run time complexity of SVM is O(N * N * m) </p>

<p>N = number of samples in data 
m = number of features in data </p>

<p>So, alternatively You can use Neural Network. You can start with nntool available in MATLAB. </p>
"
4186666;107;markan3;<machine-learning>;28488248;0;mapping math function using artificial neural network;"<p>I recently started learning about neural networks, I'm trying to modelate some economical indicators using neural networks. For example:</p>

<p>ROC = [(Close - Close n periods ago) / (Close n periods ago)] * 100</p>

<p>I'm pressuming input layer will contain closing price, but what about activation function in hidden layer?</p>

<p>any ideas?</p>
";;98975;6872;viksit;0;28492698;"<p>You should experiment with a few types of non-linear functions (tanh/sigmoid) and see which fits your data better.</p>
"
2357690;67;John;<machine-learning><mahout>;28506323;1;Mahout - converting text to vector;"<p>Given that I have a set of training text documents and a set of testing text documents. Two sets are very large so using weka is not a good choice since it costs time a lot. Hence, I use mahout - a scalable machine learning and data mining framework (<a href=""http://mahout.apache.org/"" rel=""nofollow"">http://mahout.apache.org/</a>).
  Next, I use mahout to convert training documents into mahout vector (set ngram = 1). Here I have a mahout vector representing for training documents in which the size of the each vector is the number of attributes or features and each number in that vector is the frequency of word in training documents (use tf instead of tf-idf). <strong>Does anyone know how to convert testing documents based on the features or attributes of training data I built before in mahout</strong>? </p>
";;1056563;45835;StephenBoesch;0;28512505;"<p>The ""conversion"" you refer to is actually a ""prediction"" .. no?  Given that you have already trained the data - presumably you have a model for classification available.</p>

<p>You may use the command line facilities from mahout here:</p>

<p><a href=""http://mahout.apache.org/users/basics/creating-vectors-from-text.html"" rel=""nofollow"">http://mahout.apache.org/users/basics/creating-vectors-from-text.html</a></p>
"
1115169;2518;pbu;<machine-learning><neural-network><conv-neural-network>;28508501;0;Convolution neural network prediction results are same;"<p>I am running a simple convolutional neural network, doing regression and predicting the results. It predicts 30 outputs (floats) </p>

<p>The prediction results are almost the same irrespective of any input. (converging to mean on trained outputs)</p>

<p>The training after 1000 iterations converges to maximum loss of 0.0107 (which is good one) based on this dataset.</p>

<pre><code>What is causing this? 
</code></pre>

<p>I tried to set the bias to 1.0, it brings little variables but still the same below. When i set bias to 0, the results are far worse, all outputs are 100% same. i am already using regularisation max(0,x) no improvement with results.</p>

<p>The outputs are below. As you can see, the first, second, third arrays are almost same..</p>

<pre><code> [[ 66.60850525  37.19641876  29.36295891 ...,  71.91300964  47.92261505
   85.02180481]
 [ 66.4874115   37.09647369  29.23101997 ...,  71.90777588  47.74259186
   85.10979462]
 [ 66.54870605  37.19485474  29.36085892 ...,  71.84892273  47.8970108
   85.05699921]
 ..., 
 [ 65.7435379   36.78604889  28.57537079 ...,  71.98916626  47.03699493
   85.88017273]
 [ 65.7435379   36.78604889  28.57537079 ...,  71.98916626  47.03699493
   85.88017273]
 [ 65.7435379   36.78604889  28.57537079 ...,  71.98916626  47.03699493
   85.88017273]]
</code></pre>

<p>The network model runs with this parameters</p>

<pre><code>base_lr: 0.001
lr_policy: ""fixed""
display: 100
max_iter: 1000
momentum: 0.9
</code></pre>
";;4115756;3947;zfy;0;28554341;"<p>Judging by the output and the fact that bias affect the result greatly I have a feeling that maybe you didn't normalize your input and output.</p>

<p>Try to normalize them between -1 &amp; +1.</p>
"
4545006;25;warren;<machine-learning><data-mining>;28523130;1;How to cluster docs based on their similarity measures?;"<p>I read the posts on the topic like how to cluster docs based on their similarity <a href=""https://stackoverflow.com/questions/5841282/clustering-from-the-cosine-similarity-values"">here</a>. But I still can not understand how it realizes it. My test is that I have the cos similarity measures of 10 docs. Below are some: </p>

<pre><code>D1  D2   sim(D1,D2)

d1  d10 0.6823 
d1  d2  0.6377 
d1  d8  0.0307 
d1  d9  0.0294 
d1  d7  0.0284 
d1  d3  0.0234 
d1  d4  0.0199 
d1  d6  0.0110 
d1  d5  0.0030 
d10 d2  0.7232 
d10 d3  0.3898 
d10 d4  0.3054 
d10 d9  0.0256 
d10 d7  0.0227 
d10 d8  0.0226 
d10 d6  0.0110 
d10 d5  0.0060 
d2  d3  0.7850 
...
...
</code></pre>

<p>Can I cluster these docs solely based on the similarity measures?
If I specify the number of clusters, how to do it?
If I do not specify the number of clusters, can the algorithm automatically cluster those docs, how to do it?
Thanks in advance.</p>
";28523796;278042;16592;jkff;0;28523796;"<p>Clustering is one of the biggest areas of machine learning (proportionally you could compare it to, say, ""integration"" in mathematics, or ""sorting"" in programming), and there are literally hundreds of different algorithms, focused on different problem settings and requirements. Some of them require to specify the number of clusters, some don't. Some can work with just the pairwise similarity, some require some explicit representation of the items being clustered, etc.</p>

<p>I suggest you to start with two of the classical clustering algorithms:</p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/K-means_clustering"" rel=""nofollow"">http://en.wikipedia.org/wiki/K-means_clustering</a> - here, you specify the number of clusters (""k"") in advance, however the objects being clustered have to be points in a vector space (there are ways to reduce the problem of document clustering to a vector space problem - search for ""term vector representation""). Since you're dealing with cosine similarity, looks like you already have a vector space, so you can use K-means.</li>
<li><a href=""http://en.wikipedia.org/wiki/Hierarchical_clustering"" rel=""nofollow"">http://en.wikipedia.org/wiki/Hierarchical_clustering</a> (in particular, ""single-linkage agglomerative clustering"" <a href=""http://en.wikipedia.org/wiki/Single-linkage_clustering"" rel=""nofollow"">http://en.wikipedia.org/wiki/Single-linkage_clustering</a>) - here, you only need the pairwise similarities: you build a tree by repeatedly finding the two most similar documents and joining them into the same cluster, until you have the desired number of clusters.</li>
</ul>
"
4545006;25;warren;<machine-learning><data-mining>;28523130;1;How to cluster docs based on their similarity measures?;"<p>I read the posts on the topic like how to cluster docs based on their similarity <a href=""https://stackoverflow.com/questions/5841282/clustering-from-the-cosine-similarity-values"">here</a>. But I still can not understand how it realizes it. My test is that I have the cos similarity measures of 10 docs. Below are some: </p>

<pre><code>D1  D2   sim(D1,D2)

d1  d10 0.6823 
d1  d2  0.6377 
d1  d8  0.0307 
d1  d9  0.0294 
d1  d7  0.0284 
d1  d3  0.0234 
d1  d4  0.0199 
d1  d6  0.0110 
d1  d5  0.0030 
d10 d2  0.7232 
d10 d3  0.3898 
d10 d4  0.3054 
d10 d9  0.0256 
d10 d7  0.0227 
d10 d8  0.0226 
d10 d6  0.0110 
d10 d5  0.0060 
d2  d3  0.7850 
...
...
</code></pre>

<p>Can I cluster these docs solely based on the similarity measures?
If I specify the number of clusters, how to do it?
If I do not specify the number of clusters, can the algorithm automatically cluster those docs, how to do it?
Thanks in advance.</p>
";28523796;1060350;70512;Has QUIT--Anony-Mousse;0;28525223;"<p>Various clustering algorithms operate on pairwise distsnces; and many can be adapted to work on pairwise similarities, too.</p>

<p>Hierarchical Agglomerative Clustering (HAC) is the prototype for this. It works on a distance or similarity matrix, and merges the most similar clusters starting with single documents. Other algorithms include DBSCAn, OPTICS, ...</p>

<p>k-means is the opposite. It computes means, and distances from the mean. It doesn't work with similarities or other distances than squared Euclidean well because of uding the <em>mean</em>. The mean minimizes least squares, not distances. However, sometimes you have a way out. If your data is normalized to the non-negative unit sphere, then squared Euclidean d2(a,b)= 2 - 2*cos(a,b). And thus, spherical k-means works too. Other algorithms with reliance on coordinates and means include Mean-Shift and BIRCH.</p>
"
2897844;97;xiaofei;<machine-learning><deep-learning>;28525436;7;Why the LeNet5 uses 32Ã—32 image as input?;"<p>I know that the handwritten digit images in the mnist dataset are 28Ã—28ï¼Œbut why the input in LeNet5 is 32Ã—32?</p>
";;860196;8365;runDOSrun;1;28525993;"<p>Your question is answered in the <a href=""http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf"" rel=""nofollow"">original paper</a>: <br>
The convolution step always takes a smaller input than the feature maps of the previous layer (and this holds true for the 1st layer - the input - as well):</p>

<blockquote>
  <p>Layer C1 is a convolutional layer with 6 feature maps.
  Each unit in each feature map is connected to a 5x5 neighborhood in the input. The size of the feature maps is 28x28
  which prevents connection from the input from falling off
  the boundary.</p>
</blockquote>

<p>This means that using a 5x5 neighborhood on a 32x32 input, you'll get 6 features maps of size 28x28 because there's pixels you won't use at the image boundary (you will always have a remainder with these numbers). </p>

<p>Of course they could have an exception for the first layer. The reason they're still using 32x32 images is:</p>

<blockquote>
  <p>The input is a 32x32 pixel image. This is significantly larger
  than the largest character in the database (at most 20x20
  pixels centered in a 28x28 field). The reason is that it is
  desirable that potential distinctive features such as stroke
  end-points or corner can appear in the center of the receptive field of the highest-level feature detectors.</p>
</blockquote>
"
1115169;2518;pbu;<opencv><numpy><machine-learning>;28530102;0;How to detect facial angles?;"<p>I have over 2000 grayscale images with 96x96 pixel dimensions in numpy. I have (x,y) coordinated of facial key points such as left_eye_center, right_eye_center, nose_center, mouth_left, mouth_right etc..</p>

<p>Many of the faces in the dataset are tilted either left or right or up or down. So I would like to find out the facial orientation angles towards the camera.  </p>

<p>Is there any library to detect this? I looked into opencv but it seems to do only facial detection not orientation. </p>
";;860196;8365;runDOSrun;0;28530161;"<p>I'd split the training data into 3 labels (left, center, right) and then use it with your machine learning algorithm of choice to learn the 3 classes. If you're using opencv you can e.g. use HaarCascadeClassifiers (I once had a project where I used it to classify orientations). But really, anything works. You only need to extend it from binary classification (face detection) to multiclass classification (face orientation).</p>
"
1115169;2518;pbu;<opencv><numpy><machine-learning>;28530102;0;How to detect facial angles?;"<p>I have over 2000 grayscale images with 96x96 pixel dimensions in numpy. I have (x,y) coordinated of facial key points such as left_eye_center, right_eye_center, nose_center, mouth_left, mouth_right etc..</p>

<p>Many of the faces in the dataset are tilted either left or right or up or down. So I would like to find out the facial orientation angles towards the camera.  </p>

<p>Is there any library to detect this? I looked into opencv but it seems to do only facial detection not orientation. </p>
";;2056772;35809;berak;1;28530187;"<p>cmon, it's just plain maths:</p>

<pre><code>    double eyeXdis = eye_r.x - eye_l.x;
    double eyeYdis = eye_r.y - eye_l.y;
    double angle   = atan(eyeYdis/eyeXdis);
    double degree  = angle*180/CV_PI;
</code></pre>

<p>[edit:]</p>

<p>it seems, what you're looking for is actually ""head pose estimation"" (or ""posit"")</p>

<p>you would need a (primitive) 3d head model, get the corresponding 3d points for your landmarks once, and then for each image find the 2d landmarks in the image, and get a rotation/translation vector from solvPnP() with that.</p>
"
3787253;2595;smatthewenglish;<java><io><machine-learning><hashmap><perceptron>;28535043;0;plus operation on Integer object, Read in multiple files from a directory to create bag-of-words in Java;"<p>Is bag-of-words the same thing as document term matrix?</p>

<p>I have a training data set that consists of many files. I want to read all of them into a data structure (hash map?) to create a bag-of-words model for a particular class of documents, either science, religion, sports, or sex, in preparation for a perceptron implementation. </p>

<p>Right now I have the simplest of simple Java I/o constructs, I.e.</p>

<pre><code>    String text; 
    BufferedReader br = new BufferedReader(new FileReader(""file""));

    while ((text = br.readLine()) != null) 
    {
        //read in multiple files
        //generate a hash map with each unique word
        //as a key and the frequency with which that
        //word appears as the value
    }
</code></pre>

<p>So what I want to do is read input from multiple files in a directory and save all the data to one underlying structure, how to do that? Should I write it out to a file somewhere?</p>

<p>I think a hashmap, as I described in the comments of the code above would work, based on my understanding of bag-of-words. Is that right? How could I implement such a thing to sych with the reading of input from multiple files. How should I store it so I can later incorporate that into my perceptron algorithm? </p>

<p>I've seen this done <a href=""https://stackoverflow.com/questions/12998841/java-multiple-input-files"">like so</a>:</p>

<pre><code>  String names = new String[]{""a.txt"", ""b.txt"", ""c.txt""};
  StringBuffer strContent = new StringBuffer("""");

  for (String name : names) {
      File file = new File(name); 
      int ch;
      FileInputStream stream = null;  
      try {
          stream = new FileInputStream(file);   
          while( (ch = stream.read()) != -1) {
          strContent.append((char) ch); 
          }
      } finally {
          stream.close();  
      } 
   }
</code></pre>

<p>But this is a lame solution because you need to specify in advance all the files, I think that should be more dynamic. If possible. </p>
";28535617;3344829;11037;Saravana;1;28535617;"<p>You can try below program, its dynamic, you just need to provide your directory path.</p>

<pre><code>public class BagOfWords {

ConcurrentHashMap&lt;String, Set&lt;String&gt;&gt; map = new ConcurrentHashMap&lt;String, Set&lt;String&gt;&gt;();

public static void main(String[] args) throws IOException {
    File file = new File(""F:/Downloads/Build/"");
    new BagOfWords().iterateDirectory(file);
}

private void iterateDirectory(File file) throws IOException {
    for (File f : file.listFiles()) {
        if (f.isDirectory()) {
            iterateDirectory(file);
        } else {
            // Read File
            // Split and put it in a set
            // add to map
        }
    }
}
</code></pre>

<p>}</p>
"
3787253;2595;smatthewenglish;<java><io><machine-learning><hashmap><perceptron>;28535043;0;plus operation on Integer object, Read in multiple files from a directory to create bag-of-words in Java;"<p>Is bag-of-words the same thing as document term matrix?</p>

<p>I have a training data set that consists of many files. I want to read all of them into a data structure (hash map?) to create a bag-of-words model for a particular class of documents, either science, religion, sports, or sex, in preparation for a perceptron implementation. </p>

<p>Right now I have the simplest of simple Java I/o constructs, I.e.</p>

<pre><code>    String text; 
    BufferedReader br = new BufferedReader(new FileReader(""file""));

    while ((text = br.readLine()) != null) 
    {
        //read in multiple files
        //generate a hash map with each unique word
        //as a key and the frequency with which that
        //word appears as the value
    }
</code></pre>

<p>So what I want to do is read input from multiple files in a directory and save all the data to one underlying structure, how to do that? Should I write it out to a file somewhere?</p>

<p>I think a hashmap, as I described in the comments of the code above would work, based on my understanding of bag-of-words. Is that right? How could I implement such a thing to sych with the reading of input from multiple files. How should I store it so I can later incorporate that into my perceptron algorithm? </p>

<p>I've seen this done <a href=""https://stackoverflow.com/questions/12998841/java-multiple-input-files"">like so</a>:</p>

<pre><code>  String names = new String[]{""a.txt"", ""b.txt"", ""c.txt""};
  StringBuffer strContent = new StringBuffer("""");

  for (String name : names) {
      File file = new File(name); 
      int ch;
      FileInputStream stream = null;  
      try {
          stream = new FileInputStream(file);   
          while( (ch = stream.read()) != -1) {
          strContent.append((char) ch); 
          }
      } finally {
          stream.close();  
      } 
   }
</code></pre>

<p>But this is a lame solution because you need to specify in advance all the files, I think that should be more dynamic. If possible. </p>
";28535617;3787253;2595;smatthewenglish;0;28536064;"<p>I think this is very close but there's some kind of discrepency with <code>int</code> and <code>integer</code> how to reconcile that? </p>

<p>ConcurrentHashMap> map = new ConcurrentHashMap>();</p>

<pre><code>        public static void main(String[] args) throws IOException 
        {
            String path = ""path"";
            File file = new File( path );
            new BagOfWords().iterateDirectory(file);
        }    

        private void iterateDirectory(File file) throws IOException 
        {
            for (File f : file.listFiles()) 
            {
                if (f.isDirectory()) 
                {
                    iterateDirectory(file);
                } 
                else 
                {

                    String line; 
                    BufferedReader br = new BufferedReader(new FileReader(""file""));

                    while ((line = br.readLine()) != null) 
                    {

                        String[] words = line.split("" "");//those are your words

                        // Read File
                        // Split and put it in a set
                        // add to map
                        String word;

                        for (int i = 0; i &lt; words.length; i++) 
                        {
                            word = words[i];
                            if (!map.containsKey(word))
                            {
                                map.put(word, 0);
                            }
                            map.put(word, map.get(word) + 1);
                        }

                    }

                }
            }
        }
</code></pre>
"
3787253;2595;smatthewenglish;<java><machine-learning><perceptron>;28536678;-1;run perceptron algorithm on a hash map feature vecteur: java;"<p>I have the following code, it reads in many files from a directory into a hash map, this is my <em>feature vecteur</em>. It's somewhat naive in the sense that it does no stemming but that's not my primary concern right now. I want to know how I can use this data structure as the input to the perceptron algorithm. I guess we call this a bag of words, isn't it? </p>

<pre><code>public class BagOfWords 
{
        static Map&lt;String, Integer&gt; bag_of_words = new HashMap&lt;&gt;();

        public static void main(String[] args) throws IOException 
        {
            String path = ""/home/flavius/atheism;
            File file = new File( path );
            new BagOfWords().iterateDirectory(file);

            for (Map.Entry&lt;String, Integer&gt; entry : bag_of_words.entrySet()) 
            {
                System.out.println(entry.getKey()+"" : ""+entry.getValue());
            }

        }

        private void iterateDirectory(File file) throws IOException 
        {
            for (File f : file.listFiles()) 
            {
                if (f.isDirectory()) 
                {    
                    iterateDirectory(file);
                } 
                else 
                {
                    String line; 
                    BufferedReader br = new BufferedReader(new FileReader( f ));

                    while ((line = br.readLine()) != null) 
                    {

                        String[] words = line.split("" "");//those are your words

                        String word;

                        for (int i = 0; i &lt; words.length; i++) 
                        {
                            word = words[i];
                            if (!bag_of_words.containsKey(word))
                            {
                                bag_of_words.put(word, 0);
                            }
                            bag_of_words.put(word, bag_of_words.get(word) + 1);
                        }

                    }

                }
            }
        }
    }
</code></pre>

<p>You can see that the path goes to a directory called 'atheism' there's also one called sports, I want to try to linearly seperate these two classes of documents, and then try to seperate the unseen test docs into either category. </p>

<p>How to do that? How to conceptualize that. I'd appreciate a solid reference, comprehensive explanation or some kind of pseudocode. </p>

<p>I've not found many informative and lucid references on the web. </p>
";28540733;540873;20218;Thomas Jungblut;1;28540733;"<p>Let's establish some vocabulary up front (I guess you are using the 20-newsgroup dataset):</p>

<ul>
<li>""Class Label"" is what you're trying to predict, in your binary case this is ""atheism"" vs. the rest</li>
<li>""Feature vector"" that's what you input to your classifier</li>
<li>""Document"" that is a single e-mail from the dataset</li>
<li>""Token"" a fraction of a document, usually a unigram/bigram/trigram</li>
<li>""Dictionary"" a set of ""allowed"" words for your vector </li>
</ul>

<p>So the vectorization algorithm for bag of words usually follows the following steps:</p>

<ol>
<li>Go over all the documents (across all class labels) and collect all the tokens, this is your dictionary and the dimensionality of your feature vector</li>
<li>Go over all the documents again and for each do:
<ol>
<li>Create a new feature vector with the dimensionality of your dictionary (e.g. 200, for 200 entries in that dictionary)</li>
<li>go over all the tokens in that document and set the word count (within this document) at this dimension of the feature vector</li>
</ol></li>
<li>You now have a list of feature vectors that you can feed into your algorithm</li>
</ol>

<p>Example:</p>

<pre><code>Document 1 = [""I"", ""am"", ""awesome""]
Document 2 = [""I"", ""am"", ""great"", ""great""]
</code></pre>

<p>Dictionary is:</p>

<pre><code>[""I"", ""am"", ""awesome"", ""great""]
</code></pre>

<p>So the documents as a vector would look like:</p>

<pre><code>Document 1 = [1, 1, 1, 0]
Document 2 = [1, 1, 0, 2]
</code></pre>

<p>And with that you can do all kinds of fancy math stuff and feed this into your perceptron.</p>
"
3787253;2595;smatthewenglish;<java><machine-learning><perceptron>;28536678;-1;run perceptron algorithm on a hash map feature vecteur: java;"<p>I have the following code, it reads in many files from a directory into a hash map, this is my <em>feature vecteur</em>. It's somewhat naive in the sense that it does no stemming but that's not my primary concern right now. I want to know how I can use this data structure as the input to the perceptron algorithm. I guess we call this a bag of words, isn't it? </p>

<pre><code>public class BagOfWords 
{
        static Map&lt;String, Integer&gt; bag_of_words = new HashMap&lt;&gt;();

        public static void main(String[] args) throws IOException 
        {
            String path = ""/home/flavius/atheism;
            File file = new File( path );
            new BagOfWords().iterateDirectory(file);

            for (Map.Entry&lt;String, Integer&gt; entry : bag_of_words.entrySet()) 
            {
                System.out.println(entry.getKey()+"" : ""+entry.getValue());
            }

        }

        private void iterateDirectory(File file) throws IOException 
        {
            for (File f : file.listFiles()) 
            {
                if (f.isDirectory()) 
                {    
                    iterateDirectory(file);
                } 
                else 
                {
                    String line; 
                    BufferedReader br = new BufferedReader(new FileReader( f ));

                    while ((line = br.readLine()) != null) 
                    {

                        String[] words = line.split("" "");//those are your words

                        String word;

                        for (int i = 0; i &lt; words.length; i++) 
                        {
                            word = words[i];
                            if (!bag_of_words.containsKey(word))
                            {
                                bag_of_words.put(word, 0);
                            }
                            bag_of_words.put(word, bag_of_words.get(word) + 1);
                        }

                    }

                }
            }
        }
    }
</code></pre>

<p>You can see that the path goes to a directory called 'atheism' there's also one called sports, I want to try to linearly seperate these two classes of documents, and then try to seperate the unseen test docs into either category. </p>

<p>How to do that? How to conceptualize that. I'd appreciate a solid reference, comprehensive explanation or some kind of pseudocode. </p>

<p>I've not found many informative and lucid references on the web. </p>
";28540733;3787253;2595;smatthewenglish;0;28560152;"<p><em>This is the full and complete answer to my original question, posted here for the benefit of future perusers</em></p>

<hr>

<p>Given the following files:</p>

<ul>
<li><p><strong>atheism/a_0.txt</strong></p>

<pre><code>Gott ist tot.
</code></pre></li>
<li><p><strong>politics/p_0.txt</strong></p>

<pre><code>L'Etat, c'est moi , et aussi moi .
</code></pre></li>
<li><p><strong>science/s_0.txt</strong></p>

<pre><code>If I have seen further it is by standing on the shoulders of giants.
</code></pre></li>
<li><p><strong>sports/s_1.txt</strong></p>

<pre><code>You miss 100% of the shots you don't take.
</code></pre></li>
<li><p><strong>Output data structures:</strong></p>

<pre><code>/data/train/politics/p_0.txt, [0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
/data/train/science/s_0.txt, [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0]
/data/train/atheism/a_0.txt, [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
/data/train/sports/s_1.txt, [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1]
</code></pre></li>
</ul>

<p>The code looks like this, or you can find it on <a href=""https://github.com/h1395010/perceptron/blob/master/src/file_dict_createur/FileDictCreateur.java"" rel=""nofollow"">my GitHub page</a>.</p>

<pre><code>public class FileDictCreateur 
{
    static String PATH = ""/home/matthias/Workbench/SUTD/ISTD_50.570/assignments/practice_data/data/train"";

    //the global list of all words across all articles
    static Set&lt;String&gt; GLOBO_DICT = new HashSet&lt;String&gt;();

    //is the globo dict full?
    static boolean globo_dict_fixed = false;

    // hash map of all the words contained in individual files
    static Map&lt;File, ArrayList&lt;String&gt; &gt; fileDict = new HashMap&lt;&gt;();

    //input to perceptron. final struc.
    static Map&lt;File, int[] &gt; perceptron_input = new HashMap&lt;&gt;();


    @SuppressWarnings(""rawtypes"")
    public static void main(String[] args) throws IOException 
    {
        //each of the diferent categories
        String[] categories = { ""/atheism"", ""/politics"", ""/science"", ""/sports""};

        //cycle through all categories once to populate the global dict
        for(int cycle = 0; cycle &lt;= 3; cycle++)
        {
            String general_data_partition = PATH + categories[cycle];

            File directory = new File( general_data_partition );
            iterateDirectory( directory , globo_dict_fixed);

            if(cycle == 3)
                globo_dict_fixed = true;
        }


        //cycle through again to populate the file dicts
        for(int cycle = 0; cycle &lt;= 3; cycle++)
        {
            String general_data_partition = PATH + categories[cycle];

            File directory = new File( general_data_partition );
            iterateDirectory( directory , globo_dict_fixed);

        }



        perceptron_data_struc_generateur( GLOBO_DICT, fileDict, perceptron_input );



        //print the output
        for (Map.Entry&lt;File, int[]&gt; entry : perceptron_input.entrySet()) 
        {
            System.out.println(entry.getKey() + "", "" + Arrays.toString(entry.getValue()));
        }
    }



    private static void iterateDirectory(File directory, boolean globo_dict_fixed) throws IOException 
    {
        for (File file : directory.listFiles()) 
        {
            if (file.isDirectory()) 
            {
                iterateDirectory(directory, globo_dict_fixed);
            } 
            else 
            {   
                String line; 
                BufferedReader br = new BufferedReader(new FileReader( file ));

                while ((line = br.readLine()) != null) 
                {
                    String[] words = line.split("" "");//those are your words

                    if(globo_dict_fixed == false)
                    {
                        populate_globo_dict( words );
                    }
                    else
                    {
                        create_file_dict( file, words );
                    }
                }
            }
        }
    }

    @SuppressWarnings(""unchecked"")
    public static void create_file_dict( File file, String[] words ) throws IOException
    {   

        if (!fileDict.containsKey(file))
        {
            @SuppressWarnings(""rawtypes"")
            ArrayList document_words = new ArrayList&lt;String&gt;();

            String word;

            for (int i = 0; i &lt; words.length; i++) 
            {
                word = words[i];

                document_words.add(word);
            }
            fileDict.put(file, document_words);
        }
    }

    public static void populate_globo_dict( String[] words ) throws IOException
    {
        String word;

        for (int i = 0; i &lt; words.length; i++) 
        {
            word = words[i];
            if (!GLOBO_DICT.contains(word))
            {
                GLOBO_DICT.add(word);
            }
        }   
    }

    public static void perceptron_data_struc_generateur(Set&lt;String&gt; GLOBO_DICT, 
                                                    Map&lt;File,     ArrayList&lt;String&gt; &gt; fileDict,
                                                    Map&lt;File, int[] &gt; perceptron_input)
    {
        //create a new entry in the array list 'perceptron_input'
        //with the key as the file name from fileDict
            //create a new array which is the length of GLOBO_DICT
            //iterate through the indicies of GLOBO_DICT
                //for all words in globo dict, if that word appears in fileDict,
                //increment the perceptron_input index that corresponds to that
                //word in GLOBO_DICT by the number of times that word appears in fileDict

        //so i can get the index later
        List&lt;String&gt; GLOBO_DICT_list = new ArrayList&lt;&gt;(GLOBO_DICT);

        for (Map.Entry&lt;File, ArrayList&lt;String&gt;&gt; entry : fileDict.entrySet()) 
        {
            int[] cross_czech = new int[GLOBO_DICT_list.size()];
            //initialize to zero
            Arrays.fill(cross_czech, 0);

            for (String s : GLOBO_DICT_list)
            {

                for(String st : entry.getValue()) 
                {
                    if( st.equals(s) )
                    {
                        cross_czech[ GLOBO_DICT_list.indexOf( s ) ] = cross_czech[ GLOBO_DICT_list.indexOf( s ) ] +1;
                    }
                }
            }
            perceptron_input.put( entry.getKey() , cross_czech);    
        }
    }
}
</code></pre>
"
1894980;544;Thamiar;<c++><machine-learning><gnuplot>;28542039;1;What is wrong with my gradient descent for Polynomial Regresion (C++, GNUPLOT);"<p>I tried to implement gradient descent for Polynomial regression. I thought i understand everything, but something seems wrong.</p>

<p><img src=""https://i.stack.imgur.com/VJO0V.jpg"" alt=""enter image description here""></p>

<p>Above is the formula taken from coursera Machine Learning lecutres. 
Now, what about the code:</p>

<pre><code> for(int m=1;m&lt;=degree;m++){
                for(int i = 0;i&lt;iterations;i++)
                   {    
                           getCoefs(learningStep,m);
                   }
                qDebug()&lt;&lt;""Parameters:"";
                for(int i=0;i&lt;=degree;i++) {qDebug()&lt;&lt;coefs[i];}

                    switch(m){
                    case 1:  equation = to_string(coefs[1]) + ""*x+"" + to_string(coefs[0]);
                             g1.set_style(""lines"").plot_equation(equation,""x^1""); break;
                    case 2:  equation = to_string(coefs[2])+ ""*x**2+"" + to_string(coefs[1]) + ""*x+"" + to_string(coefs[0]);
                             g1.set_style(""lines"").plot_equation(equation,""x^2""); break;
                    case 3:  equation = to_string(coefs[3])+ ""*x**3+"" +to_string(coefs[2])+ ""*x**2+"" + to_string(coefs[1]) + ""*x+"" + to_string(coefs[0]);
                             g1.set_style(""lines"").plot_equation(equation,""x^3""); break;
                    case 4:  equation = to_string(coefs[4])+ ""*x**4+"" + to_string(coefs[3])+ ""*x**3+"" +to_string(coefs[2])+ ""*x**2+"" + to_string(coefs[1]) + ""*x+"" + to_string(coefs[0]);
                             g1.set_style(""lines"").plot_equation(equation,""x^4""); break;
                    case 5:  equation = to_string(coefs[5])+ ""*x**5+"" + to_string(coefs[4])+ ""*x**4+"" + to_string(coefs[3])+ ""*x**3+"" +to_string(coefs[2])+ ""*x**2+"" + to_string(coefs[1]) + ""*x+"" + to_string(coefs[0]);
                             g1.set_style(""lines"").plot_equation(equation,""x^5""); break;
                    }

                for(int i=0;i&lt;=degree;i++) {coefs[i]=0;}
              }
</code></pre>

<p>It is the function for plotting the equations. 
<strong>Coefs</strong> is a vector for parameters {Ã¸1,Ã¸2...Ã¸n}</p>

<p><strong>degree</strong> Is an int number of the biggest degree i want to calculate.</p>

<p><strong>iterations</strong> number of steps, in my case 2000</p>

<p><strong>learningStep</strong> no need to explain (0.001 [I tried as well bigger LS 0.01 but the result was also wrong])</p>

<pre><code>void getCoefs(double learningStep, int degree){
QVector &lt; double &gt; sum;
for(int j=0;j&lt;=degree;j++){
    sum.push_back(0);
 }

double numberOfPoints = point.length();
QVector &lt; double &gt; actual;
for(int j=0;j&lt;=degree;j++){
    actual.push_back(coefs[j]);
 }

for(int i=0;i&lt;point.length();i++){

    for(int j=0;j&lt;=degree;j++){
        sum[j] += (1/numberOfPoints) *(((actual[1]*point[i].getX() + actual[0]) - (point[i].getY()))*pow(point[i].getX(), j));
    }
}
for(int j=0;j&lt;=degree;j++){

    coefs[j] -= learningStep*sum[j];
}
</code></pre>

<p>}</p>

<p>Here is my code for getting coefs. 
<strong>point</strong> is a vector of points (Objects with two variables x,y).</p>

<p><strong>actual</strong> is a vector containing coefs for our hypothesis function.</p>

<p>Here we calculate the sum showed on the picture one: </p>

<blockquote>
  <p>sum[j] += (1/numberOfPoints) *(((actual<a href=""https://i.stack.imgur.com/VJO0V.jpg"" rel=""nofollow noreferrer"">1</a>*point[i].getX() + actual[0]) - (point[i].getY()))*pow(point[i].getX(), j));</p>
</blockquote>

<p>And here we are changing each Ã¸.</p>

<blockquote>
  <p>coefs[j] -= learningStep*sum[j];</p>
</blockquote>

<p>I thought everything is done as it supposed to be. It worked fine for linear regression, but it is not working for polynimial regression.</p>

<p><img src=""https://i.stack.imgur.com/wvdvu.png"" alt=""enter image description here"">
We can see here easily that x^1 fits perfectly, but the other functions are fitted only to the middle points. It looks more clearly on the next picture:
<img src=""https://i.stack.imgur.com/z36t3.png"" alt=""enter image description here""></p>

<p>I would love to understand it, but i simply have no idea what is wrong. mathematically everything is correct (Or maybe i missed something)</p>

<p><strong>EDIT</strong>
I changed the hypothesis and it worked like a charm!
<img src=""https://i.stack.imgur.com/C97Bu.png"" alt=""enter image description here""></p>
";28542567;1190430;5186;Artem Sobolev;3;28542567;"<p>You misunderstand polynomial regression. Hypothesis should look like</p>

<p><img src=""https://i.stack.imgur.com/rLPB9.png"" alt=""enter image description here""></p>

<p>Where <code>theta</code>s are parameters that you're trying to learn. Note that there are <code>m+1</code> completely independent variables.</p>

<p>Even though your <code>coefs</code> is indeed of size <code>m+1</code>, <code>coefs[2], ..., coefs[m]</code> are never used. In fact, you always use linear model:</p>

<pre><code>actual[1]*point[i].getX() + actual[0]
</code></pre>

<p>computes</p>

<p><img src=""https://i.stack.imgur.com/3xl7K.png"" alt=""enter image description here""> </p>

<p>You should replace aforementioned code with one that computes higher-order polynomial.</p>
"
1724692;1940;Michael;<python><matplotlib><pandas><plot><machine-learning>;28542686;4;3D-plot of the error function in a linear regression;"<p>I would like to visually plot a 3D graph of the error function calculated for a given slope and y-intercept for a linear regression.
This graph will be used to illustrate a gradient descent application.</p>
<p>Letâ€™s suppose we want to model a set of points with a line. To do this weâ€™ll use the standard y=mx+b line equation where m is the lineâ€™s slope and b is the lineâ€™s y-intercept. To find the best line for our data, we need to find the best set of slope m and y-intercept b values.</p>
<p>A standard approach to solving this type of problem is to define an error function (also called a cost function) that measures how â€œgoodâ€ a given line is. This function will take in a (m,b) pair and return an error value based on how well the line fits the data. To compute this error for a given line, weâ€™ll iterate through each (x,y) point in the data set and sum the square distances between each pointâ€™s y value and the candidate lineâ€™s y value (computed at mx+b). Itâ€™s conventional to square this distance to ensure that it is positive and to make our error function differentiable. In python, computing the error for a given line will look like:</p>
<pre class=""lang-py prettyprint-override""><code># y = mx + b
# m is slope, b is y-intercept
def computeErrorForLineGivenPoints(b, m, points):
    totalError = 0
    for i in range(0, len(points)):
        totalError += (points[i].y - (m * points[i].x + b)) ** 2
    return totalError / float(len(points))
</code></pre>
<p>Since the error function consists of two parameters (m and b) we can visualize it as a two-dimensional surface.</p>
<p><b>Now my question, how can we plot such 3D-graph using python ?</b></p>
<p>Here is a skeleton code to build a 3D plot.  This code snippet is totally out of the question context but it show the basics for building a 3D plot.
For my example i would need the x-axis being the slope, the y-axis being the y-intercept and the z-axis, the error.</p>
<p>Can someone help me build such example of graph ?</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import random

def fun(x, y):
  return x**2 + y

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
x = y = np.arange(-3.0, 3.0, 0.05)
X, Y = np.meshgrid(x, y)
zs = np.array([fun(x,y) for x,y in zip(np.ravel(X), np.ravel(Y))])
Z = zs.reshape(X.shape)

ax.plot_surface(X, Y, Z)

ax.set_xlabel('X Label')
ax.set_ylabel('Y Label')
ax.set_zlabel('Z Label')

plt.show()
</code></pre>
<p>The above code produce the following plot, which is very similar to what i am looking for.
<img src=""https://i.stack.imgur.com/uHFjW.png"" alt=""Here is what the above code produce"" /></p>
";28543587;190597;706790;unutbu;4;28543587;"<p>Simply replace <code>fun</code> with <code>computeErrorForLineGivenPoints</code>:</p>

<pre><code>import numpy as np
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import collections

def error(m, b, points):
    totalError = 0
    for i in range(0, len(points)):
        totalError += (points[i].y - (m * points[i].x + b)) ** 2
    return totalError / float(len(points))

x = y = np.arange(-3.0, 3.0, 0.05)
Point = collections.namedtuple('Point', ['x', 'y'])

m, b = 3, 2
noise = np.random.random(x.size)
points = [Point(xp, m*xp+b+err) for xp,err in zip(x, noise)]

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

ms = np.linspace(2.0, 4.0, 10)
bs = np.linspace(1.5, 2.5, 10)

M, B = np.meshgrid(ms, bs)
zs = np.array([error(mp, bp, points) 
               for mp, bp in zip(np.ravel(M), np.ravel(B))])
Z = zs.reshape(M.shape)

ax.plot_surface(M, B, Z, rstride=1, cstride=1, color='b', alpha=0.5)

ax.set_xlabel('m')
ax.set_ylabel('b')
ax.set_zlabel('error')

plt.show()
</code></pre>

<p>yields
<img src=""https://i.stack.imgur.com/cJdsM.png"" alt=""enter image description here""></p>

<p>Tip: I renamed <code>computeErrorForLineGivenPoints</code> as <code>error</code>. Generally, there is no need to name a function <code>compute...</code> since almost all functions compute something. You also do not need to specify ""GivenPoints"" since the function signature shows that <code>points</code> is an argument. If you have other error functions or variables in your program, <code>line_error</code> or <code>total_error</code> might be a better name for this function.</p>
"
3787253;2595;smatthewenglish;<java><data-structures><machine-learning><perceptron>;28544144;0;data structure confusion over implementation of perceptron in java;"<p>I'm trying to implement the perceptron algorithm in java, just a one layer kind, not a fully neural net type. It's a classification problem that I'm trying to solve. </p>

<p>What I need to do is create a bag-of-words feature vector for each document in one of four categories, politics, science, sports and atheism. <a href=""https://app.box.com/s/a12g01va980blinsc43n3r2z0i0prx8g"" rel=""nofollow noreferrer"">This</a> is the data. </p>

<p>I'm trying to achieve this (a direct quote from the first answer to <a href=""https://stackoverflow.com/questions/28536678/run-perceptron-algorithm-on-a-hash-map-feature-vecteur-java/28540733#28540733"">this question</a>):</p>

<p><em>Example:</em></p>

<pre><code>Document 1 = [""I"", ""am"", ""awesome""]
Document 2 = [""I"", ""am"", ""great"", ""great""]
</code></pre>

<p><em>Dictionary is:</em></p>

<pre><code>[""I"", ""am"", ""awesome"", ""great""]
</code></pre>

<p><em>So the documents as a vector would look like:</em></p>

<pre><code>Document 1 = [1, 1, 1, 0]
Document 2 = [1, 1, 0, 2]
</code></pre>

<p><em>And with that you can do all kinds of fancy math stuff and feed this into your perceptron.</em></p>

<p>I've been able to generate the global dictionary, now I need to make one for each docuement, but how can I keep them all straight? The folder structure is pretty straight forward, i.e. `/politics/' has many articles inside, for each one I need to make a feature vector against the global dictionary. I think the iterator I'm using is what's confusing me.</p>

<p>This is the main class:</p>

<pre><code>public class BagOfWords 
{
    static Set&lt;String&gt; global_dict = new HashSet&lt;String&gt;();

    static boolean global_dict_complete = false; 

    static String path = ""/home/Workbench/SUTD/ISTD_50.570/assignments/data/train"";

    public static void main(String[] args) throws IOException 
    {
        //each of the diferent categories
        String[] categories = { ""/atheism"", ""/politics"", ""/science"", ""/sports""};

        //cycle through all categories once to populate the global dict
        for(int cycle = 0; cycle &lt;= 3; cycle++)
        {
            String general_data_partition = path + categories[cycle]; 

            File file = new File( general_data_partition );
            Iterateur.iterateDirectory(file, global_dict, global_dict_complete);
        }   

        //after the global dict has been filled up
        //cycle through again to populate a set of
        //words for each document, compare it to the
        //global dict. 
        for(int cycle = 0; cycle &lt;= 3; cycle++)
        {
            if(cycle == 3)
                global_dict_complete = true;

            String general_data_partition = path + categories[cycle]; 

            File file = new File( general_data_partition );
            Iterateur.iterateDirectory(file, global_dict, global_dict_complete);
        }

        //print the data struc              
        //for (String s : global_dict)
            //System.out.println( s );
    }
}
</code></pre>

<p>This iterates through the data structures:</p>

<pre><code>public class Iterateur 
{
    static void iterateDirectory(File file, 
                             Set&lt;String&gt; global_dict, 
                             boolean global_dict_complete) throws IOException 
    {
        for (File f : file.listFiles()) 
        {
            if (f.isDirectory()) 
            {
                iterateDirectory(file, global_dict, global_dict_complete);
            } 
            else 
            {
                String line; 
                BufferedReader br = new BufferedReader(new FileReader( f ));

                while ((line = br.readLine()) != null) 
                {
                    if (global_dict_complete == false)
                    {
                        Dictionary.populate_dict(file, f, line, br, global_dict);
                    }
                    else
                    {
                        FeatureVecteur.generateFeatureVecteur(file, f, line, br, global_dict);
                    }
                }
            }
        }
    }
}
</code></pre>

<p>This fills up that global dictionary:</p>

<pre><code>public class Dictionary 
{

    public static void populate_dict(File file, 
                                 File f, 
                                 String line, 
                                 BufferedReader br, 
                                 Set&lt;String&gt; global_dict) throws IOException
    {

        while ((line = br.readLine()) != null) 
        {
            String[] words = line.split("" "");//those are your words

            String word;

            for (int i = 0; i &lt; words.length; i++) 
            {
                word = words[i];
                if (!global_dict.contains(word))
                {
                    global_dict.add(word);
                }
            }   
        }
    }
}
</code></pre>

<p>This is an initial attempt at filling up the document specific dictionaries:</p>

<pre><code>public class FeatureVecteur 
{
    public static void generateFeatureVecteur(File file, 
                                          File f, 
                                          String line, 
                                          BufferedReader br, 
                                          Set&lt;String&gt; global_dict) throws IOException
    {
        Set&lt;String&gt; file_dict = new HashSet&lt;String&gt;();

        while ((line = br.readLine()) != null) 
        {

            String[] words = line.split("" "");//those are your words

            String word;

            for (int i = 0; i &lt; words.length; i++) 
            {
                word = words[i];
                if (!file_dict.contains(word))
                {
                    file_dict.add(word);
                }
            }   
        }
    }
}
</code></pre>
";28547411;3423866;399;ZenPylon;2;28547411;"<p>If I understand your question, you're trying to count how many instances of each word in the global dictionary occur in a given file.  I'd recommend creating an array of integers, where the index represents the index into the global dictionary and the value represents the number of occurrences of that word in the file.  </p>

<p>Then, for each word in the global dictionary, count how many times that word occurs in the file.  However, you need to be careful - feature vectors require a consistent ordering of the elements, and HashSets do not guarantee this.  In your example, for instance, ""I"" always needs to be the first element.  To solve this, you might want to convert your set to an ArrayList or some other sequential list once the global dictionary is totally finished.  </p>

<pre><code>ArrayList&lt;String&gt; global_dict_list = ArrayList&lt;String&gt;( global_dict );
</code></pre>

<p>Counting could look something like this </p>

<pre><code>int[] wordFrequency = new int[global_dict_list.size()];

for ( String globalWord : global_dict_list )
{
    for ( int i = 0; i &lt; words.length; i++ ) 
    {
         if ( words[i].equals(globalWord) ) 
         {
             wordFrequency[i]++;
         }
    }
}
</code></pre>

<p>Nest that code in the while loop that reads line by line in the feature vector code.  Hope it helps!</p>
"
4572286;11;Gabriel Ducrocq;<python><machine-learning><svm>;28545318;-1;SVM: problems with SMO algorithm;"<p>I'm currently trying to code a non linear SVM for handwritten digits recognition using the MNIST data base.</p>

<p>I chose to use the SMO algorithm (based on Platt's paper and other books), but I have some trouble implementing it.</p>

<p>When I run the code over the training set, the bias goes higher and higher, sometimes until ""Inf"" value, leading the SVM to ""classify"" every example in the same class.</p>

<p>Here is my code:</p>

<pre><code>import numpy
import gzip
import struct
import matplotlib
from sklearn import datasets
from copy import copy

class SVM:

    def __init__(self, constant, data_set, label_set):
        self._N = len(data_set)
        if self._N != len(label_set):
            raise Exception(""Data size and label size don't match."")


        self._C = constant
        self._epsilon = 0.001
        self._tol = 0.001

        self._data = [numpy.ndarray.flatten((1/255)*elt) for elt in data_set]
        self._dimension = len(self._data[0])
        self._label = label_set
        self._alphas = numpy.zeros((1, self._N))
        self._b = 0
        self._errors = numpy.ndarray((2, 0))







    def kernel(self, x1, x2):
        x1 = x1.reshape(1,self._dimension)
        result = numpy.power(numpy.dot(x1, x2), 3)

        return result






    def evaluate(self, x):
        result = 0
        i = 0
        while i &lt; self._N:
            result +=  self._alphas[0, i]*self._label[i]*self.kernel(x, self._data[i])
            i += 1

        result += self._b       
        return result






    def update(self, i1, i2, E2):
        i1 = int(i1)
        i2 = int(i2)
        if i1 == i2:
            return 0

        y1 = self._label[i1]
        y2 = self._label[i2]
        alpha1 = self._alphas[0, i1]
        alpha2 = self._alphas[0, i2]

        #If alpha1 is non-bound, its error is in the cache.
        #So we check its position to extract its error.
        #Else, we compute it.
        if alpha1 &gt; 0 and alpha1 &lt; self._C :
            position = 0
            for i, elt in enumerate(self._errors[0, :]):
                if elt == i1:
                    position = i

            E1 = self._errors[1, position]
        else:
            E1 = self.evaluate(self._data[i1]) - y1


        s = y1*y2
        H = L = 0

        if y1 != y2:
            L = max(0, alpha2 - alpha1)
            H = min(self._C, self._C + alpha2 - alpha1)
        else:
            L = max(0, alpha2 + alpha1 - self._C)
            H = min(self._C, alpha2 + alpha1)


        if H == L:
            return 0

        K11 = self.kernel(self._data[i1], self._data[i1])
        K12 = self.kernel(self._data[i1], self._data[i2])
        K22 = self.kernel(self._data[i2], self._data[i2])

        eta = K11 + K22 - 2*K12
        if eta &gt; 0:
            alpha2_new = alpha2 + (y2*(E1 - E2)/eta)
            if alpha2_new &lt; L:
                alpha2_new = L
            elif alpha2_new &gt; H:
                alpha2_new = H

        else:
            f1 = y1*(E1 + self._b) - alpha1*K11 - s*alpha2*K12
            f2 = y2*(E2 + self._b) - alpha2*K22 - s*alpha1*K12

            L1 = alpha1 + s*(alpha2 - L)
            H1 = alpha1 + s*(alpha2 - H)

            FuncL = L1*f1 + L*f2 + (1/2)*numpy.square(L1)*K11 + (1/2)*numpy.square(L)*K22 + s*L1*L*K12
            FuncH = H1*f1 + H*f2 + (1/2)*numpy.square(H1)*K11 + (1/2)*numpy.square(H)*K22 + s*H1*H*K12

            if FuncL &lt; FuncH - self._epsilon:
                alpha2_new = L
            elif FuncL &gt; FuncH + self._epsilon:
                alpha2_new = H
            else:
                alpha2_new = alpha2



        if numpy.abs(alpha2_new - alpha2) &lt; self._epsilon*(alpha2_new+alpha2+ self._epsilon):
            return 0

        alpha1_new = alpha1 + s*(alpha2 - alpha2_new)

        #Update of the threshold.
        b1 = E1 + y1*(alpha1_new - alpha1)*K11 + y2*(alpha2_new - alpha2)*K12 + self._b
        b2 = E2 + y1*(alpha1_new - alpha1)*K12 + y2*(alpha2_new - alpha2)*K22 + self._b

        if L &lt; alpha1_new &lt; H:
            b_new = b1
        elif L &lt; alpha2_new &lt; H:
            b_new = b2
        else:
            b_new = (b1+b2)/2


#Update the cache error

        #If alpha2 was bound and its new value is non-bound, we add its index and its error to the cache.
        #If alpha2 was unbound and its new value is bound, we delete it from the cache.
        if (alpha2 == 0 or alpha2 == self._C) and (alpha2_new &gt; 0 and alpha2_new &lt; self._C):
            vector_alpha2_new = numpy.array([i2, E2])
            vector_alpha2_new = vector_alpha2_new.reshape((2, 1))
            self._errors = numpy.concatenate((self._errors, vector_alpha2_new), 1)


        if (alpha2 &gt; 0 and alpha2 &lt; self._C) and (alpha2_new == 0 or alpha2_new == self._C):
            l = 0
            position = 0
            while l &lt; len(self._errors[0, :]):
                if self._errors[0, l] == i2:
                    position = l
                l += 1

            self._errors = numpy.delete(self._errors, position, 1)


        #We do the exact same thing with alpha1.
        if (alpha1 == 0 or alpha1 == self._C) and (alpha1_new &gt; 0 and alpha1_new &lt; self._C):
            vector_alpha1_new = numpy.array([i1, E1])
            vector_alpha1_new = vector_alpha1_new.reshape((2, 1))
            self._errors = numpy.concatenate((self._errors, vector_alpha1_new), 1)


        if (alpha1 &gt; 0 and alpha1 &lt; self._C) and (alpha1_new == 0 or alpha1_new == self._C):
            l = 0
            position = 0
            while l &lt; len(self._errors[0, :]):
                if self._errors[0, l] == i1:
                    position = l
                l += 1

            self._errors = numpy.delete(self._errors, position, 1)      



        #Then we update the error for each non bound point using the new values for alpha1 and alpha2.
        for i,error in enumerate(self._errors[1, :]):
            self._errors[1, i] = error + (alpha2_new - alpha2)*y2*self.kernel(self._data[i2], self._data[int(self._errors[0, i])]) + (alpha1_new - alpha1)*y1*self.kernel(self._data[i1], self._data[int(self._errors[0, i])]) - self._b + b_new


        #Storing the new values of alpha1 and alpha2:

        self._alphas[0, i1] = alpha1_new
        self._alphas[0, i2] = alpha2_new
        self._b = b_new

        print(self._errors)
        return 1




    def examineExample(self, i2):
        i2 = int(i2)
        y2 = self._label[i2]
        alpha2 = self._alphas[0, i2]

        if alpha2 &gt; 0 and alpha2 &lt; self._C:
            position = 0
            for i, elt in enumerate(self._errors[0, :]):
                if elt == i2:
                    position = i

            E2 = self._errors[1, position]
        else:
            E2 = self.evaluate(self._data[i2]) - y2

        r2 = E2*y2

        if (r2&lt; -self._tol and alpha2 &lt; self._C) or (r2 &gt; self._tol and alpha2 &gt; 0):

            n = numpy.shape(self._errors)[1]            
            if n &gt; 1:   
                i1 = 0

                if E2 &gt; 0:
                    min = self._errors[1, 0]
                    position = 0
                    for l, elt in enumerate(self._errors[1, :]):
                        if elt &lt; min:
                            min = elt
                            position = l

                    i1 = self._errors[0, position]

                else:
                    max = self._errors[1, 0]
                    position = 0
                    for l, elt in enumerate(self._errors[1, :]):
                        if elt &gt; max:
                            max = elt
                            position = l

                    i1 = self._errors[0, position]

                if self.update(i1, i2, E2):
                    return 1



            #loop over all non bound examples starting at a random point.
            list_index = [i for i in range(n)]
            numpy.random.shuffle(list_index)

            for i in list_index:
                i1 = self._errors[0, i]
                if self.update(i1, i2, E2):
                    return 1


            #Loop over all the training examples, starting at a random point.
            list_bound = [i for i in range(self._N) if not numpy.any(self._errors[0, :] == i)]
            numpy.random.shuffle(list_bound)

            for i in list_bound:
                i1 = i
                if self.update(i1, i2, E2):
                    return 1


        return 0




    def SMO(self):
        numChanged = 0
        examineAll = 1
        cpt = 1
        while(numChanged &gt; 0 or examineAll):
            numChanged = 0

            if examineAll == 1:
                for i in range(self._N):
                    numChanged += self.examineExample(i)

            else:
                for i in self._errors[0, :]:
                    numChanged += self.examineExample(i)

            if examineAll == 1:
                examineAll = 0
            elif numChanged == 0:
                examineAll = 1

            cpt += 1    

















def load_training_data(a, b):
    train = gzip.open(""train-images-idx3-ubyte.gz"", ""rb"")
    labels = gzip.open(""train-labels-idx1-ubyte.gz"", ""rb"")

    train.read(4)
    labels.read(4)

    number_images = train.read(4)
    number_images = struct.unpack(""&gt;I"", number_images)[0]

    rows = train.read(4)
    rows = struct.unpack(""&gt;I"", rows)[0]

    cols = train.read(4)
    cols = struct.unpack(""&gt;I"", cols)[0]

    number_labels = labels.read(4)
    number_labels = struct.unpack(""&gt;I"", number_labels)[0]

    image_list = []
    label_list = []
    if number_images != number_labels:
        raise Exception(""The number of labels doesn't match with the number of images"")
    else:
        for l in range(number_labels):
            if l % 1000 == 0:
                print(""l:{}"".format(l))

            mat = numpy.zeros((rows, cols), dtype = numpy.uint8)
            for i in range(rows):
                for j in range(cols):
                    pixel = train.read(1)
                    pixel = struct.unpack(""&gt;B"", pixel)[0]
                    mat[i][j] = pixel


            image_list += [mat]
            lab = labels.read(1)
            lab = struct.unpack(""&gt;B"", lab)[0]
            label_list += [lab]


    train.close()
    labels.close()


    i = 0
    index_a = []
    index_b = []
    while i &lt; number_labels:
        if label_list[i] == a:
            index_a += [i]
        elif label_list[i] == b:
            index_b += [i]

        i += 1

    image_list = [m for i,m in enumerate(image_list) if (i in index_a) | (i in index_b)]
    mean = (a+b)/2
    label_list = [ numpy.sign(m - mean) for l,m in enumerate(label_list) if l in index_a+index_b]

    return ([image_list, label_list])





def load_test_data():
    test = gzip.open(""t10k-images-idx3-ubyte.gz"", ""rb"")
    labels = gzip.open(""t10k-labels-idx1-ubyte.gz"", ""rb"")

    test.read(4)
    labels.read(4)

    number_images = test.read(4)
    number_images = struct.unpack(""&gt;I"", number_images)[0]

    rows = test.read(4)
    rows = struct.unpack(""&gt;I"", rows)[0]

    cols = test.read(4)
    cols = struct.unpack(""&gt;I"", cols)[0]

    number_labels = labels.read(4)
    number_labels = struct.unpack(""&gt;I"", number_labels)[0]

    image_list = []
    label_list = []
    if number_images != number_labels:
        raise Exception(""The number of labels doesn't match with the number of images"")
    else:
        for l in range(number_labels):
            if l % 1000 == 0:
                print(""l:{}"".format(l))

            mat = numpy.zeros((rows, cols), dtype = numpy.uint8)
            for i in range(rows):
                for j in range(cols):
                    pixel = test.read(1)
                    pixel = struct.unpack(""&gt;B"", pixel)[0]
                    mat[i][j] = pixel


            image_list += [mat]
            lab = labels.read(1)
            lab = struct.unpack(""&gt;B"", lab)[0]
            label_list += [lab]


    test.close()
    labels.close()

    return ([image_list, label_list])   

data = load_training_data(0, 7)
images_training = data[0]
labels_training = data[1]

svm = SVM(0.1, images_training[0:200], labels_training[0:200])

svm.SMO()




def view(image, label=""""):
    print(""Number : {}"".format(label))
    pylab.imshow(image, cmap = pylab.cm.gray)
    pylab.show()
</code></pre>
";;2484687;5610;Raff.Edward;3;28547591;"<p>First, SMO is a fairly complicated algorithm - it is not one easy to debug in this kind of format. </p>

<p>Second, you are starting too high up in your testing. Some advice to help you debug your problems. </p>

<p>1) First, switch to using the linear kernel. Its much easier for you to compute the exact linear solution with another algorithm and compare what you are getting with the exact solution. This way its only the weight vectors and bias term. If you stay in the dual space, you'll have to compare all the coefficients and make sure things stay in the same order. </p>

<p>2) Start with a much simpler 2D problem where you know what the general solution should look like. You can then visualize the solution, and watch as it changes at each step - this can be a visual tool to help you find where something goes wrong. </p>
"
4572286;11;Gabriel Ducrocq;<python><machine-learning><svm>;28545318;-1;SVM: problems with SMO algorithm;"<p>I'm currently trying to code a non linear SVM for handwritten digits recognition using the MNIST data base.</p>

<p>I chose to use the SMO algorithm (based on Platt's paper and other books), but I have some trouble implementing it.</p>

<p>When I run the code over the training set, the bias goes higher and higher, sometimes until ""Inf"" value, leading the SVM to ""classify"" every example in the same class.</p>

<p>Here is my code:</p>

<pre><code>import numpy
import gzip
import struct
import matplotlib
from sklearn import datasets
from copy import copy

class SVM:

    def __init__(self, constant, data_set, label_set):
        self._N = len(data_set)
        if self._N != len(label_set):
            raise Exception(""Data size and label size don't match."")


        self._C = constant
        self._epsilon = 0.001
        self._tol = 0.001

        self._data = [numpy.ndarray.flatten((1/255)*elt) for elt in data_set]
        self._dimension = len(self._data[0])
        self._label = label_set
        self._alphas = numpy.zeros((1, self._N))
        self._b = 0
        self._errors = numpy.ndarray((2, 0))







    def kernel(self, x1, x2):
        x1 = x1.reshape(1,self._dimension)
        result = numpy.power(numpy.dot(x1, x2), 3)

        return result






    def evaluate(self, x):
        result = 0
        i = 0
        while i &lt; self._N:
            result +=  self._alphas[0, i]*self._label[i]*self.kernel(x, self._data[i])
            i += 1

        result += self._b       
        return result






    def update(self, i1, i2, E2):
        i1 = int(i1)
        i2 = int(i2)
        if i1 == i2:
            return 0

        y1 = self._label[i1]
        y2 = self._label[i2]
        alpha1 = self._alphas[0, i1]
        alpha2 = self._alphas[0, i2]

        #If alpha1 is non-bound, its error is in the cache.
        #So we check its position to extract its error.
        #Else, we compute it.
        if alpha1 &gt; 0 and alpha1 &lt; self._C :
            position = 0
            for i, elt in enumerate(self._errors[0, :]):
                if elt == i1:
                    position = i

            E1 = self._errors[1, position]
        else:
            E1 = self.evaluate(self._data[i1]) - y1


        s = y1*y2
        H = L = 0

        if y1 != y2:
            L = max(0, alpha2 - alpha1)
            H = min(self._C, self._C + alpha2 - alpha1)
        else:
            L = max(0, alpha2 + alpha1 - self._C)
            H = min(self._C, alpha2 + alpha1)


        if H == L:
            return 0

        K11 = self.kernel(self._data[i1], self._data[i1])
        K12 = self.kernel(self._data[i1], self._data[i2])
        K22 = self.kernel(self._data[i2], self._data[i2])

        eta = K11 + K22 - 2*K12
        if eta &gt; 0:
            alpha2_new = alpha2 + (y2*(E1 - E2)/eta)
            if alpha2_new &lt; L:
                alpha2_new = L
            elif alpha2_new &gt; H:
                alpha2_new = H

        else:
            f1 = y1*(E1 + self._b) - alpha1*K11 - s*alpha2*K12
            f2 = y2*(E2 + self._b) - alpha2*K22 - s*alpha1*K12

            L1 = alpha1 + s*(alpha2 - L)
            H1 = alpha1 + s*(alpha2 - H)

            FuncL = L1*f1 + L*f2 + (1/2)*numpy.square(L1)*K11 + (1/2)*numpy.square(L)*K22 + s*L1*L*K12
            FuncH = H1*f1 + H*f2 + (1/2)*numpy.square(H1)*K11 + (1/2)*numpy.square(H)*K22 + s*H1*H*K12

            if FuncL &lt; FuncH - self._epsilon:
                alpha2_new = L
            elif FuncL &gt; FuncH + self._epsilon:
                alpha2_new = H
            else:
                alpha2_new = alpha2



        if numpy.abs(alpha2_new - alpha2) &lt; self._epsilon*(alpha2_new+alpha2+ self._epsilon):
            return 0

        alpha1_new = alpha1 + s*(alpha2 - alpha2_new)

        #Update of the threshold.
        b1 = E1 + y1*(alpha1_new - alpha1)*K11 + y2*(alpha2_new - alpha2)*K12 + self._b
        b2 = E2 + y1*(alpha1_new - alpha1)*K12 + y2*(alpha2_new - alpha2)*K22 + self._b

        if L &lt; alpha1_new &lt; H:
            b_new = b1
        elif L &lt; alpha2_new &lt; H:
            b_new = b2
        else:
            b_new = (b1+b2)/2


#Update the cache error

        #If alpha2 was bound and its new value is non-bound, we add its index and its error to the cache.
        #If alpha2 was unbound and its new value is bound, we delete it from the cache.
        if (alpha2 == 0 or alpha2 == self._C) and (alpha2_new &gt; 0 and alpha2_new &lt; self._C):
            vector_alpha2_new = numpy.array([i2, E2])
            vector_alpha2_new = vector_alpha2_new.reshape((2, 1))
            self._errors = numpy.concatenate((self._errors, vector_alpha2_new), 1)


        if (alpha2 &gt; 0 and alpha2 &lt; self._C) and (alpha2_new == 0 or alpha2_new == self._C):
            l = 0
            position = 0
            while l &lt; len(self._errors[0, :]):
                if self._errors[0, l] == i2:
                    position = l
                l += 1

            self._errors = numpy.delete(self._errors, position, 1)


        #We do the exact same thing with alpha1.
        if (alpha1 == 0 or alpha1 == self._C) and (alpha1_new &gt; 0 and alpha1_new &lt; self._C):
            vector_alpha1_new = numpy.array([i1, E1])
            vector_alpha1_new = vector_alpha1_new.reshape((2, 1))
            self._errors = numpy.concatenate((self._errors, vector_alpha1_new), 1)


        if (alpha1 &gt; 0 and alpha1 &lt; self._C) and (alpha1_new == 0 or alpha1_new == self._C):
            l = 0
            position = 0
            while l &lt; len(self._errors[0, :]):
                if self._errors[0, l] == i1:
                    position = l
                l += 1

            self._errors = numpy.delete(self._errors, position, 1)      



        #Then we update the error for each non bound point using the new values for alpha1 and alpha2.
        for i,error in enumerate(self._errors[1, :]):
            self._errors[1, i] = error + (alpha2_new - alpha2)*y2*self.kernel(self._data[i2], self._data[int(self._errors[0, i])]) + (alpha1_new - alpha1)*y1*self.kernel(self._data[i1], self._data[int(self._errors[0, i])]) - self._b + b_new


        #Storing the new values of alpha1 and alpha2:

        self._alphas[0, i1] = alpha1_new
        self._alphas[0, i2] = alpha2_new
        self._b = b_new

        print(self._errors)
        return 1




    def examineExample(self, i2):
        i2 = int(i2)
        y2 = self._label[i2]
        alpha2 = self._alphas[0, i2]

        if alpha2 &gt; 0 and alpha2 &lt; self._C:
            position = 0
            for i, elt in enumerate(self._errors[0, :]):
                if elt == i2:
                    position = i

            E2 = self._errors[1, position]
        else:
            E2 = self.evaluate(self._data[i2]) - y2

        r2 = E2*y2

        if (r2&lt; -self._tol and alpha2 &lt; self._C) or (r2 &gt; self._tol and alpha2 &gt; 0):

            n = numpy.shape(self._errors)[1]            
            if n &gt; 1:   
                i1 = 0

                if E2 &gt; 0:
                    min = self._errors[1, 0]
                    position = 0
                    for l, elt in enumerate(self._errors[1, :]):
                        if elt &lt; min:
                            min = elt
                            position = l

                    i1 = self._errors[0, position]

                else:
                    max = self._errors[1, 0]
                    position = 0
                    for l, elt in enumerate(self._errors[1, :]):
                        if elt &gt; max:
                            max = elt
                            position = l

                    i1 = self._errors[0, position]

                if self.update(i1, i2, E2):
                    return 1



            #loop over all non bound examples starting at a random point.
            list_index = [i for i in range(n)]
            numpy.random.shuffle(list_index)

            for i in list_index:
                i1 = self._errors[0, i]
                if self.update(i1, i2, E2):
                    return 1


            #Loop over all the training examples, starting at a random point.
            list_bound = [i for i in range(self._N) if not numpy.any(self._errors[0, :] == i)]
            numpy.random.shuffle(list_bound)

            for i in list_bound:
                i1 = i
                if self.update(i1, i2, E2):
                    return 1


        return 0




    def SMO(self):
        numChanged = 0
        examineAll = 1
        cpt = 1
        while(numChanged &gt; 0 or examineAll):
            numChanged = 0

            if examineAll == 1:
                for i in range(self._N):
                    numChanged += self.examineExample(i)

            else:
                for i in self._errors[0, :]:
                    numChanged += self.examineExample(i)

            if examineAll == 1:
                examineAll = 0
            elif numChanged == 0:
                examineAll = 1

            cpt += 1    

















def load_training_data(a, b):
    train = gzip.open(""train-images-idx3-ubyte.gz"", ""rb"")
    labels = gzip.open(""train-labels-idx1-ubyte.gz"", ""rb"")

    train.read(4)
    labels.read(4)

    number_images = train.read(4)
    number_images = struct.unpack(""&gt;I"", number_images)[0]

    rows = train.read(4)
    rows = struct.unpack(""&gt;I"", rows)[0]

    cols = train.read(4)
    cols = struct.unpack(""&gt;I"", cols)[0]

    number_labels = labels.read(4)
    number_labels = struct.unpack(""&gt;I"", number_labels)[0]

    image_list = []
    label_list = []
    if number_images != number_labels:
        raise Exception(""The number of labels doesn't match with the number of images"")
    else:
        for l in range(number_labels):
            if l % 1000 == 0:
                print(""l:{}"".format(l))

            mat = numpy.zeros((rows, cols), dtype = numpy.uint8)
            for i in range(rows):
                for j in range(cols):
                    pixel = train.read(1)
                    pixel = struct.unpack(""&gt;B"", pixel)[0]
                    mat[i][j] = pixel


            image_list += [mat]
            lab = labels.read(1)
            lab = struct.unpack(""&gt;B"", lab)[0]
            label_list += [lab]


    train.close()
    labels.close()


    i = 0
    index_a = []
    index_b = []
    while i &lt; number_labels:
        if label_list[i] == a:
            index_a += [i]
        elif label_list[i] == b:
            index_b += [i]

        i += 1

    image_list = [m for i,m in enumerate(image_list) if (i in index_a) | (i in index_b)]
    mean = (a+b)/2
    label_list = [ numpy.sign(m - mean) for l,m in enumerate(label_list) if l in index_a+index_b]

    return ([image_list, label_list])





def load_test_data():
    test = gzip.open(""t10k-images-idx3-ubyte.gz"", ""rb"")
    labels = gzip.open(""t10k-labels-idx1-ubyte.gz"", ""rb"")

    test.read(4)
    labels.read(4)

    number_images = test.read(4)
    number_images = struct.unpack(""&gt;I"", number_images)[0]

    rows = test.read(4)
    rows = struct.unpack(""&gt;I"", rows)[0]

    cols = test.read(4)
    cols = struct.unpack(""&gt;I"", cols)[0]

    number_labels = labels.read(4)
    number_labels = struct.unpack(""&gt;I"", number_labels)[0]

    image_list = []
    label_list = []
    if number_images != number_labels:
        raise Exception(""The number of labels doesn't match with the number of images"")
    else:
        for l in range(number_labels):
            if l % 1000 == 0:
                print(""l:{}"".format(l))

            mat = numpy.zeros((rows, cols), dtype = numpy.uint8)
            for i in range(rows):
                for j in range(cols):
                    pixel = test.read(1)
                    pixel = struct.unpack(""&gt;B"", pixel)[0]
                    mat[i][j] = pixel


            image_list += [mat]
            lab = labels.read(1)
            lab = struct.unpack(""&gt;B"", lab)[0]
            label_list += [lab]


    test.close()
    labels.close()

    return ([image_list, label_list])   

data = load_training_data(0, 7)
images_training = data[0]
labels_training = data[1]

svm = SVM(0.1, images_training[0:200], labels_training[0:200])

svm.SMO()




def view(image, label=""""):
    print(""Number : {}"".format(label))
    pylab.imshow(image, cmap = pylab.cm.gray)
    pylab.show()
</code></pre>
";;9329856;101;PhD Rookie;1;51901048;"<p>One important thing is you said this:</p>

<pre><code>    b1 = E1 + y1*(alpha1_new - alpha1)*K11 + y2*(alpha2_new - alpha2)*K12 + self._b
    b2 = E2 + y1*(alpha1_new - alpha1)*K12 + y2*(alpha2_new - alpha2)*K22 + self._b
</code></pre>

<p>Basically you're just adding to b every time with this code. Your b's should look more like this:</p>

<pre><code>    b1 = smo.b - E1 - y1 * (a1 - alpha1) * smo.K[i1, i1] - y2 * (a2 - alpha2) * smo.K[i1, i2]
    b2 = smo.b - E2 - y1 * (a1 - alpha1) * smo.K[i1, i2] - y2 * (a2 - alpha2) * smo.K[i2, i2]
</code></pre>

<p>This version is not perfect, but I recommend checking apex51's version on Github for pointers:
<a href=""https://github.com/apex51/SVM-and-sequential-minimal-optimization"" rel=""nofollow noreferrer"">SVM-and-sequential-minimal-optimization</a></p>

<p>The mathematical basis in the notes are very strong (despite some minor discrepancies with Platt's paper) and the code is not perfect, but a good direction for you. I would also suggest looking at other, completed SMOs and trying to tweak that code to math your needs instead of writing from scratch.</p>
"
1014747;496;edgaralienfoe;<machine-learning><nlp><normalization><feature-extraction>;28552860;0;NLP: How to correctly normalise a feature for gender classification?;"<p><strong>NOTE</strong> Before I begin, this F-measure is not related to precision and recall, and its title and definition is taken from this <a href=""http://pespmc1.vub.ac.be/papers/contextuality-fos.pdf"" rel=""nofollow"">paper</a>. </p>

<p>I have a feature known as the F-measure, which is used to measure formality in a given text. It is mostly used in gender classification of text which is what I'm working on as a project. </p>

<p>The <strong>F-measure</strong> is defined as:</p>

<p>F = 0.5 * (noun freq. + adjective freq. + preposition freq. + article freq. â€“ pronoun
freq. â€“ verb freq. â€“ adverb freq. â€“ interjection freq. + 100)</p>

<p>where the frequencies are taken from a given text (for example, a blog post).</p>

<p>I would like to normalize this feature for use in a classification task. Initially, my first thought was that since the value <em>F</em> is bound by the number of words in the given text (text_length), I thought of first taking <em>F</em> and dividing by text_length. Secondly, and finally, since this measure can take on both positive and negative values (as can be inferred from the equation) I then thought of squaring (<em>F</em>/text_length) to only get a positive value.</p>

<p>Trying this I found that the normalised values did not seem to be too correct as I started getting really small values in (below 0.10) for all the cases I tested the feature with and I am thinking that the reason might be because I am squaring the value which would essentially make it smaller since its the square of a fraction. However this is required if I want to guarantee positive values only. I am not sure what else to consider to improve the normalisation such that a nice distribution within [0,1] is produced, and would like to know if there is some kind of strategy involved to correctly normalise NLP features.</p>

<p>How should I approach the normalisation of my feature, and what might I be doing wrong?</p>
";28556247;977052;8904;Vsevolod Dyomkin;1;28556247;"<p>If you carefully read the article, you'll find that the measure is already normalized:</p>

<blockquote>
  <p>F will then vary between 0 and 100%</p>
</blockquote>

<p>The reason for this is that ""frequencies"" in the formula are calculated as follows:</p>

<blockquote>
  <p>The frequencies are here expressed as percentages of the number of words belonging to a particular category with respect to the total number of words in the excerpt.</p>
</blockquote>

<p>I.e. you should normalize them by the total number of words (just as you suggested). But afterwards don't forget to multiply each one by 100.</p>
"
1276300;125;user115188;<matlab><machine-learning><svm><libsvm>;28556266;0;Plot SVM margins using MATLAB and libsvm;"<p>I am using svmlib to classify linearly two dimensional non-separable data. I am able to train the svm and obtain w and b using svmlib. Using this information I can plot the decision boundary, along with the support vectors, but I am not sure about how to plot the margins, using the information that svmlib gives me.</p>

<p>Below is my code:</p>

<pre><code>model = svmtrain(Y,X, '-s 0 -t 0 -c 100');

w = model.SVs' * model.sv_coef;
b = -model.rho;
if (model.Label(1) == -1)
    w = -w; b = -b;
end
y_hat = sign(w'*X' + b);

sv = full(model.SVs);

% plot support vectors
plot(sv(:,1),sv(:,2),'ko', 'MarkerSize', 10);

% plot decision boundary
plot_x = linspace(min(X(:,1)), max(X(:,1)), 30);
plot_y = (-1/w(2))*(w(1)*plot_x + b);
plot(plot_x, plot_y, 'k-', 'LineWidth', 1)
</code></pre>
";28557403;501663;9232;Itamar Katz;3;28557403;"<p>It depends on what you mean by ""the margins"". It also depends on what SVM version you are talking about (separable on non-separable), but since you mentioned libsvm I'll assume you mean the more general, non-separable version.</p>

<p>The term ""margin"" can refer to the Euclidean distance from the separating hyperplane to the hyperplane defined by <code>wx+b=1</code> (or <code>wx+b=-1</code>). This distance is given by <code>1/norm(w)</code>. </p>

<p>""Margin"" can also refer to the margin of a specific sample <code>x</code>, which is the Euclidean distance of <code>x</code> from the separating hyperplane. It is given by </p>

<blockquote>
  <p>(wx+b)/norm(w)</p>
</blockquote>

<p>note that this is a signed distance, that is it is negative/positive, depending on which side of the hyperplane the point <code>x</code> resides. You can draw it as a line from the point, perpendicular to the hyperplane.</p>

<p>Another interesting value is the slack variable <code>xi</code>, which is the ""algebraic"" distance (not Euclidean) of a support vector from the ""hard"" margin defined by <code>wx+b=+1</code> (or <code>-1</code>). It is positive only for support vectors, and if a point is not a support vector, its <code>xi</code> equals 0. More compactly:</p>

<blockquote>
  <p>xi = max(0, 1 - y*(w'*x+b))</p>
</blockquote>

<p>where <code>y</code> is the label.</p>
"
3791422;21;user3791422;<machine-learning><svm>;28558540;-2;Precision and recall for outlier detection;"<p>I am trying to calculate precision, recall and f1-score for outlier detection (in my case attacks in a network) using a one-class SVM. I encounter a problem in doing that in a rigorous manner. I explain myself. Since precision is calculated like:</p>

<p>precision = true_positive /(true_positive + false_positive)</p>

<p>if I do my tests using a dataset that I already know that has a few number of attacks then the number of false_positive will be really big in comparison with the true_positive, therefore precision will be very low. 
However, if I use a dataset that I already know that has lots of attacks, without changing my detection algorithm the number of true_positive will increase and then the precision will be higher. </p>

<p>I know that something must be wrong in the way that I calculate precision. What am I missing? </p>

<p>Thanks in advance!</p>
";;270287;41194;IVlad;0;28558942;"<blockquote>
  <p>if I do my tests using a dataset that I already know that has a few number of attacks then the number of false_positive will be really big in comparison with the true_positive, therefore precision will be very low.</p>
</blockquote>

<p>That is (probably) expected behavior, because your data set is skewed. However, you should get a <a href=""http://en.wikipedia.org/wiki/Precision_and_recall"" rel=""nofollow"">recall</a> value that is acceptable.</p>

<blockquote>
  <p>However, if I use a dataset that I already know that has lots of attacks, without changing my detection algorithm the number of true_positive will increase and then the precision will be higher.</p>
</blockquote>

<p>And in this case, I bet recall will be low.</p>

<p>Based on what you describe, there are a few issues and things you can do. I can address more specific issues if you add more information to your question:</p>

<ol>
<li><p>Why are you using multiple test sets, all of which are unbalanced? You should use something that is balanced, or even better, use <a href=""http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29"" rel=""nofollow"">k-fold cross validation</a> with your entire data set. Use it to find the best parameters for your model.</p></li>
<li><p>To decide if you have a good enough balance between precision and recall, consider using the <a href=""http://en.wikipedia.org/wiki/F1_score"" rel=""nofollow"">F1 score</a>.</p></li>
<li><p>Use a <a href=""http://en.wikipedia.org/wiki/Confusion_matrix"" rel=""nofollow"">confusion matrix</a> to decide if your measures are acceptable.</p></li>
<li><p>Plot <a href=""http://en.wikipedia.org/wiki/Learning_curve"" rel=""nofollow"">learning curves</a> to help avoid overfitting.</p></li>
</ol>
"
4360034;853;Steg Verner;<java><python><c++><algorithm><machine-learning>;28560963;0;Clustering lists having maximum overlap with size restriction;"<p>I have the following group of numbers:</p>

<pre><code>group1: 12 56 57 58 59 60 61 62 63 64 75 89 91 100 105 107 108 Group Size: 40
group2: 56 57 60 71 72 73 74 91 92 93 94 100 105 107 108 110 111 Group Size: 30
group3: 57 58 91 107 108 110 112 114 117 118 120 127 129 139 184 Group Size: 15
group4: 1 2 4 6 7 8 9 10 17 18 20 41 42 43 45 47 Group Size: 40
group5: 57 58 91 201 205 207 210 212 214 216 217 218 219 220 221 225 Group Size: 30
.
groupN: 50 51 52 53 54 210 214 216 219 225 700 701 702 705 706 708 Group Size: 40
</code></pre>

<p>Now I want to cluster together groups having maximum overlap such that after clustering, maximum size within a cluster does not exceed 90. For example here, the clusters are: (group1,group2,group3),(group5,groupN) and group4. The overlapping elements in the 3 groups are shown below:</p>

<pre><code>Cluster1: (group1,group2,group3): 57 91 107 108 Cluster Size: (Group1_size+group2_size+group3_size =85 &lt;90) 
Cluster2: group4: 1 2 4 6 7 8 9 10 17 18 20 41 42 43 45 47 Cluster Size: (group4_size &lt; 40)
Cluster3: (group5,groupN): 201 214 216 219 225 Cluster Size: (group5_size + groupN_size 70 &lt;90)
</code></pre>

<p>If I include group5 in cluster1 then its size will be 85+30=115 and I want to return a size&lt;90, therefore I can not include group4 in cluster1.
The elements in the respective clusters after removing the duplicate overlapping elements are:</p>

<pre><code>Cluster1: (group1, group2, group3): 12 56 57 58 59 60 61 62 63 64 71 72 73 74 75 89 91 92 93 94 100 105 107 108 110 111 112 114 117 118 120 127 129 139 184
Cluster2: group4: 1 2 4 6 7 8 9 10 17 18 20 41 42 43 45 47
Cluster3: (group5,groupN): 50 51 52 53 54 57 58 91 201 205 207 210 212 214 216 217 218 219 220 221 225 700 701 702 705 706 708
</code></pre>

<p>Is there some existing algorithm or technique which may help me achieve this clustering with size constraint. </p>

<p>I tried to form clusters by finding the common elements between any two groups and including in the group if cluster size after inclusion is &lt;90. But is there any existing algorithm in any of the programming language libraries like C++,python,java which may help me achieve this efficiently. If not, then is there any existing algorithm which achieves the same.</p>

<p>If possible, it will be great if the algorithm is optimal also.</p>
";;786351;7539;ElKamina;0;28573080;"<p>There is no easy optimal solution. One approximation is as follows:</p>

<ol>
<li>Pick the group with the largest size. Let its size be x</li>
<li>Pick the largest group such that its size is less than 90-x</li>
<li>Keep repeating step 2 until you cannot find such a group</li>
<li>Remove the selected groups and repeat the process starting from Step 1</li>
</ol>

<p>Eg. You would pick group1 (or group4 or groupN) first is step 1. In step 2 you would pick group4. Now the size is 80 and there are no groups smaller than 90-80=10. So stop and remove these two groups. In the next iteration, you will select groupN, followed by group2, and at last group3. In the last iteration you have only one group, that is group5.</p>
"
4030468;59;Luigi Biasi;<machine-learning><speech-recognition><mfcc>;28562023;0;Simple word detector using MFCC;"<p>I am implementing a software for speech recognition using Mel Frequency Cepstrum Coefficients. In particular the system must recognize a single specified word. Since the audio file I get the MFCCs in a matrix with 12 rows(the MFCCs) and as many columns as the number of voice frames. I make the average of the rows, so I get a vector with only the 12 rows (the ith-row is the average of all ith-MFCCs of all frames). My question is how to train a classifier to detect the word? I have a training set with only positive samples, the MFCCs that i get from several audio file (several registration of the same word).</p>
";;432021;23684;Nikolay Shmyrev;1;28562928;"<blockquote>
  <p>I make the average of the rows, so I get a vector with only the 12 rows (the ith-row is the average of all ith-MFCCs of all frames). </p>
</blockquote>

<p>This is a very bad idea because you lose all information about the word, you need to analyze the whole mfcc sequence, not a part of it</p>

<blockquote>
  <p>My question is how to train a classifier to detect the word? </p>
</blockquote>

<p>The simple form would be a GMM classifier, you can check here:</p>

<p><a href=""http://www.mathworks.com/company/newsletters/articles/developing-an-isolated-word-recognition-system-in-matlab.html"" rel=""nofollow noreferrer"">http://www.mathworks.com/company/newsletters/articles/developing-an-isolated-word-recognition-system-in-matlab.html</a></p>

<p>In more complex form you need to learn more complex model like HMM. You can learn more about HMM from textbook like this one</p>

<p><a href=""https://rads.stackoverflow.com/amzn/click/com/0130151572"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">http://www.amazon.com/Fundamentals-Speech-Recognition-Lawrence-Rabiner/dp/0130151572</a></p>
"
4474553;783;fakeaccount;<c++><opencv><machine-learning><svm><training-data>;28566496;2;SVM training C++ OpenCV;"<p>I was under the impression the <code>training data</code> given to train an <code>SVM</code> consisted of image features, but after <a href=""https://stackoverflow.com/questions/14694810/using-opencv-and-svm-with-images"">reading this post</a> again, the <code>training_mat</code> that is given to the <code>SVM</code> in the example is just the <code>img_mat</code> flattened to 1-Dimension.</p>

<p>So my question is, when training an <code>SVM</code>, do you give it whole images in their entirety, row by row, or do you <code>detect</code> and <code>extract</code> the features, and then flatten a <code>Mat</code> of that into 1-Dimension?</p>
";28569577;3646384;557;Amin Suzani;3;28569577;"<p>You can extract features, or you can use pixel intensity values as the features. In this example, they have done the latter. In this case, you end up with a very high number of features that many of them may be not useful. This makes the convergence of the SVM training more difficult, but can be still possible. Based on my personal experience, SVM works better if you extract a lower number of ""good"" features that best describe your data. However, in recent years, it has been shown that state-of-the-art estimators like deep neural networks (when used instead of SVM) can perform very well with only using the pixel intensity values as features. This has eliminated the need for feature extraction in the methods that has led to state-of-the-art results on public data sets (like <a href=""http://www.image-net.org/"" rel=""nofollow"">ImageNet</a>)</p>
"
1363559;1037;danielchalef;<machine-learning><scikit-learn><classification><prediction><random-forest>;28568034;2;Getting Scikit-Learn RandomForestClassifier to output Top N results;"<p>I'd like to see the top N results for a RandomForestClassifier prediction, ordered by descending probability. </p>

<p>The answer may be predict_proba, but I have no idea how to interpret the results.</p>

<p>Help appreciated! </p>
";;333308;5345;aganders3;6;28568653;"<p>I think you're right to use <code>predict_proba</code>. Use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html#numpy.argsort"" rel=""noreferrer""><code>np.argsort</code></a> to interpret the results:</p>

<pre><code>p = rfc.predict_proba(X)
n = 3
top_n = np.argsort(p)[:,:-n-1:-1]
</code></pre>
"
743354;153;JohnDoeKazama;<machine-learning><cluster-analysis><data-mining>;28572014;0;Clustering unique datasets based on similarities (equality);"<p>I just entered into the space of data mining, machine learning and clustering. I'm having special problem, and do not know which technique to use it for solving it.</p>

<p>I want to perform clustering of observations (objects or whatever) on specific data format. All variables in each observation is numeric. My data input looks like this:</p>

<p>1   2   3   4   5   6</p>

<p>1   3   5   7</p>

<p>2   9   10  11  12  13  14</p>

<p>45  1   22  23  24</p>

<p>Let's say that <strong>n</strong> represent row (observation, or 1D vector,..) and <strong>m</strong> represents column (variable index in each vector). <strong>n</strong> could be very large number, and <strong>0 &lt; m &lt; 100</strong>. Also main point is that same observation (row) cannot have identical values (in 1st row, one value could appear only once).</p>

<p>So, I want to somehow perform clustering where I'll put observations in one cluster based on number of identical values which contain each row/observation.</p>

<p>If there are two rows like:</p>

<p>1</p>

<p>1 2 3 4 5</p>

<p>They should be clustered in same cluster, if there are no match than for sure not. Also number of each rows in one cluster should not go above 100.</p>

<p>Sick problem..? If not, just for info that I didn't mention time dimension. But let's skip that for now.</p>

<p>So, any directions from you guys,</p>

<p>Thanks and best regards,
JDK</p>
";;1060350;70512;Has QUIT--Anony-Mousse;1;28578261;"<p>Its hard to recommend anything since your problem is totally vague, and we have no information on the data. Data mining (and in particular <em>explorative</em> techniques like clustering) is <em>all</em> about understanding the data. So we cannot provide the ultimate answer.</p>

<p>Two things for you to consider:
1. if the data indicates presence of species or traits, Jaccard similarity (and other set based metrics) are worth a try.
2. if absence is less informative, maybe you should be mining association rules, not clusters</p>

<p>Either way, without understanding your data these numbers are as good as random numbers. You can easily cluster random numbers, and spend weeks to get the best useless result!</p>
"
743354;153;JohnDoeKazama;<machine-learning><cluster-analysis><data-mining>;28572014;0;Clustering unique datasets based on similarities (equality);"<p>I just entered into the space of data mining, machine learning and clustering. I'm having special problem, and do not know which technique to use it for solving it.</p>

<p>I want to perform clustering of observations (objects or whatever) on specific data format. All variables in each observation is numeric. My data input looks like this:</p>

<p>1   2   3   4   5   6</p>

<p>1   3   5   7</p>

<p>2   9   10  11  12  13  14</p>

<p>45  1   22  23  24</p>

<p>Let's say that <strong>n</strong> represent row (observation, or 1D vector,..) and <strong>m</strong> represents column (variable index in each vector). <strong>n</strong> could be very large number, and <strong>0 &lt; m &lt; 100</strong>. Also main point is that same observation (row) cannot have identical values (in 1st row, one value could appear only once).</p>

<p>So, I want to somehow perform clustering where I'll put observations in one cluster based on number of identical values which contain each row/observation.</p>

<p>If there are two rows like:</p>

<p>1</p>

<p>1 2 3 4 5</p>

<p>They should be clustered in same cluster, if there are no match than for sure not. Also number of each rows in one cluster should not go above 100.</p>

<p>Sick problem..? If not, just for info that I didn't mention time dimension. But let's skip that for now.</p>

<p>So, any directions from you guys,</p>

<p>Thanks and best regards,
JDK</p>
";;1148123;387;Ddavid;1;28654476;"<p>Can your problem be treated as a Bag-of-words model, where each article  (observation row) has no more than 100 terms?</p>

<p>Anyway, I think your have to give more information and examples about ""why"" and ""how"" you want to cluster these data. For example, we have:</p>

<pre><code>1 2 3
2 3 4
2 3 4 5
1 2 3 4
3 4 6
6 7 8
9 10
9 11
10 12 13 14
</code></pre>

<p>What is your expected clustering? How many clusters are there in this clustering? Only two clusters?</p>

<p>Before you give more information, according to you current description, I think you do not need a cluster algorithm, but a structure of connected components. The first round you process the dataset to get the information of connected components, and you need a second round to check each row belong to which connected components. Take the example above, first round:</p>

<pre><code>1 2 3   : 1 &lt;- 1, 1 &lt;- 2, 1 &lt;- 3 (all point linked to the smallest point to
          represent they are belong to the same cluster of the smallest point)
2 3 4   : 2 &lt;- 4 (2 and 3 have already linked to 1 which is &lt;= 2, so they do
          not need to change)
2 3 4 5 : 2 &lt;- 5
1 2 3 4 : 1 &lt;- 4 (in fact this change are not essential because we have
          1 &lt;- 2 &lt;- 4, but change this can speed up the second round)
3 4 6   : 3 &lt;- 6
6 7 8   : 6 &lt;- 7, 6 &lt;- 8
9 10    : 9 &lt;- 9, 9 &lt;- 10
9 11    : 9 &lt;- 11
10 11 12 13 14 : 10 &lt;- 12, 10 &lt;- 13, 10 &lt;- 14
</code></pre>

<p>Now we have a forest structure to represent the connected components of points. The second round you can easily pick up one point in each row (the smallest one is the best) and trace its root in the forest. The rows which have the same root are in the same, in your words, cluster. For example:</p>

<pre><code>1 2 3   : 1 &lt;- 1, cluster root 1
2 3 4 5 : 1 &lt;- 1 &lt;- 2, cluster root 1
6 7 8   : 1 &lt;- 1 &lt;- 3 &lt;- 6, cluster root 1
9 10    : 9 &lt;- 9, cluster root 9
10 11 12 13 14 : 9 &lt;- 9 &lt;- 10, cluster root 9
</code></pre>

<p>This process takes <em>O(k)</em> space where <em>k</em> is the number of points, and <em>O(nm + nh)</em> time, where <em>r</em> is the height of the forest structure, where <em>r</em> &lt;&lt; <em>m</em>.
I am not sure if this is the result you want.</p>
"
4566073;235;frog1944;<c++><machine-learning><dlib>;28577371;1;Dlib Element Specific Operations;"<p>Using the dlib library how do you do element specific operations? E.g.</p>

<p>A = [1 2 3; 4 5 6]</p>

<p>Instead of it going A*A, can I get it to square the elements of the matrix so that the answer would be</p>

<p>ans = [1 4 9; 16 25 36]</p>

<p>In matlab you could simply go A.^2</p>

<p>Thanks</p>
";;4566073;235;frog1944;1;28714348;"<p>You can use pointwise_multiply(), which is in the library ""matrix.h"".
e.g.</p>

<pre><code>matrix&lt;double&gt; A(3,2);
A = 1,    2,
    3,    4,
    5,    6;

matrix&lt;double&gt; B(3,2);
B = 1,    2,
    3,    4,
    5,    6;

matrix&lt;double&gt; answer = pointwise_multiply(A,B);
</code></pre>

<p>Or alternatively</p>

<pre><code>matrix&lt;double&gt; answer = squared(A);
</code></pre>
"
891823;1201;Erick Asto Oblitas;<opencv><machine-learning><neural-network><classification><encog>;28582559;0;Image Classification example in ENCOG(or any Framework)?;"<p>I need classify images from a video camera, the main features to consider are:</p>

<ul>
<li>Object Form (basic shape like triangle, square, etc)</li>
<li>Object Color</li>
<li>Few Deformations</li>
</ul>

<p>I'm already working in shape recognition with opencv, following this <a href=""https://www.youtube.com/watch?v=X6rPdRZzgjg"" rel=""nofollow noreferrer"">Real Time Tracking Tutorial</a> and <a href=""https://stackoverflow.com/questions/11424002/how-to-detect-simple-geometric-shapes-using-opencv"">this</a>:</p>

<p>My goal is, if I show a tiny or big square shape in front of camera, then it would recognized it as a square of color '....', if I show a eared/deformed paper(square or triangle) then it would recognized this shape as a triangle of color '....'.</p>

<p>I'm searching how to do Image Classification with Encog, but what I found was classification using quantitative attributes like, measure (lenght, width) not by shape form.</p>

<p>The encog example are <a href=""http://www.heatonresearch.com/wiki/Workbench_Classification_Example"" rel=""nofollow noreferrer"">this</a> (available in Pluralsight).</p>

<p>In this encog example the training data are like:</p>

<pre><code>Sepal Length    Sepal Width Petal Length    Petal Width Species
5.1             3.5         1.4             0.2         setosa
4.9             3.0         1.4             0.2         setosa
4.7             3.2         1.3             0.2         setosa
7.0             3.2         4.7             1.4         versicolor
6.4             3.2         4.5             1.5         versicolor
6.9             3.1         4.9             1.5         versicolor
6.3             3.3         6.0             2.5         virginica
5.8             2.7         5.1             1.9         virginica
7.1             3.0         5.9             2.1         virginica
</code></pre>

<p>In my case the training data would be pixel (mat type of encog) also my evaluation data.</p>

<p>How to normalize pixel for encog training data?</p>

<p>I need some clue, tutorial.
Many thanks.</p>
";28602568;4367179;1031;Denis Tarasov;2;28602568;"<p>Short answer:
From purely technical perspective you need to subsample images to about 100x100 pixels, convert to grayscale (for shape recognition), get all pixels to a single vector and normalize largest integer pixels value to 1.0 (for example if you pixel values are in range [0..255] you divide everything by 255). For color images  one usually creates three vectors, one for each channel (RGB), normalize them in the same way, concatenate and feed into neural network (MLP) classifier with at least one hidden layer. Thats all very similar to simple example you provide, only uses lot more data.</p>

<p>Long answer: above is probably the best thing you can do with Encog and given enough samples and suffucient CPU/GPU resources, this should work for your task. However, image recognition is currently open problem and no single universal method exists that will solve everything. Most work is done nowdays with convolutional neural nets (not supported by Encog), and there are certain number of important things to consider, so you might want to read some classic image recogition papers to get some important ideas. In case you need help with theory, I think its best to ask your question <a href=""https://stats.stackexchange.com/"">here</a> as theory like that and tutorials are to my best knowledge outside the scope of SO</p>
"
4580819;33;Rob;<azure><azure-machine-learning-studio>;28590690;2;Azure ML App - Complete Experince - Train automatically and Consume;"<p>I played a bit around with Azure ML studio. So as I understand the process goes like this:</p>

<p>a) Create training experiment. Train it with data. </p>

<p>b) Create Scoring experiment. This will include the trained model from the training experiment. Expose this as a service to be consumed over REST.</p>

<p>Maybe a stupid question but what is the recommended way to get the complete experience like the one i get when I use an app like <a href=""https://datamarket.azure.com/dataset/amla/mba"" rel=""nofollow"">https://datamarket.azure.com/dataset/amla/mba</a> (Frequently Bought Together API built with Azure Machine Learning). </p>

<p>I mean the following:</p>

<p>a) Expose 2 or more services - one to train the model and the other to consume (test) the trained model. </p>

<p>b) User periodically sends training data to train the model </p>

<p>c) The trained model/models now gets saved available for consumption</p>

<p>d) User is now able to send a dataframe to get the predicted results.</p>

<p>Is there an additional wrapper that needs to be built?</p>

<p>If there is a link documenting this please point me to the same. </p>
";28844298;4630014;91;Roope;1;28844298;"<p>The Azure ML retraining API is designed to handle the workflow you describe:</p>

<p><a href=""http://azure.microsoft.com/en-us/documentation/articles/machine-learning-retrain-models-programmatically/"" rel=""nofollow"">http://azure.microsoft.com/en-us/documentation/articles/machine-learning-retrain-models-programmatically/</a></p>

<p>Hope this helps,</p>

<p>Roope - Microsoft Azure ML Team</p>
"
4580819;33;Rob;<azure><azure-machine-learning-studio>;28590690;2;Azure ML App - Complete Experince - Train automatically and Consume;"<p>I played a bit around with Azure ML studio. So as I understand the process goes like this:</p>

<p>a) Create training experiment. Train it with data. </p>

<p>b) Create Scoring experiment. This will include the trained model from the training experiment. Expose this as a service to be consumed over REST.</p>

<p>Maybe a stupid question but what is the recommended way to get the complete experience like the one i get when I use an app like <a href=""https://datamarket.azure.com/dataset/amla/mba"" rel=""nofollow"">https://datamarket.azure.com/dataset/amla/mba</a> (Frequently Bought Together API built with Azure Machine Learning). </p>

<p>I mean the following:</p>

<p>a) Expose 2 or more services - one to train the model and the other to consume (test) the trained model. </p>

<p>b) User periodically sends training data to train the model </p>

<p>c) The trained model/models now gets saved available for consumption</p>

<p>d) User is now able to send a dataframe to get the predicted results.</p>

<p>Is there an additional wrapper that needs to be built?</p>

<p>If there is a link documenting this please point me to the same. </p>
";28844298;655312;1952;Ravi Singh;1;29339066;"<p>You need to take a look at <a href=""http://azure.microsoft.com/en-in/services/data-factory/"" rel=""nofollow"">Azure Data Factory</a>. </p>

<p>I have written a <a href=""http://azure.microsoft.com/en-in/documentation/articles/data-factory-use-custom-activities/"" rel=""nofollow"">Custom Activity</a> to do the same. </p>

<p>And used the logic to <a href=""http://azure.microsoft.com/en-us/documentation/articles/machine-learning-retrain-models-programmatically/"" rel=""nofollow"">retrain the model</a> in the custom activity.</p>
"
4192446;123;Linguist;<machine-learning><libsvm><liblinear>;28598670;1;Liblinear Error;"<p>I am training my datasets in Liblinear: train heart_scale, but i am getting this error</p>

<pre><code>can't open /home/linguistics/.train//train.ini
ERROR: Init file not found (/home/linguistics/.train//train.ini)
train&gt;
</code></pre>

<p>Reference: 
README file from Liblinear's source code. I downloaded it from here:
<a href=""http://www.csie.ntu.edu.tw/~cjlin/cgi-bin/liblinear.cgi?+http://www.csie.ntu.edu.tw/~cjlin/liblinear+tar.gz"" rel=""nofollow"">http://www.csie.ntu.edu.tw/~cjlin/cgi-bin/liblinear.cgi?+http://www.csie.ntu.edu.tw/~cjlin/liblinear+tar.gz</a></p>
";28599113;3839900;1007;Bhindi;1;28599113;"<p>try  ./train dataset_name   instead of  train dataset_name</p>
"
4140027;3985;tumbleweed;<python><machine-learning><nlp><scikit-learn>;28599754;2;Problems with an unbalanced dataset with scikit-learn Random forest?;"<p>I have an unbalanced textual dataset this is how it looks:</p>

<pre><code>label | texts(documents)
----------
5     |1190
4     |839
3     |239
1     |204
2     |127
</code></pre>

<p>I tried to use the <code>fit(X, y[, sample_weight])</code> parameter but I did not understand in the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"" rel=""nofollow"">documentation</a> how does this is expected. I tried the following:</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import balance_weights

classifier=RandomForestClassifier(n_estimators=10,criterion='entropy')
classifier.fit(X_train, y_train,sample_weight = balance_weights(y))
prediction = classifier.predict(X_test)
</code></pre>

<p>But I get this exception:</p>

<pre><code>/usr/local/lib/python2.7/site-packages/sklearn/utils/__init__.py:93: DeprecationWarning: Function balance_weights is deprecated; balance_weights is an internal function and will be removed in 0.16
  warnings.warn(msg, category=DeprecationWarning)
Traceback (most recent call last):
  File ""/Users/user/RF_classification.py"", line 34, in &lt;module&gt;
    classifier.fit(X_train, y_train,sample_weight = balance_weights(y))
  File ""/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.py"", line 279, in fit
    for i in range(n_jobs))
  File ""/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 653, in __call__
    self.dispatch(function, args, kwargs)
  File ""/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 400, in dispatch
    job = ImmediateApply(func, args, kwargs)
  File ""/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 138, in __init__
    self.results = func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.py"", line 85, in _parallel_build_trees
    curr_sample_weight *= sample_counts
ValueError: operands could not be broadcast together with shapes (2599,) (1741,) (2599,) 
</code></pre>

<p>How can I balance this estimator for this ""unbalanced data""?.</p>
";;2362381;10431;JAB;3;28600656;"<p>I think the issue is you are using <code>balanced_weights</code> on the full data set. <code>y</code> before you split it into test and training sets. Try:</p>

<pre><code>classifier.fit(X_train, y_train,sample_weight = balance_weights(y_train))
</code></pre>
"
4140027;3985;tumbleweed;<python><machine-learning><nlp><scikit-learn>;28599754;2;Problems with an unbalanced dataset with scikit-learn Random forest?;"<p>I have an unbalanced textual dataset this is how it looks:</p>

<pre><code>label | texts(documents)
----------
5     |1190
4     |839
3     |239
1     |204
2     |127
</code></pre>

<p>I tried to use the <code>fit(X, y[, sample_weight])</code> parameter but I did not understand in the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"" rel=""nofollow"">documentation</a> how does this is expected. I tried the following:</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import balance_weights

classifier=RandomForestClassifier(n_estimators=10,criterion='entropy')
classifier.fit(X_train, y_train,sample_weight = balance_weights(y))
prediction = classifier.predict(X_test)
</code></pre>

<p>But I get this exception:</p>

<pre><code>/usr/local/lib/python2.7/site-packages/sklearn/utils/__init__.py:93: DeprecationWarning: Function balance_weights is deprecated; balance_weights is an internal function and will be removed in 0.16
  warnings.warn(msg, category=DeprecationWarning)
Traceback (most recent call last):
  File ""/Users/user/RF_classification.py"", line 34, in &lt;module&gt;
    classifier.fit(X_train, y_train,sample_weight = balance_weights(y))
  File ""/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.py"", line 279, in fit
    for i in range(n_jobs))
  File ""/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 653, in __call__
    self.dispatch(function, args, kwargs)
  File ""/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 400, in dispatch
    job = ImmediateApply(func, args, kwargs)
  File ""/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 138, in __init__
    self.results = func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/site-packages/sklearn/ensemble/forest.py"", line 85, in _parallel_build_trees
    curr_sample_weight *= sample_counts
ValueError: operands could not be broadcast together with shapes (2599,) (1741,) (2599,) 
</code></pre>

<p>How can I balance this estimator for this ""unbalanced data""?.</p>
";;2246788;2494;Gilles Louppe;6;28649249;"<p>Update to 0.16-dev. Random Forests now support <code>class_weight=""auto""</code>, which basically rebalances classes automatically for you. </p>
"
3761297;7062;committedandroider;<java><machine-learning><computer-science><prediction><supervised-learning>;28600811;-1;How to start the process of coming up with the predicted math score?;"<p>This concerns a ""software algorithm"" from <a href=""https://stackoverflow.com/help/on-topic"">https://stackoverflow.com/help/on-topic</a></p>

<p>I am working on a problem(non competition) from hacker rank <a href=""https://www.hackerrank.com/challenges/predict-missing-grade"" rel=""nofollow noreferrer"">https://www.hackerrank.com/challenges/predict-missing-grade</a></p>

<p>Basically you're given test data of a bunch of students of their scores in other subjects not including math and you are to predict their score in math based off all their other test scores. Say you were passed data of</p>

<p>{""SerialNumber"":1,""English"":1,""Physics"":2,""Chemistry"":3,""ComputerScience"":2}</p>

<p>How would you go about generating that student's score in mathematics or coming up with a prediction engine to generate the math score? I know that's the whole point of this question but can someone give me a hint or a resource to go to so I can have a chance of figuring this out and actually get started? I really want to learn.</p>
";28642487;1608226;3918;Nuclearman;0;28642487;"<p>There are a lot of possible strategies to do this, the purpose in asking the question seems to be to try to determine which works the best.</p>

<p>It seems impossible to get 100% correct, so the goal is really to come up with a strategy that provides the correct answer more often than not, and hopefully beats the current best. On that note, the best at the time of writing, seems to give about 46% more correct answers than wrong ones, if I'm reading the scoring right.</p>

<p>A quick glance at the first few training numbers and looks like the English score might be a good indicator of the Mathematics score. The reason being that it seems to often be within 1 of the English score (<code>English Score + 1</code> seems to yield slightly better results). Thus a simple strategy might be to simply return the English score as the Mathematics score.
However, you'd want to go through the data and see if that is actually the case that the Mathematics score is within 1 of the English score more often than not This ends up looking to be significantly better than random, but roughly equal in terms of correct VS wrong answers, so it would need to be improved upon by a fair bit to get close to the current best. So you then might want to look into what the other scores look like when using the English score would be counted as wrong. Alternatively, you could look for a different indicator.</p>
"
1070422;871;Kannan Lg;<csv><machine-learning><nlp><weka><arff>;28602598;0;CSV input file format for a (Sparse) ARFF which has a relational attribute;"<p>I want to convert the CSV file to ARFF format (using CSVToARFFConverter) before other processing in WEKA. 
My ARFF file is in the below format:</p>

<pre><code>@relation Sample

 @attribute CLS string
 @attribute SCLS string
 @attribute key relational
   @attribute key1 string
   @attribute key2 string
   @attribute key3 string
 @end key
 @attribute class {-5,-4,-3,-2,-1,0,1,2,3,4,5}


@data
{0 type, 1 beta, 2 ""3 keyword1\nkeyword2\nkeyword3"", -5}
{0 typeA, 1 gamma, 2 ""3 keyword11\nkeyword21\nkeyword31"", 0}
{0 typeB, 1 alpha, 2 ""3 keyword21\nkeyword22\nkeyword23"", 3}
</code></pre>

<p>What is the equivalent CSV representation of the above Sparse ARFF file? Please assist. Thanks.</p>
";;4573749;3;simpleman91;0;28609315;"<p>I don't know if I understand exactly your question, but if you already have the arff file why do you need a csv input? just use the arff in weka.</p>

<p>The equivalent CSV representation could probably be one of these:</p>

<pre><code>CLS, SCLS, key1, key2, key3, class
0 type, 1 beta, keyword1\n, keyword2\n, keyword3, -5
0 typeA, 1 gamma, keyword11\n, keyword21\n, keyword31, 0
0 typeB, 1 alpha, keyword21\n, keyword22\n, keyword23, 3
</code></pre>

<p>or</p>

<pre><code>CLS, SCLS, key, key1, key2, key3, class
0 type, 1 beta, 2, keyword1\n, keyword2\n, keyword3, -5
0 typeA, 1 gamma, 2, keyword11\n, keyword21\n, keyword31, 0
0 typeB, 1 alpha, 2, keyword21\n, keyword22\n, keyword23, 3
</code></pre>

<p>or</p>

<pre><code>CLS, SCLS, key, class
0 type, 1 beta, keyword1\n;keyword2\n;keyword3\n, -5
0 typeA, 1 gamma, keyword11\n;keyword21\n;keyword31\n, 0
0 typeB, 1 alpha, keyword21\n;keyword22\n;keyword23\n, 3
</code></pre>

<p>Read this about relations in csv </p>

<p><a href=""http://en.wikipedia.org/wiki/Comma-separated_values#General_functionality"" rel=""nofollow"">http://en.wikipedia.org/wiki/Comma-separated_values#General_functionality</a></p>

<p>hope it helps you</p>
"
4141061;1299;MLMLTL;<c++><opencv><machine-learning><svm><image-recognition>;28606952;3;SVM OpenCV c++ Predict returning nothing but 1's;"<p>I believe I have successfully trained an <code>SVM</code>, but when I try to predict with it, the output is entirely 1's.</p>

<p>My code for training looks like this:</p>

<pre><code>for(size_t i = 0; i &lt; (testPosArraySize); i++){
    testGivenImg = imread(imagePosDir[i]);
    detector-&gt;detect(testGivenImg, testKeypointsPos);
    bowDE.compute(testGivenImg, testKeypointsPos, testFeaturesPos);
    testFeaturesPos.reshape(1, 1);
    testFeaturesVec.push_back(testFeaturesPos);
}
for(size_t i = 0; i &lt; (testNegaArraySize); i++){
    testGivenImg = imread(image[i]);
    detector-&gt;detect(testGivenImg, testKeypointsNega);
    bowDE.compute(testGivenImg, testKeypointsNega, testFeaturesNega);
    testFeaturesNega.reshape(1, 1);
    testFeaturesVec.push_back(testFeaturesNega);
}

Mat labels(numSamples, 1, CV_32F);
labels.rowRange(0, testPosArraySize).setTo(1);
labels.rowRange(testPosArraySize + 1, numSamples).setTo(-1);
SVM.model.train(fileTestFeat, labels, Mat(), Mat(), SVMParams());
</code></pre>

<p>My code for prediction looks like this:</p>

<pre><code>vector&lt;Mat&gt; predictMatVec(predictArraySize); // -- amount of testing images

for(size_t i = 0; i &lt; (predictArraySize); i++){
    predictImg = imread(imageNegaDir[i]);
    detector-&gt;detect(predictImg, predictKeypoints);
    bowDE.compute(predictImg, predictKeypoints, predictFeatures);
    predictFeatures.reshape(1, 1);
    predictMatVec[i].push_back(predictFeatures);

    Mat predictMat = Mat(predictMatVec);
    float* predictFloat1D = (float*)predictMat.data;
    Mat predictMat1D(1, fileTestFeat.cols, CV_32FC1, predictFloat1D);
    float predictFloat = model.predict(predictMat1D);
    cout &lt;&lt; "" -- SVM output: "" &lt;&lt; predictFloat &lt;&lt; endl; 
}
</code></pre>

<p>But it is returning nothing but 1's. </p>

<p><img src=""https://i.stack.imgur.com/ZfC82.png"" alt=""enter image description here""></p>

<p>What is wrong with it?</p>
";28625439;1348388;4898;Kornel;2;28625439;"<p>So, the vocabulary has been already created (e.g. by <a href=""http://docs.opencv.org/modules/features2d/doc/object_categorization.html#bowkmeanstrainer"" rel=""nofollow""><code>BOWKMeansTrainer</code></a>) and you start to train you SVM classifier, right?</p>

<p>At this point you have a feature detector, extractor, matcher and a BOW image descriptor extractor (to compute an image descriptor using the bag of visual words) such as:</p>

<pre><code>cv::Ptr&lt;cv::FeatureDetector&gt; detector = cv::FeatureDetector::create(""SURF"");
cv::Ptr&lt;cv::DescriptorExtractor&gt; extractor = cv::DescriptorExtractor::create(""SURF"");
cv::Ptr&lt;cv::DescriptorMatcher&gt; matcher = cv::DescriptorMatcher::create(""BruteForce "");

cv::BOWImgDescriptorExtractor bowide(extractor, matcher);
bowide-&gt;setVocabulary(vocabulary);
</code></pre>

<p>First of all we need to scour the training set for our histograms:</p>

<pre><code>cv::Mat samples;
cv::Mat labels(0, 1, CV_32FC1);

for(auto&amp; it : imagePosDir)
{
    cv::Mat image = cv::imread(it);

    std::vector&lt;cv::KeyPoint&gt; keypoints;
    detector-&gt;detect(image, keypoints);

    if(keypoints.empty()) continue;

    // Responses to the vocabulary
    cv::Mat imgDescriptor;
    bowide.compute(image, keypoints, imgDescriptor);

    if(imgDescriptor.empty()) continue;

    if(samples.empty())
    {
        samples.create(0, imgDescriptor.cols, imgDescriptor.type());
    }

    // Copy class samples and labels
    std::cout &lt;&lt; ""Adding "" &lt;&lt; imgDescriptor.rows &lt;&lt; "" positive sample."" &lt;&lt; std::endl;
    samples.push_back(imgDescriptor);

    cv::Mat classLabels = cv::Mat::ones(imgDescriptor.rows, 1, CV_32FC1);
    labels.push_back(classLabels);
}
</code></pre>

<p>Do the same for <code>imagePosNeg</code> except that <code>classLabels</code> will have zero values, such as:</p>

<pre><code>...
cv::Mat classLabels = cv::Mat::zeros(imgDescriptor.rows, 1, CV_32FC1);
labels.push_back(classLabels);
...
</code></pre>

<p>Note how I build the samples and the labels, I marked the positive samples with labels '1', and then the negatives with label '0'. So we have the training data for each class (here for positives and negatives) in <code>samples</code>. Lets's get training:</p>

<pre><code>cv::Mat samples_32f; 
samples.convertTo(samples_32f, CV_32F);

CvSVM svm; 
svm.train(samples_32f, labels);
// Do something with the classifier, like saving it to file
</code></pre>

<p>Then testing let's get testing the classifier:</p>

<pre><code>for(auto&amp; it : testDir)
{
    cv::Mat image = cv::imread(it);

    std::vector&lt;cv::KeyPoint&gt; keypoints;
    detector-&gt;detect(image, keypoints);

    if(keypoints.empty()) continue;

    // Responses to the vocabulary
    cv::Mat imgDescriptor;
    bowide.compute(image, keypoints, imgDescriptor);

    if(imgDescriptor.empty()) continue;

    float res = svm.predict(imgDescriptor, true);

    std::cout &lt;&lt; ""- Result of prediction: "" &lt;&lt; res &lt;&lt; std::endl;
}
</code></pre>

<p>Is it working?</p>

<hr>

<p><strong>Update #1:</strong></p>

<p>Here I made a simple example about BOW+SVM under OpenCV 3.0:
<a href=""https://github.com/bkornel/OpenCV_BOW_SVM/blob/master/main.cpp"" rel=""nofollow"">https://github.com/bkornel/OpenCV_BOW_SVM/blob/master/main.cpp</a></p>

<p>This works me fine for classifying bottles of Coca Cola / Pepsi. I also published the binaries so you can have a try on your database. Hope it works :)</p>
"
693272;13022;AD.Net;<matlab><machine-learning><quadratic-programming>;28608070;0;Matlab quadprog parameters for L1 Regularization;"<p>I've been struggling with the parameterization of quadprog function in Matlab for a problem like this:</p>

<p><img src=""https://i.stack.imgur.com/Ze5Pe.png"" alt=""enter image description here""></p>

<p>where x is a matrix of x 0 w 0 +x 1 w 1 +x 2 2 w 2   and y is the target vector containing a value for each row of x, w is the weight vector, lambda is a scalar value. </p>

<p>I have tried this, but I'm sure it's not correct:</p>

<pre><code>N = size(x, 2);
Sigma = cov(x);
H = 2.0*Sigma;
c = zeros(N,1);
quadprog(H, c)
</code></pre>

<p>Could someone please guide me with what the parameters should be for the quadprog function?</p>
";;501663;9232;Itamar Katz;2;28608413;"<p>This optimization problem is known as Lasso, and as you wrote it it is not formally a quadratic program. You can either convert it to a quadratic program, see <a href=""https://stats.stackexchange.com/questions/119795/quadratic-programming-and-lasso"">this link</a>, or you can use Matlab's builtin <a href=""http://www.mathworks.com/help/stats/lasso.html"" rel=""nofollow noreferrer"">lasso</a> (part of Statistics toolbox)</p>
"
2993035;3730;ralzaul;<search><elasticsearch><machine-learning><e-commerce>;28608778;4;Effect of randomness on search results;"<p>I am currently working on a search ranking algorithm which will be applied to elastic search queries (domain: e-commerce). It assigns scores on several entities returned and finally sorts them based on the score assigned.</p>

<p><strong>My question is:</strong> Has anyone ever tried to introduce a certain level of randomness to any search algorithm and has experienced a positive effect of it. I am thinking that it might be useful to reduce bias and promote the lower ranking items to give them a chance to be seen easier and get popular if they deserve it. I know that some machine learning algorithms are introducing some randomization to reduce the bias so I thought it might be applied to search as well. </p>

<p>Closest I can get here is this but not exactly what I am hoping to get answers for:
<a href=""https://stackoverflow.com/questions/5891727/randomness-in-artificial-intelligence-machine-learning"">Randomness in Artificial Intelligence &amp; Machine Learning</a></p>
";;3498062;48158;Andrei Stefan;1;30607939;"<p>I don't see this mentioned in your post... Elasticsearch offers a random scoring feature: <a href=""https://www.elastic.co/guide/en/elasticsearch/guide/master/random-scoring.html"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/elasticsearch/guide/master/random-scoring.html</a></p>
<blockquote>
<p>As the owner of the website, you want to give your advertisers as much exposure as possible. With the current query, results with the same <code>_score</code> would be returned in the same order every time. It would be good to introduce some randomness here, to ensure that all documents in a single score level get a similar amount of exposure.</p>
<p>We want every user to see a different random order, but we want the same user to see the same order when clicking on page 2, 3, and so forth. This is what is meant by consistently random.</p>
<p>The <code>random_score</code> function, which outputs a number between 0 and 1, will produce consistently random results when it is provided with the same seed value, such as a userâ€™s session ID</p>
</blockquote>
"
2993035;3730;ralzaul;<search><elasticsearch><machine-learning><e-commerce>;28608778;4;Effect of randomness on search results;"<p>I am currently working on a search ranking algorithm which will be applied to elastic search queries (domain: e-commerce). It assigns scores on several entities returned and finally sorts them based on the score assigned.</p>

<p><strong>My question is:</strong> Has anyone ever tried to introduce a certain level of randomness to any search algorithm and has experienced a positive effect of it. I am thinking that it might be useful to reduce bias and promote the lower ranking items to give them a chance to be seen easier and get popular if they deserve it. I know that some machine learning algorithms are introducing some randomization to reduce the bias so I thought it might be applied to search as well. </p>

<p>Closest I can get here is this but not exactly what I am hoping to get answers for:
<a href=""https://stackoverflow.com/questions/5891727/randomness-in-artificial-intelligence-machine-learning"">Randomness in Artificial Intelligence &amp; Machine Learning</a></p>
";;4748557;756;stefan.schroedl;0;37131387;"<p>Your intuition is right - randomization can help surface results that get a lower than deserved score due to uncertainty in the estimation. Empirically, Google search ads seemed to have sometimes been randomized, and e.g. <a href=""https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf"" rel=""nofollow"">this paper</a> is hinting at it (see Section 6).</p>

<p>This problem describes an instance of a class of problems called <em>Explore/Exploit algorithms</em>, or <em>Multi-Armed Bandit</em> problems; see e.g. <a href=""http://en.wikipedia.org/wiki/Multi-armed_bandit"" rel=""nofollow"">http://en.wikipedia.org/wiki/Multi-armed_bandit</a>. There is a large body of mathematical theory and algorithmic approaches. A general idea is to not always order by expected, ""best"" utility, but by an <em>optimistic</em> estimate that takes the degree of uncertainty into account. A readable, motivating blog post can be found <a href=""https://jeremykun.com/2013/10/28/optimism-in-the-face-of-uncertainty-the-ucb1-algorithm/"" rel=""nofollow"">here</a>.</p>
"
1215889;6399;neel;<machine-learning><nlp><classification><keyword-search>;28609735;0;Word Sense Disambiguation on Selected words;"<p>I have a given set of <code>keywords</code>, which I know are only related to my application. But these keywords can have different meaning at different contexts. Only one meaning is useful to me, which I know in advance. How can I <code>disambiguate</code> their meanings at runtime?<br>
I tried using different <code>Word Sense Disambiguation</code> methods in the market but these are giving poor results?<br>
Can anyone help me here?</p>
";;1981700;31;Eric B.;2;28616936;"<p>Word sense disambiguation is an open problem, so the success of any approach will depend a lot on your particular data.  If you have enough context surrounding your keywords that are supplied at runtime, you could calculate the tf-idf (<a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow"">http://en.wikipedia.org/wiki/Tf%E2%80%93idf</a>) and compare it to a pre-established tf-idf of the word sense you are interested in: of course, this means having training data where only the sense you're interested in occurs.  You could then compare the two tf-idf vectors, and if they are similar (<a href=""http://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow"">http://en.wikipedia.org/wiki/Cosine_similarity</a>) enough according to some threshold that you could establish experimentally, then you could conclude they are the same sense.  Good luck.</p>
"
1215889;6399;neel;<machine-learning><nlp><classification><keyword-search>;28609735;0;Word Sense Disambiguation on Selected words;"<p>I have a given set of <code>keywords</code>, which I know are only related to my application. But these keywords can have different meaning at different contexts. Only one meaning is useful to me, which I know in advance. How can I <code>disambiguate</code> their meanings at runtime?<br>
I tried using different <code>Word Sense Disambiguation</code> methods in the market but these are giving poor results?<br>
Can anyone help me here?</p>
";;4588780;4531;Nikita Astrakhantsev;2;28637032;"<p>Disambiguation is the task of choosing one meaning from pre-specified set for the term (word/collocation, or <code>keyword</code>) depending on the context. The main idea here is to compute similarity between each meaning and the context, and then choose the closest meaning. Also it is very useful to have a priori distribution over meanings - for example, how often each meaning was used for the term; most common sense algorithm is a pretty good baseline, by the way.</p>
<p>So, you task is to set a priori distribution, to define similarity measure, and to choose the context. Often it is enough to consider only local context - 3 to 5 closest words from each side.
Similarity measure highly depends on your dictionary (set of meanings per term) and your domain. One example - cosine over tf-idf vectors - is proposed above.</p>
<p>Having this, you can create a binary classifier; ideally, to train machine learning one like Logistic regression, if you have train set where you know precisely for each keyword if it has useful meaning or not.
If you have only positive examples (which seems to be assumed by user1981700), then you have something like <a href=""http://en.wikipedia.org/wiki/One-class_classification"" rel=""nofollow noreferrer"">one class classifiction</a>, which usually has worse performance.</p>
<p>Hope this helps. If you provide with more details about your domain and kind of dictionary, it will be much easier to think out more appropriate solution.</p>
"
1512848;469;Soroosh;<r><machine-learning>;28611179;0;How ROCR package of R can be used to handle label (output) with just one class?;"<p>I am using ROCR package of R to measure the performance of a model. A simple snippet of my code could be assumed as the following:</p>

<pre><code>predic &lt;- c(0.30, 0.59, 0.60, 0.71, 0.86)
label &lt;- c(1,    1,    1,    1,     1)

pred &lt;- ROCR::prediction(predic, label)
</code></pre>

<p>But unfortunately it sounds that I cannot use ROCR for label(output) with just one class and I received the following error:</p>

<pre><code>Error in ROCR::prediction(predic, label) : 
  Number of classes is not equal to 2.
ROCR currently supports only evaluation of binary classification tasks. 
</code></pre>

<p>Is there anyway to fix this problem?</p>
";;1519199;8302;Jthorpe;0;28615534;"<p>I think you're out of luck.  Even instantiating a 'Prediction' object
directly raises errors...</p>

<pre><code>cutoffs = sort(c(Inf,sort(predic,decreasing=TRUE)))
len = length(cutoffs)
tp &lt;- apply(outer(sort(predic),cutoffs,`&lt;`),2,sum)
fn=

new(""prediction"", 
    predictions = predic,
    labels = label,
    cutoffs = ,
    tp = tp, #cases
    fn = len-tp-1,#cases
    fp = rep.int(0,len),#controls
    tn = rep.int(0,len),#controls 
    n.pos = length(predic),
    n.neg = 0,
    n.pos.pred = tp,
    n.neg.pred = len-tp-1)


#&gt; Error in initialize(value, ...) : argument is missing, with no default
#&gt; Calls: new -&gt; initialize -&gt; initialize
</code></pre>
"
1551596;857;assafmo;<machine-learning><weka><supervised-learning><unsupervised-learning><elki>;28611228;1;Using ELKI MiniGUI for anomaly detection with training set and test set;"<p>I have:</p>

<ol>
<li><p>A file <code>training.arff</code> which contains only samples with normal behavior.</p></li>
<li><p>A file <code>test.arff</code> which contains samples both with normal and abnormal behavior.</p></li>
</ol>

<p>I would like to use ELKI MiniGUI for anomaly detection using semi-supervised learning.</p>

<p>I believe usually I should build/train a model using <code>training.arff</code> and then apply the model on the <code>test.arff</code>.</p>

<p>It does not matter which algorithm I use.</p>

<p>I just can't seem to find where to put those two files in ELKI MiniGUI so I would get my desired result. (There's only<code>dbc.in</code>)</p>

<p>*PS: after a week of trying using weka I gave up, but I am not limited to ELKI.</p>

<p>Thanks!!</p>
";28630033;1939754;8298;Erich Schubert;3;28630033;"<p>Your scenario is a <em>supervised learning</em> approach.</p>

<p>ELKI currently only includes <em>unsupervised</em> outlier detection methods, that do not make use of the prior information of ""normal only"" training data.</p>

<p>You could concatenate training and test files into one file, and then run outlier detection. Most published algorithms in this domain are unsupervised. In unsupervised learning, there is no training data set - there is only one kind of data.</p>

<p>Note that most algorithms available in ELKI as of 2014 are designed for numerical data. If your data is categorial, you will be able to use many of them, but you will need to implement data types and distance functions that fit your data type. There are some parsers and distances for non-numerical data available (e.g. for textual data) but this is not supported by the ARFF parser, and there is currently no distance function for mixed data either.</p>
"
557022;3277;picmate æ¶…;<python><algorithm><machine-learning><nlp><cluster-analysis>;28615421;1;Cluster a Distance Matrix in Python;"<p>I have a distance matrix of the form:</p>

<pre><code>        str1    str2    str3    str4    ...     strn
str1    0.8     0.4     0.6     0.1     ...     0.2
str2    0.4     0.7     0.5     0.1     ...     0.1
str3    0.6     0.5     0.6     0.1     ...     0.1
str4    0.1     0.1     0.1     0.5     ...     0.6
.       .       .       .       .       ...     .
.       .       .       .       .       ...     .
.       .       .       .       .       ...     .
strn    0.2     0.1     0.1     0.6     ...     0.7
</code></pre>

<p>Each element contains a distance between two strings, string i and string j that has been calculated based on their similarity. If the strings are similar the value is higher. As it can be seen from the matrix, the same string would not get a 1 or 0. However, the value is high.</p>

<p>My requirement is to cluster the strings based on their values so that most similar strings are clustered together. For example, the five strings here should be clusterd: [str1, str2, str3], [str4, strn].</p>

<p>I am looking for a python library to do this. </p>
";;4588780;4531;Nikita Astrakhantsev;0;28647484;"<p>Since you already have similarity values, try <a href=""http://en.wikipedia.org/wiki/Hierarchical_clustering"" rel=""nofollow"">hierachical clustering</a>.
For example, <a href=""http://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage"" rel=""nofollow"">scipy lib</a> provides several methods for it.</p>

<p>*don't forget to convert your similarity matrix to distance one.</p>
"
1014747;496;edgaralienfoe;<machine-learning><naivebayes>;28619979;0;Accuracy increases using cross-validation and decreases without;"<p>I have a question regarding cross validation: I'm using a Naive Bayes classifier to classify blog posts by author. When I validate my dataset without k-fold cross validation I get an accuracy score of 0.6, but when I do k-fold cross validation, each fold renders a much higher accuracy (greater than 0.8).</p>

<p>For Example:</p>

<blockquote>
  <p>(splitting manually): Validation Set Size: 1452,Training Set Size: 13063, Accuracy: 0.6033057851239669 </p>
</blockquote>

<p>and then</p>

<blockquote>
  <p>(with k-fold): Fold 0 -> Training Set Size: 13063, Validation Set Size: 1452 Accuracy: 0.8039702233250621 (all folds are over 0.8) </p>
</blockquote>

<p>etc...</p>

<p>Why does this happen?</p>
";28625932;270287;41194;IVlad;1;28625932;"<p>There are a few reasons this could happen:</p>

<ol>
<li><p>Your ""manual"" split is not random, and you happen to select more outliers that are hard to predict. How are you doing this split?</p></li>
<li><p>What is the <code>k</code> in k-fold CV? I'm not sure what you mean by Validation Set Size, you have a fold size in k-fold CV. There is no validation set, you run the cross validation using your entire data. Are you sure you're running k-fold cross validation correctly?</p></li>
</ol>

<p>Usually, one picks <code>k = 10</code> for k-fold cross validation. If you run it correctly using your entire data, you should rely on its results instead of other results.</p>
"
3847160;98;user3847160;<opencv><machine-learning><training-data><haar-classifier><cascade-classifier>;28620111;1;opencv cascade classifier detects background;"<p>I have been using cascade classifier to train some kind of plants. Here is a sample image for what I want to detect</p>

<p><a href=""https://i.stack.imgur.com/8jbDA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8jbDA.png"" alt=""Litle green plants sample""></a></p>

<p>I sampled the little green plants for positives, and made negatives out of images with similar background and no green plants (as suggested by many sources). Used many images similar to this one for sampling.</p>

<p>I did not have a lot of training data so of course I did not expect some idealistic classification results.</p>

<p>I have set the usual parameters min_hit_rate 0.95 max_false_alarm 0.5 etc. I have tried training with 5,6,7,8,9 and 10 stages. The strange thing that happens to me is that during the training process I get hit rate of 1 during all stages, and after 5 stages I get good acceptance ratio 0.004 (similar for later stages 6,7,8...).
I tried testing my classifier on the same image which I used for the training samples and there is very illogical behavior:</p>

<ol>
<li>the classifier detects almost everything BUT the positive samples i took from it (the same samples in the training with HIT RATION EQUAL TO 1).</li>
<li>the classifier is really but really slow it took over an hour for single input image (down-sampled scale factor 1.1).</li>
</ol>

<p>I do not get it how could the same samples be classified as positives during training (through all the stages) and then NONE of it as positive on the image (there are a lot of false positives around it).</p>

<p>I checked everything a million times (I thought that I somehow mixed positives and negatives but I did not).</p>

<p>Can someone help me with this issue?</p>
";;464273;6376;JohnAllen;2;36438770;"<p>I can try and help but of course I can't train this thing for you unless you send me your images.</p>

<p>In my experience if you aren't getting the desired results, you are simply giving <code>traincascade</code> the wrong or not enough images (either or both positives or negatives).</p>

<p>I did not get great results until I created an annotation file using the built-in <code>opencv_annotation</code> tool.  Have you done that?  How many positives?</p>

<p>Did your negatives contain the background that you are attempting to detect your object in?  This is key and can't be overlooked.</p>

<p>Also, I would use LBP, it's much faster.  </p>

<p>If you or anyone is still stuck and have some positives created, send them to me and I'll see if I can train this thing.</p>

<p>And also, I have written hopefully a one-stop tutorial about this stuff after my experiences with it:</p>

<p><a href=""http://johnallen.github.io/opencv-object-detection-tutorial/"" rel=""nofollow"">http://johnallen.github.io/opencv-object-detection-tutorial/</a></p>
"
4542142;171;dangerous;<python><machine-learning><scikit-learn>;28621518;5;what is the difference between class weight = none and auto in svm scikit learn;"<p>In scikit learn svm classifier what is the difference between class_weight = None and class_weight = Auto. </p>

<p>From the documentation it is given as </p>

<blockquote>
  <p>Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The â€˜autoâ€™ mode uses the values of y to automatically adjust weights inversely proportional to class frequencies.</p>
</blockquote>

<pre><code>class sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, random_state=None)
</code></pre>

<p>But what is the advantage of using auto mode. I couldnt understand its implementation.</p>
";;270287;41194;IVlad;9;28625807;"<p>This takes place in the <a href=""https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/class_weight.py"" rel=""noreferrer"">class_weight.py file</a>:</p>

<pre><code>elif class_weight == 'auto':
    # Find the weight of each class as present in y.
    le = LabelEncoder()
    y_ind = le.fit_transform(y)
    if not all(np.in1d(classes, le.classes_)):
        raise ValueError(""classes should have valid labels that are in y"")

    # inversely proportional to the number of samples in the class
    recip_freq = 1. / bincount(y_ind)
    weight = recip_freq[le.transform(classes)] / np.mean(recip_freq)
</code></pre>

<p>This means that each class you have (in <code>classes</code>) gets a weight equal to <code>1</code> divided by the number of times that class appears in your data (<code>y</code>), so classes that appear more often will get lower weights. This is then further divided by the mean of all the inverse class frequencies.</p>

<p>The advantage is that you no longer have to worry about setting the class weights yourself: this should already be good for most applications.</p>

<p>If you look above in the source code, for <code>None</code>, <code>weight</code> is filled with ones, so each class gets equal weight.</p>
"
4542142;171;dangerous;<python><machine-learning><scikit-learn>;28621518;5;what is the difference between class weight = none and auto in svm scikit learn;"<p>In scikit learn svm classifier what is the difference between class_weight = None and class_weight = Auto. </p>

<p>From the documentation it is given as </p>

<blockquote>
  <p>Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The â€˜autoâ€™ mode uses the values of y to automatically adjust weights inversely proportional to class frequencies.</p>
</blockquote>

<pre><code>class sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, random_state=None)
</code></pre>

<p>But what is the advantage of using auto mode. I couldnt understand its implementation.</p>
";;6507566;81;chakrr;5;44801433;"<p>This is quite an old post, but for all those who've just encountered this problem, note that class_weight == 'auto' has been deprecated as of version 0.17. 
Use class_weight == 'balanced' instead. </p>

<p><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"" rel=""noreferrer"">http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a></p>

<p>This is implemented as follows:</p>

<p>n_samples / (n_classes * np.bincount(y))</p>

<p>Cheers!</p>
"
1826542;449;U-571;<machine-learning><deep-learning>;28625822;1;Can we use Deep learning techniques in binary classification?;"<p>Recently, I started reading about the deep learning. Mainly the weights are pre-trained using unsupervised RBM network and after that, they use neural network networks with many hidden layers to address their task.
So my question is, Whether we can use DNN with for 2 class classification problem.
Thanks to the people who are going to respond.</p>
";28625876;540873;20218;Thomas Jungblut;2;28625876;"<p>Yes, you can do that with a simple logistic regression on top of your hidden layers (whatever you choose for that, RBMs other autoencoders).</p>
"
1826542;449;U-571;<machine-learning><deep-learning>;28625822;1;Can we use Deep learning techniques in binary classification?;"<p>Recently, I started reading about the deep learning. Mainly the weights are pre-trained using unsupervised RBM network and after that, they use neural network networks with many hidden layers to address their task.
So my question is, Whether we can use DNN with for 2 class classification problem.
Thanks to the people who are going to respond.</p>
";28625876;58811;5769;Upul Bandara;1;37888816;"<p>Absolutely Yes!</p>

<p>As mentioned by Thomas you can use a Logistic Regression as you output layers. Also, another approach is you can use a Sotmax layer with two classes as you output layers.</p>

<p>Good Luck!</p>
"
4588374;9;Preist;<machine-learning><scikit-learn>;28632136;0;PYTHON: Memory Error - MultinomialNB.partial_fit() - 17k classes;"<p><strong>Hi i am new to Python SKLearn and ML in general. Im encountering a Memory Error when using MultinomialNB partial fit, Im trying to do Multi Label Classification on the DMOZ directory data.</strong> </p>

<p><strong>My questions</strong>:</p>

<ul>
<li>What am i doing wrong? Is it my lack of memory or is the data wrong?</li>
<li>Am i using the right approach ?</li>
<li>Anything i can do to improve my appraoch ?</li>
</ul>

<p><strong>Approach</strong>:</p>

<p>Store DMOZ DB directories into MongoDB/TokuMX</p>

<pre class=""lang-json prettyprint-override""><code>{
  ""_id"": {
    ""$oid"": ""54e758c91d41c804d8ace196""
  },
  ""docs"": [
    {
      ""url"": ""http://www.awn.com/"",
      ""description"": ""Provides information resources to the international animation community. Features include searchable database archives, monthly magazine, web animation guide, the Animation Village, discussion forums and other useful resources."",
      ""title"": ""Animation World Network""
    }
  ],
  ""labels"": [
    ""Top"",
    ""Arts"",
    ""Animation""
  ]
}
</code></pre>

<p>Itterate over the <code>docs</code> array and pass <code>docs</code> elements into my classifier function.</p>

<p><strong>Vectorizer and Classifier</strong></p>



<pre><code>    classifier = MultinomialNB()
    vectorizer = HashingVectorizer(
            stop_words='english', 
            strip_accents='unicode', 
            norm='l2'
         )
</code></pre>

<p><strong>My classifier function</strong></p>



<pre><code>def classify(doc, labels, classifier, vectorizer, *args):

    r = requests.get(doc['url'], verify=False)

    print ""Retrieving URL = {0}\n"".format(doc['url'])

    if r.status_code == 200:
        html = lxml.html.fromstring(r.text)
        doc['content'] = []


        tags = ['font', 'td', 'h1', 'h2', 'h3', 'p', 'title']
        for tag in tags:
            for x in html.xpath('//'+tag):
                try:
                    bag_of_words = nltk.word_tokenize(x.text_content())
                    pos_tagged = nltk.pos_tag(bag_of_words)

                    for word, pos in pos_tagged:
                        if pos[:2] == 'NN':
                            doc['content'].append(word)

                except AttributeError as e:
                    print e

        x_train = vectorizer.fit_transform(doc['content'])

        #if we are the first one to run partial_fit, pass all classes
        if len(args) == 1:
            classifier.partial_fit(x_train, labels, classes=args[0])
        else:
            classifier.partial_fit(x_train, labels)

        return doc
</code></pre>

<p>X: <code>doc['content']</code> consists of a array with NOUNS. (600)</p>

<p>Y: <code>labels</code> consists of a array with labels inside the mongo document showed above. (3)</p>

<p>Classes <code>args[0]</code> consists of array with all the (UNIQUE)<code>labels</code> in the database. ( 17490)</p>

<p>Running inside VirtualBox on a Quadcore laptop with 4gb ram assigned to VM.</p>
";;676634;23217;Andreas Mueller;0;28663762;"<p>What are the 17490 unique labels? There will be one coefficient for each label and each feature, which is likely where your memory error comes from.</p>
"
843036;2130;StuckInPhDNoMore;<image><matlab><matrix><machine-learning>;28635290;3;Difference between observations and variables in Matlab;"<p>I'm kind of ashamed to even ask this but here goes. In every Matlab help file where the input matrix is a <code>NxD</code> matrix <code>X</code> Matlab describes the matrix arrangement as </p>

<blockquote>
  <p>Data, specified as a numeric matrix. The rows of X correspond to
  observations, and the columns correspond to variables.</p>
</blockquote>

<p>Above taken from help of <a href=""http://uk.mathworks.com/help/stats/kmeans.html#inputarg_X"" rel=""nofollow"">kmeans</a> </p>

<p>I'm kind of confused as to what does Matlab mean by observations and variables.</p>

<p>Suppose I have a data matrix composed of 100 images. Each image is represented by a feature vector of size <code>128 x 1</code>. So here is 100 my observations and 128 the variables or is it the other way around?</p>

<p>Will my data matrix be of the size <code>128 x 100</code> or <code>100 x 128</code></p>
";28637115;4253229;16252;Eugene Sh.;1;28635354;"<p>It looks like you are talking about some specific statistical/probabilistic functions. In statistics or probability theory there are some random variables that are results of some kind of measurements/observations over time (or some other dimension). So such a matrix is just a collection of N measurements of D different random variables.</p>
"
843036;2130;StuckInPhDNoMore;<image><matlab><matrix><machine-learning>;28635290;3;Difference between observations and variables in Matlab;"<p>I'm kind of ashamed to even ask this but here goes. In every Matlab help file where the input matrix is a <code>NxD</code> matrix <code>X</code> Matlab describes the matrix arrangement as </p>

<blockquote>
  <p>Data, specified as a numeric matrix. The rows of X correspond to
  observations, and the columns correspond to variables.</p>
</blockquote>

<p>Above taken from help of <a href=""http://uk.mathworks.com/help/stats/kmeans.html#inputarg_X"" rel=""nofollow"">kmeans</a> </p>

<p>I'm kind of confused as to what does Matlab mean by observations and variables.</p>

<p>Suppose I have a data matrix composed of 100 images. Each image is represented by a feature vector of size <code>128 x 1</code>. So here is 100 my observations and 128 the variables or is it the other way around?</p>

<p>Will my data matrix be of the size <code>128 x 100</code> or <code>100 x 128</code></p>
";28637115;3250829;96436;rayryeng;5;28637115;"<p>Eugene's explanation in a statistical and probability construct is great, but I would like to explain it more in the viewpoint of data analysis and image processing.</p>

<p>Think of an observation as one <strong>sample</strong> from your data set.  In this case, one observation is one image.  For each sample, it has some dimensionality associated to it or a number of variables used to represent such a sample.  </p>

<p>For example, if we had a set of 100 2D Cartesian points, the amount of <strong>observations</strong> is 100, while the dimensionality or the total number of <strong>variables</strong> used to describe the point is 2: We have a <code>x</code> point and a <code>y</code> point.  As such, in the MATLAB universe, we'd place all of these data points into a single matrix. Each <strong>row</strong> of the matrix denotes one point in your data set.  Therefore, the matrix you would create here is <code>100 x 2</code>.</p>

<p>Now, go back to your problem.  We have 100 images and each image can be expressed by 128 features.  This suspiciously looks like you are trying to use SIFT or SURF to represent an image so think of this situation where each image can be described by a 128-dimensional vector, or a histogram with bins of 128 elements.  Each feature is part of the dimensionality makeup that makes up the image.  Therefore, you would have a <code>100 x 128</code> matrix.  Each row represents <strong>one image</strong>, where each image is represented as a <code>1 x 128</code> feature vector.</p>

<p>In general, MATLAB's machine learning and data analysis algorithms assume that your matrix is <code>M x N</code>, where <code>M</code> is the total number of points that make up your data set while <code>N</code> is the dimensionality of one such point in your data set.  In MATLAB's universe, the total number of observations is equal to the total number of points in your data set, while the total number of features / distinct attributes to represent one sample is the total number of variables.</p>

<h2><code>tl:dr</code></h2>

<ul>
<li>Observation: One sample from your data set</li>
<li>Variable: One feature / attribute that helps describe an observation or sample in your data set.</li>
<li>Number of observations: Total number of points in your data set</li>
<li>Number of variables: Total number of features / attributes that make up an observation or sample in your data set.</li>
</ul>
"
4589561;5;Vishal;<machine-learning><svm>;28637610;0;How to get the data to Feature Space Y from Input Space I;"<p>I am trying to implement a Support Vector Machine to understand in and out of it but I am stuck on how to implement it.
Everywhere it is explained how to get a hyper-plane such that we are able to separate different classes. My question is <strong>how to get the data to Feature Space <em>Y</em> from Input Space <em>I</em>.</strong> </p>

<p>Like for example consider below data:</p>

<pre><code>date                 userId      pc        activity

01/04/2010 07:12:31  RES0962     PC-3736   Connect
01/04/2010 07:35:40  RES0962     PC-2588   Disconnect 
01/04/2010 08:02:14  ZKH0388     PC-1021   Connect
01/04/2010 08:20:17  ZKH0388     PC-3736   Disconnect
</code></pre>

<p>Q) Assuming we are trying to build a User behavior model. We can extract features of each user and use it to train but in terms of code how its working? I have no idea about that. If someone could explain that it would be of great help.</p>
";29559491;4095437;62;Positive;1;29559491;"<p>Mapping to feature space requires you to have a weight for each of the distinct feature that determine the classes of your input. Getting the weight is a function of clearly understood the theoretical basis of your project e.g Your financial worth is determined by Money in bank and Investment. The weight of money in bank might be 2; while for investment mightt be 5. therefore, somebody with more investment and less money will likely be with more networths. </p>

<p>Now, the two features money in bank and investment will now be treated as a cordinate x and y respectively as you wished for each inputed data(of course with two features). Imagine you plot the graph after knowing each data (x, y) cordinate based on your weight. Then, getting the hyperplane will be the next challenge. I hope this help. Good luck</p>
"
3804098;4336;lserlohn;<machine-learning><statistics><data-mining>;28643418;0;Does it help to duplicate original data in order to make more data for building model?;"<p>I just got an interview question.</p>

<p>""Assume you want to build a statistical or machine learning model, but you have very limited data on hand. Your boss told you can duplicate original data several times, to make more data for building the model"" Does it help?</p>

<p>Intuitively, it does not help, because duplicating original data doesn't create more ""information"" to feed the model. </p>

<p>But is there anyone can explain it more statistically? Thanks </p>
";28663640;1060350;70512;Has QUIT--Anony-Mousse;1;28644613;"<p>Consider e.g. variance. The data set with the duplicated data will have the exact same variance - you don't have a more precise estimate of the distrbution afterwards.</p>

<p>There are, however, some exceptions. For example bootstrap validation helps when evaluating your model, but you have very little data.</p>
"
3804098;4336;lserlohn;<machine-learning><statistics><data-mining>;28643418;0;Does it help to duplicate original data in order to make more data for building model?;"<p>I just got an interview question.</p>

<p>""Assume you want to build a statistical or machine learning model, but you have very limited data on hand. Your boss told you can duplicate original data several times, to make more data for building the model"" Does it help?</p>

<p>Intuitively, it does not help, because duplicating original data doesn't create more ""information"" to feed the model. </p>

<p>But is there anyone can explain it more statistically? Thanks </p>
";28663640;871096;14606;Robert Dodier;0;28663640;"<p>Well, it depends on exactly what one means by ""duplicating the data"". </p>

<p>If one is exactly duplicating the whole data set a number of times, then methods based on maximum likelihood (as with many models in common use) must find exactly the same result since the log likelihood function of the duplicated data is exactly a multiple of the unduplicated data's log likelihood, and therefore has the same maxima. (This argument doesn't apply to methods which aren't based on the likelihood function; I believe that CART and other tree models, and SVM's,  are such models. In that case you'll have to work out a different argument.)</p>

<p>However, if by duplicating, one means duplicating the positive examples in a classification problem (which is common enough, since there are often many more negative examples than positive), then that does make a difference, since the likelihood function is modified.</p>

<p>Also if one means bootstrapping, then that, too, makes a difference.</p>

<p>PS. Probably you'll get more interest in this question on stats.stackexchange.com.</p>
"
3696321;139;user3696321;<python><machine-learning><scikit-learn><text-classification>;28644177;2;CountVectorizer deleting features that only appear once;"<p>I'm using the sklearn python package, and I am having trouble creating a <code>CountVectorizer</code> with a pre-created dictionary, where the <code>CountVectorizer</code> doesn't delete features that only appear once or don't appear at all.</p>

<p>Here is the sample code that I have:</p>

<pre><code>train_count_vect, training_matrix, train_labels = setup_data(train_corpus, query, vocabulary=None)
test_count_vect, test_matrix, test_labels = setup_data(test_corpus, query, vocabulary=train_count_vect.get_feature_names())

print(len(train_count_vect.get_feature_names()))
print(len(test_count_vect.get_feature_names()))
</code></pre>

<p><code>len(train_count_vect.get_feature_names())</code> outputs <code>89967</code>
<code>len(test_count_vect.get_feature_names())</code> outputs <code>9833</code></p>

<p>Inside the <code>setup_data()</code> function, I am just initializing <code>CountVectorizer</code>. For training data, I'm initializing it without a preset vocabulary. Then, for test data, I'm initializing CountVectorizer with the vocabulary I retrieved from my training data.</p>

<p>How do I get the vocabularies to be the same lengths? I think sklearn is deleting features because they only appear once or don't appear at all in my test corpus. I need to have the same vocabulary because otherwise, my classifier will be of a different length from my test data points.</p>
";28649116;1287834;19655;Slater Victoroff;4;28649116;"<p>So, it's impossible to say without actually seeing the source code of <code>setup_data</code>, but I have a pretty decent guess as to what is going on here. <code>sklearn</code> follows the <code>fit_transform</code> format, meaning there are two stages, specifically <code>fit</code>, and <code>transform</code>.</p>

<p>In the example of the <code>CountVectorizer</code> the <code>fit</code> stage effectively creates the vocabulary, and the <code>transform</code> step transforms your input text into that vocabulary space.</p>

<p>My guess is that you're calling <code>fit</code> on both datasets instead of just one, you need to be using the same ""fitted"" version of <code>CountVectorizer</code> on both if you want the results to line up. e.g.:</p>

<pre><code>model = CountVectorizer()
transformed_train = model.fit_transform(train_corpus)
transformed_test = model.transform(test_corpus)
</code></pre>

<p>Again, this can only be a guess until you post the <code>setup_data</code> function, but having seen this before I would guess you're doing something more like this:</p>

<pre><code>model = CountVectorizer()
transformed_train = model.fit_transform(train_corpus)
transformed_test = model.fit_transform(test_corpus)
</code></pre>

<p>which will effectively make a new vocabulary for the <code>test_corpus</code>, which unsurprisingly won't give you the same vocabulary length in both cases.</p>
"
2925607;1409;kylerthecreator;<python><numpy><machine-learning>;28651801;1;IndexError when fitting SSVM model in PyStruct;"<p>I'm using the <code>pystruct</code> Python module for a structured learning problem in classifying posts in discussion threads, and I've run into an issue when tying to train the <code>OneSlackSSVM</code> for use with the <code>LinearChainCRF</code>. I'm following the <a href=""https://pystruct.github.io/auto_examples/plot_letters.html"" rel=""nofollow"">OCR example from the docs</a>, but can't seem to call the <code>.fit()</code> method on the SSVM. Here is the error I'm getting:</p>

<pre><code>Traceback (most recent call last):

File ""&lt;ipython-input-47-da804d135818&gt;"", line 1, in &lt;module&gt;
ssvm.fit(X_train, y_train)

File ""/Users/kylefth/anaconda/lib/python2.7/site-  
packages/pystruct/learners/one_slack_ssvm.py"", line 429, in fit
joint_feature_gt = self.model.batch_joint_feature(X, Y)

File ""/Users/kylefth/anaconda/lib/python2.7/site-       
packages/pystruct/models/base.py"", line 40, in batch_joint_feature      
joint_feature_ += self.joint_feature(x, y)

File ""/Users/kylefth/anaconda/lib/python2.7/site-    
packages/pystruct/models/graph_crf.py"", line 197, in joint_feature
unary_marginals[gx, y] = 1

IndexError: index 7 is out of bounds for axis 1 with size 7
</code></pre>

<p>Below is the code I've written. I've tired to structure the data as in the docs example where the overall data structure is a <code>dict</code> with keys for <code>data</code>, <code>labels</code>, and <code>folds</code>.</p>

<pre><code>from pystruct.models import LinearChainCRF
from pystruct.learners import OneSlackSSVM

# Printing out keys of overall data structure
print threads.keys()
&gt;&gt;&gt; ['folds', 'labels', 'data']

# Creating instances of models
crf = LinearChainCRF()
ssvm = OneSlackSSVM(model=crf)

# Splitting up data into training and test sets as in example
X, y, folds = threads['data'], threads['labels'], threads['folds']
X_train, X_test = X[folds == 1], X[folds != 1]
y_train, y_test = y[folds == 1], y[folds != 1]

# Print out dimensions of first element in data and labels
print X[0].shape, y[0].shape
&gt;&gt;&gt; (8, 211), (8,)

# Fitting the ssvm model
ssvm.fit(X_train, y_train)
&gt;&gt;&gt; see error above
</code></pre>

<p>Directly after trying to fit the model, I get the above error. All instances of <code>X_train</code>, <code>X_test</code>, <code>y_train</code>, and <code>y_test</code> have 211 columns and all the label dimensions appear to match up with their corresponding training and testing data. Any help would be greatly appreciated. </p>
";28665832;676634;23217;Andreas Mueller;2;28665832;"<p>I think everything you are doing is right, this is <a href=""https://github.com/pystruct/pystruct/issues/114"" rel=""nofollow"">https://github.com/pystruct/pystruct/issues/114</a>.
Your labels y need to start from 0 to n_labels. I think yours start at 1.</p>
"
4589561;5;Vishal;<machine-learning><scikit-learn><svm>;28680509;-1;Sklearn One Class SVM;"<p>I am trying to use OneClassSVM in <a href=""http://scikit-learn.org/"" rel=""nofollow"">Sklearn</a> for outlier detection.
A user is visiting websites everyday but one day he visits a website which has never been visited before. I want to catch this outlier using OneClassSVM. Below is a sample data:</p>

<pre><code>`([[www.makeuseof.com,
www.kickstater.com,
www.google.com,
www.mashable.com`
</code></pre>

<p>Below is sample test data </p>

<pre><code>`test_data = ['www.makeuseof.com','www.google.com','www.abc.com',]`
</code></pre>

<p>I am hashing all the string using python built in hashing library.
<code>abs(hash('string'))</code></p>

<p>I am expecting it to return <code>-1</code> for <code>www.abc.com but its</code>-1` for all.</p>
";28682666;270287;41194;IVlad;4;28682666;"<p>There are probably some implementation bugs as well, but in general I think hashing will cause your data to be way too spread out, so basic hashing will not let you accurately predict outliers in this case since everything will be so far away from everything else that, well, everything will kinda be an outlier.</p>

<p>For your task, I don't know why you even need machine learning. Use a dictionary that stores the visited websites and when you get a new site check whether or not it's in the dictionary. Fast, efficient and easy.</p>
"
4589561;5;Vishal;<machine-learning><scikit-learn><svm>;28680509;-1;Sklearn One Class SVM;"<p>I am trying to use OneClassSVM in <a href=""http://scikit-learn.org/"" rel=""nofollow"">Sklearn</a> for outlier detection.
A user is visiting websites everyday but one day he visits a website which has never been visited before. I want to catch this outlier using OneClassSVM. Below is a sample data:</p>

<pre><code>`([[www.makeuseof.com,
www.kickstater.com,
www.google.com,
www.mashable.com`
</code></pre>

<p>Below is sample test data </p>

<pre><code>`test_data = ['www.makeuseof.com','www.google.com','www.abc.com',]`
</code></pre>

<p>I am hashing all the string using python built in hashing library.
<code>abs(hash('string'))</code></p>

<p>I am expecting it to return <code>-1</code> for <code>www.abc.com but its</code>-1` for all.</p>
";28682666;5143272;507;MaxBenChrist;0;34809807;"<p>Here a short explanation why you should not use machine learning methods:</p>

<p>This is not a machine learning task. In essence machine learning is used to learn unknown patterns from noisy data. It is non-deterministic process.
An examples for such a pattern is </p>

<blockquote>
  <p>""If a client is unemployed, under 25 years old and does not have high school diploma, his risk for credit loan default increases by 30% compared to all male clients."" </p>
</blockquote>

<p>In your case the pattern is known and can be described as </p>

<blockquote>
  <p>""User visits a page that he never visited before"". </p>
</blockquote>

<p>It is a deterministic pattern, hence you should not use machine learning methods here.</p>
"
4598501;11;ChasingDingos;<python><api><twitter><oauth><machine-learning>;28682599;1;Why am I getting this error when using Twitter Rest API;"<p>This is my first post here so I'm sorry if my question is weird/dumb.</p>

<p>I'm using this book -> Building Machine Learning Systems with Python to learn.</p>

<p>In chapter 6 we have to download this corpora of tweets by Niek Sanders ( ~ 5000ish tweets), I found the code online on the authors github --><a href=""https://github.com/luispedro/BuildingMachineLearningSystemsWithPython/tree/master/ch06"" rel=""nofollow""> here </a>and used my twitter app account and entered all the keys and tokens in the twitter auth.py file. However when I run the install.py file, it downloads 5 tweets and spits out the following error at me:</p>

<pre><code>We will skip 1107 tweets that are not available or visible any more on twitter
We have already downloaded 5 tweets.
Fetching 4779 tweets...
['apple', 'positive', '126394830791254016']
--&gt; downloading tweet #126394830791254016 (1 of 4779)
Twitter sent status 404 for URL: 1.1/statuses/show.json using parameters: (id=blank&amp;oauth_consumer_key=blank&amp;oauth_nonce=blank&amp;oauth_signature_method=blank&amp;oauth_timestamp=1424721838&amp;oauth_token=blank&amp;oauth_version=1.0&amp;oauth_signature=blank)
details: {""errors"":[{""code"":144,""message"":""No status found with that ID.""}]}
</code></pre>

<p>i replaced all the keys with the word <strong>blank</strong> here to keep them secret</p>
";;4480227;1099;Daniel Timberlake;2;28683128;"<p>By the look of it, it appears you are trying to pull a tweet with an invalid ID. More than likely that tweet was deleted, thereby making the id invalid.</p>
"
4598501;11;ChasingDingos;<python><api><twitter><oauth><machine-learning>;28682599;1;Why am I getting this error when using Twitter Rest API;"<p>This is my first post here so I'm sorry if my question is weird/dumb.</p>

<p>I'm using this book -> Building Machine Learning Systems with Python to learn.</p>

<p>In chapter 6 we have to download this corpora of tweets by Niek Sanders ( ~ 5000ish tweets), I found the code online on the authors github --><a href=""https://github.com/luispedro/BuildingMachineLearningSystemsWithPython/tree/master/ch06"" rel=""nofollow""> here </a>and used my twitter app account and entered all the keys and tokens in the twitter auth.py file. However when I run the install.py file, it downloads 5 tweets and spits out the following error at me:</p>

<pre><code>We will skip 1107 tweets that are not available or visible any more on twitter
We have already downloaded 5 tweets.
Fetching 4779 tweets...
['apple', 'positive', '126394830791254016']
--&gt; downloading tweet #126394830791254016 (1 of 4779)
Twitter sent status 404 for URL: 1.1/statuses/show.json using parameters: (id=blank&amp;oauth_consumer_key=blank&amp;oauth_nonce=blank&amp;oauth_signature_method=blank&amp;oauth_timestamp=1424721838&amp;oauth_token=blank&amp;oauth_version=1.0&amp;oauth_signature=blank)
details: {""errors"":[{""code"":144,""message"":""No status found with that ID.""}]}
</code></pre>

<p>i replaced all the keys with the word <strong>blank</strong> here to keep them secret</p>
";;3820361;123;Ashnet;0;60754292;"<p>You must use <code>$tweetObject-&gt;{""id_str""}</code> instead of <code>""id""</code>.</p>
"
3787253;2595;smatthewenglish;<java><machine-learning><perceptron>;28686932;0;how to implement perceptron test;"<p>I have an implementation of the perceptron algorithm, which operates according to the bag-of-words model, defining a series of weights to seperate two feature vectors. </p>

<p>Example:</p>

<pre><code>Document 1 = [""I"", ""am"", ""awesome""]
Document 2 = [""I"", ""am"", ""great"", ""great""]
</code></pre>

<p>Dictionary is:</p>

<pre><code>[""I"", ""am"", ""awesome"", ""great""]
</code></pre>

<p>So the documents as a vector would look like:</p>

<pre><code>Document 1 = [1, 1, 1, 0]
Document 2 = [1, 1, 0, 2]
</code></pre>

<p>The algorithm then learns a Decision boundary equation, i.e.:</p>

<pre><code>feature_0 * weight_0 +
feature_1 * weight_1 +
feature_2 * weight_2 +
feature_3 * weight_3 +
bias
</code></pre>

<p>Now I have a test set which, in format is very similar to the training set depicted above. What is the psuedocode to test these values against my decision boundary equation and thereby assign them labels?</p>

<p>I guess it's something like (pseudocode): </p>

<pre><code>For each word in the test set
    if that word exists in the global dict
        value = the frequency of that word * the learned weight
            if value &gt;= 0
                return 1
            else
                return -1
</code></pre>

<p>But I want to capture the class of the entire feature vector, not just one word, so I guess it must be a summation?</p>
";28687353;860196;8365;runDOSrun;1;28687353;"<p>Not sure if I understand you correctly. Training and test set need to have the <em>exact</em> same format. To test, you just solve the equation for known weights and features (of your test set).</p>

<p>In principle, you should generate test and training data together to ensure they're as equal as possible - and then split them into two sets. Generating test data (i.e. the labels) depending on how the decision boundary is set is a very bad idea: The main idea of the test set is to test the current trained boundary against data that follows an unknown, <em>real</em> boundary. By inducing knowledge into the system your test results will badly reflect the <em>real</em> accuracy. </p>
"
372066;167;sparkle;<c++><opencv><machine-learning><hierarchical-clustering>;28689219;0;OpenCV machine learning library for agglomerative hierarchical clustering;"<p>I want to cluster some (x,y) coordinates based on distance using agglomerative hierarchical clustering as number of clusters are not known before. Is there any library that supports this task? 
Iam doing in c++ using Opencv libraries.</p>
";28691521;3343569;1479;Aphire;1;28691521;"<p><a href=""http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_ml/py_kmeans/py_kmeans_opencv/py_kmeans_opencv.html#kmeans-opencv"" rel=""nofollow"">http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_ml/py_kmeans/py_kmeans_opencv/py_kmeans_opencv.html#kmeans-opencv</a></p>

<p>This is a link for K-Means clustering in OpenCV for Python.
Shouldn't be too hard to convert this to c++ code once you understand the logic</p>
"
372066;167;sparkle;<c++><opencv><machine-learning><hierarchical-clustering>;28689219;0;OpenCV machine learning library for agglomerative hierarchical clustering;"<p>I want to cluster some (x,y) coordinates based on distance using agglomerative hierarchical clustering as number of clusters are not known before. Is there any library that supports this task? 
Iam doing in c++ using Opencv libraries.</p>
";28691521;1348388;4898;Kornel;1;28692777;"<p>In <a href=""http://www.nickgillian.com/software/grt"" rel=""nofollow"">Gesture Recognition Toolkit</a> (GRT) there is a simple module for <a href=""https://github.com/nickgillian/grt/blob/master/GRT/ClusteringModules/HierarchicalClustering/HierarchicalClustering.h"" rel=""nofollow"">hierarchical clustering</a>. This is a ""bottom up"" approach as you need, where each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.</p>

<p>You can train the method by:</p>

<ol>
<li><p><a href=""http://www.nickgillian.com/wiki/pmwiki.php/GRT/UnlabelledData"" rel=""nofollow""><code>UnlabelledData</code></a>: The only thing you really need to know about the <code>UnlabelledData</code> class is that you must set the number of input dimensions of your dataset before you try and add samples to the training dataset.</p></li>
<li><p><a href=""http://www.nickgillian.com/wiki/pmwiki.php/GRT/ClassificationData"" rel=""nofollow""><code>ClassificationData</code></a>:</p>

<ul>
<li>You must set the number of input dimensions of your dataset before you try and add samples to the training dataset,</li>
<li>You can not use the class label of <code>0</code> when you add a new sample to your dataset. This is because the class label of <code>0</code> is reserved for the special null gesture class.</li>
</ul></li>
<li><p><a href=""http://www.nickgillian.com/wiki/pmwiki.php/GRT/MatrixDouble"" rel=""nofollow""><code>MatrixDouble</code></a>: <code>MatrixDouble</code> is the default datatype for storing <code>M</code> by <code>N</code> dimensional data, where <code>M</code> is the number of rows and <code>N</code> is the number of columns.</p></li>
</ol>

<p>Furthermore you can save or load your models from/to a file and get the clusters by <a href=""https://github.com/nickgillian/grt/blob/master/GRT/ClusteringModules/HierarchicalClustering/HierarchicalClustering.h#L230"" rel=""nofollow""><code>getClusters()</code></a>.</p>
"
3443615;101;user3443615;<python><machine-learning><statistics><probability><dirichlet>;28689687;0;Normalizing constant of mixture of dirichlet distribution goes unbounded;"<p>I need to calculate PDFs of mixture of Dirichlet distribution in python. But for each mixture component there is the normalizing constant, which is the inverse beta function which has gamma function of sum of the hyper-parameters as the numerator. So even for a sum of hyper-parameters of size '60' it goes unbounded. Please suggest me a work around for this problem. What happens when I ignore the normalizing constant?</p>

<p>First its not the calculation of NC itself that is the problem. For a single dirichlet I have no problem . But what I have here is a mixture of product of dirichlets, so each mixture component is a product of many dirichlets each with its own NCs. So the product of these goes unbounded. Regarding my objective, I have a joint distribution of p(s,T,O), where 's' is discrete, 'T' and 'O' are the dirichlet variables i.e. a set of vectors of parameters which sum to '1'. Now since 's' is discrete and finite I have |S| set of mixture of product of dirichlet components for each 's'. Now my objective here is to find p(s|T,O). So I directly substitute a particular (T,O) and calculate the value of each p('s'|T,O). For this I need to calc the NCs. If there is only one mixture component then I can ignore the norm constant, calc. and renormalise finally, but since I have several mixture components each components will have different scaling and so I can't renormalise. This is my conundrum. </p>
";;871096;14606;Robert Dodier;0;28703110;"<p>Some ideas. (1) To calculate the normalizing factor exactly, maybe you can rewrite the gamma function via gamma(a_i + 1) = a_i gamma(a_i) (a_i need not be an integer, let the base case be a_i &lt; 1) and then you'll have sum(a_i, i, 1, n) terms in the numerator and denominator and you can reorder them so that you divide the largest term by the largest term and multiply those single ratios together instead of computing an enormous numerator and an enormous denominator and dividing those. (2) If you don't need to be exact, maybe you can apply Stirling's approximation. (3) Maybe you don't need the pdf at all. For some purposes, you just need a function which is proportional to the pdf. I believe Markov chain Monte Carlo is like that. So, what is the larger goal you are trying to achieve here?</p>
"
4599969;73;xgmaker;<matlab><machine-learning><standardized>;28689807;1;How does this code for standardizing data work?;"<p>I have a provided <code>standardize</code> function for a machine learning course that wasn't well documented and I'm still new to MATLAB so I'm just trying to break down the function. Any explanation of the syntax or the general idea of standardizing would greatly help. We use this function to standardize a set of training data provided in a large matrix. A break down of most of the lines of the code snippet would help me greatly. Thank you so much. </p>

<pre><code>function [X, mean_X, std_X] = standardize(varargin)
switch nargin
    case 1
        mean_X = mean(varargin{1});
        std_X = std(varargin{1});

        X = varargin{1} - repmat(mean_X, [size(varargin{1}, 1) 1]);

        for i = 1:size(X, 2)
            X(:, i) =  X(:, i) / std(X(:, i));
        end     
    case 3
        mean_X = varargin{2};
        std_X = varargin{3};
        X = varargin{1} - repmat(mean_X, [size(varargin{1}, 1) 1]);
        for i = 1:size(X, 2)
            X(:, i) =  X(:, i) / std_X(:, i);
        end 
end
</code></pre>
";28690441;3250829;96436;rayryeng;2;28690441;"<p>This code accepts a data matrix of size <code>M x N</code>, where <code>M</code> is the dimensionality of one data sample from this matrix and <code>N</code> is the total number of samples.  Therefore, one column of this matrix is <strong>one</strong> data sample.  Data samples are all stacked horizontally and are columns.  </p>

<p>Now, the true purpose of this code is to take all of the columns of your matrix and <strong>standardize / normalize</strong> the data so that each data sample exhibits <a href=""http://en.wikipedia.org/wiki/Feature_scaling#Standardization"" rel=""nofollow noreferrer"">zero mean and unit variance</a>.  This means that after this transform, if you found the mean value of any column in this matrix, it would be 0 and the variance would be 1.  This is a very standard method for normalizing values in statistical analysis, machine learning, and computer vision.</p>

<p>This actually comes from the <a href=""http://en.wikipedia.org/wiki/Standard_score"" rel=""nofollow noreferrer"">z-score</a> in statistical analysis.  Specifically, the equation for normalization is:</p>

<p><img src=""https://upload.wikimedia.org/math/8/4/6/8463971a22cc96a1e0612588e5656bce.png"" alt=""""></p>

<p>Given a set of data points, we subtract the value in question by the mean of these data points, then divide by the respective standard deviation.  How you'd call this code is the following.  Given this matrix, which we will call <code>X</code>, there are two ways you can call this code:</p>

<ul>
<li>Method #1: <code>[X, mean_X, std_X] = standardize(X);</code></li>
<li>Method #2: <code>[X, mean_X, std_X] = standardize(X, mu, sigma);</code></li>
</ul>

<p>The first method automatically infers the mean of each column of <code>X</code>  and the standard deviation of each column of <code>X</code>.  <code>mean_X</code> and <code>std_X</code> will both return <code>1 x N</code> vectors that give you the mean and standard deviation of each column in the matrix <code>X</code>.  The second method allows you to manually specify a mean (<code>mu</code>) and standard deviation (<code>sigma</code>) for each column of <code>X</code>.  This is possibly for use in debugging, but you would specify both <code>mu</code> and <code>sigma</code> as <code>1 x N</code> vectors in this case.  What is returned for <code>mean_X</code> and <code>std_X</code> is identical to <code>mu</code> and <code>sigma</code>.</p>

<p>The code is a bit poorly written IMHO, because you can certainly achieve this vectorized, but the gist of the code is that it finds the mean of every column of the matrix <code>X</code> if we are are using Method #1, duplicates this vector so that it becomes a <code>M x N</code> matrix, then we subtract this matrix with <code>X</code>.  This will subtract each column by its respective mean.  We also compute the standard deviation of each column before the mean subtraction.  </p>

<p>Once we do that, we then normalize our <code>X</code> by dividing each column by its respective standard deviation.  BTW, doing <code>std_X(:, i)</code> is superfluous as <code>std_X</code> is already a <code>1 x N</code> vector.  <code>std_X(:, i)</code> means to grab all of the rows at the <code>i</code><sup>th</sup> column.  If we already have a <code>1 x N</code> vector, this can simply be replaced with <code>std_X(i)</code> - a bit overkill for my taste.</p>

<p>Method #2 performs the same thing as Method #1, but we provide our own mean and standard deviation for each column of <code>X</code>.</p>

<p>For the sake of documentation, this is how I would have commented the code:</p>

<pre><code>function [X, mean_X, std_X] = standardize(varargin)
switch nargin %// Check how many input variables we have input into the function
    case 1 %// If only one variable - this is the input matrix
        mean_X = mean(varargin{1}); %// Find mean of each column
        std_X = std(varargin{1}); %// Find standard deviation of each column

        %// Take each column of X and subtract by its corresponding mean
        %// Take mean_X and duplicate M times vertically
        X = varargin{1} - repmat(mean_X, [size(varargin{1}, 1) 1]);

        %// Next, for each column, normalize by its respective standard deviation
        for i = 1:size(X, 2)
            X(:, i) =  X(:, i) / std(X(:, i));
        end     
    case 3 %// If we provide three inputs
        mean_X = varargin{2}; %// Second input is a mean vector
        std_X = varargin{3}; %// Third input is a standard deviation vector

        %// Apply the code as seen in the first case
        X = varargin{1} - repmat(mean_X, [size(varargin{1}, 1) 1]);
        for i = 1:size(X, 2)
            X(:, i) =  X(:, i) / std_X(:, i);
        end 
end
</code></pre>

<hr>

<p>If I can suggest another way to write this code, I would use the mighty and powerful <a href=""http://www.mathworks.com/help/matlab/ref/bsxfun.html"" rel=""nofollow noreferrer""><code>bsxfun</code></a> function.  This avoids having to do any duplication of elements and we can do this under the hood.  I would rewrite this function so that it looks like this:</p>

<pre><code>function [X, mean_X, std_X] = standardize(varargin)
switch nargin
    case 1
        mean_X = mean(varargin{1}); %// Find mean of each column
        std_X = std(varargin{1}); %// Find std. dev. of each column

        X = bsxfun(@minus, varargin{1}, mean_X); %// Subtract each column by its respective mean
        X = bsxfun(@rdivide, X, std_X); %// Take each column and divide by its respective std dev.

    case 3
        mean_X = varargin{2};
        std_X = varargin{3};

        %// Same code as above
        X = bsxfun(@minus, varargin{1}, mean_X);
        X = bsxfun(@rdivide, X, std_X);
end
</code></pre>

<p>I would argue that the new code above is much faster than using <code>for</code> and <code>repmat</code>.  In fact, it is known that <code>bsxfun</code> is faster than the former approach - especially for larger matrices.</p>
"
2993924;4635;neelshiv;<machine-learning><recommendation-engine>;28697588;0;Item based collaborative filtering when items have been available for different lengths of time;"<p>I am attempting to use item based collaborative filtering for product recommendation. The matrix is all 1s and 0s based on whether or not a buyer purchased an item, and I am using cosine similarity to identify similar products.</p>

<p>In the data I am testing this on, there are over 2K products, and many were available for different amounts of time. Some have been available the entire time, some were available in the past but are no longer available, and some have only recently become available.</p>

<p>I've noticed that products that have been available for a long time tend to match up to other products that have been available a long time (popular greatest-hits sort of items). Similarly, items that are less popular and have not been available as long tend to get good matches that are also not quite as popular. In many cases, the popular products were available at times when the less popular products were not.</p>

<p><a href=""http://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf"" rel=""nofollow"">http://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf</a> This paper about Amazon has pseudo code that seems to suggest only considering people who bought item X when looking for items similar to X (although that might be a misinterpretation on my part). This corrects the issue to some extent, but might even over-correct the issue because it ignores the many people who may have bought item Y and not bought item X.</p>

<p>Has anybody attempted item based collaborative filtering under these conditions, and do you have any advice on how to deal with products that have had different availability?</p>
";;3633250;4320;Maksim Khaitovich;0;28702626;"<p>I recall playing with mahout library on some similar dataset and using a recommender. I ended up injecting domain-specific knowledge into recommender engine as described in 'Mahout in action book' (chapter 'Injecting domain-specific information'). I made a similar thing with 'IDRescorer' class (refer to mentioned book to understand context) which drops out some of recommendations based on the logic you provide and also could boost recommendation rate based on some other logic. In my case I just decided to drop out recommendations of items which are not available anymore and boost recommendations of items which are similar to those of interest for specific user (specified in user's profile). I beleive you could use same ideas for your task.</p>
"
2335990;1712;monster;<scala><apache-spark><machine-learning>;28703808;1;Apache Spark ALS - how is it solving the least square?;"<p>The source code for the Apache Spark ALS can be found <a href=""https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/recommendation/ALS.scala"" rel=""nofollow"">here</a>.</p>

<p>I am wondering where the Least Squares solving is going on in this source code? I can't find it for the life of me.
When following a tutorial/walkthrough on <a href=""http://spark.apache.org/docs/1.1.1/mllib-collaborative-filtering.html"" rel=""nofollow"">Collaborative Filtering</a>, it shows that to perform the ALS function on some ratings you call <code>ALS.train(ratings, rank, numIterations, lambda)</code>. Checking the source code and the <code>train</code> function calls the <code>run</code> function which returns a <code>MatrixFactorizationModel</code> with the predicted ratings in it.</p>

<p>Additionally, the API for ALS <a href=""http://spark.apache.org/docs/1.1.1/api/scala/index.html#org.apache.spark.mllib.recommendation.ALS"" rel=""nofollow"">(found here)</a> says there is a method called <code>solveLeastSquares</code> but it isn't in the source code found in the <a href=""https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/recommendation/ALS.scala"" rel=""nofollow"">first link</a>. I would like to learn how the least squares problem is being solved so I can adjust it as necessary.</p>
";;779513;61890;Justin Pihony;1;28703987;"<p>From the <a href=""https://github.com/apache/spark/blob/4a17eedb16343413e5b6f8bb58c6da8952ee7ab6/docs/mllib-guide.md"" rel=""nofollow"">documentation</a>:</p>

<blockquote>
  <p>(Breaking change) In ALS, the extraneous method solveLeastSquares has been removed. The DeveloperApi method analyzeBlocks was also removed.</p>
</blockquote>

<p>However, you can change the branch <a href=""https://github.com/apache/spark/blob/branch-1.1/mllib/src/main/scala/org/apache/spark/mllib/recommendation/ALS.scala"" rel=""nofollow"">to be 1.1 per the docs</a> you referenced and you will see the <code>solveLeastSquares</code> method</p>
"
3450594;1628;JChao;<python><search><random><machine-learning><indices>;28706223;0;How to get the indices for randomly selected rows in a list (Python);"<p>Okay, I don't know if I phrased it badly or something, but I can't seem to find anything similar here for my problem.</p>

<p>So I have a 2D list, each row representing a case and each column representing a feature (for machine learning). In addition, I have a separated list (column) as labels.</p>

<p>I want to randomly select the rows from the 2D list to train a classifier while using the rest to test for accuracy. Thus I want to be able to know all the indices of rows I used for training to avoid repeats.</p>

<p>I think there are 2 parts of the question:
1) how to randomly select
2) how to get indices</p>

<p>again I have no idea why I can't find good info here by searching (maybe I just suck)</p>

<p>Sorry I'm still new to the community so I might have made a lot of format mistake. If you have any suggestion, please let me know.</p>

<p>Here's the part of code I'm using to get the 2D list</p>

<pre><code>#273 = number of cases
feature_list=[[0]*len(mega_list)]*273
#create counters to use for index later
link_count=0
feature_count=0
#print len(mega_list)
for link in url_list[:-1]:

    #setup the url
    samp_url='http://www.mtsamples.com'+link
    samp_url = ""%20"".join( samp_url.split() )

    #soup it for keywords
    samp_soup=BeautifulSoup(urllib2.urlopen(samp_url).read())
    keywords=samp_soup.find('meta')['content']
    keywords=keywords.split(',')

    for keys in keywords:
        #print 'megalist: '+ str(mega_list.index(keys))
        if keys in mega_list:
            feature_list[link_count][mega_list.index(keys)]=1 
</code></pre>

<p>mega_list: a list with all keywords</p>

<p>feature_list: the 2D list, with any word in mega_list, that specific cell is set to 1, otherwise 0</p>
";28706721;4598462;21;Marshall Atchison;1;28706705;"<p>As I understand the problem, you have a list and you want to sample the list and save the indices for future use.
See: <a href=""https://docs.python.org/2/library/random.html"" rel=""nofollow"">https://docs.python.org/2/library/random.html</a></p>

<p>you could do a random.sample(xrange(sizeoflist),sizeofsample) which will return the indices of your sample. You can then use that sample for training and skip over them (or get fancy and do a Set difference) for validation.</p>

<p>Hope this helps</p>
"
3450594;1628;JChao;<python><search><random><machine-learning><indices>;28706223;0;How to get the indices for randomly selected rows in a list (Python);"<p>Okay, I don't know if I phrased it badly or something, but I can't seem to find anything similar here for my problem.</p>

<p>So I have a 2D list, each row representing a case and each column representing a feature (for machine learning). In addition, I have a separated list (column) as labels.</p>

<p>I want to randomly select the rows from the 2D list to train a classifier while using the rest to test for accuracy. Thus I want to be able to know all the indices of rows I used for training to avoid repeats.</p>

<p>I think there are 2 parts of the question:
1) how to randomly select
2) how to get indices</p>

<p>again I have no idea why I can't find good info here by searching (maybe I just suck)</p>

<p>Sorry I'm still new to the community so I might have made a lot of format mistake. If you have any suggestion, please let me know.</p>

<p>Here's the part of code I'm using to get the 2D list</p>

<pre><code>#273 = number of cases
feature_list=[[0]*len(mega_list)]*273
#create counters to use for index later
link_count=0
feature_count=0
#print len(mega_list)
for link in url_list[:-1]:

    #setup the url
    samp_url='http://www.mtsamples.com'+link
    samp_url = ""%20"".join( samp_url.split() )

    #soup it for keywords
    samp_soup=BeautifulSoup(urllib2.urlopen(samp_url).read())
    keywords=samp_soup.find('meta')['content']
    keywords=keywords.split(',')

    for keys in keywords:
        #print 'megalist: '+ str(mega_list.index(keys))
        if keys in mega_list:
            feature_list[link_count][mega_list.index(keys)]=1 
</code></pre>

<p>mega_list: a list with all keywords</p>

<p>feature_list: the 2D list, with any word in mega_list, that specific cell is set to 1, otherwise 0</p>
";28706721;3945856;481;jay s;1;28706721;"<p>I would store the data in a pandas data frame instead of a 2D list. If I understand your data right you could do that like this:</p>

<pre><code>import pandas as pd

df = pd.DataFrame(feature_list, columns = mega_list)
</code></pre>

<p>I don't see any mention of a dependent variable, but I'm assuming you have one because you mentioned a classifier algorithm. If your dependent variable is called ""Y"" and is in a list format with indices that align with your features,  then this code will work for you:</p>

<pre><code>from sklearn import cross_validation

x_train, x_test, y_train, y_test = cross_validation.train_test_split(
    df, Y, test_size=0.8, random_state=0)
</code></pre>
"
4118635;414;Geronimo;<opencv><machine-learning><filesize>;28706381;1;How do I reduce the size of the learning model in opencv (for CvBoost)?;"<p>I'm using opencv's CvBoost to classify. I've trained the classifier with several gigabytes of data and then I save it off. The model has a tree of 1000 weak learners with a depth of 20 (the default settings). Now I want to load it up to predict classes in real time production code. However, the size of the learning model is HUGE (nearly a gigabyte). I believe this is because the save function saves off all of the data used for learning so the training model can be properly updated. However, I don't need this functionality at run-time, I just want to use the fixed parameters (1000 weak learners, etc) which shouldn't be much data.</p>

<p><strong>Is there a way to save off and load just the weak learner parameters into CvBoost?</strong></p>

<p><strong>Does anyone have experience reducing the learning model data size with this or another opencv learning model?</strong> <em>Note: CvBoost inherits from CvStatModel which has the save/load functions.</em> </p>
";;4118635;414;Geronimo;0;28889937;"<p>I realized that with 1000 learners and a depth of 20, that's potentially 2^20*1000 learning parameters, i.e. about a billion or 1 gigabyte. So turns out that the learning model needs all of that space to store all of the trees.</p>

<p>To reduce the size I must lower the tree depth and/or number of learners. For example, reducing tree depth to 5 used only 21 mb (though it seemed to take around the same amount of time to build the learning model). Perhaps decreasing the weight trim rate would result in more trees that are pruned before reaching depth 20 (and thus reduce memory size as well). I haven't tested this yet. </p>

<p>Case closed.</p>
"
4118635;414;Geronimo;<opencv><machine-learning><filesize>;28706381;1;How do I reduce the size of the learning model in opencv (for CvBoost)?;"<p>I'm using opencv's CvBoost to classify. I've trained the classifier with several gigabytes of data and then I save it off. The model has a tree of 1000 weak learners with a depth of 20 (the default settings). Now I want to load it up to predict classes in real time production code. However, the size of the learning model is HUGE (nearly a gigabyte). I believe this is because the save function saves off all of the data used for learning so the training model can be properly updated. However, I don't need this functionality at run-time, I just want to use the fixed parameters (1000 weak learners, etc) which shouldn't be much data.</p>

<p><strong>Is there a way to save off and load just the weak learner parameters into CvBoost?</strong></p>

<p><strong>Does anyone have experience reducing the learning model data size with this or another opencv learning model?</strong> <em>Note: CvBoost inherits from CvStatModel which has the save/load functions.</em> </p>
";;7060625;1;lee;0;40540423;"<p>CvBoostParams has a 'use_surrogates' parameter, it's default value is ture. Set it false can reduce the size of learning model</p>
"
3670186;21;robowolf;<python><machine-learning><scikit-learn><classification><decision-tree>;28706567;0;is it proper to use float64 data type with scikit-learn ML algorithms?;"<p>I am trying to execute Decision Tree and SVM for a dataset given <a href=""https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data"" rel=""nofollow"">here</a> using scikit-learn. My purpose is to compare these two algorithms so that I am using KFold cross-validation method for both algorithms and show the difference. But the dataset I am using, consist real number like 0.00057. I get accuracy that I can say there is no overfitting, but I am not sure if real numbers effect the results.</p>

<p>Is it a problem to give scikit-learn built-in classification functions real numbers ? If it is , what should I do get better results ?</p>

<p>PS: when I check the type of a single data in python I see it is float64.</p>
";;676634;23217;Andreas Mueller;2;28731829;"<p>DecisionTreeClassifier and SVC internally use float32 to represent the features. They will convert any input data into this format. For machine learning tasks, that is usually more than enough precision.</p>
"
3001408;302;user3001408;<machine-learning><scikit-learn><decision-tree>;28709023;3;strings as features in decision tree/random forest;"<p>I am new to machine learning!</p>

<p>Right now I am doing some problems on application of decision tree/random forest. I am trying to fit a problem which has numbers as well as strings (such as country name) as features. Now the library, scikit-learn takes only numbers as parameters, but I want to inject the strings as well as they carry significant amount of knowledge.</p>

<p>How do I handle such scenario, I can convert string to numbers by some mechanism such as hashing in python. But I would like to know the best practice on how strings are handled in decision tree problems.</p>
";;2484687;5610;Raff.Edward;5;28710380;"<p>1) How to add ""strings"" as features. </p>

<p>Very few algorithms can natively handle strings in any form, and decision trees are not one of them. You have to convert them to something that the decision tree knows about (generally numeric or categorical variables). </p>

<p>How to convert them to features: This very much depends on the nature of the strings. If the strings are sentences, you can use things like <a href=""http://en.wikipedia.org/wiki/Bag-of-words_model"" rel=""nofollow"">bag of words</a> to map each word to a numeric feature. There are numerous different strategies for determining what numeric value to use, but just using 0/1 for not present / present is often a decent baseline. </p>

<p>For countries, this doesn't make sense as you're representing your feature in the wrong way. A country is more akin to a categorical variable. There are only X countries and you must have a value that is in X (this may not be strictly absolutely true, but that's beyond the point). scikit-learn doesn't have support for categorical variables. You can ""fake"" it by using a <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"" rel=""nofollow"">one-hot-encoding</a>, but it likely will not work quite as well as a library that fully supports categorical variables. </p>

<p>Note that just because countries can be represented as categories doesn't mean that it is the best way to handle them. It depends highly on what your data is and what you are doing. No one can answer it for you without knowing all the details. </p>
"
3001408;302;user3001408;<machine-learning><scikit-learn><decision-tree>;28709023;3;strings as features in decision tree/random forest;"<p>I am new to machine learning!</p>

<p>Right now I am doing some problems on application of decision tree/random forest. I am trying to fit a problem which has numbers as well as strings (such as country name) as features. Now the library, scikit-learn takes only numbers as parameters, but I want to inject the strings as well as they carry significant amount of knowledge.</p>

<p>How do I handle such scenario, I can convert string to numbers by some mechanism such as hashing in python. But I would like to know the best practice on how strings are handled in decision tree problems.</p>
";;1863229;383077;Tim Biegeleisen;1;28746243;"<p>The way to handle your problem is to use Breiman's <code>randomForest</code> implementation in R.  This implementation would allow you to use actual strings as inputs.  For example, you could include a string <code>country</code> column in your input data frame.  The resulting model would be built assuming that the only values for <code>country</code> were the ones in your training data.  One other cool thing which <code>randomForest</code> in R can offer is something called partial dependence plots.  The R function is called <code>partialPlot</code>, and it generates a plot showing how your <strong>response</strong> variable (i.e. what you are trying to predict) depends on a certain predictor.  In the case of <code>country</code>, you could actually generate a plot which shows how your response varies depending on the <code>country</code> you choose.</p>

<p>The answer by Raff Edward also makes sense in the case where you don't want to use your strings directly.  One example of this might be where you have as input people's names as strings.  However, you are really interested (for whatever reason) in how the <strong>length</strong> of a person's name influences your response variable.  In this case, you would just take <code>nchar(name)</code>, where <code>name</code> is the vector containing the name strings.</p>

<p>To get started using <code>randomForest</code> in R, you can visit <a href=""http://www.r-project.org/"" rel=""nofollow"">this site</a> to download it.  It's free for personal use, and I don't believe we paid for it either when I used it at work a few years ago.</p>

<p>There is just one more step you need to get started.  You have to install the <code>randomForest</code> package into your R console.  You can do that by typing:</p>

<pre><code>install.packages(""randomForest"")
</code></pre>

<p>If you have an active internet connection, the R console should be able to resolve that request and prompt you with a list of mirrors from which to choose.</p>

<p>There is a wealth of sites on how to use <code>randomForest</code> and <code>partialPlot</code> which you access by Googling for these terms.</p>
"
3787253;2595;smatthewenglish;<java><machine-learning>;28713492;1;What does theta values of gradient descent mean?;"<p>I have all the components, I just am not quite sure This is my output:</p>

<pre><code>Theta--&gt;: 0.09604203456288299, 1.1864676227195392
</code></pre>

<p>How do I interpret that? What does it mean?</p>

<p>I essentially just modified the example from <a href=""http://berlinbrowndev.blogspot.sg/2011/12/implementing-example-one-from-machine.html"" rel=""nofollow""><strong>this description</strong></a>. But I'm not sure if it's really applicable to my problem. I'm trying to perform binary classification on a set of documents. The documents are rendered as bag-of-words style feature vectors of the form:</p>

<p>Example:</p>

<pre><code>Document 1 = [""I"", ""am"", ""awesome""]
Document 2 = [""I"", ""am"", ""great"", ""great""]
</code></pre>

<p>Dictionary is:</p>

<pre><code>[""I"", ""am"", ""awesome"", ""great""]
</code></pre>

<p>So the documents as a vector would look like:</p>

<pre><code>Document 1 = [1, 1, 1, 0]
Document 2 = [1, 1, 0, 2]
</code></pre>

<p>This is my gradient descent code:</p>

<pre><code>public static double [] gradientDescent(final double [] theta_in, final double alpha, final int num_iters, double[][] data ) 
{
    final double m = data.length;   
    double [] theta = theta_in;
    double theta0 = 0;
    double theta1 = 0;
    for (int i = 0; i &lt; num_iters; i++) 
    {                        
        final double sum0 = gradientDescentSumScalar0(theta, alpha, data );
        final double sum1 = gradientDescentSumScalar1(theta, alpha, data);                                   
        theta0 = theta[0] - ( (alpha / m) * sum0 ); 
        theta1 = theta[1] - ( (alpha / m) * sum1 );                        
        theta = new double [] { theta0, theta1 };
    }
    return theta;
}


//data is the feature vector
//this theta is weight
protected static double [] matrixMultipleHthetaByX( final double [] theta, double[][] data ) 
{
    final double [] vector = new double[ data.length ];
    int i = 0;                 
    for (final double [] d : data) 
    {
        vector[i] = (1.0 * theta[0]) + (d[0] * theta[1]);            
        i++;
    } // End of the for // 
    return vector;
}


protected static double gradientDescentSumScalar0(final double [] theta, final double alpha, double[][] data ) 
{        
    double sum = 0;
    int i = 0;
    final double [] hthetaByXArr = matrixMultipleHthetaByX(theta, data ); 
    for (final double [] d : data) 
    {
        final double X = 1.0;
        final double y = d[1];
        final double hthetaByX = hthetaByXArr[i];    
        sum = sum + ( (hthetaByX - y) * X );
        i++;
    } // End of the for //
    return sum;
}
protected static double gradientDescentSumScalar1(final double [] theta, final double alpha, double[][] data ) 
{        
    double sum = 0;
    int i = 0;
    final double [] hthetaByXArr = matrixMultipleHthetaByX(theta, data );
    for (final double [] d : data) 
    {
        final double X = d[0];
        final double y = d[1];            
        final double hthetaByX = hthetaByXArr[i];         
        sum = sum + ( (hthetaByX - y) * X );
        i++;
    } // End of the for //
    return sum;
}

public static double [] batchGradientDescent( double [] weights, double[][] data ) 
{
    /*
     * From tex:
     * \theta_j := \theta_j - \alpha\frac{1}{m} \sum_{i=1}^m ( h_\theta (x^{(i)})
     */
    final double [] theta_in = weights;
    double [] theta = gradientDescent(theta_in, alpha, iterations, data );
    lastTheta = theta;
    System.out.println(""Theta--&gt;: "" + theta[0] + "", "" + theta[1]);
    return theta;
}
</code></pre>

<p>I call it like this:</p>

<pre><code>   final int globoDictSize = globoDict.size(); // number of features

   double[] weights = new double[globoDictSize + 1];
   for (int i = 0; i &lt; weights.length; i++) 
   {
       //weights[i] = Math.floor(Math.random() * 10000) / 10000;
       //weights[i] = randomNumber(0,1);
       weights[i] = 0.0;
   }


   int inputSize = trainingPerceptronInput.size();
   double[] outputs = new double[inputSize];
   final double[][] a = Prcptrn_InitOutpt.initializeOutput(trainingPerceptronInput, globoDictSize, outputs, LABEL);



       for (int p = 0; p &lt; inputSize; p++) 
       {

           Gradient_Descent.batchGradientDescent( weights, a );
       }
</code></pre>

<p>How can I verify that this code is doing what I want? Shouldn't it be outputting a predicted label or something? I've heard I can also apply to it an error function, such as <a href=""http://en.wikipedia.org/wiki/Hinge_loss"" rel=""nofollow""><strong>hinge loss</strong></a>, that would come after the call to batch gradient descent as a seperate component, isn't it? </p>
";28730029;3633250;4320;Maksim Khaitovich;0;28730029;"<p>You code is complicated (I used to implement batch gradient descent in Octave, not in OO programming languages). But as far as I see in your code (and it is a common to use this notation) Theta is a parameter vector. After grad descend algorithm converges it returns you optimal Theta vector. After that you could claculate output of your new example with formula:</p>

<p>theta_transposed * X,</p>

<p>where theta_trasponsed is a transposed vector of theta, X is a vector of input features.</p>

<p>On a side note, the example you have referred to is a regression task (it is about linear regression). While the task you describe is a classification problem, where instead of predicting some value (some number - weight, length, smth else) you need to assign a label to input set. It can be completed with lots of different algorithms, but defenetily not with linear regression which is described in article you posted.</p>

<p>I also need to mentioned that it is absolutely not clear what kind of classification you try to perform. In your exmaple you have a bag of words description (matrixes of word counts). But where are classificaiton labels? Is it multi-output classification? Or just multi-class? Or binary?</p>

<p>I really suggest you to take a course on ml. Maybe on coursera. This one is good:
<a href=""https://www.coursera.org/course/ml"" rel=""nofollow"">https://www.coursera.org/course/ml</a>
It also covers full implementaion of gradient descent.</p>
"
2514986;1599;Adithya Upadhya;<opencv><image-processing><machine-learning>;28716086;0;Image recognition OpenCV;"<p>My project is to analyze a dataset containing images of birds. After that, the program should recognize whether the input image is a bird or not. </p>

<p><strong>I plan to use OpenCV with C++. Could someone suggest me the modules required and the procedure in which this program should be made for recognizing if the input is a bird or not after reading the image dataset.  (I'm a beginner in OpenCV)</strong></p>
";28718277;4448958;105;Eni;0;28718148;"<p>In order to accomplish this task, you will have to train your very own cascade classifier which by no mean is an easy task for a beginner.</p>

<p>For more information on how this can be achieved please perform your own research as they are loads of examples online. I have listed a link below to get you started.</p>

<p><a href=""http://docs.opencv.org/doc/user_guide/ug_traincascade.html"" rel=""nofollow"">http://docs.opencv.org/doc/user_guide/ug_traincascade.html</a></p>
"
2514986;1599;Adithya Upadhya;<opencv><image-processing><machine-learning>;28716086;0;Image recognition OpenCV;"<p>My project is to analyze a dataset containing images of birds. After that, the program should recognize whether the input image is a bird or not. </p>

<p><strong>I plan to use OpenCV with C++. Could someone suggest me the modules required and the procedure in which this program should be made for recognizing if the input is a bird or not after reading the image dataset.  (I'm a beginner in OpenCV)</strong></p>
";28718277;3343569;1479;Aphire;0;28718277;"<p>There are plenty of ways to go about solving this problem, however the solution I think would probably have the best chance of success would be to train your own haar cascade.</p>

<p>You will need <em>alot</em> of images containing birds and a <em>helluva lot</em> of images <strong>without</strong> birds in. If you already have access to this dataset you need to analyze, then you already got alot of images (some of which you can train with)</p>

<p>A haar cascade essentially searches the image for patterns it has been trained to recognize (such as a face, eyes, nose etc which comes with OpenCV)</p>

<p><a href=""http://nayakamitarup.blogspot.in/2011/07/how-to-make-your-own-haar-trained-xml.html"" rel=""nofollow"">Here</a> is a link to a tutorial/tools for making training your own cascade as painless as possible.</p>

<p>Follow the instructions and at the end you should have an XML file that will be your Cascade for birds.</p>

<p><a href=""http://docs.opencv.org/doc/tutorials/objdetect/cascade_classifier/cascade_classifier.html"" rel=""nofollow"">Here</a> is a tutorial on how to <em>use</em> your new cascade. I suggest you read up on how cascade classifiers work in general as it will help you understand what kind of images to train on, and what tweaking you can do to your images to ensure the best possible chance of detection.</p>

<p>Another option you could pursue would be to just use OpenCV for preprocessing and then send your image to a neural network for classification.</p>

<p>This could involve removing as much ""background noise"" information as possible from the image such as removing the colour bright green (unless you have bright green birds of course), smooth and scale down the image (take some weight off the network) and then plug the rest of the info to your network. I have no idea how successful this might be as I have never used a network for detection, only for face recognition, but it did a pretty damn good job at that, so give it a shot.</p>

<p>Good luck and I hope this helps.</p>
"
492372;24872;London guy;<machine-learning><scikit-learn><classification><logistic-regression>;28716241;28;Controlling the threshold in Logistic Regression in Scikit Learn;"<p>I am using the  <code>LogisticRegression()</code> method in <code>scikit-learn</code> on a highly unbalanced data set. I have even turned the <code>class_weight</code> feature to <code>auto</code>.</p>

<p>I know that in Logistic Regression it should be possible to know what is the threshold value for a particular pair of classes. </p>

<p>Is it possible to know what the threshold value is in each of the One-vs-All classes the <code>LogisticRegression()</code> method designs?</p>

<p>I did not find anything in the documentation page.</p>

<p>Does it by default apply the <code>0.5</code> value as threshold for all the classes regardless of the parameter values?</p>
";53974769;4588780;4531;Nikita Astrakhantsev;18;28724763;"<p>Logistic regression chooses the class that has the biggest probability. In case of 2 classes, the threshold is 0.5: if P(Y=0) > 0.5 then obviously P(Y=0) > P(Y=1). The same stands for the multiclass setting: again, it chooses the class with the biggest probability (see e.g. <a href=""http://www.holehouse.org/mlclass/06_Logistic_Regression.html"" rel=""noreferrer"">Ng's lectures</a>, the bottom lines).</p>

<p>Introducing special thresholds only affects in the proportion of false positives/false negatives (and thus in precision/recall tradeoff), but it is not the parameter of the LR model. See also <a href=""https://stackoverflow.com/questions/19984957/scikit-predict-default-threshold"">the similar question</a>.</p>
"
492372;24872;London guy;<machine-learning><scikit-learn><classification><logistic-regression>;28716241;28;Controlling the threshold in Logistic Regression in Scikit Learn;"<p>I am using the  <code>LogisticRegression()</code> method in <code>scikit-learn</code> on a highly unbalanced data set. I have even turned the <code>class_weight</code> feature to <code>auto</code>.</p>

<p>I know that in Logistic Regression it should be possible to know what is the threshold value for a particular pair of classes. </p>

<p>Is it possible to know what the threshold value is in each of the One-vs-All classes the <code>LogisticRegression()</code> method designs?</p>

<p>I did not find anything in the documentation page.</p>

<p>Does it by default apply the <code>0.5</code> value as threshold for all the classes regardless of the parameter values?</p>
";53974769;3937550;402;jazib jamil;22;50345409;"<p>There is a little trick that I use, instead of using <code>model.predict(test_data)</code> use <code>model.predict_proba(test_data)</code>. Then use a range of values for thresholds to analyze the effects on the prediction;</p>

<pre><code>pred_proba_df = pd.DataFrame(model.predict_proba(x_test))
threshold_list = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
for i in threshold_list:
    print ('\n******** For i = {} ******'.format(i))
    Y_test_pred = pred_proba_df.applymap(lambda x: 1 if x&gt;i else 0)
    test_accuracy = metrics.accuracy_score(Y_test.as_matrix().reshape(Y_test.as_matrix().size,1),
                                           Y_test_pred.iloc[:,1].as_matrix().reshape(Y_test_pred.iloc[:,1].as_matrix().size,1))
    print('Our testing accuracy is {}'.format(test_accuracy))

    print(confusion_matrix(Y_test.as_matrix().reshape(Y_test.as_matrix().size,1),
                           Y_test_pred.iloc[:,1].as_matrix().reshape(Y_test_pred.iloc[:,1].as_matrix().size,1)))
</code></pre>

<p>Best!</p>
"
492372;24872;London guy;<machine-learning><scikit-learn><classification><logistic-regression>;28716241;28;Controlling the threshold in Logistic Regression in Scikit Learn;"<p>I am using the  <code>LogisticRegression()</code> method in <code>scikit-learn</code> on a highly unbalanced data set. I have even turned the <code>class_weight</code> feature to <code>auto</code>.</p>

<p>I know that in Logistic Regression it should be possible to know what is the threshold value for a particular pair of classes. </p>

<p>Is it possible to know what the threshold value is in each of the One-vs-All classes the <code>LogisticRegression()</code> method designs?</p>

<p>I did not find anything in the documentation page.</p>

<p>Does it by default apply the <code>0.5</code> value as threshold for all the classes regardless of the parameter values?</p>
";53974769;9194965;491;sb2020;14;53974769;"<p>Yes, Sci-Kit learn is using a threshold of P&gt;=0.5 for binary classifications. I am going to build on some of the answers already posted with two options to check this:</p>
<p>One simple option is to extract the probabilities of each classification using the output from model.predict_proba(test_x) segment of the code below along with class predictions (output from model.predict(test_x) segment of code below).  Then, append class predictions and their probabilities to your test dataframe as a check.</p>
<p>As another option, one can graphically view precision vs. recall at various thresholds using the following code.</p>
<pre><code>### Predict test_y values and probabilities based on fitted logistic 
regression model

pred_y=log.predict(test_x) 

probs_y=log.predict_proba(test_x) 
  # probs_y is a 2-D array of probability of being labeled as 0 (first 
  column of 
  array) vs 1 (2nd column in array)

from sklearn.metrics import precision_recall_curve
precision, recall, thresholds = precision_recall_curve(test_y, probs_y[:, 
1]) 
   #retrieve probability of being 1(in second column of probs_y)
pr_auc = metrics.auc(recall, precision)

plt.title(&quot;Precision-Recall vs Threshold Chart&quot;)
plt.plot(thresholds, precision[: -1], &quot;b--&quot;, label=&quot;Precision&quot;)
plt.plot(thresholds, recall[: -1], &quot;r--&quot;, label=&quot;Recall&quot;)
plt.ylabel(&quot;Precision, Recall&quot;)
plt.xlabel(&quot;Threshold&quot;)
plt.legend(loc=&quot;lower left&quot;)
plt.ylim([0,1])
</code></pre>
"
811921;2303;com;<machine-learning><cluster-analysis><knn>;28721279;-3;kNN classifier in multi-label settings with WEKA;"<p>WEKA has profound support for kNN classifiers (many different distances and etc.)</p>

<p>Unfortunately WEKA doesn't support <a href=""http://en.wikipedia.org/wiki/Multi-label_classification"" rel=""nofollow"">multi-label problems</a>.</p>

<p>One of the solutions can be to use binary relevance approach.</p>

<p>I am not sure whether it's a correct workaround? What do you think?</p>
";28726428;968064;2363;Rushdi Shams;1;28726428;"<p>You can try <a href=""http://meka.sourceforge.net/"" rel=""nofollow"">Meka</a> which is based on Weka and is expected to handle multilabel classification problems. </p>
"
1040006;9060;tusharmath;<machine-learning><linear-regression>;28723777;0;How to manage a huge number of values for a categorical feature in linear regression;"<p>I am new to machine learning and trying to write a linear regression algorithm where I have a categorical feature - Keywords. I can have around 10 million keywords in my model.</p>

<p>As per the instructions given here - <a href=""http://www.psychstat.missouristate.edu/multibook/mlt08m.html"" rel=""nofollow"">http://www.psychstat.missouristate.edu/multibook/mlt08m.html</a></p>

<p>It seems like I should dichotomize categorical features. Does it mean I will have 23 feature variables (Dummy coding with 10M levels)?</p>

<p>Is this the best way to handle such a scenario?</p>
";28728520;1693354;1471;Abhimanu Kumar;1;28728520;"<p>Yes. You will essentially have 10 Million predictor variables. This is unavoidable if you are doing regression/classification unless you want to club ""similar"" keywords together to reduce the number of predictor variables. E.g. you can club keyword_1, keyword_2, keyword_3 into a single keyword if they share a specific relation among themselves and so on.</p>

<p>To cut down this huge number of keywords (10 M) you have techniques like LASSO (<a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"" rel=""nofollow"">http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html</a>) and RIDGE (<a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"" rel=""nofollow"">http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html</a>) penalties. The idea is that most of these 10 M predictor variables are not important in predicting the results and hence you want to remove them early in the learning phase to increase interpretability and avoid overfitting of your results.</p>

<p>Strictly speaking RIDGE is to avoid overfitting and LASSO is to reduce the number of predictors.</p>
"
1040006;9060;tusharmath;<machine-learning><linear-regression>;28723777;0;How to manage a huge number of values for a categorical feature in linear regression;"<p>I am new to machine learning and trying to write a linear regression algorithm where I have a categorical feature - Keywords. I can have around 10 million keywords in my model.</p>

<p>As per the instructions given here - <a href=""http://www.psychstat.missouristate.edu/multibook/mlt08m.html"" rel=""nofollow"">http://www.psychstat.missouristate.edu/multibook/mlt08m.html</a></p>

<p>It seems like I should dichotomize categorical features. Does it mean I will have 23 feature variables (Dummy coding with 10M levels)?</p>

<p>Is this the best way to handle such a scenario?</p>
";28728520;3923448;8157;Amrita Sawant;1;28779718;"<p>You will get better results if you transform your data to a Sparse Matrix , techniques such as One Hot Encoding and then use any Feature Selection technique(Forward/backward selection,Lasso). Hope this helps!</p>
"
2488820;247;AnishM;<python><machine-learning><apache-spark>;28724639;18;How to augment matrix factors in Spark ALS recommender?;"<p>I am a beginner to the world of Machine Learning and the usage of Apache Spark. <br>
I have followed the tutorial at <a href=""https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html#augmenting-matrix-factors"">https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html#augmenting-matrix-factors</a>, and was succesfully able to develop the application. Now, as it is required that today's web application need to be powered by real time recommendations, I would like my model to be ready for new data that keeps coming on the server.
The site has quoted:</p>

<blockquote>
  <p>A better way to get the recommendations for you is training a matrix factorization model first and then augmenting the model using your ratings.</p>
</blockquote>

<p>How do I do that? I am using Python to develop my application.
Also, please tell me how do I persist the model to use it again, or an idea how do I interface this with a web service.
Thanking you</p>
";;8805315;5740;pissall;1;46844934;"<p>I don't think online learning is possible for ALS in Spark. That means you can't update the model while getting the data in real time. However, you can use the model to get the predictions.</p>

<p>Also, refer to:
<a href=""https://stackoverflow.com/questions/30509335/how-to-update-spark-matrixfactorizationmodel-for-als"">How to update Spark MatrixFactorizationModel for ALS</a></p>
"
4594855;11;shahid Afridi;<algorithm><machine-learning><supervised-learning>;28725232;0;Weight Vectors in Perceptron Algorithm in Machine learning;"<p>I'm researching on the perceptron Algorithm in machine learning.Till now,I understood the following things about the perceptron</p>

<pre><code>1)It's a supervised learning technique
2)It tries to create a hyper plane that linearly separates the class   
  labels ,which is when the perceptron converges
3)if the predicted output and the obtained output from the algorithm   
  doesnot match it adjusts it's weight vector and bias.
</code></pre>

<p>However,I couldnot understand  what happens to the weight vector if the<br>
   perceptron doesnot acheive convergence? Do the algorithm keeps on<br>
   updating the weight vector?</p>
";;2484687;5610;Raff.Edward;0;28733634;"<p>The preceptron can only converge <em>if and only if</em> the classes are linearly separable. If this is true the algorithm will converge to <strong><em>a</em></strong> solution, it has not guarantees about the quality of the solution in any way. The preceptron's hyperplane does not minimize any particular objective and it has no unique solution, any line that separates the two classes is equally valid to the perceptron. </p>

<p>If this is not true, the perceptron will never converge to a final solution. It may bounce around a reasonable area, but that is not guaranteed - and it could just fly off into a useless place. It will simply keep updating its weights until you force some stopping condition (most common is max number of passes through the data). </p>
"
3633250;4320;Maksim Khaitovich;<machine-learning><scikit-learn><ensemble-learning>;28727709;18;Ensemble of different kinds of regressors using scikit-learn (or any other python framework);"<p>I am trying to solve the regression task. I found out that 3 models are working nicely for different subsets of data: LassoLARS, SVR and Gradient Tree Boosting. I noticed that when I make predictions using all these 3 models and then make a table of 'true output' and outputs of my 3 models I see that each time at least one of the models is really close to the true output, though 2 others could be relatively far away.</p>

<p>When I compute minimal possible error (if I take prediction from 'best' predictor for each test example) I get a error which is much smaller than error of any model alone. So I thought about trying to combine predictions from these 3 diffent models into some kind of ensemble. Question is, how to do this properly? All my 3 models are build and tuned using scikit-learn, does it provide some kind of a method which could be used to pack models into ensemble? The problem here is that I don't want to just average predictions from all three models, I want to do this with weighting, where weighting should be determined based on properties of specific example.</p>

<p>Even if scikit-learn not provides such functionality, it would be nice if someone knows how to property address this task - of figuring out the weighting of each model for each example in data. I think that it might be done by a separate regressor built on top of all these 3 models, which will try output optimal weights for each of 3 models, but I am not sure if this is the best way of doing this.</p>
";35170149;676634;23217;Andreas Mueller;5;28731749;"<p>What you describe is called ""stacking"" which is not implemented in scikit-learn yet, but I think contributions would be welcome. An ensemble that just averages will be in pretty soon: <a href=""https://github.com/scikit-learn/scikit-learn/pull/4161"" rel=""noreferrer"">https://github.com/scikit-learn/scikit-learn/pull/4161</a></p>
"
3633250;4320;Maksim Khaitovich;<machine-learning><scikit-learn><ensemble-learning>;28727709;18;Ensemble of different kinds of regressors using scikit-learn (or any other python framework);"<p>I am trying to solve the regression task. I found out that 3 models are working nicely for different subsets of data: LassoLARS, SVR and Gradient Tree Boosting. I noticed that when I make predictions using all these 3 models and then make a table of 'true output' and outputs of my 3 models I see that each time at least one of the models is really close to the true output, though 2 others could be relatively far away.</p>

<p>When I compute minimal possible error (if I take prediction from 'best' predictor for each test example) I get a error which is much smaller than error of any model alone. So I thought about trying to combine predictions from these 3 diffent models into some kind of ensemble. Question is, how to do this properly? All my 3 models are build and tuned using scikit-learn, does it provide some kind of a method which could be used to pack models into ensemble? The problem here is that I don't want to just average predictions from all three models, I want to do this with weighting, where weighting should be determined based on properties of specific example.</p>

<p>Even if scikit-learn not provides such functionality, it would be nice if someone knows how to property address this task - of figuring out the weighting of each model for each example in data. I think that it might be done by a separate regressor built on top of all these 3 models, which will try output optimal weights for each of 3 models, but I am not sure if this is the best way of doing this.</p>
";35170149;3633250;4320;Maksim Khaitovich;10;28746157;"<p>Ok, after spending some time on googling 'stacking' (as mentioned by @andreas earlier) I found out how I could do the weighting in python even with scikit-learn. Consider the below:</p>

<p>I train a set of my regression models (as mentioned SVR, LassoLars and GradientBoostingRegressor). Then I run all of them on training data (same data which was used for training of each of these 3 regressors). I get predictions for examples with each of my algorithms and save these 3 results into pandas dataframe with columns 'predictedSVR', 'predictedLASSO' and 'predictedGBR'. And I add the final column into this datafrane which I call 'predicted' which is a real prediction value.</p>

<p>Then I just train a linear regression on this new dataframe:</p>

<pre><code>#df - dataframe with results of 3 regressors and true output
from sklearn linear_model
stacker= linear_model.LinearRegression()
stacker.fit(df[['predictedSVR', 'predictedLASSO', 'predictedGBR']], df['predicted'])
</code></pre>

<p>So when I want to make a prediction for new example I just run each of my 3 regressors separately and then I do: </p>

<pre><code>stacker.predict() 
</code></pre>

<p>on outputs of my 3 regressors. And get a result.</p>

<p>The problem here is that I am finding optimal weights for regressors 'on average, the weights will be same for each example on which I will try to make prediction.</p>
"
3633250;4320;Maksim Khaitovich;<machine-learning><scikit-learn><ensemble-learning>;28727709;18;Ensemble of different kinds of regressors using scikit-learn (or any other python framework);"<p>I am trying to solve the regression task. I found out that 3 models are working nicely for different subsets of data: LassoLARS, SVR and Gradient Tree Boosting. I noticed that when I make predictions using all these 3 models and then make a table of 'true output' and outputs of my 3 models I see that each time at least one of the models is really close to the true output, though 2 others could be relatively far away.</p>

<p>When I compute minimal possible error (if I take prediction from 'best' predictor for each test example) I get a error which is much smaller than error of any model alone. So I thought about trying to combine predictions from these 3 diffent models into some kind of ensemble. Question is, how to do this properly? All my 3 models are build and tuned using scikit-learn, does it provide some kind of a method which could be used to pack models into ensemble? The problem here is that I don't want to just average predictions from all three models, I want to do this with weighting, where weighting should be determined based on properties of specific example.</p>

<p>Even if scikit-learn not provides such functionality, it would be nice if someone knows how to property address this task - of figuring out the weighting of each model for each example in data. I think that it might be done by a separate regressor built on top of all these 3 models, which will try output optimal weights for each of 3 models, but I am not sure if this is the best way of doing this.</p>
";35170149;3510736;66448;Ami Tavory;20;35170149;"<p>This is a known interesting (and often painful!) problem with hierarchical predictions. A problem with training a number of predictors over the train data, then training a higher predictor over them, again using the train data - has to do with the bias-variance decomposition. </p>

<p>Suppose you have two predictors, one essentially an overfitting version of the other, then the former will appear over the train set to be better than latter. The combining predictor will favor the former for no true reason, just because it cannot distinguish overfitting from true high-quality prediction.</p>

<p>The known way of dealing with this is to prepare, for each row in the train data, for each of the predictors, a prediction for the row, based on a model <em>not</em> fit for this row. For the overfitting version, e.g., this won't produce a good result for the row, on average. The combining predictor will then be able to better assess a fair model for combining the lower-level predictors.</p>

<p>Shahar Azulay &amp; I wrote a transformer stage for dealing with this:</p>

<pre><code>class Stacker(object):
    """"""
    A transformer applying fitting a predictor `pred` to data in a way
        that will allow a higher-up predictor to build a model utilizing both this 
        and other predictors correctly.

    The fit_transform(self, x, y) of this class will create a column matrix, whose 
        each row contains the prediction of `pred` fitted on other rows than this one. 
        This allows a higher-level predictor to correctly fit a model on this, and other
        column matrices obtained from other lower-level predictors.

    The fit(self, x, y) and transform(self, x_) methods, will fit `pred` on all 
        of `x`, and transform the output of `x_` (which is either `x` or not) using the fitted 
        `pred`.

    Arguments:    
        pred: A lower-level predictor to stack.

        cv_fn: Function taking `x`, and returning a cross-validation object. In `fit_transform`
            th train and test indices of the object will be iterated over. For each iteration, `pred` will
            be fitted to the `x` and `y` with rows corresponding to the
            train indices, and the test indices of the output will be obtained
            by predicting on the corresponding indices of `x`.
    """"""
    def __init__(self, pred, cv_fn=lambda x: sklearn.cross_validation.LeaveOneOut(x.shape[0])):
        self._pred, self._cv_fn  = pred, cv_fn

    def fit_transform(self, x, y):
        x_trans = self._train_transform(x, y)

        self.fit(x, y)

        return x_trans

    def fit(self, x, y):
        """"""
        Same signature as any sklearn transformer.
        """"""
        self._pred.fit(x, y)

        return self

    def transform(self, x):
        """"""
        Same signature as any sklearn transformer.
        """"""
        return self._test_transform(x)

    def _train_transform(self, x, y):
        x_trans = np.nan * np.ones((x.shape[0], 1))

        all_te = set()
        for tr, te in self._cv_fn(x):
            all_te = all_te | set(te)
            x_trans[te, 0] = self._pred.fit(x[tr, :], y[tr]).predict(x[te, :]) 
        if all_te != set(range(x.shape[0])):
            warnings.warn('Not all indices covered by Stacker', sklearn.exceptions.FitFailedWarning)

        return x_trans

    def _test_transform(self, x):
        return self._pred.predict(x)
</code></pre>

<hr>

<p>Here is an example of the improvement for the setting described in @MaximHaytovich's answer.</p>

<p>First, some setup:</p>

<pre><code>    from sklearn import linear_model
    from sklearn import cross_validation
    from sklearn import ensemble
    from sklearn import metrics

    y = np.random.randn(100)
    x0 = (y + 0.1 * np.random.randn(100)).reshape((100, 1)) 
    x1 = (y + 0.1 * np.random.randn(100)).reshape((100, 1)) 
    x = np.zeros((100, 2)) 
</code></pre>

<p>Note that <code>x0</code> and <code>x1</code> are just noisy versions of <code>y</code>. We'll use the first 80 rows for train, and the last 20 for test.</p>

<p>These are the two predictors: a higher-variance gradient booster, and a linear predictor:</p>

<pre><code>    g = ensemble.GradientBoostingRegressor()
    l = linear_model.LinearRegression()
</code></pre>

<p>Here is the methodology suggested in the answer:</p>

<pre><code>    g.fit(x0[: 80, :], y[: 80])
    l.fit(x1[: 80, :], y[: 80])

    x[:, 0] = g.predict(x0)
    x[:, 1] = l.predict(x1)

    &gt;&gt;&gt; metrics.r2_score(
        y[80: ],
        linear_model.LinearRegression().fit(x[: 80, :], y[: 80]).predict(x[80: , :]))
    0.940017788444
</code></pre>

<p>Now, using stacking:</p>

<pre><code>    x[: 80, 0] = Stacker(g).fit_transform(x0[: 80, :], y[: 80])[:, 0]
    x[: 80, 1] = Stacker(l).fit_transform(x1[: 80, :], y[: 80])[:, 0]

    u = linear_model.LinearRegression().fit(x[: 80, :], y[: 80])

    x[80: , 0] = Stacker(g).fit(x0[: 80, :], y[: 80]).transform(x0[80:, :])
    x[80: , 1] = Stacker(l).fit(x1[: 80, :], y[: 80]).transform(x1[80:, :])

    &gt;&gt;&gt; metrics.r2_score(
        y[80: ],
        u.predict(x[80:, :]))
    0.992196564279
</code></pre>

<p>The stacking prediction does better. It realizes that the gradient booster is not that great.</p>
"
3633250;4320;Maksim Khaitovich;<machine-learning><scikit-learn><ensemble-learning>;28727709;18;Ensemble of different kinds of regressors using scikit-learn (or any other python framework);"<p>I am trying to solve the regression task. I found out that 3 models are working nicely for different subsets of data: LassoLARS, SVR and Gradient Tree Boosting. I noticed that when I make predictions using all these 3 models and then make a table of 'true output' and outputs of my 3 models I see that each time at least one of the models is really close to the true output, though 2 others could be relatively far away.</p>

<p>When I compute minimal possible error (if I take prediction from 'best' predictor for each test example) I get a error which is much smaller than error of any model alone. So I thought about trying to combine predictions from these 3 diffent models into some kind of ensemble. Question is, how to do this properly? All my 3 models are build and tuned using scikit-learn, does it provide some kind of a method which could be used to pack models into ensemble? The problem here is that I don't want to just average predictions from all three models, I want to do this with weighting, where weighting should be determined based on properties of specific example.</p>

<p>Even if scikit-learn not provides such functionality, it would be nice if someone knows how to property address this task - of figuring out the weighting of each model for each example in data. I think that it might be done by a separate regressor built on top of all these 3 models, which will try output optimal weights for each of 3 models, but I am not sure if this is the best way of doing this.</p>
";35170149;6549363;81;sf631;5;44316688;"<p>Late response, but I wanted to add one practical point for this sort of stacked regression approach (which I use this frequently in my work).  </p>

<p>You may want to choose an algorithm for the stacker which allows positive=True (for example, ElasticNet).  I have found that, when you have one relatively stronger model, the unconstrained LinearRegression() model will often fit a larger positive coefficient to the stronger and a negative coefficient to the weaker model.  </p>

<p>Unless you actually believe that your weaker model has negative predictive power, this is not a helpful outcome.  Very similar to having high multi-colinearity between features of a regular regression model.  Causes all sorts of edge effects.  </p>

<p>This comment applies most significantly to noisy data situations.  If you're aiming to get RSQ of 0.9-0.95-0.99, you'd probably want to throw out the model which was getting a negative weighting.  </p>
"
200317;14774;add-semi-colons;<python><algorithm><machine-learning><precision-recall>;28734607;3;Evaluation of lists: AvgP@K and R@K are they same?;"<p>My goal is to understand Average <code>Precision at K</code>, and <code>Recall at K</code>. I have two lists, one is predicted and other is actual (ground truth) </p>

<p>lets call these two lists as predicted and actual. Now I want to do <code>precision@k</code> and <code>recall@k</code>.</p>

<p>Using python I implemented Avg precision at K as follows: </p>

<pre><code>def apk(actual, predicted, k=10):
    """"""
    Computes the average precision at k.

    This function computes the average precision at k between two lists of items.

    Parameters
    ----------
    actual: list
            A list of elements that are to be predicted (order doesn't matter)
    predicted : list
            A list of predicted elements (order does matter)
    k: int, optional

    Returns
    -------
    score : double
            The average precision at k over the input lists

    """"""
    if len(predicted) &gt; k:
        predicted = predicted[:k]

    score = 0.0
    num_hits = 0.0

    for i,p in enumerate(predicted):
        if p in actual and p not in predicted[:i]:
            num_hits += 1.0
            score += num_hits / (i + 1.0)

    if not actual:
        return 1.0
    if min(len(actual), k) == 0:
        return 0.0
    else:
        return score / min(len(actual), k)
</code></pre>

<p>lets assume that our predicted has 5 strings in following order: 
<code>predicted = ['b','c','a','e','d'] and</code>actual = ['a','b','e']<code>since we are doing @k would the precision@k is same as</code>recall@k<code>? If not how would I do</code>recall@k`</p>

<p>If I want to do <code>f-measure (f-score)</code> what would be the best route to do for above mention list?</p>
";28752478;4588780;4531;Nikita Astrakhantsev;8;28752478;"<p>I guess, you've already checked <a href=""http://en.wikipedia.org/wiki/Information_retrieval#Average_precision"" rel=""noreferrer"">wiki</a>. Based on its formula, the 3rd and the biggest one (after the words 'This finite sum is equivalent to:'), let's see at your example for each iteration:</p>

<ol>
<li>i=1 p = 1</li>
<li>i=2 rel = 0</li>
<li>i=3 p = 2/3</li>
<li>i=4 p = 3/4</li>
<li>i=5 rel = 0</li>
</ol>

<p>So, avp@4 = avp@5 = (1 + 0.66 + 0.75) / 3 = 0.805; avp@3 = (1 + 0.66) / 3 and so on.</p>

<p>Recall@5 = Recall@4 = 3/3 = 1; Recall@3 = 2/3; Recall@2 =Recall@1 = 1/3</p>

<p>Below is the code for precision@k and recall@k. I kept your notation, while it seems to be more common to use <code>actual</code> for observed/returned value and <code>expected</code> for ground truth (see for example JUnit defaults).</p>

<pre><code>def precision(actual, predicted, k):
    act_set = set(actual)
    pred_set = set(predicted[:k])
    result = len(act_set &amp; pred_set) / float(k)
    return result

def recall(actual, predicted, k):
    act_set = set(actual)
    pred_set = set(predicted[:k])
    result = len(act_set &amp; pred_set) / float(len(act_set))
    return result
</code></pre>
"
17965;29141;Luke Francl;<machine-learning><mahout>;28735445;1;Baseline classification accuracy with Mahout;"<p>In his <a href=""https://weka.waikato.ac.nz/dataminingwithweka/preview"" rel=""nofollow"">Data Mining with Weka</a> class, Prof. Witten stresses the importance of checking your classifier against simpler ones, like the ZeroR classifier which picks the most common class (if your fancy machine learning algorithm is barely beating ZeroR's accuracy, it's probably not working very well).</p>

<p>Is there a way to check baseline accuracy of a classifier built with Apache Mahout, either using ZeroR or some thing else?</p>
";;540873;20218;Thomas Jungblut;0;28738219;"<p>Take your data, count how often the classes occur. </p>

<p>And that's literally what ZeroR does. Since it is so simple I don't think Mahout includes it in their Framework. </p>

<p><strong>Writing a MapReduce job to do this is rather simple:</strong></p>

<p>Mapper: </p>

<ul>
<li>emit the class as key, 1 as value (let the mapper precompute the sum over his whole input for network efficiency or use a combiner)</li>
</ul>

<p>Reducer </p>

<ul>
<li>sum over all keys, take the max and divide by the sum over all classes</li>
</ul>

<p>Then you would know what baseline accuracy you would get from predicting the majority class. </p>

<p><strong>The Spark implementation is similar:</strong> </p>

<p>Group by the class and then count per class and divide by the sum over all classes. Pick the max, that's the baseline.</p>
"
4608966;21;imran khan;<machine-learning><logistic-regression>;28736050;1;Logistic Regression in Machine Learning;"<p>I'm researching on the topic of ""logistic Regression"" in machine learning.I could understand the entire concept that it's trying to maximize the likelihood of an instance belonging to a particular class label </p>

<p>The  algorithm, if run for many iterations, finds a weight vector that separates the instances and then keeps increasing the magnitude of the weight vector.  I donot understand why would it try to increase the magnitude of weight vector</p>

<p>Any Help would be highly appreciable!</p>
";;3054114;269;Rishabh;0;28736134;"<p>I think what you are asking about is Regularization in Machine learning, From my understanding of it this is done so as to avoid the phenomena known as overfitting i.e when the hypothesis fits the training data almost perfectly at the cost of giving a poor hypothesis for the test data.</p>

<p>I hope this helps to an extent.</p>
"
4608966;21;imran khan;<machine-learning><logistic-regression>;28736050;1;Logistic Regression in Machine Learning;"<p>I'm researching on the topic of ""logistic Regression"" in machine learning.I could understand the entire concept that it's trying to maximize the likelihood of an instance belonging to a particular class label </p>

<p>The  algorithm, if run for many iterations, finds a weight vector that separates the instances and then keeps increasing the magnitude of the weight vector.  I donot understand why would it try to increase the magnitude of weight vector</p>

<p>Any Help would be highly appreciable!</p>
";;70915;15178;Niki;1;28737833;"<p>I'm guessing your data is linearly separable? IIRC, logistic regression breaks down in that case. I think this is a well-known problem. Quoting from <a href=""http://pages.cs.wisc.edu/%7Ejerryzhu/cs769/lr.pdf"" rel=""nofollow noreferrer"">here</a> (the first hit on google for &quot;logistic regression separable data&quot;):</p>
<blockquote>
<p>...However, when the training data is linearly separable, two bad things happen: 1. |Î¸| goes to infinity; 2. There are infinite number of MLEâ€™s. To see this, note any step function (sigmoid with |Î¸|=âˆž) that is in the gap between the two classes is an MLE</p>
<p>One way to avoid this is to incorporate a prior on Î¸ in the form of a zero-mean Gaussian with covariance 1/(2Î»)I</p>
</blockquote>
"
1256369;2338;Adorn;<c++><image-processing><machine-learning><neural-network><gradient>;28746889;22;Neural Network not learning - MNIST data - Handwriting recognition;"<p>I have written a Neural Network Program. It works for Logic Gates, but when I try to use it for recognizing handwritten digits - it simply does not learn.</p>

<p>Please find the code below:</p>

<p>// This is a single neuron; this might be necessary in order to understand remaining code</p>

<pre><code>typedef struct SingleNeuron
{
    double                  outputValue;
    std::vector&lt;double&gt;     weight;
    std::vector&lt;double&gt;     deltaWeight;
    double                  gradient;
    double                  sum;
}SingleNeuron;
</code></pre>

<p>Then I initialize the net. I set weights to be random value between -0.5 to +0.5, sum to 0, deltaWeight to 0</p>

<p>Then comes the FeedForward:</p>

<pre><code>for (unsigned i = 0; i &lt; inputValues.size(); ++i)
{
    neuralNet[0][i].outputValue = inputValues[i];
    neuralNet[0][i].sum = 0.0;
    //  std::cout &lt;&lt; ""o/p Val = "" &lt;&lt; neuralNet[0][i].outputValue &lt;&lt; std::endl;
}

for (unsigned i = 1; i &lt; neuralNet.size(); ++i)
{
    std::vector&lt;SingleNeuron&gt; prevLayerNeurons = neuralNet[i - 1];
    unsigned j = 0;
    double thisNeuronOPVal = 0;
    //  std::cout &lt;&lt; std::endl;
    for (j = 0; j &lt; neuralNet[i].size() - 1; ++j)
    {
        double sum = 0;
        for (unsigned k = 0; k &lt; prevLayerNeurons.size(); ++k)
        {
            sum += prevLayerNeurons[k].outputValue * prevLayerNeurons[k].weight[j];
        }
        neuralNet[i][j].sum = sum;
        neuralNet[i][j].outputValue = TransferFunction(sum);
        //      std::cout &lt;&lt; neuralNet[i][j].outputValue &lt;&lt; ""\t"";
    }
    //      std::cout &lt;&lt; std::endl;
}
</code></pre>

<p>My transfer function and its derivative is mentioned at the end.</p>

<p>After this I try to back-propagate using:</p>

<pre><code>// calculate output layer gradients
for (unsigned i = 0; i &lt; outputLayer.size() - 1; ++i)
{
    double delta = actualOutput[i] - outputLayer[i].outputValue;
    outputLayer[i].gradient = delta * TransferFunctionDerivative(outputLayer[i].sum);
}
//  std::cout &lt;&lt; ""Found Output gradients ""&lt;&lt; std::endl;
// calculate hidden layer gradients
for (unsigned i = neuralNet.size() - 2; i &gt; 0; --i)
{
    std::vector&lt;SingleNeuron&gt;&amp; hiddenLayer = neuralNet[i];
    std::vector&lt;SingleNeuron&gt;&amp; nextLayer = neuralNet[i + 1];

    for (unsigned j = 0; j &lt; hiddenLayer.size(); ++j)
    {
        double dow = 0.0;
        for (unsigned k = 0; k &lt; nextLayer.size() - 1; ++k)
        {
            dow += nextLayer[k].gradient * hiddenLayer[j].weight[k];
        }
        hiddenLayer[j].gradient = dow * TransferFunctionDerivative(hiddenLayer[j].sum);
    }
}
//  std::cout &lt;&lt; ""Found hidden layer gradients ""&lt;&lt; std::endl;

// from output to 1st hidden layer, update all weights
for (unsigned i = neuralNet.size() - 1; i &gt; 0; --i)
{
    std::vector &lt;SingleNeuron&gt;&amp; currentLayer = neuralNet[i];
    std::vector &lt;SingleNeuron&gt;&amp; prevLayer = neuralNet[i - 1];

    for (unsigned j = 0; j &lt; currentLayer.size() - 1; ++j)
    {
        for (unsigned k = 0; k &lt; prevLayer.size(); ++k)
        {
            SingleNeuron&amp; thisNeueon = prevLayer[k];
            double oldDeltaWeight = thisNeueon.deltaWeight[j];
            double newDeltaWeight = ETA * thisNeueon.outputValue * currentLayer[j].gradient + (ALPHA * oldDeltaWeight);
            thisNeueon.deltaWeight[j] = newDeltaWeight;
            thisNeueon.weight[j] += newDeltaWeight;
        }
    }
}
</code></pre>

<p>These are the TransferFuntion and its derivative;</p>

<pre><code>double TransferFunction(double x)
{
    double val;
    //val = tanh(x);
    val = 1 / (1 + exp(x * -1));
    return val;
}

double TransferFunctionDerivative(double x)
{
    //return 1 - x * x;
    double val = exp(x * -1) / pow((exp(x * -1) + 1), 2);
    return val;
}
</code></pre>

<p>One thing I observed If i use standard sigmoid function to be my transfer function AND if I pass output of neuron to transfer function - Result is INFINITY. But tanh(x) works fine with this value</p>

<p>So if I am using 1/1+e^(-x) as transfer function I have to pass <code>Sum of Net Inputs</code> and with <code>tanh</code> being my transfer function I have to pass <code>output</code> of current neuron. </p>

<p>I do not completely understand why this is the way it is, may be this calls for a different question.</p>

<p>But this question is really about something else: <strong>NETWORK IS WORKING FOR LOGIC GATES BUT NOT FOR CHARACTER RECOGNITION</strong></p>

<p>I have tried many variations/combinations of <code>Learning Rate</code> and <code>Acceleration</code> and <code># hidden layers</code> and <code>their sizes</code>. Please find the results below:</p>

<pre><code>AvgErr: 0.299399          #Pass799
AvgErr : 0.305071         #Pass809
AvgErr : 0.303046         #Pass819
AvgErr : 0.299569         #Pass829
AvgErr : 0.30413          #Pass839
AvgErr : 0.304165         #Pass849
AvgErr : 0.300529         #Pass859
AvgErr : 0.302973         #Pass869
AvgErr : 0.299238         #Pass879
AvgErr : 0.304708         #Pass889
AvgErr : 0.30068          #Pass899
AvgErr : 0.302582         #Pass909
AvgErr : 0.301767         #Pass919
AvgErr : 0.303167         #Pass929
AvgErr : 0.299551         #Pass939
AvgErr : 0.301295         #Pass949
AvgErr : 0.300651         #Pass959
AvgErr : 0.297867         #Pass969
AvgErr : 0.304221         #Pass979
AvgErr : 0.303702         #Pass989
</code></pre>

<p>After looking at the results you might feel this guy is simply stuck into local minima, but please wait and read through:</p>

<pre><code>Input = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]          
Output = 0.0910903, 0.105674, 0.064575, 0.0864824, 0.128682, 0.0878434, 0.0946296, 0.154405, 0.0678767, 0.0666924

Input = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Output = 0.0916106, 0.105958, 0.0655508, 0.086579, 0.126461, 0.0884082, 0.110953, 0.163343, 0.0689315, 0.0675822

Input = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]          
Output = 0.105344, 0.105021, 0.0659517, 0.0858077, 0.123104, 0.0884107, 0.116917, 0.161911, 0.0693426, 0.0675156

Input = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]          
Output = , 0.107113, 0.101838, 0.0641632, 0.0967766, 0.117149, 0.085271, 0.11469, 0.153649, 0.0672772, 0.0652416
</code></pre>

<p>Above is the output of epoch #996, #997,#998 and #999</p>

<p>So simply network is not learning. For this e.g. I have used ALPHA = 0.4, ETA = 0.7, 10 hidden layers each of 100 neurons and average is over 10 epochs. If you are worried about Learning Rate being 0.4 or so many hidden layers I have already tried their variations. For e.g. for learning rate being 0.1 and 4 hidden layers - each of 16</p>

<pre><code>Input = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]          
Output = 0.0883238, 0.0983253, 0.0613749, 0.0809751, 0.124972, 0.0897194, 0.0911235, 0.179984, 0.0681346, 0.0660039

Input = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]          
Output = 0.0868767, 0.0966924, 0.0612488, 0.0798343, 0.120353, 0.0882381, 0.111925, 0.169309, 0.0676711, 0.0656819

Input = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]          
Output = 0.105252, 0.0943837, 0.0604416, 0.0781779, 0.116231, 0.0858496, 0.108437, 0.1588, 0.0663156, 0.0645477

Input = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]          
Output = 0.102023, 0.0914957, 0.059178, 0.09339, 0.111851, 0.0842454, 0.104834, 0.149892, 0.0651799, 0.063558
</code></pre>

<p>I am so damn sure that I have missed something. I am not able to figure it out. I have read Tom Mitchel's algorithm so many times, but I don't know what is wrong. Whatever example I solve by hand - works! (Please don't ask me to solve MNIST data images by hand ;) ) I do not know where to change the code, what to do.. please help out..</p>

<h2>EDIT -- Uploading more data as per suggestions in comments</h2>

<p><a href=""http://textuploader.com/4a57"" rel=""nofollow"">1 Hidden Layer of 32</a> -- still no learning.</p>

<p>Expected Output -- Input is images between 0-9, so a simple vector describing which is current image, that bit is 1 all others are 0. So i would want output to be as close to 1 for that particular bit and others being close to 0 For e.g. if input is <code>Input = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]</code> I would want output to be something like <code>Output = 0.002023, 0.0914957, 0.059178, 0.09339, 0.011851, 0.0842454, 0.924834, 0.049892, 0.0651799, 0.063558</code> (THis is vague, hand-generated)</p>

<p>Here are the links of other researcher's work. </p>

<p><a href=""http://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html"" rel=""nofollow"">Stanford</a> </p>

<p><a href=""http://eblearn.sourceforge.net/old/demos/mnist/index.shtml"" rel=""nofollow"">SourceForge</a> -- This is rather a library</p>

<p>Not only these 2, there are so many sites showing the demos.</p>

<p>Things are working quite fine for them. If I set my network parameters(Alpha, ETA) like them I am not getting results like them, so this is reassurance that something is wrong with my code.</p>

<h2>EDIT 2</h2>

<p>Adding more failure cases</p>

<p><a href=""http://m.uploadedit.com/ba3a/1425032342920.txt"" rel=""nofollow"">Accelaration - 0.7, Learning Rate 0.1</a></p>

<p><a href=""http://m.uploadedit.com/ba3a/1425032545677.txt"" rel=""nofollow"">Accelaration - 0.7, Learning Rate 0.6</a></p>

<p>In both of the above cases Hidden layers were 3, each of 32 neurons.</p>
";29781669;1666357;1170;Tokkot;4;29781669;"<p><em>This answer is copied from the OP's comment on the question.</em></p>

<p>I solved the puzzle. I had made the worst possible mistake. I was giving wrong input. I have used opencv to scan the images, instead of using <code>reshape</code> I was using <code>resize</code> and so input was linear interpolation of images. So my input was wrong. There was nothing wrong with the code. My network is <code>784 - 65 - 10</code> giving 96.43% accuracy.</p>
"
4611375;1065;WÎ”_;<python><machine-learning><artificial-intelligence>;28748957;0;How can I allow an API to remotely access my code without revealing it?;"<p>A friend is building an API. He wants my machine learning algorithm (written in Python) to be incorporated into this API. I have heard that his API can <strong>'<em>remotely</em> access'</strong> my algorithm and in this way I don't have to reveal what the code actually is.</p>

<p>If this is all true, can someone point me in the direction of building this. I know very little about API's!</p>

<p>Apologies, I'm a newbie to this!</p>
";28749293;1054573;2456;Leonard Pauli;2;28749293;"<p>You could upload your code to a server and create your own API for your friend's API to use.</p>

<p>An example of how to create a python server: <a href=""http://www.acmesystems.it/python_httpserver"" rel=""nofollow"">http://www.acmesystems.it/python_httpserver</a></p>

<p>Basically, when you get a request, call your algorithm script and pass the data received from the request, then return an appropriate response.</p>

<p>For better security, you could require some identification in the request to prove that it really is your friend that's sending it. If you're afraid of your code getting leaked if the server would be hacked, you could use tools like docker.com to host the server script and your algorithm script in two different virtual machines.</p>
"
2335004;121;user2335004;<java><machine-learning><weka><liblinear>;28752700;1;How to use liblinear with weka gui?;"<p>I am using weka to build one model. I have requirement of using liblinear for our model. For testing using weka GUI, I need to add liblinear jar into my classpath but after adding liblinear jar in classpath, Weka GUI still give me error liblinear classes not found. I have searched through internet and found nothing.
Is there anybody else who face same problem as me. Also, any help will be appreciated. </p>
";30310849;2335004;121;user2335004;0;30310849;"<p>I found that to use liblinear in weka gui, you have ti install weka-3-7 version. Then click on tools--> package manager--> install liblinear.</p>
"
4277287;11;Karen;<machine-learning><weka><nlp>;28752724;1;Weka - StringtoVector Filter Not working;"<p>I am practicing Weka using the Reuters data.  The StringtoVector Classifier works for converting my string data (shown below), so I can analyze the articles to understand what words predict the article type. If the article type is true, the original dataset said TRUE/FALSE, but I converted it to 0/1. However, it refuses to work for this one arff file using the StringtoVector filter on the ""review"" string.  </p>

<p>I used the following StringtoVector filter while ONLY checking the review attribute: </p>

<pre><code>weka.filters.unsupervised.attribute.StringToWordVector -R first-last -W 1000 -prune-rate -1.0 -N 0 -stemmer weka.core.stemmers.NullStemmer -M 1 -tokenizer ""weka.core.tokenizers.WordTokenizer -delimiters \"" \\r\\n\\t.,;:\\\'\\\""()?!\""""
</code></pre>

<p>I get this error: 
<b>""Problem filtering instances: attribute names are not unique. Cause: sentiment"" when only review is checked for the filter.</b></p>

<p>Here is the header of my dataset/formatting for a few of the cases:</p>

<pre><code>@relation text_files
@attribute review string
@attribute sentiment {0, 1}
@data   ""cocoa the the cocoa the early the levels its the the this the ended the mln against at the that cocoa the to crop cocoa to crop around mln sales at mln the to this cocoa export the their cocoa prices to to per to offer sales at to dlrs per to to crop sales to at dlrs at dlrs at dlrs per sales at at at at to dlrs at at dlrs the currency sales at to dlrs dlrs dlrs the currency sales at at dlrs at at dlrs at at sales at mln against the crop mln against the the to to the cocoa commission reuter"", 0""prices reserve the agriculture department reported the reserve price loan call price price wheat corn 1986 loan call price price reserves grain wheat per reuter"", 0""grain crop their products to to wheat export the export wheat oil oil reuter"", 0""inc the stock corp its dlrs oil to dlrs production its the company to its to profit to reuter"", 0""products stock split products inc its stock split its common shares shareholders the company its to to shareholders at the the stock mln to mln reuter"", 0
</code></pre>

<p><b>Anyone have any ideas on why this is happening?</b> I was thinking there might be a conflict with the fact the data might contain 0 and 1s as part of the words occurring naturally in the text.  I'm also thinking I might need an additional space before the quote for the string after the previous string.</p>
";;968064;2363;Rushdi Shams;2;28782516;"<p>Hi the problem is the filter converts every term in a string into an attribute. Now there must be a term ""review"" or ""sentiment"" in your data section. Therefore the attributes are duplicated. </p>

<p>So, change the names of these two attributes like ""myreview"" and ""mysentiment"" or to something that is unlikely to occur in your data. It should work.</p>
"
4277287;11;Karen;<machine-learning><weka><nlp>;28752724;1;Weka - StringtoVector Filter Not working;"<p>I am practicing Weka using the Reuters data.  The StringtoVector Classifier works for converting my string data (shown below), so I can analyze the articles to understand what words predict the article type. If the article type is true, the original dataset said TRUE/FALSE, but I converted it to 0/1. However, it refuses to work for this one arff file using the StringtoVector filter on the ""review"" string.  </p>

<p>I used the following StringtoVector filter while ONLY checking the review attribute: </p>

<pre><code>weka.filters.unsupervised.attribute.StringToWordVector -R first-last -W 1000 -prune-rate -1.0 -N 0 -stemmer weka.core.stemmers.NullStemmer -M 1 -tokenizer ""weka.core.tokenizers.WordTokenizer -delimiters \"" \\r\\n\\t.,;:\\\'\\\""()?!\""""
</code></pre>

<p>I get this error: 
<b>""Problem filtering instances: attribute names are not unique. Cause: sentiment"" when only review is checked for the filter.</b></p>

<p>Here is the header of my dataset/formatting for a few of the cases:</p>

<pre><code>@relation text_files
@attribute review string
@attribute sentiment {0, 1}
@data   ""cocoa the the cocoa the early the levels its the the this the ended the mln against at the that cocoa the to crop cocoa to crop around mln sales at mln the to this cocoa export the their cocoa prices to to per to offer sales at to dlrs per to to crop sales to at dlrs at dlrs at dlrs per sales at at at at to dlrs at at dlrs the currency sales at to dlrs dlrs dlrs the currency sales at at dlrs at at dlrs at at sales at mln against the crop mln against the the to to the cocoa commission reuter"", 0""prices reserve the agriculture department reported the reserve price loan call price price wheat corn 1986 loan call price price reserves grain wheat per reuter"", 0""grain crop their products to to wheat export the export wheat oil oil reuter"", 0""inc the stock corp its dlrs oil to dlrs production its the company to its to profit to reuter"", 0""products stock split products inc its stock split its common shares shareholders the company its to to shareholders at the the stock mln to mln reuter"", 0
</code></pre>

<p><b>Anyone have any ideas on why this is happening?</b> I was thinking there might be a conflict with the fact the data might contain 0 and 1s as part of the words occurring naturally in the text.  I'm also thinking I might need an additional space before the quote for the string after the previous string.</p>
";;15634609;1;Eason;0;67091026;"<p>I also encountered the same problem because the word &quot;domain&quot; appeared in the data, causing the filter to misunderstand when recognizing it. My solution was to remove all the &quot;domain&quot; from the data and keep only the &quot;domain&quot; in @attribute.</p>
"
4277287;11;Karen;<machine-learning><weka><nlp>;28752724;1;Weka - StringtoVector Filter Not working;"<p>I am practicing Weka using the Reuters data.  The StringtoVector Classifier works for converting my string data (shown below), so I can analyze the articles to understand what words predict the article type. If the article type is true, the original dataset said TRUE/FALSE, but I converted it to 0/1. However, it refuses to work for this one arff file using the StringtoVector filter on the ""review"" string.  </p>

<p>I used the following StringtoVector filter while ONLY checking the review attribute: </p>

<pre><code>weka.filters.unsupervised.attribute.StringToWordVector -R first-last -W 1000 -prune-rate -1.0 -N 0 -stemmer weka.core.stemmers.NullStemmer -M 1 -tokenizer ""weka.core.tokenizers.WordTokenizer -delimiters \"" \\r\\n\\t.,;:\\\'\\\""()?!\""""
</code></pre>

<p>I get this error: 
<b>""Problem filtering instances: attribute names are not unique. Cause: sentiment"" when only review is checked for the filter.</b></p>

<p>Here is the header of my dataset/formatting for a few of the cases:</p>

<pre><code>@relation text_files
@attribute review string
@attribute sentiment {0, 1}
@data   ""cocoa the the cocoa the early the levels its the the this the ended the mln against at the that cocoa the to crop cocoa to crop around mln sales at mln the to this cocoa export the their cocoa prices to to per to offer sales at to dlrs per to to crop sales to at dlrs at dlrs at dlrs per sales at at at at to dlrs at at dlrs the currency sales at to dlrs dlrs dlrs the currency sales at at dlrs at at dlrs at at sales at mln against the crop mln against the the to to the cocoa commission reuter"", 0""prices reserve the agriculture department reported the reserve price loan call price price wheat corn 1986 loan call price price reserves grain wheat per reuter"", 0""grain crop their products to to wheat export the export wheat oil oil reuter"", 0""inc the stock corp its dlrs oil to dlrs production its the company to its to profit to reuter"", 0""products stock split products inc its stock split its common shares shareholders the company its to to shareholders at the the stock mln to mln reuter"", 0
</code></pre>

<p><b>Anyone have any ideas on why this is happening?</b> I was thinking there might be a conflict with the fact the data might contain 0 and 1s as part of the words occurring naturally in the text.  I'm also thinking I might need an additional space before the quote for the string after the previous string.</p>
";;4698227;741;fracpete;0;67098880;"<p>The easiest solution to avoid these attribute name clashes, is to use a prefix for the generated attributes.</p>
<p>The prefix can be supplied via the <code>-P</code> command-line option, the <code>attributeNamePrefix</code> option in the GenericObjectEditor or the <code>setAttributeNamePrefix</code> method from Java code.</p>
<p>See Javadoc of <a href=""https://weka.sourceforge.io/doc.dev/weka/filters/unsupervised/attribute/StringToWordVector.html"" rel=""nofollow noreferrer"">StringToWordVector</a> filter.</p>
"
4520962;85;Jae ;<matlab><machine-learning><contour><knn>;28757295;1;k-NN Classify in Matlab;"<p>So once I extracted the image which I want from the original image as attached here,<img src=""https://i.stack.imgur.com/ErmOX.jpg"" alt=""enter image description here""> I considered contour length as feature 1 and contour area as feature 2. I used sum(sum(ds)) to count the number of white pixels in binary images for each species. ds is the attached image here. And then I found the contour area, the number of pixels inscribed by the conotur by the following code.</p>

<pre><code>area=0;
for col=1:464 
    temp=find(ds(:,col)==1);  
    if temp 
        area= area + (temp(end)-temp(1)+1); 
    end
end
area;
</code></pre>

<p>Now I should plot feature 1 vs. feature 2 plot and check if there are separable clusters and Use k-NN to classify. Can anyone tell me how to plot it, check and classify. Thanks ! </p>
";;4050550;2152;Steffen;0;28760423;"<p>You can plot it by</p>

<pre><code>figure(1);
plot(feature1_classA,feature2_classA,'b.');
plot(feature1_classOther,feature2_classOther,'r.');
</code></pre>

<p>then check out if the blue dots are somehow separated from the blue dots.</p>

<p>KNN can be easily implemented. You need some labeled data set.
When classifying a new data point, find the K closest points in the labeled data set and check from which class the majority is. Assing this label to new data point. Shouldn't be that hard to implement.</p>
"
1840877;1072;kundan;<machine-learning><nltk><stanford-nlp><text-classification><naivebayes>;28764459;6;How to train a naive bayes classifier with pos-tag sequence as a feature?;"<p>I have two classes of sentences. Each has reasonably distinct pos-tag sequence. How can I train a Naive-Bayes classifier with POS-Tag sequence as a feature? Does Stanford CoreNLP/NLTK (Java or Python) provide any method for building a classifier with pos-tag as a feature?  I know in python <code>NaiveBayesClassifier</code> allows for building a NB classifier but it uses <code>contains-a-word</code> as feature but can it be extended to use pos-tag-sequence as a feature ?</p>
";28783642;4611541;400;char bugs;6;28783642;"<p>If you know how to train and predict texts (or sentences in your case) using nltk's naive bayes classifier and words as features, than you can easily extend this approach in order to classify texts by pos-tags. This is because the classifier don't care about whether your feature-strings are words or tags. So you can simply replace the words of your sentences by pos-tags using for example nltk's standard pos tagger:</p>

<pre class=""lang-py prettyprint-override""><code>sent = ['So', 'they', 'have', 'internet', 'on', 'computers' , 'now']
tags = [t for w, t in nltk.pos_tag(sent)]
print tags
</code></pre>

<p>['IN', 'PRP', 'VBP', 'JJ', 'IN', 'NNS', 'RB']</p>

<p>As from now you can proceed with the ""contains-a-word"" approach.</p>
"
1169020;31156;Diego;<r><machine-learning><glm><prediction><random-forest>;28764742;1;R prediction within an interval;"<p>quick question on prediction.</p>

<p>The value Iâ€™m trying to predict is either 0 or 1 (it is set as numeric, not as a factor) so when I run my random forest:</p>

<pre><code>fit &lt;- randomForest(PredictValue ~ &lt;variables&gt;, data=trainData, ntree=50) 
</code></pre>

<p>and predict:</p>

<pre><code>pred&lt;-predict(fit, testData)
</code></pre>

<p>all my predictions are between 0 and 1 â€“ which is what I expect and - I Imagine - can be interpreted as the probability of being 1.</p>

<p>Now, If I go through the same process using the gbm algorithm:</p>

<pre><code>fitgbm &lt;- gbm(PredictValue~ &lt;variables&gt;, data=trainData, distribution = ""bernoulli"", n.trees = 500,   bag.fraction = 0.75, cv.folds = 5, interaction.depth = 3)
predgbm &lt;- predict(fitgbm, testData)
</code></pre>

<p>the values are from -0.5 to 0.5</p>

<p>I also tried glm and the range was worst, from around -3 to 3.</p>

<p>So, my question is: is it possible to set the algorithms to predict between 0 and 1?</p>

<p>Thanks</p>
";28765067;4130044;33919;LyzandeR;1;28765067;"<p>You need to specify <code>type='response'</code> for this to happen:</p>

<p>Check this example:</p>

<pre><code>y &lt;- rep(c(0,1),c(100,100))
x &lt;- runif(200)
df &lt;- data.frame(y,x)


fitgbm &lt;- gbm(y ~ x, data=df, 
              distribution = ""bernoulli"", n.trees = 100)

predgbm &lt;- predict(fitgbm, df, n.trees=100, type='response')
</code></pre>

<p>Too simplistic but look at the summary of <code>predgbm</code>:</p>

<pre><code>&gt; summary(predgbm)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.4936  0.4943  0.5013  0.5000  0.5052  0.5073 
</code></pre>

<p>And as the documentation mentions this is the probability of y being 1:</p>

<blockquote>
  <p>If type=""response"" then gbm converts back to the same scale as the outcome. Currently the only effect this will have is returning probabilities for bernoulli and expected counts for poisson.</p>
</blockquote>
"
843036;2130;StuckInPhDNoMore;<matlab><machine-learning><cluster-analysis><sift><feature-extraction>;28769234;0;My observations are less than the feature vector of each. Any solution to overcome this?;"<p>I'm using GMM to fit my data to 256 Gaussians. I'm using Matlab's <code>fitgmdist</code> to achieve this.</p>

<pre><code>gmm{i} = fitgmdist(model_feats, gaussians, 'Options',statset('MaxIter',1000), ...
            'CovType','diagonal', 'SharedCov',false, 'Regularize',0.01, 'Start',cInd);
</code></pre>

<p>I am using RootSIFT to extract the features of each image. This produces a vector of <code>1x128</code> for each image.</p>

<p>Now I have 45 images maximum for each writer. So after feature extraction and everything the size of my <code>model_feats</code> is <code>45 x 128</code>.</p>

<p>According to the help file for data arrangement for <code>X</code> is:</p>

<blockquote>
  <p>The rows of X correspond to observations, and columns correspond to
  variables.</p>
</blockquote>

<p>The problem I have is when I run the above function I am told that:</p>

<blockquote>
  <p>""X must have more rows than columns.""</p>
</blockquote>

<p>I have a total of 45 images for each writer. How will I be able to get this function to run? This is a strange limitation for such a function, I mean even if I am able to get 100 images for each writer it will not work.</p>

<p>I will appreciate any workaround to this.</p>

<p>P.S. I've tried the same with VL_Feat's <code>vl_gmm</code> and it works without any problems but I need this to work in Matlab not VL_FEAT.</p>
";28889893;1060350;70512;Has QUIT--Anony-Mousse;1;28889893;"<p>With SIFT you usually don't compute the feature for the whole image, but literally <strong>hundreds of keypoints for each image.</strong> Then you won't have this problem anymore.</p>

<p>Next step then probably is a â€œbag of visual wordsâ€ mapping of each image.</p>
"
1019129;5193;sten;<machine-learning><regression>;28774321;-1;Best model for learning function with shape 1/x^2?;"<p>What model/estimator would you guys use to learn data which has the shape like 1/x^2 ?
Example :</p>

<pre><code>http://www.wolframalpha.com/input/?i=1%2Fx^2%2C+from+0+to+1
</code></pre>

<p>The best behaved from what I tested up to now is Support Vector Regression (SVR)</p>
";;270287;41194;IVlad;1;28775797;"<p>I think you've answered your own question: if you know it looks like <code>1 / x^2</code>, then use <code>1 / x^2</code>.</p>

<p>Support Vector Machines are likely to work well or even best in a lot of problems. </p>

<p>Since your data is obviously not generated by a linear process, I suggest you don't use any linear estimator. Other than that, a lot of estimators can work well. Pick one and see if you're happy with it.</p>
"
3992452;764;user168983;<azure><machine-learning><azure-machine-learning-studio>;28786290;1;How to use SMOTE in Microsoft Azure;"<p>There is a module named <a href=""https://msdn.microsoft.com/library/azure/9f3fe1c4-520e-49ac-a152-2e104169912a"" rel=""nofollow"">SMOTE</a>(Synthetic Minority Oversampling Technique ) which increase the number of samples of under sampled data, I guess we should choose a feature(feature to be predicted) which is under represented. How to choose it? There seems to be no option on choosing the coloumn.</p>
";;4621861;66;Said Bleik;3;28802147;"<p>I guess you are referring to the target variable (label column). You can set that using a Metadata Editor module. Choose your label column using the column selector and set the Fields property to Labels.</p>
"
3992452;764;user168983;<azure><machine-learning><azure-machine-learning-studio>;28786290;1;How to use SMOTE in Microsoft Azure;"<p>There is a module named <a href=""https://msdn.microsoft.com/library/azure/9f3fe1c4-520e-49ac-a152-2e104169912a"" rel=""nofollow"">SMOTE</a>(Synthetic Minority Oversampling Technique ) which increase the number of samples of under sampled data, I guess we should choose a feature(feature to be predicted) which is under represented. How to choose it? There seems to be no option on choosing the coloumn.</p>
";;4589073;1231;neerajkh;2;28883359;"<p>Here is the SMOTE definition - SMOTE is an approach for the construction of classifiers from imbalanced datasets, which is when classification categories are not approximately equally represented. The classification category is the feature that the classifier is trying to learn. There is not an option of choosing the column in the SMOTE module because it should be the label column</p>

<p>Here is the details on how to use SMOTE in Azure Machine Learning - <a href=""https://msdn.microsoft.com/en-us/library/azure/dn913076.aspx?f=255&amp;MSPPError=-2147217396"" rel=""nofollow"">https://msdn.microsoft.com/en-us/library/azure/dn913076.aspx?f=255&amp;MSPPError=-2147217396</a></p>
"
3992452;764;user168983;<azure><machine-learning><azure-machine-learning-studio>;28786290;1;How to use SMOTE in Microsoft Azure;"<p>There is a module named <a href=""https://msdn.microsoft.com/library/azure/9f3fe1c4-520e-49ac-a152-2e104169912a"" rel=""nofollow"">SMOTE</a>(Synthetic Minority Oversampling Technique ) which increase the number of samples of under sampled data, I guess we should choose a feature(feature to be predicted) which is under represented. How to choose it? There seems to be no option on choosing the coloumn.</p>
";;623419;1436;benjguin;1;34357854;"<p>You can do it thru the column selector. In the sample below, the blood donation data (a sample dataset in Azure ML) has 25% of people who donated (class 1).</p>

<p><a href=""https://i.stack.imgur.com/4IEzg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4IEzg.png"" alt=""enter image description here""></a></p>
"
2102292;765;Cheknov;<python><machine-learning><nlp><tagged-corpus>;28788845;0;Error generating a model reading corpus from a big .txt file;"<p>i'm trying to read the file corpus.txt (training set) and generate a model, the output must be called lexic.txt and contain the word, the tag and the number of ocurrences...for small training sets it works, but for the university given training set (30mb .txt file, millions of lines) the code does not work,I imagine it will be a problem with the efficiency and therefore the system runs out of memory...can anybody help me with the code please?</p>

<p>Here I attach my code:</p>

<pre><code>from collections import Counter

file=open('corpus.txt','r')
data=file.readlines()
file.close()

palabras = []
count_list = []

for linea in data:
   linea.decode('latin_1').encode('UTF-8') # para los acentos
   palabra_tag = linea.split('\n')
   palabras.append(palabra_tag[0])

cuenta = Counter(palabras) # dictionary for count ocurrences for a word + tag 

#Assign for every word + tag the number of times appears
for palabraTag in palabras:
    for i in range(len(palabras)):
        if palabras[i] == palabraTag:       
            count_list.append([palabras[i], str(cuenta[palabraTag])])


#We delete repeated ones
finalList = []
for i in count_list:
    if i not in finalList:
        finalList.append(i)


outfile = open('lexic.txt', 'w') 
outfile.write('Palabra\tTag\tApariciones\n')

for i in range(len(finalList)):
    outfile.write(finalList[i][0]+'\t'+finalList[i][1]+'\n') # finalList[i][0] is the word + tag and finalList[i][1] is the numbr of ocurrences

outfile.close()
</code></pre>

<p>And here you can see a sample of the corpus.txt:</p>

<pre><code>Al  Prep
menos   Adv
cinco   Det
reclusos    Adj
murieron    V
en  Prep
las Det
Ãºltimas Adj
24  Num
horas   NC
en  Prep
las Det
cÃ¡rceles    NC
de  Prep
Valencia    NP
y   Conj
Barcelona   NP
en  Prep
incidentes  NC
en  Prep
los Det
que Pron
su  Det
</code></pre>

<p>Thanks in advance!</p>
";28794445;3436849;660;David;0;28788983;"<p>You may be able to reduce your memory usage if you combine these two chunks of code.</p>

<pre><code>#Assign for every word + tag the number of times appears
for palabraTag in palabras:
    for i in range(len(palabras)):
        if palabras[i] == palabraTag:       
            count_list.append([palabras[i], str(cuenta[palabraTag])])


#We delete repeated ones
finalList = []
for i in count_list:
    if i not in finalList:
        finalList.append(i) 
</code></pre>

<p>You can check to see if an item exists in the count list already and by doing so, not add duplicates in the first place. This should reduce your memory usage. See below;</p>

<pre><code>#Assign for every word + tag the number of times appears
for palabraTag in palabras:
    for i in range(len(palabras)):
        if palabras[i] == palabraTag and
           [palabras[i], str(cuenta[palabraTag])] not in count_list:
                count_list.append([palabras[i], str(cuenta[palabraTag])])
</code></pre>
"
2102292;765;Cheknov;<python><machine-learning><nlp><tagged-corpus>;28788845;0;Error generating a model reading corpus from a big .txt file;"<p>i'm trying to read the file corpus.txt (training set) and generate a model, the output must be called lexic.txt and contain the word, the tag and the number of ocurrences...for small training sets it works, but for the university given training set (30mb .txt file, millions of lines) the code does not work,I imagine it will be a problem with the efficiency and therefore the system runs out of memory...can anybody help me with the code please?</p>

<p>Here I attach my code:</p>

<pre><code>from collections import Counter

file=open('corpus.txt','r')
data=file.readlines()
file.close()

palabras = []
count_list = []

for linea in data:
   linea.decode('latin_1').encode('UTF-8') # para los acentos
   palabra_tag = linea.split('\n')
   palabras.append(palabra_tag[0])

cuenta = Counter(palabras) # dictionary for count ocurrences for a word + tag 

#Assign for every word + tag the number of times appears
for palabraTag in palabras:
    for i in range(len(palabras)):
        if palabras[i] == palabraTag:       
            count_list.append([palabras[i], str(cuenta[palabraTag])])


#We delete repeated ones
finalList = []
for i in count_list:
    if i not in finalList:
        finalList.append(i)


outfile = open('lexic.txt', 'w') 
outfile.write('Palabra\tTag\tApariciones\n')

for i in range(len(finalList)):
    outfile.write(finalList[i][0]+'\t'+finalList[i][1]+'\n') # finalList[i][0] is the word + tag and finalList[i][1] is the numbr of ocurrences

outfile.close()
</code></pre>

<p>And here you can see a sample of the corpus.txt:</p>

<pre><code>Al  Prep
menos   Adv
cinco   Det
reclusos    Adj
murieron    V
en  Prep
las Det
Ãºltimas Adj
24  Num
horas   NC
en  Prep
las Det
cÃ¡rceles    NC
de  Prep
Valencia    NP
y   Conj
Barcelona   NP
en  Prep
incidentes  NC
en  Prep
los Det
que Pron
su  Det
</code></pre>

<p>Thanks in advance!</p>
";28794445;2102292;765;Cheknov;0;28794445;"<p>Finally I improved the code using dictionary, here is the result working 100% fine:</p>

<pre><code>file=open('corpus.txt','r')
data=file.readlines()
file.close()

diccionario = {}

for linea in data:
    linea.decode('latin_1').encode('UTF-8') # para los acentos
    palabra_tag = linea.split('\n')
    cadena = str(palabra_tag[0])
    if(diccionario.has_key(cadena)):
        aux = diccionario.get(cadena)
        aux += 1
        diccionario.update({cadena:aux})
    else:
        diccionario.update({cadena:1})

outfile = open('lexic.txt', 'w')
outfile.write('Palabra\tTag\tApariciones\n')

for key, value in diccionario.iteritems() :
    s = str(value)
    outfile.write(key +"" ""+s+'\n')
outfile.close()
</code></pre>
"
2245718;440;IrishDog;<machine-learning><scikit-learn><svm>;28788864;1;Scikit-learn SVR Speed;"<p>I'm trying to build a kernel model for some training data. The model used is Support Vector Regression and the input data-set is about 58 samples with X a vector of size 5 and Y a double value. Example: X = [300678, 10,6,1,3] - Y = [18.38]. The training(fit) for this small data set is taking too long(over 5 minutes) is that reasonable or something goes wrong?</p>
";;1615070;800;404pio;0;28791892;"<p>You should normalize your data. Look at first feature of X, it is 300k. Another features are relatively smaller than the first feature. <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html"" rel=""nofollow"">http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html</a></p>
"
3808898;391;VoidZero;<python><pandas><machine-learning><data-manipulation>;28792043;0;How to replace values in pandas with column names;"<p>I am trying to make a recommendation system using Last.fm data to recommend user songs a user will like to hear.</p>

<p>I am using NearestNeighbors Algorithm to predict feature songs user will like to hear.
I have made the model but problem I am facing is I am getting Integer values instead of the song names in the Pandas DataFrame.
This is a screenshot of what I am getting </p>

<p><img src=""https://i.stack.imgur.com/SP4U0.png"" alt=""enter image description here""></p>

<p>Instead of numbers in the value section I want the song name in the dataset.
How do I achieve this.
This is what the output should look like:</p>

<p><img src=""https://i.stack.imgur.com/7aJGL.png"" alt=""enter image description here"">
Here is the link to my Ipython notebook: <a href=""http://nbviewer.ipython.org/github/kartikjagdale/Last.fm-Song-Recommender/blob/master/Ipython%20Notebook/Last.Fm%20Song%20Recommeder.ipynb"" rel=""nofollow noreferrer"">http://nbviewer.ipython.org/github/kartikjagdale/Last.fm-Song-Recommender/blob/master/Ipython%20Notebook/Last.Fm%20Song%20Recommeder.ipynb</a></p>

<p>and link to my github project: <a href=""https://github.com/kartikjagdale/Last.fm-Song-Recommender/"" rel=""nofollow noreferrer"">https://github.com/kartikjagdale/Last.fm-Song-Recommender/</a></p>
";28792317;772649;83400;HYRY;3;28792317;"<p><code>DataFrame.columns</code> is an <code>Index</code> object, which can be used as array. </p>

<p>You can use <code>pd.DataFrame(df.columns[model])</code> to get names, here is an exampleï¼š</p>

<pre><code>import pandas as pd
import numpy as np
from sklearn.neighbors import NearestNeighbors
df = pd.DataFrame(np.random.randint(0, 5, (10, 5)), columns=list(""ABCDE""))
neigh = NearestNeighbors(n_neighbors=3)
neigh.fit(df.T) # Fit the data
model = neigh.kneighbors(df.T, return_distance=False)
pd.DataFrame(df.columns[model])
</code></pre>
"
4030468;59;Luigi Biasi;<matlab><machine-learning><statistics><gaussian>;28794049;1;GMM in MATLAB gives different results for the same file;"<p>I constructed a Gaussian Mixture Model in Matlab with a dataset:</p>

<pre><code>model = gmdistribution.fit(data,M,'Replicates',5);
</code></pre>

<p>with <code>M = 3</code> Gaussian components. I tested new data with:</p>

<pre><code>[P, l] = posterior(model,new_data);
</code></pre>

<p>I ran the program several times and didn't get the same result. Each run produces different log-likelihood values. I use the log-likelihood to make decisions, and this value for the same data (<code>new_data</code>) differs for each run. What does it depend on? How can I resolve this problem?</p>
";;2278029;17884;horchler;0;32191971;"<p>First, assuming that you're using a newish <code>version</code> of Matlab, the <a href=""http://mathworks.com/help/stats/gmdistribution.fit.html"" rel=""nofollow""><code>gmdistribution.fit</code></a> documentation  indicates that the <code>fit</code> method is deprecated and that <a href=""http://mathworks.com/help/stats/fitgmdist.html"" rel=""nofollow""><code>fitgmdist</code></a> should be used. See <a href=""http://www.mathworks.com/help/stats/gmdistribution-class.html#bty1gzy-1"" rel=""nofollow""><code>here</code></a> for an example.</p>

<p>Second, the documentation for <code>gmdistribution.fit</code> indicates that if the <code>'Replicates'</code> option is larger than 1, the <code>'randSample'</code> start method will be used to produce the initial parameters. This may be the cause (or at least one of the causes) of your observed variability.</p>

<p>Finally, you can also try using <a href=""http://www.mathworks.com/help/matlab/ref/rng.html"" rel=""nofollow""><code>rng</code></a> before calling <code>gmdistribution.fit</code> to set the seed of the global random number stream (assuming the function doesn't use it's own stream internally). Alternatively, you can try specifying an <a href=""http://www.mathworks.com/help/stats/gmdistribution.fit.html#zmw57dd0e207301"" rel=""nofollow""><code>'Options'</code></a> parameter via <a href=""http://www.mathworks.com/help/stats/statset.html"" rel=""nofollow""><code>statset</code></a>:</p>

<pre><code>seed = 1;
s = RandStream('mt19937ar','Seed',seed);
opts = statset('Streams',s);
model = gmdistribution.fit(data,M,'Replicates',5,'Options',opts);
</code></pre>

<p>I can't test this fully myself â€“ see the <a href=""http://www.mathworks.com/help/stats/gmdistribution-class.html"" rel=""nofollow""><code>gmdistribution</code></a> class documentation for further details.</p>
"
2755321;79;CSK;<machine-learning><test-data><supervised-learning>;28803228;0;Purpose of test data in supervised learning?;"<p>So this question may seem a little stupid but I couldn't wrap my head around it.
What is the purpose of test data? Is it only to calculate accuracy of the classifier? I'm using Naive Bayes for sentiment analysis of tweets. Once I train my classifier using training data, I use test data just to calculate accuracy of the classifier. How can I use the test data to improve classifier's performance?</p>
";;874188;137145;tripleee;0;28803292;"<p>You don't -- like you surmise, the test data is used for testing, and mustn't be used for anything else, lest you skew your accuracy measurements.  This is an important cornerstone of any machine learning -- you only fool yourself if you use your test data for training.</p>

<p>If you are considering desperate measures like that, the proper way forward is usually to re-examine your problem space and the solution you have.  Does it adequately model the problem you are trying to solve?  If not, can you devise a better model which captures the essence of the problem?</p>

<p>Machine learning is not a silver bullet.  It will not solve your problem for you.  Too many failed experiments prove over and over again, ""garbage in -- garbage out"".</p>
"
2755321;79;CSK;<machine-learning><test-data><supervised-learning>;28803228;0;Purpose of test data in supervised learning?;"<p>So this question may seem a little stupid but I couldn't wrap my head around it.
What is the purpose of test data? Is it only to calculate accuracy of the classifier? I'm using Naive Bayes for sentiment analysis of tweets. Once I train my classifier using training data, I use test data just to calculate accuracy of the classifier. How can I use the test data to improve classifier's performance?</p>
";;1863229;383077;Tim Biegeleisen;1;28803377;"<p>In doing general supervised machine learning, the test data set plays a critical role in determining how well your model is performing.  You typically will build a model with say 90% of your input data, leaving 10% aside for testing.  You then check the accuracy of that model by seeing how well it does against the 10% training set.  The performance of the model against the test data is meaningful because the model has never ""seen"" this data.  If the model be statistically valid, then it should perform well on both the training and test data sets.  This general procedure is called <strong>cross validation</strong> and you can read more about it <a href=""http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29"" rel=""nofollow"">here</a>.</p>
"
811921;2303;com;<machine-learning><implementation><knn>;28808059;3;kNN state-of-the-art implementation;"<p>I am looking for a kNN implementation package.</p>

<p>This package should:</p>

<ul>
<li><p>deal with multi-label problem</p></li>
<li><p>give a detailed output, what are those k nearest neiboneighbors points 
that classify the test point.</p></li>
<li><p>Implement all kinds of distance metrics.</p></li>
<li><p>Implement all kind of instance weighting. </p></li>
</ul>

<p>So far, I have not found one (WEKA doesn't support multi-labeling). </p>
";28825304;968064;2363;Rushdi Shams;2;28825304;"<p>Try Mulan for Multi label knn. In Mulan it is called MLKNN. A detailed link to start with knn in Mulan can be found <a href=""http://mulan.sourceforge.net/starting.html"" rel=""nofollow"">here</a></p>
"
1357509;275;sk1ll3r;<machine-learning><neural-network>;28810240;0;Software packages for neural network;"<p>I am looking for a very <strong>lightweight</strong> neural network package to solve the following problem:</p>

<ul>
<li>2 input units, 4 hidden units, 2 output units</li>
<li>different activation functions for different connections</li>
<li>different cost (error) functions for the two output units</li>
</ul>

<p>Could somebody, perhaps with more experience, please help?</p>
";;4623467;2682;alesc;0;28810241;"<p>I can recommend Weka, where you have <a href=""http://weka.sourceforge.net/doc.dev/weka/classifiers/functions/MultilayerPerceptron.html"" rel=""nofollow"">MultilayerPerceptron</a>, which is an out-of-the-box neural network classifier. But this is probably not suitable for your problem, so you can use the basic class <a href=""http://weka.sourceforge.net/doc.dev/weka/classifiers/functions/neural/NeuralNode.html"" rel=""nofollow"">NeuralNode</a>, which is used by the MultilayerPerceptron to build it's neural network.</p>

<p>If you manage to separate all the needed code in and pack it into your own custom classes, you will end with a very lightweight solution - not counting the JVM needed to run it.</p>

<p>In which programming language or environment do you plan to implement this? If you already use Java, then the JVM will not be an overhead, since you already use it.</p>
"
2993924;4635;neelshiv;<algorithm><machine-learning><recommendation-engine><collaborative-filtering>;28817653;1;evaluating the performance of item-based collaborative filtering for binary (yes/no) product recommendations;"<p>I'm attempting to write some code for item based collaborative filtering for product recommendations. The input has buyers as rows and products as columns, with a simple 0/1 flag to indicate whether or not a buyer has bought an item. The output is a list similar items for a given purchased, ranked by cosine similarities.</p>

<p>I am attempting to measure the accuracy of a few different implementations, but I am not sure of the best approach. Most of the literature I find mentions using some form of mean square error, but this really seems more applicable when your collaborative filtering algorithm predicts a rating (e.g. 4 out of 5 stars) instead of recommending which items a user will purchase.</p>

<p>One approach I was considering was as follows...</p>

<ul>
<li>split data into training/holdout sets, train on training data</li>
<li>For each item (A) in the set, select data from the holdout set where users bought A</li>
<li>Determine which percentage of A-buyers bought one of the top 3 recommendations for A-buyers</li>
</ul>

<p>The above seems kind of arbitrary, but I think it could be useful for comparing two different algorithms when trained on the same data.</p>
";28835186;1384012;121;Kim.Net;0;28817956;"<p>Best way to test a recommender is always to manually verify that the results. However some kind of automatic verification is also good. </p>

<p>In the spirit of a recommendation system, you should split your data in time, and see if you algorithm can predict what future buys the user does. this should be done for all users. </p>

<p>Don't expect that it can predict everything, a 100% correctness is usually a sign of over-fitting. </p>
"
2993924;4635;neelshiv;<algorithm><machine-learning><recommendation-engine><collaborative-filtering>;28817653;1;evaluating the performance of item-based collaborative filtering for binary (yes/no) product recommendations;"<p>I'm attempting to write some code for item based collaborative filtering for product recommendations. The input has buyers as rows and products as columns, with a simple 0/1 flag to indicate whether or not a buyer has bought an item. The output is a list similar items for a given purchased, ranked by cosine similarities.</p>

<p>I am attempting to measure the accuracy of a few different implementations, but I am not sure of the best approach. Most of the literature I find mentions using some form of mean square error, but this really seems more applicable when your collaborative filtering algorithm predicts a rating (e.g. 4 out of 5 stars) instead of recommending which items a user will purchase.</p>

<p>One approach I was considering was as follows...</p>

<ul>
<li>split data into training/holdout sets, train on training data</li>
<li>For each item (A) in the set, select data from the holdout set where users bought A</li>
<li>Determine which percentage of A-buyers bought one of the top 3 recommendations for A-buyers</li>
</ul>

<p>The above seems kind of arbitrary, but I think it could be useful for comparing two different algorithms when trained on the same data.</p>
";28835186;2837393;693;mokarakaya;1;28835186;"<p>Actually your approach is quiet similar with the literature but I think you should consider to use recall and precision as most of the papers do.</p>

<p><a href=""http://en.wikipedia.org/wiki/Precision_and_recall"" rel=""nofollow"">http://en.wikipedia.org/wiki/Precision_and_recall</a></p>

<p>Moreover if you will use Apache Mahout there is an implementation for recall and precision in this class; GenericRecommenderIRStatsEvaluator</p>
"
4623371;40;technobadboy;<python><c++><opencv><machine-learning><viola-jones>;28817768;0;opencv train classifier never contiueus 3 stage;"<p>I am trying to learn the cascade classifier to learn license plates. I'm running OpenCV 2.4.9 on a Windows 7 server with 16 GB RAM and i7 processor of 3,4 GHz. I want to train the classifier, but it never continuous stage 3.</p>

<p>This is how I made my .vec file on the xxxx there is personal information:</p>

<pre><code>D:\xxxx\trainingBinairImages\build\x64\vc12\bin\opencv_createsamples.exe -info D:\xxxx\trainingBinairImages\positiveNew.dat -bg D:\xxxx\trainingBinairImages\NegativeNew.bg -vec D:\xxxx\trainingBinairImages\output_vec.vec -maxxangle 0 -maxyangle 0 -maxzangle 0 -w 140 -h 40 -num 3311

pause
</code></pre>

<p>Parameters:</p>

<pre><code>D:\xxxx\trainingBinairImages\build\x64\vc12\bin\opencv_traincascade.exe -data D:\xxxx\trainingBinairImages\result -vec output_vec.vec -bg negativeNew.bg -numPos 2980 -numNeg 3311 -miniHitRate 0.995 -maxFalseAlarmRate 0.5 -w 140 -h 40 -featureType HAAR -precalcValBufSize 2048 -precalcIdxBufSize 2048
pause
</code></pre>

<p>From cmd:</p>

<pre><code> ===== TRAINING 3-stage =====
&lt;BEGIN
POS count : consumed   2980 : 2981
Train dataset for temp stage can not be filled. Branch training terminated.
</code></pre>

<p>If needed to get a look in the files I will edit the extra's. I tried to change the positive and negative values the numStages. Change the directory paths etc. Online there are question about this, that I tried without a solved solutions. </p>
";28830359;2743112;908;Martijn van Wezel;1;28830359;"<p>The answer is solved on another form <a href=""http://answers.opencv.org/question/56716/opencv-train-classifier-never-contiueus-3-stage/"" rel=""nofollow"">the_link</a></p>

<blockquote>
  <p>Hmm that is your problem. You need to supply larger images as
  negative instead of cropped windows. Imagine that you will need a lot
  of windows! Each negative window that is classified correctly by the
  previous stages cannot be used again for the new stage. So most people
  just supply tons of not object images, the software will do its own
  job of cutting out the negatives randomly for you.</p>
</blockquote>
"
354979;6775;rhombidodecahedron;<machine-learning><neural-network><deep-learning>;28819104;7;How to do machine learning when the inputs are of different sizes?;"<p>In standard cookbook machine learning, we operate on a rectangular matrix; that is, all of our data points have the same number of features. How do we cope with situations in which all of our data points have different numbers of features? For example, if we want to do visual classification but all of our pictures are of different dimensions, or if we want to do sentiment analysis but all of our sentences have different amounts of words, or if we want to do stellar classification but all of the stars have been observed a different number of times, etc.</p>

<p>I think the normal way would be to extract features of regular size from these irregularly sized data. But I attended a talk on deep learning recently where the speaker emphasized that instead of hand-crafting features from data, deep learners are able to learn the appropriate features themselves. But how do we use e.g. a neural network if the input layer is not of a fixed size?</p>
";;270287;41194;IVlad;2;28819450;"<p>You can usually make the number of features the same for all instances quite easily:</p>

<blockquote>
  <p>if we want to do visual classification but all of our pictures are of different dimensions</p>
</blockquote>

<p>Resize them all to a certain dimension / number of pixels.</p>

<blockquote>
  <p>if we want to do sentiment analysis but all of our sentences have different amounts of words</p>
</blockquote>

<p>Keep a dictionary of the <code>k</code> words in all of your text data. Each instance will consist of a boolean vector of size <code>k</code> where the <code>i</code>-th entry is true if word <code>i</code> from the dictionary appears in that instance (this is not the best representation, but many are based on it). See the bag of words model.</p>

<blockquote>
  <p>if we want to do stellar classification but all of the stars have been observed a different number of times</p>
</blockquote>

<p>Take the features that have been observed for all the stars.</p>

<blockquote>
  <p>But I attended a talk on deep learning recently where the speaker emphasized that instead of hand-crafting features from data deep learners are able to learn the appropriate features themselves.</p>
</blockquote>

<p>I think the speaker probably referred to higher level features. For example, you shouldn't manually extract the feature ""contains a nose"" if you want to detect faces in an image. You should feed it the raw pixels, and the deep learner will learn the ""contains a nose"" feature somewhere in the deeper layers.</p>
"
354979;6775;rhombidodecahedron;<machine-learning><neural-network><deep-learning>;28819104;7;How to do machine learning when the inputs are of different sizes?;"<p>In standard cookbook machine learning, we operate on a rectangular matrix; that is, all of our data points have the same number of features. How do we cope with situations in which all of our data points have different numbers of features? For example, if we want to do visual classification but all of our pictures are of different dimensions, or if we want to do sentiment analysis but all of our sentences have different amounts of words, or if we want to do stellar classification but all of the stars have been observed a different number of times, etc.</p>

<p>I think the normal way would be to extract features of regular size from these irregularly sized data. But I attended a talk on deep learning recently where the speaker emphasized that instead of hand-crafting features from data, deep learners are able to learn the appropriate features themselves. But how do we use e.g. a neural network if the input layer is not of a fixed size?</p>
";;4367179;1031;Denis Tarasov;3;28829102;"<p>Since you are asking about deep learning, I assume you are more interested in end-to-end systems, rather then feature design. Neural networks that can handle variable data inputs are:</p>

<p>1) Convolutional neural networks with pooling layers. They are usually used in image recognition context, but recently were applied to modeling sentences as well. ( I think they should also be good at classifiying stars ).</p>

<p>2) Recurrent neural networks. (Good for sequential data, like time series,sequence labeling tasks, also good for machine translation). </p>

<p>3) Tree-based autoencoders (also called recursive autoencoders) for data arranged in tree-like structures (can be applied to sentence parse trees)</p>

<p>Lot of papers describing example applications can readily be found by googling. </p>

<p>For uncommon tasks you can select one of these based on the structure of your data, or you can design some variants and combinations of these systems. </p>
"
4557261;1;Ramseyl;<machine-learning><cluster-analysis><measurement>;28821357;0;How can we say that a clustering quality measure is good?;"<p>There are few well known measures like silhouette width (SW), the Davies- Bouldin index (DB), the Calinski-Harabasz  index (CH), and the Dunn index .
How can we say that a clustering quality measure is good?<br />
Is there some kind of metric for the clustering quality measure to be good?</p>
<p>Also ,</p>
<blockquote>
<p>&quot;algorithms that produce clusters with high Dunn index are more desirable&quot; -Wikipedia</p>
<p>&quot;Objects with a high silhouette value are considered well clustered&quot; -Wikipedia</p>
<p>&quot;clustering algorithm that produces a collection of clusters with the smallest Daviesâ€“Bouldin index is considered the best algorithm&quot;   -Wikipedia</p>
</blockquote>
<p>How  high or low these values should be ?Is there a metric number ?</p>
<p>Can any one provide me a small example using a clustering quality measure  on a dataset or IRIS dataset to say that the particular clustering quality measure is good?</p>
";;1056563;45835;StephenBoesch;0;28821748;"<p>Maybe a simple starting point  would be: </p>

<blockquote>
  <p>""Are the elements within a cluster alike and are they different from
  elements in a different cluster"".</p>
</blockquote>

<p>There are obviously a variety of metrics to quantify similarity vs difference - as well as considerations like density vs distance.  </p>

<p>The Stanford NLP project has a useful reference that is approachable: <a href=""http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html"" rel=""nofollow"">http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html</a></p>
"
2777099;141;Srinidhi Shankar;<java><machine-learning><logistic-regression><gradient-descent>;28824327;3;Implementation of Logistic regression with Gradient Descent in Java;"<p>I have implemented Logistic Regression with Gradient Descent in Java. It doesn't seem to work well (It does not classify records properly; the probability of y=1 is a lot.)   I don't know whether my implementation is correct.I have gone through the code several times and i am unable to find any bug. I have been following Andrew Ng's tutorials on Machine learning on Course Era. My Java implementation has 3 classes. namely :</p>

<ol>
<li>DataSet.java : To read the data set</li>
<li>Instance.java : Has two members : 1. double[ ] x  and 2. double label  </li>
<li>Logistic.java : This is the main class that implements Logistic Regression with Gradient Descent.            </li>
</ol>

<p>This is my cost function:
<h2>J(Î˜) = (- 1/m ) [Î£<sup>m</sup><sub>i=1</sub> y<sup>(i)</sup> log( h<sub>Î˜</sub>( x<sup>(i)</sup> ) ) + (1 - y<sup>(i) </sup>) log(1 - h<sub>Î˜</sub> (x<sup>(i)</sup>) )]
</h2>For the above Cost function, this is my Gradient Descent algorithm:
<h1>Repeat (</h1><h1> Î˜<sub>j</sub> := Î˜<sub>j</sub> - Î± Î£<sup>m</sup><sub>i=1</sub> ( h<sub>Î˜</sub>( x<sup>(i)</sup>) - y<sup>(i)</sup> ) x<sup>(i)</sup><sub>j</sub></h1>
(Simultaneously update all Î˜<sub>j</sub> )<h1>
)
</h1></p>

<pre><code>import java.io.FileNotFoundException;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;

public class Logistic {

    /** the learning rate */
    private double alpha;

    /** the weight to learn */
    private double[] theta;

    /** the number of iterations */
    private int ITERATIONS = 3000;

    public Logistic(int n) {
        this.alpha = 0.0001;
        theta = new double[n];
    }

    private double sigmoid(double z) {
        return (1 / (1 + Math.exp(-z)));
    }

    public void train(List&lt;Instance&gt; instances) {

    double[] temp = new double[3];

    //Gradient Descent algorithm for minimizing theta
    for(int i=1;i&lt;=ITERATIONS;i++)
    {
       for(int j=0;j&lt;3;j++)
       {      
        temp[j]=theta[j] - (alpha * sum(j,instances));
       }

       //simulataneous updates of theta  
       for(int j=0;j&lt;3;j++)
       {
         theta[j] = temp[j];
       }
        System.out.println(Arrays.toString(theta));
    }

    }

    private double sum(int j,List&lt;Instance&gt; instances)
    {
        double[] x;
        double prediction,sum=0,y;


       for(int i=0;i&lt;instances.size();i++)
       {
          x = instances.get(i).getX();
          y = instances.get(i).getLabel();
          prediction = classify(x);
          sum+=((prediction - y) * x[j]);
       }
         return (sum/instances.size());

    }

    private double classify(double[] x) {
        double logit = .0;
        for (int i=0; i&lt;theta.length;i++)  {
            logit += (theta[i] * x[i]);
        }
        return sigmoid(logit);
    }


    public static void main(String... args) throws FileNotFoundException {

      //DataSet is a class with a static method readDataSet which reads the dataset
      // Instance is a class with two members: double[] x, double label y
      // x contains the features and y is the label.

        List&lt;Instance&gt; instances = DataSet.readDataSet(""data.txt"");
      // 3 : number of theta parameters corresponding to the features x 
      // x0 is always 1   
        Logistic logistic = new Logistic(3);
        logistic.train(instances);

        //Test data
        double[]x = new double[3];
        x[0]=1;
        x[1]=45;
        x[2] = 85;

        System.out.println(""Prob: ""+logistic.classify(x));


    }
}
</code></pre>

<p>Can anyone tell me what am I doing wrong? 
Thanks in advance! :)</p>
";;2003537;8656;Daishi;1;54343588;"<p>As I am studying logistic regression, I took the time to review your code in detail.</p>

<p><strong>TLDR</strong></p>

<p>In fact, it appears the algorithm is correct.</p>

<p>The reason you had so much false negatives or false positives is, I think, because of the hyper parameters you choose.</p>

<p>The model was under-trained so the hypothesis was under-fitting.</p>

<p><strong>Details</strong></p>

<p>I had to create the <code>DataSet</code> and <code>Instance</code> classes because you did not publish them, and set up a train data set and a test data set based on the Cryotherapy dataset.
See <a href=""http://archive.ics.uci.edu/ml/datasets/Cryotherapy+Dataset+"" rel=""nofollow noreferrer"">http://archive.ics.uci.edu/ml/datasets/Cryotherapy+Dataset+</a>.</p>

<p>Then, using your same exact code (for the logistic regression part) and by choosing an alpha rate of <code>0.001</code> and a number of iterations of <code>100000</code>, I got a precision rate of <code>80.64516129032258</code> percent on the test data set, which is not so bad.</p>

<p>I tried to get a better precision rate by tweaking manualy those hyper parameters but could not obtain any better result.</p>

<p>At this point, an enhancement would be to implement regularization, I suppose.</p>

<p><strong>Gradient descent formula</strong></p>

<p>In Andrew Ng's video about the the cost function and gradient descent, it is correct that the <code>1/m</code> term is omitted.
A possible explanation is that the <code>1/m</code> term is included in the <code>alpha</code> term.
Or maybe it's just an oversight.
See <a href=""https://www.youtube.com/watch?v=TTdcc21Ko9A&amp;index=36&amp;list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&amp;t=6m53s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=TTdcc21Ko9A&amp;index=36&amp;list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&amp;t=6m53s</a> at 6m53s.</p>

<p>But if you watch Andrew Ng's video about regularization and logistic regression you'll notice that the term <code>1/m</code> is clearly present in the formula.
See <a href=""https://www.youtube.com/watch?v=IXPgm1e0IOo&amp;index=42&amp;list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&amp;t=2m19s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=IXPgm1e0IOo&amp;index=42&amp;list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&amp;t=2m19s</a> at 2m19s.</p>
"
2810596;51;Stiege;<maven><machine-learning><pom.xml>;28831177;0;Maven build for machine learning - how do I get it working?;"<p>I'm trying to compile and run some basic Java code in this repository:
<a href=""https://github.com/jasebell/mlbookexamples"" rel=""nofollow"">https://github.com/jasebell/mlbookexamples</a></p>

<p>As an example, inside the Java folder, chapter2.BritneyDilemma.</p>

<p>I'm trying to do this with maven (as there's a pom.xml) but I feel that there's a few things holding me back. I don't know much about Maven; my background is C and cmake. At first glance, I can see why people hate Maven. The pom.xml for the project doesn't seem fully configured / appears quite non-standard:</p>

<ul>
<li>It only pulls down some dependencies, for example BritneyDilemma requires classifier4J but this isn't mentioned in the pom.xml.</li>
<li>When I do any of the simple commands mvn install/compile etc, the output into target is basically nothing (it's an almost empty jar).</li>
</ul>

<p>I've tried various approaches such as:</p>

<ul>
<li>Modifying the pom.xml to add in the <a href=""http://maven.apache.org/plugins/maven-assembly-plugin/usage.html"" rel=""nofollow"">maven assembly plugin</a> and running a <code>mvn clean compile package</code>. At most I managed to get a jar that packaged the pom.xml dependencies, but still couldn't compile BritneyDilemma due to not having classifier4j (this isn't mentioned in the pom.xml so I'm unsure how maven is supposed to know it's a requirement/where to pull it from).</li>
<li>Moving away from maven, downloading classifer4j and compiling via the command line. Run time error and a bit of sleuthing pointed out I was missing some <a href=""http://sourceforge.net/p/classifier4j/mailman/classifier4j-devel/?viewmonth=200411"" rel=""nofollow"">classifier4j dependency</a>, I assume this was because I moved away from maven and this would otherwise have been taken care of by some recursive building.</li>
</ul>

<p>Here is a stock build attempt, making no modifications/taking the pom.xml as is:</p>

<pre><code>~/projects/mlbookexamples/java (master) $ mvn install
[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Building mlbook
[INFO]    task-segment: [install]
[INFO] ------------------------------------------------------------------------
[INFO] [resources:resources {execution: default-resources}]
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory /home/alex/projects/mlbookexamples/java/src/main/resources
Downloading: http://repo.springsource.org/libs-milestone//javax/batch/javax.batch-api/1.0/javax.batch-api-1.0.jar
34K downloaded  (javax.batch-api-1.0.jar)
[INFO] [compiler:compile {execution: default-compile}]
[INFO] No sources to compile
[INFO] [resources:testResources {execution: default-testResources}]
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory /home/alex/projects/mlbookexamples/java/src/test/resources
[INFO] [compiler:testCompile {execution: default-testCompile}]
[INFO] No sources to compile
[INFO] [surefire:test {execution: default-test}]
[INFO] No tests to run.
[INFO] Surefire report directory: /home/alex/projects/mlbookexamples/java/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] [jar:jar {execution: default-jar}]
[WARNING] JAR will be empty - no content was marked for inclusion!
[INFO] Building jar: /home/alex/projects/mlbookexamples/java/target/mlbookexamples-1.0-SNAPSHOT.jar
[INFO] [install:install {execution: default-install}]
[INFO] Installing /home/alex/projects/mlbookexamples/java/target/mlbookexamples-1.0-SNAPSHOT.jar to /home/alex/.m2/repository/com/datasentiment/mlbook/mlbookexamples/1.0-SNAPSHOT/mlbookexamples-1.0-SNAPSHOT.jar
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESSFUL
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 6 seconds
[INFO] Finished at: Wed Mar 04 08:38:56 NZDT 2015
[INFO] Final Memory: 33M/249M
[INFO] ------------------------------------------------------------------------
~/projects/mlbookexamples/java/target (master) $ java -jar mlbookexamples-1.0-SNAPSHOT.jar 
no main manifest attribute, in mlbookexamples-1.0-SNAPSHOT.jar
</code></pre>

<p>I'm running arch linux.</p>
";28889389;2810596;51;Stiege;0;28889389;"<p>To compile Chapter 2 it appears there's no requirement to grab the dependencies listed in the pom.xml.</p>

<p>You can simply compile BritneyDilemma from the chapter2 folder (remembering to include the Classifier4J dependency downloaded from
<a href=""http://sourceforge.net/projects/classifier4j/files/"" rel=""nofollow"">http://sourceforge.net/projects/classifier4j/files/</a> , this can be kept in the chapter2 folder).</p>

<pre><code>~/projects/mlbookexamples/java/src/chapter2 (master) $ javac -cp Classifier4J-0.6.jar BritneyDilemma.java 
</code></pre>

<p>You can then go to the src folder and run the class, again with a reference to the classifier4j jar inside the chapter2 folder.</p>

<pre><code>~/projects/mlbookexamples/java/src (master) $ java -classpath .:chapter2/Classifier4J-0.6.jar chapter2.BritneyDilemma 
brittany spears = 0.7071067811865475
brittney spears = 0.7071067811865475
britany spears = 0.7071067811865475
britny spears = 0.7071067811865475
briteny spears = 0.7071067811865475
britteny spears = 0.7071067811865475
briney spears = 0.7071067811865475
brittny spears = 0.7071067811865475
brintey spears = 0.7071067811865475
britanny spears = 0.7071067811865475
britiny spears = 0.7071067811865475
britnet spears = 0.7071067811865475
britiney spears = 0.7071067811865475
christina aguilera = 0.0
britney spears = 0.9999999999999998
</code></pre>

<p>You will likely already have the required logging library, but it can be grabbed through a package manager as libcommons-logging-java.</p>

<p>Alternatively it is much easier to use eclipse in general. Simply create a new Java project but change the working directory from the default to the java directory inside the repository. Create a new folder 'lib' here and put in the classifier4j jar. In eclipse, right click on classifier4j and 'add to build path'. You can now simply click BritneyDilemma or any other source (adding dependencies to lib and adding to build path when necessary), and run as required.</p>

<p>Happy machine learning.</p>
"
4474553;783;fakeaccount;<opencv><machine-learning><svm><training-data>;28836418;1;SVM training OpenCV;"<p>I've been using a small number of images (around 20) to train an <code>SVM</code>, and I've noticed that when it comes to training, one picture can have a really big difference on the outcome.</p>

<p>Sometimes it will be kind of accurate, other times it'll say everything is a match, other times nothing is a match. Now working with a smaller set of images, I understand that the affects of one image are going to be proportionately greater, but what other than trial and error is there to discern what a good training set is? How do you go about choosing several hundred images for training?</p>

<p>This seems like its a bit <em>too</em> important of a step for it to just be left up to going</p>

<p><em>""well it seems to run better with these images and not those, now what if I include these...""</em> </p>

<p>i.e. guesswork.</p>
";28839032;860196;8365;runDOSrun;2;28839032;"<p>There can't be a broad definitive answer to your question because (as always) it's depending on the specifics of the application scenario. </p>

<p>So <strong>tl;dr</strong> version: Analyze your problem.</p>

<p>3 methods come to mind to gather evidence that your training set is useful:</p>

<ul>
<li><p>Using unsupervised techniques on your training data to verify their usefulness (needs you to actually analyze the problem and define specific criteria).</p></li>
<li><p>Taking training data without selective assumptions but preprocessing them <em>all</em> in the same way according to your scenario. This can be anything from a normalization to transfomations in order to ensure that the data grows more stable and e.g. noise or invariance is learnt.</p></li>
<li><p>In certain scenarios (e.g. real-time systems that operate highly diverse data) you don't want to make assumptions. You want your classifier to consider a broad range of input, so you choose your training set broad as well. </p></li>
</ul>

<p>In general: If 1 image in your training set breaks your classifier, you need to identify what makes that certain picture special and preferably train with more pictures similar to that one to make it more robust. Unless you can ensure for a fact that the trained system will never encounter images of this type.</p>
"
4069391;119;thesisstudent;<python><machine-learning><scikit-learn><svm>;28839663;0;Scikit SVM: create training dataset;"<p>I'm using this site <a href=""http://scikit-learn.org/stable/datasets/"" rel=""nofollow"">http://scikit-learn.org/stable/datasets/</a> (subtitle 5.5) to create my custom dataset for performing SVM with scikit. Summary of my day: I basically have no idea what I'm doing. </p>

<p>For my thesis I want to predict stock return direction, i.e. the output of SVM should be 1 (UP) or -1 (DOWN). At the moment I'm trying to figure out SVM with a random sample (because I do get how the tutorials work).</p>

<p>As on the mentioned website it says that each line takes the form  <code>&lt;label&gt; &lt;feature-id&gt;:&lt;feature-value&gt; &lt;feature-id&gt;:&lt;feature-value&gt;</code>, I thought that the training set I provide should take the same formatting. Hence I created following training sample in Notepad++:</p>

<pre><code>&lt;1&gt; &lt;1&gt;:&lt;0&gt;, &lt;1&gt;:&lt;19260800&gt;, &lt;1&gt;:&lt;77.83&gt;
&lt;1&gt; &lt;2&gt;:&lt;-1&gt;, &lt;2&gt;:&lt;20110000&gt;, &lt;2&gt;:&lt;75.78&gt;
&lt;-1&gt; &lt;3&gt;:&lt;1&gt;, &lt;3&gt;:&lt;53306400&gt;, &lt;3&gt;:&lt;76.24&gt;
&lt;1&gt; &lt;4&gt;:&lt;0&gt;, &lt;4&gt;:&lt;61293500&gt;, &lt;4&gt;:&lt;78.00&gt;
&lt;-1&gt; &lt;5&gt;:&lt;-1&gt;, &lt;5&gt;:&lt;42649500&gt;, &lt;5&gt;:&lt;75.91&gt;
</code></pre>

<p>For example, the second line:</p>

<p>&lt;1> means that stock went up since the day before, &lt;2> is the  of the data of the second line, &lt;-1> is a negative Twitter sentiment for that day for the specific firm, &lt;20110000> is the stock volume for that day, &lt;75.78> is the adjusted closing price of that day.</p>

<p>I hope you understand what I'm trying to say. And I hope even more somebody can help me out.</p>

<p>Thanks in advance!</p>
";28839812;190597;706790;unutbu;0;28839812;"<p>Take a look at the related links referenced in <a href=""http://scikit-learn.org/stable/datasets/#datasets-in-svmlight-libsvm-format"" rel=""nofollow noreferrer"">the docs</a>:</p>
<blockquote>
<p>Public datasets in svmlight / libsvm format: <a href=""http://www.csie.ntu.edu.tw/%7Ecjlin/libsvmtools/datasets/"" rel=""nofollow noreferrer"">http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/</a></p>
<p>Faster API-compatible implementation: <a href=""https://github.com/mblondel/svmlight-loader"" rel=""nofollow noreferrer"">https://github.com/mblondel/svmlight-loader</a></p>
</blockquote>
<p>If you click on the first link you'll find <a href=""http://www.csie.ntu.edu.tw/%7Ecjlin/libsvmtools/datasets/binary.htm"" rel=""nofollow noreferrer"">example data sets</a> such as <a href=""http://www.csie.ntu.edu.tw/%7Ecjlin/libsvmtools/datasets/binary/a1a"" rel=""nofollow noreferrer"">this one</a>:</p>
<pre><code>-1 3:1 11:1 14:1 19:1 39:1 42:1 55:1 64:1 67:1 73:1 75:1 76:1 80:1 83:1 
-1 3:1 6:1 17:1 27:1 35:1 40:1 57:1 63:1 69:1 73:1 74:1 76:1 81:1 103:1 
-1 4:1 6:1 15:1 21:1 35:1 40:1 57:1 63:1 67:1 73:1 74:1 77:1 80:1 83:1 
-1 5:1 6:1 15:1 22:1 36:1 41:1 47:1 66:1 67:1 72:1 74:1 76:1 80:1 83:1 
-1 2:1 6:1 16:1 22:1 36:1 40:1 54:1 63:1 67:1 73:1 75:1 76:1 80:1 83:1 
-1 2:1 6:1 14:1 20:1 37:1 41:1 47:1 64:1 67:1 73:1 74:1 76:1 82:1 83:1 
</code></pre>
<p>So you don't need the brackets, <code>&lt;&gt;</code>. Just fill the file with a numeric label,
and the pairs of numbers separated by a colon. There are no commas between the pairs.</p>
<p><a href=""http://scikit-learn.org/stable/datasets/#datasets-in-svmlight-libsvm-format"" rel=""nofollow noreferrer"">Per the docs</a>, you can then load the data set with</p>
<pre><code>&gt;&gt;&gt; from sklearn.datasets import load_svmlight_file
&gt;&gt;&gt; X_train, y_train = load_svmlight_file(&quot;/path/to/train_dataset.txt&quot;)
</code></pre>
"
3147711;964;Alex Walczak;<numpy><machine-learning><computer-vision><signal-processing><scientific-computing>;28839927;1;Tips on Improving Peak Analysis of a Signal. (peak widths);"<ul>
<li>Hi all. I've got hundreds of signals of this form on which I have
detected peaks above some threshold.</li>
</ul>

<p><img src=""https://i.stack.imgur.com/w3IGU.png"" alt=""enter image description here""></p>

<p>I define a <strong>peak width as FWHM (full width at half maximum)</strong>. However, I've fitted a cubic polynomial to the valleys of the signal, and so I've defined a peak as the distance from this baseline at same index as the peak to the peak.</p>

<p>I'm calculating <strong>peak width as the greatest distance between the intersection of the signal and a line at half the max</strong>. It looks like this:</p>

<pre><code>roots = indeces_of_intersections

intersection_lengths=[abs(y - x) for x, y in it.combinations(roots, 2)]

calculated_width = max(intersection_lengths)
</code></pre>

<p>I'm having problems calculating peak width consistently and that's because sometimes the line intersects with points on different peaks. </p>

<p><img src=""https://i.stack.imgur.com/H3nol.png"" alt=""enter image description here""></p>

<p>I've <strong>restricted the domain on which this intersecting line is defined:</strong> </p>

<p><em>Domain = [a little to the left of the peak, a little to the right of it]</em></p>

<p>but this domain restriction is the same for all peaks.</p>

<p>I've thought about somehow having this domain change for different peaks but not sure how to implement that. My code is almost fully automated, and I have to keep it that way. </p>

<p><img src=""https://i.stack.imgur.com/U16sO.png"" alt=""enter image description here""></p>
";28840925;3147711;964;Alex Walczak;0;28840925;"<p>Posting my question onto here helped me realize an easy solution:</p>

<pre><code>more_than_peak=[x for x in it.ifilter(lambda x: x if x&gt;peaks[i] else 0, roots)]
less_than_peak=[x for x in it.ifilter(lambda x: x if x&lt;peaks[i] else 0, roots)]
if len(more_than_peak)&gt;0 or len(less_than_peak)&gt;0:
     width = min(more_than_peak)-max(less_than_peak)
</code></pre>

<p>Here, I'm finding the intersections to the left and right of the peak index. Then, I'm finding the smallest one to the right of the peak and the biggest one to the left (x axis increases to the right). So simple and fast!</p>
"
200317;14774;add-semi-colons;<python><matrix><pandas><machine-learning><scipy>;28842922;0;python: Building a matrix with weights - using scikit-learn;"<p>I have large number of files each file represent an item and file contains labels and their weights reflecting how these labels are relevant to the item.
for example </p>

<pre><code>file -&gt; 0001.txt has
skiing-0.789
snow-0.65
winter-0.56

file -&gt; 0002.txt has
drama-0.89
comedy-0.678
action-0.12
</code></pre>

<p>I want to build a following matrix:</p>

<pre><code>     skiing, snow, winter, drama, comedy, action
0001  0.789, 0.65, 0.56, 0, 0, 0
0002  0, 0, 0, 0.89, 0.678, 0.12
</code></pre>

<p>I went about doing this by reading all the files and generating following dictionary: </p>

<pre><code>{0001:[(skiing,0.789),(snow,0.65),(winter,0.56)], 0002:[(drama, 0.89),(comedy, 0.678),(action,0.12)]
</code></pre>

<p>How can I build matrix from above. I did some research using <code>TfidfVectorizer()</code> I can get the counts by passing a list of items to it but I want the matrix with the weights. </p>
";;2476444;11959;Oliver W.;1;28843542;"<p>There is no need for external libraries, such as <code>scikit-learn</code>, as Python provides the required functionality easily with dictionary methods. Under the hood, any external library will be following a similar algorithm, described in the next paragraph.</p>

<p>First you'll want to generate a list of all the headers that are distributed over those different files. Then, it would be a cleaner idea to switch the internal structure into a <code>dict</code>, such that you can easily retrieve the filling value <code>0</code> when the header is not present in a specific file:</p>

<pre><code>u = {'0001': [('skiing',0.789),('snow',0.65),('winter',0.56)],
     '0002': [('drama', 0.89),('comedy', 0.678),('action',0.12)]}

headers = []
for key, labelweights in u.items():
    u[key] = dict(labelweights)
    headers.extend(u[key].keys())

print('user, {}'.format(', '.join(headers)))
for k in u:
    print('{}, {}'.format(k, ', '.join(str(u[k].get(header,0)) for header in headers)))
</code></pre>

<p>You'll want to change the last <code>print</code> function into a <code>matrix.append()</code> form, if your eventual goal is to present an array of arrays (a 2D matrix) to some other function in <code>scikit-learn</code>. I'll leave that fairly easy challenge to you.</p>

<p>Mind you, a very similar functionality, intended for writing to a csv file (or any object which has a <code>write</code> method), is provided by Python's <a href=""https://docs.python.org/2/library/csv.html#csv.DictWriter"" rel=""nofollow"">csv DictWriter class</a>, which you could use like this:</p>

<pre><code>headers = ['user']    
data = []
for key, labelweights in u.items():
    tmp = dict(labelweights)
    headers.extend(tmp.keys())
    tmp['user'] = key    
    data.append(tmp)

with open(outputfile, 'w') as fh:
    dw = csv.DictWriter(fh, headers, restval='0', delimiter=',')
    dw.writeheader()
    dw.writerows(data)
</code></pre>

<p>If there are duplicate headers in the files, then you'll want to get rid of the duplicates, e.g. by calling <code>headers = list(set(headers))</code>, just before the writing/printing code blocks.</p>
"
2805564;675;ukejoe;<machine-learning><scikit-learn><logistic-regression>;28844716;1;sklearn Logistic Regression probability;"<p>I have a dataset that determines whether a student will be admitted given two scores. I train my model with this data and can determine if a student will be admitted or not using the following code:</p>

<pre><code>model.predict([score1, score2])
</code></pre>

<p>This results in the answer:</p>

<pre><code>[1]
</code></pre>

<p>How can I get the probability of that? If I try predict_proba, I get:</p>

<pre><code>model.predict_proba([score1, score2])
&gt;&gt;[[ 0.38537034  0.61462966]]
</code></pre>

<p>I'd really like to see something like:</p>

<pre><code>&gt;&gt; [0.75]
</code></pre>

<p>to indicate that P(admittance | score1, score2) = 0.75</p>
";;2484687;5610;Raff.Edward;2;28847211;"<p>You may notice that 0.38537034+ 0.61462966 = 1. This is because you are getting the probabilities for both classes (admitted and not admitted) from the output of <em>predict_proba</em>. If you had 7 classes, you would instead get something like [[p1, p2, p3, p4, p5, p6, p7]] where p1+p2+p3+p4+p5+p6+p7 = 1 and pi >= 0. So if you want the probability of output i, you go index into your result and look at what pi is. Thats just how that works. </p>

<p>So if you had something where the probability was 0.75 of being not admitted, you would get a result that looks like [[0.25, 0.75]]. </p>

<p>(I may have reversed the ordering you used in your code for admitted/not admitted, but it doesn't matter - that just changes the index you look at). </p>
"
2805564;675;ukejoe;<machine-learning><scikit-learn><logistic-regression>;28844716;1;sklearn Logistic Regression probability;"<p>I have a dataset that determines whether a student will be admitted given two scores. I train my model with this data and can determine if a student will be admitted or not using the following code:</p>

<pre><code>model.predict([score1, score2])
</code></pre>

<p>This results in the answer:</p>

<pre><code>[1]
</code></pre>

<p>How can I get the probability of that? If I try predict_proba, I get:</p>

<pre><code>model.predict_proba([score1, score2])
&gt;&gt;[[ 0.38537034  0.61462966]]
</code></pre>

<p>I'd really like to see something like:</p>

<pre><code>&gt;&gt; [0.75]
</code></pre>

<p>to indicate that P(admittance | score1, score2) = 0.75</p>
";;8917942;51;lugq;1;47404370;"<p>If you want to sklearn's Lr model and you want to get the 2 classes' predicted probability, you should use this:</p>

<pre><code>model.predict_proba(xtest)
</code></pre>

<p>You will get the array of two classes <code>prob(shape N*2)</code>.</p>
"
3787253;2595;smatthewenglish;<machine-learning><data-visualization>;28849320;0;data visualization. 3D, precison, recall, and f-measure. maybe using ocatve?;"<p>I've been running a machine learning algorithm, I have output in the form of Precision, Recall, and F-Measure. </p>

<p>I'd like to graph this data so I can get a clearer conception of how things are really going, but I don't really know how to do that. I suppose I can use Octave? I heard about it in that Andrew Ng course and I've already got it on my machine, but I don't really know how to use it to visualize data. </p>

<p>Does anyone with experience in this know how I might best proceed or some helpful resources on the best way to go about this?</p>

<pre><code>0.011723329425556858 P 0.6000000238418579 R 0.010416666977107525 F1 0.02047781631341665
0.012895662368112544 P 0.6363636255264282 R 0.01215277798473835 F1 0.023850085569817648
0.01406799531066823 P 0.6666666865348816 R 0.013888888992369175 F1 0.027210884568890845
0.015240328253223915 P 0.6153846383094788 R 0.013888888992369175 F1 0.02716468612858015
0.016412661195779603 P 0.6428571343421936 R 0.015625 F1 0.03050847456668239
0.017584994138335287 P 0.6000000238418579 R 0.015625 F1 0.03045685282259509
0.01875732708089097 P 0.5625 R 0.015625 F1 0.030405405405405407
0.01992966002344666 P 0.529411792755127 R 0.015625 F1 0.030354131580674088
0.021101992966002344 P 0.5555555820465088 R 0.0173611119389534 F1 0.03367003527554599
0.022274325908558032 P 0.5263158082962036 R 0.0173611119389534 F1 0.03361344696816966
0.023446658851113716 P 0.5 R 0.0173611119389534 F1 0.033557048526295
0.0246189917936694 P 0.4761904776096344 R 0.0173611119389534 F1 0.03350083906570289
</code></pre>
";28866037;1667256;15373;greeness;1;28866037;"<p>I suppose the first column is some threshold you varied between lines.
The precision-recall graph is precision-vs-recall. Thus we can first retrieve those two columns from your data: (suppose your data are saved in <code>prf.data</code>). </p>

<pre><code>cat prf.data | awk '{print $3,$5}'
</code></pre>

<p>You will get below two columns only and you can initialize a 2d matrix in octave:</p>

<pre><code>data = [
0.6000000238418579 0.010416666977107525
0.6363636255264282 0.01215277798473835
0.6666666865348816 0.013888888992369175
0.6153846383094788 0.013888888992369175
0.6428571343421936 0.015625
0.6000000238418579 0.015625
0.5625 0.015625
0.529411792755127 0.015625
0.5555555820465088 0.0173611119389534
0.5263158082962036 0.0173611119389534
0.5 0.0173611119389534
0.4761904776096344 0.0173611119389534];
</code></pre>

<p>Then under octave, below command will print each row as a data point in the graph:</p>

<pre><code>plot(data(:,2), data(:,1), 'x')
ylabel('precision')
xlabel('recall')
</code></pre>

<p><img src=""https://i.stack.imgur.com/0kFxM.png"" alt=""enter image description here""></p>

<p>Looks like with some threshold increase, you are decreasing precision and the recall stays the same (for example, when threshold = 0.021, 0.022, 0.023, 0.024). </p>
"
4631203;43;bfengineer;<machine-learning><artificial-intelligence><neural-network><lstm>;28850154;4;LSTM network learning;"<p>I have attempted to program my own LSTM (long short term memory) neural network. I would like to verify that the basic functionality is working. I have implemented a Back propagation through time BPTT algorithm to train a single cell network.</p>

<p>Should a single cell LSTM network be able to learn a simple sequence, or are more than one cells necessary? The network does not seem to be able to learn a simple sequence such as 1 0 0 0 1 0 0 0 1 0 0 0 1.</p>

<p>I am sending the the sequence 1's and 0's one by one, in order, into the network, and feeding it forward. I record each output for the sequence.</p>

<p>After running the whole sequence through the LSTM cell, I feed the mean error signals back into the cell, saving the weight changes internal to the cell, in a seperate collection, and after running all the errors one by one through and calculating the new weights after each error, I average  the new weights together to get the new weight, for each weight in the cell.</p>

<p>Am i doing something wrong? I would very appreciate any advice.</p>

<p>Thank you so much!</p>
";34148259;1259448;953;Jing;3;34148259;"<p>Having only one cell (one hidden unit) is not a good idea even if you are just testing the correctness of your code. You should try 50 even for such simple problem. This paper here: <a href=""http://arxiv.org/pdf/1503.04069.pdf"" rel=""nofollow"">http://arxiv.org/pdf/1503.04069.pdf</a> gives you very clear gradient rules for updating the parameters. Having said that, there is no need to implement your own even if your dataset and/or the problem you are working on is new LSTM. Pick from the existing library (Theano, mxnet, Torch etc...) and modify from there I think is a easier way, given that it's less error prone and it supports gpu computing which is essential for training lstm within a reasonable amount of time.</p>
"
4631203;43;bfengineer;<machine-learning><artificial-intelligence><neural-network><lstm>;28850154;4;LSTM network learning;"<p>I have attempted to program my own LSTM (long short term memory) neural network. I would like to verify that the basic functionality is working. I have implemented a Back propagation through time BPTT algorithm to train a single cell network.</p>

<p>Should a single cell LSTM network be able to learn a simple sequence, or are more than one cells necessary? The network does not seem to be able to learn a simple sequence such as 1 0 0 0 1 0 0 0 1 0 0 0 1.</p>

<p>I am sending the the sequence 1's and 0's one by one, in order, into the network, and feeding it forward. I record each output for the sequence.</p>

<p>After running the whole sequence through the LSTM cell, I feed the mean error signals back into the cell, saving the weight changes internal to the cell, in a seperate collection, and after running all the errors one by one through and calculating the new weights after each error, I average  the new weights together to get the new weight, for each weight in the cell.</p>

<p>Am i doing something wrong? I would very appreciate any advice.</p>

<p>Thank you so much!</p>
";34148259;4755986;52;Vespa;0;36991043;"<p>I haven't tried 1 hidden unit before, but I am sure 2  or 3 hidden units will work for sequence 0,1,0,1,0,1. It is not necessarily the more the cells, the better the result. Training difficulty also increases with the number of cells. </p>

<p>You said you averaged new weights together to get the new weight. Does that mean you run many training sessions and take the average of the trained   weights? </p>

<p>There are many possibilities your LSTM did not work, even if you implemented it correctly. The weights are not easy to train by simple gradient descent.</p>

<p>Here are my suggestion for weight optimization.</p>

<ol>
<li><p>Using Momentum method for gradient descent.</p></li>
<li><p>Add some gaussian noise to your training set to prevent overfitting.</p></li>
<li><p>using adaptive learning rates for each unit.</p></li>
</ol>

<p>Maybe you can take a look at Coursera's course Neural Network offered by Toronto University, and discuss with people there.</p>

<p>Or you can take a look at other examples on GitHub. For instance :</p>

<p><a href=""https://github.com/JANNLab/JANNLab/tree/master/examples/de/jannlab/examples"" rel=""nofollow"">https://github.com/JANNLab/JANNLab/tree/master/examples/de/jannlab/examples</a></p>
"
4631203;43;bfengineer;<machine-learning><artificial-intelligence><neural-network><lstm>;28850154;4;LSTM network learning;"<p>I have attempted to program my own LSTM (long short term memory) neural network. I would like to verify that the basic functionality is working. I have implemented a Back propagation through time BPTT algorithm to train a single cell network.</p>

<p>Should a single cell LSTM network be able to learn a simple sequence, or are more than one cells necessary? The network does not seem to be able to learn a simple sequence such as 1 0 0 0 1 0 0 0 1 0 0 0 1.</p>

<p>I am sending the the sequence 1's and 0's one by one, in order, into the network, and feeding it forward. I record each output for the sequence.</p>

<p>After running the whole sequence through the LSTM cell, I feed the mean error signals back into the cell, saving the weight changes internal to the cell, in a seperate collection, and after running all the errors one by one through and calculating the new weights after each error, I average  the new weights together to get the new weight, for each weight in the cell.</p>

<p>Am i doing something wrong? I would very appreciate any advice.</p>

<p>Thank you so much!</p>
";34148259;6650024;36;Pranav Shyam;0;38638577;"<p>The best way to test an LSTM implementation (after gradient checking) is to try it out on the toy memory problems described in the original LSTM paper itself.</p>

<p>The best one that I often use is the 'Addition Problem':</p>

<p>We give a sequence of tuples of the form (value, mask). Value is a real valued scalar number between 0 and 1. Mask is a binary value - either 0 or 1. </p>

<p><code>
0.23, 0
0.65, 0
...
0.86, 0
0.13, 1
0.76, 0
...
0.34, 0
0.43, 0
0.12, 1
0.09, 0
..
0.83, 0    -&gt;  0.125
</code></p>

<p>In the entire sequence of such tuples (usually of length 100), only 2 tuples should have mask as 1, the rest of the tuples should have the mask as 0. The target at the final time step is the a average of the two values for which the mask was 1. The outputs at all other time steps, other than the last one is ignored. The values and the positions of the mask are arbitrarily chosen. Thus, this simple task shows if your implementation can actually remember things over long periods of time.</p>
"
3633250;4320;Maksim Khaitovich;<machine-learning><neural-network><pybrain>;28855024;1;Pybrain recurrent network for regression - how to properly kickstart trained network for predictions;"<p>I am trying to solve regression task using recurrent neural network (I use pybrain to build it). After my network is fit I want to use it to make predictions. But prediction of recurrent network is affected by its previous prediction (whih in turn is affected by prediction before it etc).</p>

<p>Question is - once network is trained and I want to make predictions with it on a dataset, how to properly kickstart the prediction process. If I will just call .activate() on first example from a dataset for predictions that means that the recurrent connection will pass 0 to network and it will affect the subsequent predictions in an undesireable way. Is there a way to force fully trained recurrent network to think that previous activation result was of a some special value? If yes, which value is the best here (maybe mean of possible activation output values or smth like it?)</p>

<p>UPDATE. Ok, since no one had any ideas within a day on how to do this with recurrent network in pybrain, let me maybe a bit change a formulation to forget about pybrain. Consider that I build a pybrain network for regression (for example, predicting price of a stock). Network will be used with a dataset which has 10 features. I add one additional feature into the dataset and fill it with previous price of from a dataset. Thus I replicate a recurrent network (aditional input neuron replicates recurrent connection). The questions are:</p>

<p>1) In the dataset for training I fill this additional feature with previous price. But what to do with the FIRST record in a training dataset (I don't know previous price). Should leave it 0? It should a bad idea, previous price WAS NOT zero. Should I use mean of prices in training dataset? Any other suggestions?
2) Again, same question as #1 but for running fully trained network against test dataset. While running my network against test dataset I should always pick up its prediction and put the result into this new 11th input neuron before making next prediction. But again, what to do when I need to run first prediction in dataset (since I don't know previous price)?</p>
";31192731;1998457;91;VirtualGreg;2;31192731;"<p>This isn't my understanding of recurrent networks at all.</p>

<p>When you initially create a recurrent network the recurrent connections (say middle layer to middle layer) will be randomized, as with any other connection. This is their starting value. Each time you activate a recurrent network you'll alter those connections and thus your output will be altered.</p>

<p>Carrying this logic forwards, if you wrote some code to train a recurrent network and saved it to a file, you'd have in that file a recurrent network ready to go with your real data, albeit the first invocation will contain the recurrent feedback from your last activation during the training. </p>

<p>The thing you want to do is make sure that you re-save your recurrent network anytime you wish to persist it's state. For a simple FFN this wouldn't be an issue because you only change the state during training, but for a recurrent network you'll want to persist the state after any activation because the recurrent weights will have updated.</p>

<p>I don't think it's the case that a recurrent network will be poisoned because of the initial value of the recurrent connections; certainly I wouldn't trust the first invocation, but given they're designed for sequences that shouldn't be an issue in either case.</p>

<p>Regarding your updated question, I'm not at all convinced that arbitrarily adding a single input node will simulate this. In point of fact I suspect you'd absolutely break the networks predictive capabilities. In your example, starting with 10 input nodes, and lets pretend you have 20 middle nodes, just by adding an extra input node you'll generate an additional 20 connections to the network, that will be initially randomized. Every additional point will compound this change, and after 10 additional input nodes you'll have as many randomized connections as trained.</p>

<p>I don't see this working, and I certainly don't believe it would simulate recurrent learning in the way you think.</p>
"
4257646;73;kim;<image-processing><machine-learning><computer-vision><object-detection><object-recognition>;28873847;0;The steps to do object detection in natural image?;"<p>I am new to computer vision, can anyone tell me the steps to do object detection in natural image? (Here object refers to logo). I drafted the following steps based on my own understanding:</p>

<p><strong>Problem statement:</strong> Suppose there are 20 reference logos, given an natural image, tell out which logo exists in the image and in which location (bounding box).</p>

<p><strong>Step1:</strong> Collect many (i.e. 100) images containing corresponding logos, and crop out the logo region. Hence, there are 100 examples for each logo. The purpose of this step is to deal with logos under different conditions, such as illumination, rotations etc. </p>

<p><strong>Step2:</strong> Collect random images that don't contain any logos.</p>

<p><strong>Step3:</strong> Extract features for example logos and random images, use SIFT feature.</p>

<p><strong>Step4:</strong> Now, the problem becomes a multi-class classification problem. There are 21 classes, 20 classes corresponds to 20 logos, and 1 class corresponds random images. </p>

<p><strong><em>Question1:</em></strong>  use which classifier?  what is input and what is output? </p>

<p><strong>Step5:</strong> Given a test image, extract SIFT features, use all the features as input?</p>

<p><strong><em>Question2:</em></strong> For the test image, use what as input and how to do the classification to tell out whether it contains a logo or not, and which logo it is?</p>

<p><strong><em>Question3:</em></strong>  How to determine the location of the detected logo?</p>

<p><strong><em>Question4:</em></strong> Any image labeling or cropping tool?  </p>

<p>If my procedure is not correct, please tell me how to do this step by step. Thanks in advance!!</p>
";;2944190;11;SkyHawk;0;28878279;"<p><strong>Question1:</strong> I can advice you to use <a href=""http://en.wikipedia.org/wiki/Support_vector_machine"" rel=""nofollow noreferrer"">Support Vector Machine</a>. It's simple but powerful classifier for tasks with small dataset. It's easy to find implementations of SVM for most of popular programming languages. You should extract SIFT (or any other) features for patches with or without logos of same size and use them as classifiers input. Ground truth classification labels are logo names and some label for clean patches. So, if you have 20 logos, you will have 21 different class labels.</p>

<p><strong>Question2 and 3:</strong> You should use sliding window technique. Its essence lies in the fact that you can crop patches of the test image with some stride and use your classifier to predict if there is some logo or not. You can read more about it, for example, <a href=""http://cs.brown.edu/courses/cs143/proj4/"" rel=""nofollow noreferrer"">here</a>.</p>

<p><strong>Question4:</strong> Seems like that thread has the answer: <a href=""https://stackoverflow.com/questions/8317787/image-labelling-and-annotation-tool"">image labelling and annotation tool</a> </p>

<p>Some advices:</p>

<ul>
<li>Bootstrapping can help you to find the most difficult for classifier patches without logos</li>
<li>Use cross-validation to determine the best parameters of SIFT, SVM or optimal patch size.</li>
</ul>

<p>Good luck!</p>
"
4636736;93;Thomas;<python><pandas><machine-learning><classification><recommendation-engine>;28878987;-1;How to use machine learning to ask users dynamically and recommend a result;"<p>I want to devolope a system which recommends a sport after asking multiple questions to an user. Which questions are asked and their fittest sorting should be decided dynamically by the system so that the best answer for each user is found.</p>

<p>I have a threefold data structure: sports are linked to different attributes and questions can refer to several attributes. Both relations are weighted.</p>

<p>On a scale from 0-1, soccer is linked to 'indoors' with 0.6, but to 'outdoors' with 0.8, as soccer is played more often outdoors and with 1.0 to 'ball' which is in any case necessary for soccer. Question 2 is linked to 'racket'(0.7) and 'bow' (0.6) as both items can be used with hands, however 'racket' is heigher weighted as it is more likely that someone thinks of 'racket' answering this question.</p>

<pre><code># table1: sports (result set)
1 | badminton
2 | soccer
3 | fishing

# table2: attributes (describes different results)
1 | indoors
2 | outdoors
3 | ball
4 | racket
5 | bow

# table3: questions (checks for needed attributes)
1 | Do you like doing sports outdoors?
2 | Do you want to hold a gadget while performing the sport?
</code></pre>

<p>I could try to build this based on a relational database and some ranking system implemented by me in python / pandas. However, I feel like I am neglecting recent developments in machine learning and apparently one should not build a recommendation engine for smaller projects by yourself:</p>

<blockquote>
  <p><a href=""http://www.datacommunitydc.org/blog/2013/05/recommendation-engines-why-you-shouldnt-build-one"" rel=""nofollow"">http://www.datacommunitydc.org/blog/2013/05/recommendation-engines-why-you-shouldnt-build-one</a></p>
</blockquote>

<p>As far as I understood this could be partially a decission tree problem, am I right? </p>

<blockquote>
  <p><a href=""http://scikit-learn.org/stable/modules/tree.html"" rel=""nofollow"">http://scikit-learn.org/stable/modules/tree.html</a></p>
</blockquote>

<p>What is the concise problem statement of my matter in terms of machine learning? Which libraries could support me in my python-pandas environment?</p>

<p><em>I am really sorry for being that naive, I promise to learn as much as I can if someone could guide me towards the right direction (regarding the theory and technologies to apply).</em></p>
";28879926;2886575;4261;Him;0;28879926;"<p>The machine learning task in question would not be the task of finding the best sport given already determined weights between questions and sports, it would be to find the weights given already answered questions and favorite sports.</p>

<p>Viz: You have a group of people (the training set) who have answered your questions and have then ranked their favorite sports using some scale (you pick).  Then, you train some algorithm to pick sports probabilistically to predict the results of the training set as closely as possible.</p>

<p>The attributes are not really useful if they already exist as questions (your indoor v outdoor example), and even then they are of questionable value.</p>

<p>For example, you fill out the questionairre: indoor v outdoor? o, gadget? y
Then you say how much you like various sports: soccer? 5, tennis? 8, bball? 2</p>

<p>From this your algorithm should determine that people who answer o,y like tennis, are indifferent to soccer, and hate basketball.  Of course, more data will produce more accurate results.</p>

<p>Probably the model to go with for modeling discrete inputs is a bayesian network.  You *can* include the attributes as an intermediate hidden layer in a bayesian network, but I would start with a simple bayesian network and then see if an intermediate layer adds value.  If you are confident that things like ""using a ball"" are really, in some sense, the <em>underlying reasons</em> that people like one sport v. another, then, yes, use a hidden layer in your network, otherwise, it's not going to be useful.</p>

<p><a href=""http://en.wikipedia.org/wiki/Bayesian_network"" rel=""nofollow"">http://en.wikipedia.org/wiki/Bayesian_network</a></p>
"
3749854;75;user3749854;<machine-learning><neural-network><mathematical-optimization><hessian-matrix>;28884485;1;Backwards Propagation: are analytical second derivatives worth calculating?;"<p>I understand the reason why we typically do not use second derivative information is that the Hessian can be very large with more layers and weights, leading to high computational expense, so Hessian-free methods are preferred. My question is <strong>whether the computational expense that the literature refers to is a result of the assumption that second derivatives are being calculated numerically</strong>? So if instead the objective function under evaluation gave rise to neat analytical second derivatives, then could a Newton based method be computationally tractable for estimating ANN parameters even with a large number of hidden layers? Or is it purely the size of the Hessian - and the need/expense to invert it for the updating algorithm (!) - that is the determining factor for using gradient based methods instead?</p>
";;1190430;5186;Artem Sobolev;0;28887030;"<p>In order to use 2nd order optimization methods, you should calculate <strong>inverse</strong> of the Hessian matrix. There are 2 problems with that:</p>

<ol>
<li><p>The Hessian has O(N<sup>2</sup>) parameters, which makes it harder to compute (both in terms of memory and time complexity). Even if you calculate each one in O(1) time (that is, you compute derivatives using an analytic formula), you still need to compute quadratic amount of them.</p></li>
<li><p>Matrix inversion is <a href=""http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra"" rel=""nofollow"">quite slow</a> operation, which takes even more time (O(N<sup>3</sup> in case of Gaussian Elimination), for example, in case of ) than just calculating all the values of the Hessian.</p></li>
</ol>
"
492372;24872;London guy;<machine-learning><classification>;28904180;1;What kind of classifier is used in the following scenario?;"<p>If I am building a weather predictor that will predict if it is will snow tomorrow, it is very easy to just straight away answer by saying ""NO"". </p>

<p>Obviously, if you evaluate such a classifier on every day of the year, it would be correct with an accuracy at 95% (considering that I build it and test it in a region where it snows very rarely). </p>

<p>Of course, that is such a stupid classifier even if it has an accuracy of 95% because it is obviously more important to predict if it will snow during the winter months (Jan &amp; Feb) as opposed to any other months.</p>

<p>So, if I have a lot of features that I collect about the previous day to predict if it will snow the next day or not, considering that there will be a feature that says which month/week of the year it is, how can I weigh this particular feature and design the classifier to solve this practical problem?</p>
";28904719;270287;41194;IVlad;2;28904719;"<blockquote>
  <p>Of course, that is such a stupid classifier even if it has an accuracy of 95% because it is obviously more important to predict if it will snow during the winter months (Jan &amp; Feb) as opposed to any other months.</p>
</blockquote>

<p>Accuracy might not be the best measurement to use in your case. Consider using <a href=""http://en.wikipedia.org/wiki/Precision_and_recall"" rel=""nofollow noreferrer"">precision, recall</a> and <a href=""http://en.wikipedia.org/wiki/F1_score"" rel=""nofollow noreferrer"">F1 score</a>.</p>

<blockquote>
  <p>how can I weigh this particular feature and design the classifier to solve this practical problem?</p>
</blockquote>

<p>I don't think <strong>you</strong> should weight any particular feature in any way. You should let your algorithm do that and use <a href=""http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29"" rel=""nofollow noreferrer"">cross validation</a> to decide on the best parameters for your model, in order to also avoid overfitting.</p>

<p>If you say jan and feb are the most important months, consider only applying your model for those two months. If that's not possible, look into giving different weights to your <strong>classes</strong> (going to rain / not going to rain), based on their number. <a href=""https://stackoverflow.com/questions/28621518/what-is-the-difference-between-class-weight-none-and-auto-in-svm-scikit-learn/28625807#28625807"">This question</a> discusses that issue - the concept should be understandable regardless of your language of choice.</p>
"
3787253;2595;smatthewenglish;<java><machine-learning>;28913062;2;modify perceptron to become gradient descent;"<p>According to <a href=""https://www.udacity.com/course/viewer#!/c-ud675/l-315142919/e-432088676/m-432088677"" rel=""nofollow""><strong>this</strong></a> video the substantive difference between the perceptron and gradient descent algorithms are quite minor. They specified it as essentially:</p>

<p><strong>Perceptron:</strong> &Delta;w<sub>i</sub> = &eta;(y - &ycirc;)x<sub>i</sub></p>

<p><strong>Gradient Descent:</strong> &Delta;w<sub>i</sub> = &eta;(y - &alpha;)x<sub>i</sub></p>

<p>I've implemented a working version of the perceptron algorithm, but I don't understand what sections I need to change to turn it into gradient descent. </p>

<p>Below is the load-bearing portions of my perceptron code, I suppose that these are the components I need to modify. But where? What do I need to change? I don't understand. </p>

<p><em>This is left for pedagogical reasons, I've sort of figured this out but am still confused about the gradient, please see</em> <strong>UPDATE</strong> <em>below</em></p>

<pre><code>      iteration = 0;
      do 
      {
          iteration++;
          globalError = 0;
          //loop through all instances (complete one epoch)
          for (p = 0; p &lt; number_of_files__train; p++) 
          {
              // calculate predicted class
              output = calculateOutput( theta, weights, feature_matrix__train, p, globo_dict_size );
              // difference between predicted and actual class values
              localError = outputs__train[p] - output;
              //update weights and bias
              for (int i = 0; i &lt; globo_dict_size; i++) 
              {
                  weights[i] += ( LEARNING_RATE * localError * feature_matrix__train[p][i] );
              }
              weights[ globo_dict_size ] += ( LEARNING_RATE * localError );

              //summation of squared error (error value for all instances)
              globalError += (localError*localError);
          }

          /* Root Mean Squared Error */
          if (iteration &lt; 10) 
              System.out.println(""Iteration 0"" + iteration + "" : RMSE = "" + Math.sqrt( globalError/number_of_files__train ) );
          else
              System.out.println(""Iteration "" + iteration + "" : RMSE = "" + Math.sqrt( globalError/number_of_files__train ) );
      } 
      while(globalError != 0 &amp;&amp; iteration&lt;=MAX_ITER);
</code></pre>

<p>This is the crux of my perceptron:</p>

<pre><code>  static int calculateOutput( int theta, double weights[], double[][] feature_matrix, int file_index, int globo_dict_size )
  {
     //double sum = x * weights[0] + y * weights[1] + z * weights[2] + weights[3];
     double sum = 0;

     for (int i = 0; i &lt; globo_dict_size; i++) 
     {
         sum += ( weights[i] * feature_matrix[file_index][i] );
     }
     //bias
     sum += weights[ globo_dict_size ];

     return (sum &gt;= theta) ? 1 : 0;
  }
</code></pre>

<p>Is it just that I replace that <code>caculateOutput</code> method with something like this:</p>

<pre><code>public static double [] gradientDescent(final double [] theta_in, final double alpha, final int num_iters, double[][] data ) 
{
    final double m = data.length;   
    double [] theta = theta_in;
    double theta0 = 0;
    double theta1 = 0;
    for (int i = 0; i &lt; num_iters; i++) 
    {                        
        final double sum0 = gradientDescentSumScalar0(theta, alpha, data );
        final double sum1 = gradientDescentSumScalar1(theta, alpha, data);                                   
        theta0 = theta[0] - ( (alpha / m) * sum0 ); 
        theta1 = theta[1] - ( (alpha / m) * sum1 );                        
        theta = new double [] { theta0, theta1 };
    }
    return theta;
}
</code></pre>

<hr>

<p><strong>UPDATE EDIT</strong></p>

<hr>

<p>At this point I think I'm very close. </p>

<p>I understand how to calculate the hypothesis and I think I've done that correctly, but nevertheless, something remains terribly wrong with this code. I'm pretty sure it has something to do with my calculation of the <code>gradient</code>. When I run it the error fluctuates wildly and then goes to <code>infinity</code> then just <code>NaaN</code>. </p>

<pre><code>  double cost, error, hypothesis;
  double[] gradient;
  int p, iteration;

  iteration = 0;
  do 
  {
    iteration++;
    error = 0.0;
    cost = 0.0;

    //loop through all instances (complete one epoch)
    for (p = 0; p &lt; number_of_files__train; p++) 
    {

      // 1. Calculate the hypothesis h = X * theta
      hypothesis = calculateHypothesis( theta, feature_matrix__train, p, globo_dict_size );

      // 2. Calculate the loss = h - y and maybe the squared cost (loss^2)/2m
      cost = hypothesis - outputs__train[p];

      // 3. Calculate the gradient = X' * loss / m
      gradient = calculateGradent( theta, feature_matrix__train, p, globo_dict_size, cost, number_of_files__train);

      // 4. Update the parameters theta = theta - alpha * gradient
      for (int i = 0; i &lt; globo_dict_size; i++) 
      {
          theta[i] = theta[i] - LEARNING_RATE * gradient[i];
      }

    }

    //summation of squared error (error value for all instances)
    error += (cost*cost);       

  /* Root Mean Squared Error */
  if (iteration &lt; 10) 
      System.out.println(""Iteration 0"" + iteration + "" : RMSE = "" + Math.sqrt(  error/number_of_files__train  ) );
  else
      System.out.println(""Iteration "" + iteration + "" : RMSE = "" + Math.sqrt( error/number_of_files__train ) );
  //System.out.println( Arrays.toString( weights ) );

  } 
  while(cost != 0 &amp;&amp; iteration&lt;=MAX_ITER);


}

static double calculateHypothesis( double[] theta, double[][] feature_matrix, int file_index, int globo_dict_size )
{
    double hypothesis = 0.0;

     for (int i = 0; i &lt; globo_dict_size; i++) 
     {
         hypothesis += ( theta[i] * feature_matrix[file_index][i] );
     }
     //bias
     hypothesis += theta[ globo_dict_size ];

     return hypothesis;
}

static double[] calculateGradent( double theta[], double[][] feature_matrix, int file_index, int globo_dict_size, double cost, int number_of_files__train)
{
    double m = number_of_files__train;

    double[] gradient = new double[ globo_dict_size];//one for bias?

    for (int i = 0; i &lt; gradient.length; i++) 
    {
        gradient[i] = (1.0/m) * cost * feature_matrix[ file_index ][ i ] ;
    }

    return gradient;
}
</code></pre>
";;4643209;120;ejr;1;28918214;"<p>The perceptron rule is just an approximation to the gradient descent when you have non-differentiable activation functions like <code>(sum &gt;= theta) ? 1 : 0</code>. As they ask in the end of the video, you cannot use gradients there because this threshold function isn't differentiable (well, its gradient is not defined for x=0 and the gradient is zero everywhere else). If, instead of this thresholding, you had a smooth function like the <a href=""http://en.wikipedia.org/wiki/Sigmoid_function"" rel=""nofollow"">sigmoid</a> you could calculate the actual gradients. </p>

<p>In that case your weight update would be <code>LEARNING_RATE * localError * feature_matrix__train[p][i] * output_gradient[i]</code>. For the case of sigmoid, the link I sent you also shows how to calculate the <code>output_gradient</code>.</p>

<p>In summary to change from perceptrons to gradient descent you have to</p>

<ol>
<li>Use an activation function whose derivative (gradient) is not zero
everywhere.   </li>
<li>Apply the chain rule to define the new update rule</li>
</ol>
"
1732062;1587;djWann;<java><algorithm><gps><machine-learning>;28913778;1;How to select a region of GPS coordinates from multiple tracks?;"<p>I have multiple GPS tracks(vector of 2d latitude, longitude coordinates) that I have created using my mobile device. They have different lengths and directions. I want to average this tracks and create just a single one. As a first step I would like to select only the points that are in a certain area. For example in the image bellow I want to select only the points that are between the grey lines.</p>

<p><img src=""https://i.stack.imgur.com/aOqsJ.jpg"" alt=""enter image description here""></p>

<p>Given the fact that the tracks might have different shapes and positioning would a bounding rectangle approach make sense? Are there better algorithms to do this? </p>
";28915784;4623467;2682;alesc;1;28914151;"<p>I would try clustering points from multiple tracks. After that, I would use the center point of each cluster to get my <em>average</em> path.</p>

<p>For clustering, you can use kNN or any other principle where you group points that are near each other.</p>

<p>After getting your <em>average</em> path, you can apply bounds to it (you could also filter your points before clustering).</p>
"
1732062;1587;djWann;<java><algorithm><gps><machine-learning>;28913778;1;How to select a region of GPS coordinates from multiple tracks?;"<p>I have multiple GPS tracks(vector of 2d latitude, longitude coordinates) that I have created using my mobile device. They have different lengths and directions. I want to average this tracks and create just a single one. As a first step I would like to select only the points that are in a certain area. For example in the image bellow I want to select only the points that are between the grey lines.</p>

<p><img src=""https://i.stack.imgur.com/aOqsJ.jpg"" alt=""enter image description here""></p>

<p>Given the fact that the tracks might have different shapes and positioning would a bounding rectangle approach make sense? Are there better algorithms to do this? </p>
";28915784;1137065;544;uahakan;1;28915784;"<p>I would suggest taking a look into these classes for practical use :</p>

<p><a href=""https://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/stat/clustering/DBSCANClusterer.html"" rel=""nofollow"">https://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/stat/clustering/DBSCANClusterer.html</a></p>

<p><a href=""http://commons.apache.org/proper/commons-math/userguide/filter.html"" rel=""nofollow"">http://commons.apache.org/proper/commons-math/userguide/filter.html</a></p>
"
3787253;2595;smatthewenglish;<java><machine-learning><gradient-descent>;28916191;0;gradient descent as applied to feature vector bag of words classification task;"<p>I've watched the <a href=""https://www.youtube.com/watch?v=ZgXjKa0ChDw"" rel=""nofollow noreferrer""><strong>Andrew Ng videos</strong></a> over and over and still I don't understand how to apply gradient descent to my problem. </p>

<p>He deals pretty much exclusively in the realm of high level conceptual explanations but what I need are ground level tactical insights. </p>

<p>My input are feature vectors of the form: </p>

<p>Example:</p>

<pre><code>Document 1 = [""I"", ""am"", ""awesome""]
Document 2 = [""I"", ""am"", ""great"", ""great""]
</code></pre>

<p>Dictionary is:</p>

<pre><code>[""I"", ""am"", ""awesome"", ""great""]
</code></pre>

<p>So the documents as a vector would look like:</p>

<pre><code>Document 1 = [1, 1, 1, 0]
Document 2 = [1, 1, 0, 2]
</code></pre>

<p>According to what I've seen the algorithm for gradient descent looks like this:</p>

<p><img src=""https://i.stack.imgur.com/UHgMr.png"" alt=""enter image description here""></p>

<p>It is my current understanding that &alpha; is the learning rate, x<sup>(i)</sup> is a feature, in the above example for <code>Document 2</code>, x<sup>(3)</sup>=2. </p>

<p>y<sup>(i)</sup> is the label, in my case I'm trying to predict the <code>Document</code> associated with a particular feature vector so for instance y<sup>(0)</sup> would be associated with <code>Document 1</code>, &amp; y<sup>(1)</sup> would represent <code>Document 2</code>. </p>

<p>There will be potentially many documents, let's say 10, so I could have 5 docuements associated with y<sup>(0)</sup> and 5 documents associated with y<sup>(1)</sup>, in such case <code>m = 10</code>.</p>

<p><em>The first thing I don't really understand is, what is the role of &Theta;<sub>0</sub> &amp; &Theta;<sub>1</sub>?</em></p>

<p>I suppose that they are the weight values, as with the perceptron algorithm, I apply them to the value of the feature in an effort to coax that feature, regardless of its inherent value, to output the value of the label with which it is associated. Is that correct?
So I've been equating the &Theta; values with the weight values of perceptron, is this accurate? </p>

<p><em>Moreover I don't understand what we're taking the gradient of. I really don't care to hear another high level explaination about walking on hills and whatnot, practically speaking, for the situation I've just detailed above, what are we taking the gradient of? Weights in two subsequent iterations? The value of a feature and it's true label?</em></p>

<p>Thank you for your consideration, any insight would be greatly appreciated.  </p>
";;270287;41194;IVlad;3;28920368;"<blockquote>
  <p>He deals pretty much exclusively in the realm of high level conceptual explanations but what I need are ground level tactical insights.</p>
</blockquote>

<p>I found his videos the most practical and ""ground level"", especially since there is also code you can look at. Have you looked at it?</p>

<blockquote>
  <p>It is my current understanding that Î± is the learning rate, x(i) is a feature, in the above example for Document 2, x(3)=2.</p>
</blockquote>

<p>Correct about Î±, wrong about <code>x(i)</code>: <code>x(i)</code> is an <strong>instance</strong> or a <strong>sample</strong>. In your example, you have:</p>

<pre><code>Document 1 = [1, 1, 1, 0] = x(1)
Document 2 = [1, 1, 0, 2] = x(2)
</code></pre>

<p>A <strong>feature</strong> would be <code>x(1, 2) = 1</code>, for example.</p>

<blockquote>
  <p>y(i) is the label, in my case I'm trying to predict the Document associated with a particular feature vector so for instance y(0) would be associated with Document 1, &amp; y(1) would represent Document 2.</p>
</blockquote>

<p>Correct. Although I believe Andrew Ng's lectures use 1-based indexing, so that would be <code>y(1)</code> and <code>y(2)</code>.</p>

<blockquote>
  <p>There will be potentially many documents, let's say 10, so I could have 5 docuements associated with y(0) and 5 documents associated with y(1), in such case m = 10.</p>
</blockquote>

<p>That's not how you should look at it. Each document will have its own label (an <code>y</code> value). Whether or not the labels are equal among them is another story. Document 1 will have label <code>y(1)</code> and document <code>5</code> will have label <code>y(5)</code>. Whether or not <code>y(1) == y(5)</code> is irrelevant so far.</p>

<blockquote>
  <p>The first thing I don't really understand is, what is the role of Î˜0 &amp; Î˜1?</p>
</blockquote>

<p><code>Theta0</code> and <code>Theta1</code> represent your <strong>model</strong>, which is the thing you use to predict your labels:</p>

<pre><code>prediction = Theta * input
           = Theta0 * input(0) + Theta1 * input(1)
</code></pre>

<p>Where <code>input(i)</code> is the value of a feature, and <code>input(0)</code> is usually defined as always being equal to <code>1</code>.</p>

<p>Of course, since you have more than one feature, you will need more than two <code>Theta</code> values. Andrew Ng goes on to generalize this process for more features in the lectures following the one where he presents the formula you posted.</p>

<blockquote>
  <p>I suppose that they are the weight values, as with the perceptron algorithm, I apply them to the value of the feature in an effort to coax that feature, regardless of its inherent value, to output the value of the label with which it is associated. Is that correct? So I've been equating the Î˜ values with the weight values of perceptron, is this accurate?</p>
</blockquote>

<p>Yes, that is correct.</p>

<blockquote>
  <p>Moreover I don't understand what we're taking the gradient of. I really don't care to hear another high level explaination about walking on hills and whatnot, practically speaking, for the situation I've just detailed above, what are we taking the gradient of? Weights in two subsequent iterations? The value of a feature and it's true label?</p>
</blockquote>

<p>First of all, do you know what a gradient is? It's basically an array of partial derivatives, so it's easier to explain what we're taking the partial derivative of and with respect to what.</p>

<p>We are taking the partial derivative of the <strong>cost function</strong> (defined in Andrew Ng's lecture as the difference squared) with respect to each <code>Theta</code> value. All of these partial derivatives make up the gradient.</p>

<p>I really don't know how to explain it more practically. The closest from what you listed would be ""the value of a feature and its true label"", because the cost function tells us how good our model is, and its partial derivatives with respect to the weight of each feature kinda tell us how bad each weight is so far.</p>

<p>You seem to be confusing features and samples again. A feature does not have labels. Samples or instances have labels. Samples or instances also <strong>consist of</strong> features.</p>
"
823859;6395;Adam_G;<python><vector><machine-learning><scikit-learn>;28921807;0;Scikit-learn DictVectorizer to Classifier;"<p>I am trying to load a dictionary, and then perform classification. However, I get the error:</p>

<pre><code>  File ""train_classifier.py"", line 49, in &lt;module&gt;
    clf.fit(page_vecs.data[:-1],page_vecs.target[:-1])
  File ""/usr/local/lib/python3.4/site-packages/scipy/sparse/base.py"", line 505, in __getattr__
    raise AttributeError(attr + "" not found"")
AttributeError: target not found
</code></pre>

<p>How can I load the targets? Here is my code:</p>

<pre><code>vec = DictVectorizer()
page_vecs = vec.fit_transform(feature_dict_list)
clf = svm.SVC(gamma=0.001, C=100)
clf.fit(page_vecs.data[:-1],page_vecs.target[:-1])
print(clf.predict(page_vecs[-1]))
</code></pre>
";28925234;270287;41194;IVlad;1;28925234;"<p>Look at the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer.fit_transform"" rel=""nofollow"">DictVectorizer</a> class, specifically its fit_transform method:</p>

<blockquote>
  <p>Returns:<br>
  Xa : {array, sparse matrix}</p>
  
  <p>Feature vectors; always 2-d.</p>
</blockquote>

<p>So it returns a 2d array. </p>

<p>In your code, this line:</p>

<blockquote>
  <p>page_vecs = vec.fit_transform(feature_dict_list)</p>
</blockquote>

<p>Will cause <code>page_vecs</code> to be such a 2d array. 2d numpy arrays have no <code>target</code> attribute, which you try to use here:</p>

<blockquote>
  <p>clf.fit(page_vecs.data[:-1],page_vecs.target[:-1])</p>
</blockquote>

<p>That is why you get the error. In fact, you shouldn't even do <code>.data</code>, you should work with the numpy array directly. If you want to ignore the last row, do:</p>

<pre><code>page_vecs[:-1, :]
</code></pre>

<p>Your labels (or targets) have nothing to do with the <code>DictVectorizer</code> class, which only vectorizes your samples, not your labels. You should have a separate vector for the labels.</p>
"
3371423;118;ninja;<matlab><machine-learning><classification><cluster-analysis><mixture-model>;28923865;1;Clustering an image using Gaussian mixture models;"<p><strong>I want to use GMM(Gaussian mixture models for clustering a binary image and also want to plot the cluster centroids on the binary image itself.</strong></p>

<p>I am using this as my reference:
  <a href=""http://in.mathworks.com/help/stats/gaussian-mixture-models.html"" rel=""nofollow noreferrer"">http://in.mathworks.com/help/stats/gaussian-mixture-models.html</a></p>

<p>This is my initial code</p>

<pre><code> I=im2double(imread('sil10001.pbm'));
  K = I(:);
  mu=mean(K);
  sigma=std(K);
  P=normpdf(K, mu, sigma);
   Z = norminv(P,mu,sigma);
  X = mvnrnd(mu,sigma,1110);
  X=reshape(X,111,10);


 scatter(X(:,1),X(:,2),10,'ko');

options = statset('Display','final');
gm = fitgmdist(X,2,'Options',options);



idx = cluster(gm,X);
cluster1 = (idx == 1);
cluster2 = (idx == 2);


 scatter(X(cluster1,1),X(cluster1,2),10,'r+');
 hold on

  scatter(X(cluster2,1),X(cluster2,2),10,'bo');
  hold off
  legend('Cluster 1','Cluster 2','Location','NW')


  P = posterior(gm,X);

 scatter(X(cluster1,1),X(cluster1,2),10,P(cluster1,1),'+')
 hold on
 scatter(X(cluster2,1),X(cluster2,2),10,P(cluster2,1),'o')
 hold off
 legend('Cluster 1','Cluster 2','Location','NW')
 clrmap = jet(80); colormap(clrmap(9:72,:))
 ylabel(colorbar,'Component 1 Posterior Probability')
</code></pre>

<p>But the problem is that I am unable to plot the cluster centroids received from GMM in the primary binary image.How do i do this?
<a href=""https://i.stack.imgur.com/QB4h4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QB4h4.jpg"" alt=""enter image description here""></a></p>

<p>**Now suppose i have 10 such images in a sequence And i want to store the information of their mean position in two cell array then how do i do that.This is my code foe my new question ** </p>

<pre><code>    images=load('gait2go.mat');%load the matrix file
    for i=1:10

   I{i}=images.result{i};
  I{i}=im2double(I{i});

   %determine 'white' pixels, size of image can be [M N], [M N 3] or [M N 4]
  Idims=size(I{i});
  whites=true(Idims(1),Idims(2));

    df=I{i};
      %we add up the various color channels
 for colori=1:size(df,3)
  whites=whites &amp; df(:,:,colori)&gt;0.5;
 end

%choose indices of 'white' pixels as coordinates of data
[datax datay]=find(whites);

%cluster data into 10 clumps
  K = 10;               % number of mixtures/clusters
  cInd = kmeans([datax datay], K, 'EmptyAction','singleton',...
   'maxiter',1000,'start','cluster');

%get clusterwise means
 meanx=zeros(K,1);
 meany=zeros(K,1);  
  for i=1:K
   meanx(i)=mean(datax(cInd==i));
   meany(i)=mean(datay(cInd==i));

 end

 xc{i}=meanx(i);%cell array contaning the position of the mean for the 10    
 images
  xb{i}=meany(i);

figure;
gscatter(datay,-datax,cInd); %funky coordinates for plotting according to      
 image
 axis equal;
  hold on;
  scatter(meany,-meanx,20,'+'); %same funky coordinates


 end
</code></pre>

<p>I am able to get 10 images segmented but no the values of themean stored in the cell arrays xc and xb.They r only storing [] in place of the values of means</p>
";32181171;1763823;11;Mallo;0;32171788;"<p>I do believe you must had made a naive mistake in the plot and that's why you see just a straight line: You are plotting only the <code>x</code> values. </p>

<p>In my opinion, the second argument in the scatter command should be <code>X(cluster1,2)</code> or <code>X(cluster2,2)</code> depending on which <code>scatter</code> command is being used in the code.</p>
"
3371423;118;ninja;<matlab><machine-learning><classification><cluster-analysis><mixture-model>;28923865;1;Clustering an image using Gaussian mixture models;"<p><strong>I want to use GMM(Gaussian mixture models for clustering a binary image and also want to plot the cluster centroids on the binary image itself.</strong></p>

<p>I am using this as my reference:
  <a href=""http://in.mathworks.com/help/stats/gaussian-mixture-models.html"" rel=""nofollow noreferrer"">http://in.mathworks.com/help/stats/gaussian-mixture-models.html</a></p>

<p>This is my initial code</p>

<pre><code> I=im2double(imread('sil10001.pbm'));
  K = I(:);
  mu=mean(K);
  sigma=std(K);
  P=normpdf(K, mu, sigma);
   Z = norminv(P,mu,sigma);
  X = mvnrnd(mu,sigma,1110);
  X=reshape(X,111,10);


 scatter(X(:,1),X(:,2),10,'ko');

options = statset('Display','final');
gm = fitgmdist(X,2,'Options',options);



idx = cluster(gm,X);
cluster1 = (idx == 1);
cluster2 = (idx == 2);


 scatter(X(cluster1,1),X(cluster1,2),10,'r+');
 hold on

  scatter(X(cluster2,1),X(cluster2,2),10,'bo');
  hold off
  legend('Cluster 1','Cluster 2','Location','NW')


  P = posterior(gm,X);

 scatter(X(cluster1,1),X(cluster1,2),10,P(cluster1,1),'+')
 hold on
 scatter(X(cluster2,1),X(cluster2,2),10,P(cluster2,1),'o')
 hold off
 legend('Cluster 1','Cluster 2','Location','NW')
 clrmap = jet(80); colormap(clrmap(9:72,:))
 ylabel(colorbar,'Component 1 Posterior Probability')
</code></pre>

<p>But the problem is that I am unable to plot the cluster centroids received from GMM in the primary binary image.How do i do this?
<a href=""https://i.stack.imgur.com/QB4h4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QB4h4.jpg"" alt=""enter image description here""></a></p>

<p>**Now suppose i have 10 such images in a sequence And i want to store the information of their mean position in two cell array then how do i do that.This is my code foe my new question ** </p>

<pre><code>    images=load('gait2go.mat');%load the matrix file
    for i=1:10

   I{i}=images.result{i};
  I{i}=im2double(I{i});

   %determine 'white' pixels, size of image can be [M N], [M N 3] or [M N 4]
  Idims=size(I{i});
  whites=true(Idims(1),Idims(2));

    df=I{i};
      %we add up the various color channels
 for colori=1:size(df,3)
  whites=whites &amp; df(:,:,colori)&gt;0.5;
 end

%choose indices of 'white' pixels as coordinates of data
[datax datay]=find(whites);

%cluster data into 10 clumps
  K = 10;               % number of mixtures/clusters
  cInd = kmeans([datax datay], K, 'EmptyAction','singleton',...
   'maxiter',1000,'start','cluster');

%get clusterwise means
 meanx=zeros(K,1);
 meany=zeros(K,1);  
  for i=1:K
   meanx(i)=mean(datax(cInd==i));
   meany(i)=mean(datay(cInd==i));

 end

 xc{i}=meanx(i);%cell array contaning the position of the mean for the 10    
 images
  xb{i}=meany(i);

figure;
gscatter(datay,-datax,cInd); %funky coordinates for plotting according to      
 image
 axis equal;
  hold on;
  scatter(meany,-meanx,20,'+'); %same funky coordinates


 end
</code></pre>

<p>I am able to get 10 images segmented but no the values of themean stored in the cell arrays xc and xb.They r only storing [] in place of the values of means</p>
";32181171;5067311;27675;Andras Deak;11;32181171;"<p>I decided to post an answer to your question (where your question was determined by a maximum-likelihood guess:P), but I wrote an extensive introduction. Please read carefully, as I think you have difficulties understanding the methods you want to use, and you have difficulties understanding why others can't help you with your usual approach of asking questions. There are several problems with your question, both code-related and conceptual. Let's start with the latter.</p>

<h2>The problem with the problem</h2>

<p>You say that you want to cluster your image with Gaussian mixture modelling. While I'm generally not familiar with clustering, after a look through your <a href=""http://in.mathworks.com/help/stats/gaussian-mixture-models.html"" rel=""nofollow noreferrer"">reference</a> and <a href=""https://stackoverflow.com/a/26070081/5067311"">the wonderful SO answer you cited elsewhere</a> (and a quick 101 from <a href=""https://stackoverflow.com/users/3250829/rayryeng"">@rayryeng</a>) I think you are on the wrong track altogether. </p>

<p>Gaussian mixture modelling, as its name suggests, models your data set with a mixture of Gaussian (i.e. normal) distributions. The reason for the popularity of this method is that when you do measurements of all sorts of quantities, in many cases you will find that your data is mostly distributed like a normal distribution (which is actually the reason why it's called <em>normal</em>). The reason behind this is the <a href=""https://en.wikipedia.org/wiki/Central_limit_theorem"" rel=""nofollow noreferrer"">central limit theorem</a>, which implies that the sum of reasonably independent random variables tends to be normal in many cases.</p>

<p>Now, <em>clustering</em>, on the other hand, simply means separating your data set into disjoint smaller bunches based on some criteria. The main criterion is usually (some kind of) distance, so you want to find ""close lumps of data"" in your larger data set. You usually need to cluster your data before performing a GMM, because it's already hard enough to find the Gaussians underlying your data without having to guess the clusters too. I'm not familiar enough with the procedures involved to tell how well GMM algorithms can work if you just let them work on your raw data (but I expect that many implementations start with a clustering step anyway).</p>

<p>To get closer to your question: I guess you want to do some kind of image recognition. Looking at the picture, you want to get more strongly correlated lumps. This is <em>clustering</em>. If you look at a picture of a zoo, you'll see, say, an elephant and a snake. Both have their distinct shapes, and they are well separated from one another. If you cluster your image (and the snake is not riding the elephant, <a href=""https://i.stack.imgur.com/I5zwe.jpg"" rel=""nofollow noreferrer"">neither did it eat it</a>), you'll find two lumps: one lump elephant-shaped, and one lump snake-shaped. Now, it wouldn't make sense to use GMM on these data sets: elephants, and especially snakes, are not shaped like multivariate Gaussian distributions. But you don't need this in the first place, if you just want to know where the distinct animals are located in your picture.</p>

<p>Still staying with the example, you should make sure that you cluster your data into an appropriate number of subsets. If you try to cluster your zoo picture into 3 clusters, you might get a second, spurious snake: the nose of the elephant. With an increasing number of clusters your partitioning might make less and less sense.</p>

<h2>Your approach</h2>

<p>Your code doesn't give you anything reasonable, and there's a very good reason for that: it doesn't make sense from the start. Look at the beginning:</p>

<pre><code>I=im2double(imread('sil10001.pbm'));
K = I(:);
mu=mean(K);
sigma=std(K);
X = mvnrnd(mu,sigma,1110);
X=reshape(X,111,10);
</code></pre>

<p>You read your binary image, convert it to double, then <strong>stretch it out into a vector and compute the mean and deviation of that vector</strong>. You basically smear your intire image into 2 values: an average intensity and a deviation. And THEN you generate <code>111*10</code> standard normal points with these parameters, and try to do GMM on the first two sets of 111. Which are both independently normal with the same parameter. So you probably get two overlapping Gaussians around the same mean with the same deviation.</p>

<p>I think the examples you found online confused you. When you do GMM, you already have your data, so no pseudo-normal numbers should be involved. But when people post examples, they also try to provide reproducible inputs (well, some of them do, <em>nudge nudge wink wink</em>). A simple method for this is to generate a union of simple Gaussians, which can then be fed into GMM.</p>

<p>So, my point is, that you don't have to generate random numbers, but have to use the image data itself as input to your procedure. <em>And</em> you probably just want to cluster your image, instead of actually using GMM to draw potatoes over your cluster, since you want to cluster body parts in an image about a human. Most body parts are <strong>not</strong> shaped like multivariate Gaussians (with a few distinct exceptions for men and women).</p>

<h2>What I think you should do</h2>

<p>If you really want to cluster your image, like in the figure you added to your question, then you should use a method like k-means. But then again, you already have a program that does that, don't you? So I don't really think I can answer the question saying ""How can I cluster my image with GMM?"". Instead, here's an answer to ""How can I cluster my image?"" with k-means, but at least there will be a piece of code here.</p>

<pre><code>%set infile to what your image file will be
infile='sil10001.pbm';

%read file
I=im2double(imread(infile));

%determine 'white' pixels, size of image can be [M N], [M N 3] or [M N 4]
Idims=size(I);
whites=true(Idims(1),Idims(2));

%we add up the various color channels
for colori=1:Idims(3)
    whites=whites &amp; I(:,:,colori)&gt;0.5;
end

%choose indices of 'white' pixels as coordinates of data
[datax datay]=find(whites);

%cluster data into 10 clumps
K = 10;               % number of mixtures/clusters
cInd = kmeans([datax datay], K, 'EmptyAction','singleton',...
    'maxiter',1000,'start','cluster');

%get clusterwise means
meanx=zeros(K,1);
meany=zeros(K,1);
for i=1:K
    meanx(i)=mean(datax(cInd==i));
    meany(i)=mean(datay(cInd==i));
end

figure;
gscatter(datay,-datax,cInd); %funky coordinates for plotting according to image
axis equal;
hold on;
scatter(meany,-meanx,20,'ko'); %same funky coordinates
</code></pre>

<p>Here's what this does. It first reads your image as double like yours did. Then it tries to determine ""white"" pixels by checking that each color channel (of which can be either 1, 3 or 4) is brighter than 0.5. Then your input data points to the clustering will be the <code>x</code> and <code>y</code> ""coordinates"" (i.e. indices) of your white pixels.</p>

<p>Next it does the clustering via <code>kmeans</code>. This part of the code is loosely based on <a href=""https://stackoverflow.com/a/26070081/5067311"">the already cited answer of Amro</a>. I had to set a large maximal number of iterations, as the problem is ill-posed in the sense that there aren't 10 clear clusters in the picture. Then we compute the <code>mean</code> for each cluster, and plot the clusters with <code>gscatter</code>, and the means with <code>scatter</code>. Note that in order to have the picture facing in the right directions in a <code>scatter</code> plot you have to shift around the input coordinates. Alternatively you could define <code>datax</code> and <code>datay</code> correspondingly at the beginning.</p>

<p>And here's my output, run with the already processed figure you provided in your question:
<a href=""https://i.stack.imgur.com/Hxe83.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Hxe83.png"" alt=""output""></a></p>
"
3371423;118;ninja;<matlab><machine-learning><classification><cluster-analysis><mixture-model>;28923865;1;Clustering an image using Gaussian mixture models;"<p><strong>I want to use GMM(Gaussian mixture models for clustering a binary image and also want to plot the cluster centroids on the binary image itself.</strong></p>

<p>I am using this as my reference:
  <a href=""http://in.mathworks.com/help/stats/gaussian-mixture-models.html"" rel=""nofollow noreferrer"">http://in.mathworks.com/help/stats/gaussian-mixture-models.html</a></p>

<p>This is my initial code</p>

<pre><code> I=im2double(imread('sil10001.pbm'));
  K = I(:);
  mu=mean(K);
  sigma=std(K);
  P=normpdf(K, mu, sigma);
   Z = norminv(P,mu,sigma);
  X = mvnrnd(mu,sigma,1110);
  X=reshape(X,111,10);


 scatter(X(:,1),X(:,2),10,'ko');

options = statset('Display','final');
gm = fitgmdist(X,2,'Options',options);



idx = cluster(gm,X);
cluster1 = (idx == 1);
cluster2 = (idx == 2);


 scatter(X(cluster1,1),X(cluster1,2),10,'r+');
 hold on

  scatter(X(cluster2,1),X(cluster2,2),10,'bo');
  hold off
  legend('Cluster 1','Cluster 2','Location','NW')


  P = posterior(gm,X);

 scatter(X(cluster1,1),X(cluster1,2),10,P(cluster1,1),'+')
 hold on
 scatter(X(cluster2,1),X(cluster2,2),10,P(cluster2,1),'o')
 hold off
 legend('Cluster 1','Cluster 2','Location','NW')
 clrmap = jet(80); colormap(clrmap(9:72,:))
 ylabel(colorbar,'Component 1 Posterior Probability')
</code></pre>

<p>But the problem is that I am unable to plot the cluster centroids received from GMM in the primary binary image.How do i do this?
<a href=""https://i.stack.imgur.com/QB4h4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QB4h4.jpg"" alt=""enter image description here""></a></p>

<p>**Now suppose i have 10 such images in a sequence And i want to store the information of their mean position in two cell array then how do i do that.This is my code foe my new question ** </p>

<pre><code>    images=load('gait2go.mat');%load the matrix file
    for i=1:10

   I{i}=images.result{i};
  I{i}=im2double(I{i});

   %determine 'white' pixels, size of image can be [M N], [M N 3] or [M N 4]
  Idims=size(I{i});
  whites=true(Idims(1),Idims(2));

    df=I{i};
      %we add up the various color channels
 for colori=1:size(df,3)
  whites=whites &amp; df(:,:,colori)&gt;0.5;
 end

%choose indices of 'white' pixels as coordinates of data
[datax datay]=find(whites);

%cluster data into 10 clumps
  K = 10;               % number of mixtures/clusters
  cInd = kmeans([datax datay], K, 'EmptyAction','singleton',...
   'maxiter',1000,'start','cluster');

%get clusterwise means
 meanx=zeros(K,1);
 meany=zeros(K,1);  
  for i=1:K
   meanx(i)=mean(datax(cInd==i));
   meany(i)=mean(datay(cInd==i));

 end

 xc{i}=meanx(i);%cell array contaning the position of the mean for the 10    
 images
  xb{i}=meany(i);

figure;
gscatter(datay,-datax,cInd); %funky coordinates for plotting according to      
 image
 axis equal;
  hold on;
  scatter(meany,-meanx,20,'+'); %same funky coordinates


 end
</code></pre>

<p>I am able to get 10 images segmented but no the values of themean stored in the cell arrays xc and xb.They r only storing [] in place of the values of means</p>
";32181171;3371423;118;ninja;0;32810155;"<p>The code can be made more simple:</p>

<pre><code>%read file

I=im2double(imread('sil10340.pbm'));
%choose indices of 'white' pixels as coordinates of data
[datax datay]=find(I);
%cluster data into 10 clumps
 K = 10;               % number of mixtures/clusters
[cInd, c] = kmeans([datax datay], K, 'EmptyAction','singleton',...
'maxiter',1000,'start','cluster');
 figure;
gscatter(datay,-datax,cInd); %funky coordinates for plotting according to    
image
axis equal;
hold on;
 scatter(c(:,2),-c(:,1),20,'ko'); %same funky coordinates
</code></pre>

<p>I don't think there is nay need for the looping as the c itself return a 10x2 double array which contains the position of the means</p>

<p><a href=""https://i.stack.imgur.com/2IndY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2IndY.png"" alt=""enter image description here""></a></p>
"
2480013;458;Stralo;<machine-learning><nlp><text-classification><text-analysis>;28924194;-3;Choosing machine learning algorithm for baseline;"<p>I'm working on a machine learning project aimed at predicting the quality/helpfulness of a review. For each review in the dataset, I have the review text, a number 'm' for the number of people who have voted on the review and a number 'n' for the number of positive votes on the review. </p>

<p>The goal is to predict the percentage of votes that are positive:</p>

<pre><code>n/m
</code></pre>

<p>I'm using a random forest for the main algorithm, and trying to decide on what would be a good algorithm to use for the baseline. </p>

<p>A feature vector for each review comprises a word presence representation of the review and a number representing the total number of words in the review. </p>

<p>I would appreciate any suggestions on what algorithm would be good for a baseline method to compare against my random forest implementation. </p>

<p>Thanks!</p>
";;4588780;4531;Nikita Astrakhantsev;1;28926750;"<p>Depends on why do you need this comparison.
If you are writing a research article, then you should read several papers (e.g. <a href=""http://www.aclweb.org/anthology/W06-16#page=445"" rel=""nofollow"">2006</a>, <a href=""http://www.researchgate.net/profile/Aijun_An/publication/224368502_HelpMeter_A_Nonlinear_Model_for_Predicting_the_Helpfulness_of_Online_Reviews/links/00b7d521240dad7066000000.pdf"" rel=""nofollow"">2008</a>, <a href=""http://home.gwu.edu/~wduan/Paper/DSS012011.pdf"" rel=""nofollow"">2011</a>) and find out the popular baseline; if no, just take the oldest approach. (However, in this case you should compare with state-of-the-art instead).</p>

<p>Another option is to compare with modifications of your own method: different feature spaces or different ML algorithms. For example, leave just one feature like length - btw, it is rather common, so maybe it is indeed the baseline you want.</p>
"
3723346;5568;plambre;<scala><apache-spark><machine-learning><k-means><apache-spark-mllib>;28929704;4;Spark MLlib / K-Means intuition;"<p>I'm very new to machine learning algorithms and Spark. I'm follow the
Twitter Streaming Language Classifier found here:</p>

<p><a href=""http://databricks.gitbooks.io/databricks-spark-reference-applications/content/twitter_classifier/README.html"" rel=""nofollow"">http://databricks.gitbooks.io/databricks-spark-reference-applications/content/twitter_classifier/README.html</a></p>

<p>Specifically this code:</p>

<p><a href=""http://databricks.gitbooks.io/databricks-spark-reference-applications/content/twitter_classifier/scala/src/main/scala/com/databricks/apps/twitter_classifier/ExamineAndTrain.scala"" rel=""nofollow"">http://databricks.gitbooks.io/databricks-spark-reference-applications/content/twitter_classifier/scala/src/main/scala/com/databricks/apps/twitter_classifier/ExamineAndTrain.scala</a></p>

<p>Except I'm trying to run it in batch mode on some tweets it pulls out
of Cassandra, in this case 200 total tweets.</p>

<p>As the example shows, I am using this object for ""vectorizing"" a set of tweets:</p>

<pre><code>object Utils{
  val numFeatures = 1000
  val tf = new HashingTF(numFeatures)

  /**
   * Create feature vectors by turning each tweet into bigrams of
   * characters (an n-gram model) and then hashing those to a
   * length-1000 feature vector that we can pass to MLlib.
   * This is a common way to decrease the number of features in a
   * model while still getting excellent accuracy (otherwise every
   * pair of Unicode characters would potentially be a feature).
   */
  def featurize(s: String): Vector = {
    tf.transform(s.sliding(2).toSeq)
  }
}
</code></pre>

<p>Here is my code which is modified from ExaminAndTrain.scala:</p>

<pre><code> val noSets = rawTweets.map(set =&gt; set.mkString(""\n""))

val vectors = noSets.map(Utils.featurize).cache()
vectors.count()

val numClusters = 5
val numIterations = 30

val model = KMeans.train(vectors, numClusters, numIterations)

  for (i &lt;- 0 until numClusters) {
    println(s""\nCLUSTER $i"")
    noSets.foreach {
        t =&gt; if (model.predict(Utils.featurize(t)) == 1) {
          println(t)
        }
      }
    }
</code></pre>

<p>This code runs and each Cluster prints ""Cluster 0"" ""Cluster 1"" etc
with nothing printing beneath. If i flip</p>

<pre><code>models.predict(Utils.featurize(t)) == 1 
</code></pre>

<p>to</p>

<pre><code>models.predict(Utils.featurize(t)) == 0
</code></pre>

<p>the same thing happens except every tweet is printed beneath every cluster.</p>

<p>Here is what I intuitively think is happening (please correct my
thinking if its wrong): This code turns each tweet into a vector,
randomly picks some clusters, then runs kmeans to group the tweets (at
a really high level, the clusters, i assume, would be common
""topics""). As such, when it checks each tweet to see if models.predict
== 1, different sets of tweets should appear under each cluster (and
because its checking the training set against itself, every tweet
should be in a cluster). Why isn't it doing this? Either my
understanding of what kmeans does is wrong, my training set is too
small or I'm missing a step.</p>

<p>Any help is greatly appreciated</p>
";;2169083;1079;uberwach;3;28930382;"<p>Well, first of all KMeans is a clustering algorithm and as such unsupervised. So there is no ""checking of the training set against itself"" (well okay you can do it manually ;).</p>

<p>Your understanding is quite good actually, just that you miss the point that model.predict(Utils.featurize(t)) gives you the cluster that t belongs as assigned by KMeans. I think you want to check </p>

<p><code>models.predict(Utils.featurize(t)) == i</code></p>

<p>in your code since i iterates through all cluster labels.</p>

<p>Also a small remark: The feature vector is created on a 2-gram model of <b>characters</b> of the tweets. This intermediate step is important ;)</p>

<p>2-gram (for words) means: ""A bear shouts at a bear"" => {(A, bear), (bear, shouts), (shouts, at), (at, a), (a bear)} i.e. ""a bear"" is counted twice. Chars would be (A,[space]), ([space], b), (b, e) and so on.</p>
"
4402708;717;Hezi Resheff;<python><azure><azure-machine-learning-studio>;28929813;0;Python support for Azure ML -- speed issue;"<p>We are trying to create an Azure ML web-service that will receive a (.csv) data file, do some processing, and return two similar files. The Python support recently added to the azure ML platform was very helpful and we were able to successfully port our code, run it in experiment mode and publish the web-service.</p>

<p>Using the ""batch processing"" API, we are now able to direct a file from blob-storage to the service and get the desired output. However, run-time for small files (a few KB) is significantly slower than on a local machine, and more importantly, the process seems to never return for slightly larger input data files (40MB). Processing time on my local machine for the same file is under 1 minute. </p>

<p>My question is if you can see anything we are doing wrong, or if there is a way to get this to speed up. Here is the DAG representation of the experiment:</p>

<p><img src=""https://i.stack.imgur.com/MalhQ.png"" alt=""The DAG representation of the experiment""></p>

<p>Is this the way the experiment should be set up? </p>
";;4402708;717;Hezi Resheff;1;29058978;"<p>It looks like the problem was with processing of a <em>timestamp</em> column in the input table. The successful workaround was to explicitly force the column to be processed as string values, using the ""Metadata Editor"" block. The final model now looks like this:</p>

<p><img src=""https://i.stack.imgur.com/deXws.png"" alt=""final model""></p>
"
4609181;397;huskywolf;<machine-learning><artificial-intelligence><reinforcement-learning><q-learning>;28937803;30;What is the difference between Q-learning and Value Iteration?;"<p>How is Q-learning different from value iteration in reinforcement learning? </p>

<p>I know Q-learning is model-free and training samples are transitions <code>(s, a, s', r)</code>. But since we know the transitions and the reward for every transition in Q-learning, is it not the same as model-based learning where we know the reward for a state and action pair, and the transitions for every action from a state (be it stochastic or deterministic)? I do not understand the difference.</p>
";28955191;1560599;5782;seaotternerd;46;28955191;"<p>You are 100% right that if we knew the transition probabilities and reward for every transition in Q-learning, it would be pretty unclear why we would use it instead of model-based learning or how it would even be fundamentally different. After all, transition probabilities and rewards are the two components of the model used in value iteration - if you have them, you have a model.</p>

<p>The key is that, <strong>in Q-learning, the agent does not know state transition probabilities or rewards</strong>. The agent only discovers that there is a reward for going from one state to another via a given action when it does so and receives a reward. Similarly, it only figures out what transitions are available from a given state by ending up in that state and looking at its options. If state transitions are stochastic, it learns the probability of transitioning between states by observing how frequently different transitions occur.</p>

<p>A possible source of confusion here is that you, as the programmer, might know exactly how rewards and state transitions are set up. In fact, when you're first designing a system, odds are that you do as this is pretty important to debugging and verifying that your approach works. But you never tell the agent any of this - instead you force it to learn on its own through trial and error. <strong>This is important if you want to create an agent that is capable of entering a new situation that you don't have any prior knowledge about and figuring out what to do.</strong> Alternately, if you don't care about the agent's ability to learn on its own, <strong>Q-learning might also be necessary if the state-space is too large to repeatedly enumerate.</strong> Having the agent explore without any starting knowledge can be more computationally tractable.</p>
"
4609181;397;huskywolf;<machine-learning><artificial-intelligence><reinforcement-learning><q-learning>;28937803;30;What is the difference between Q-learning and Value Iteration?;"<p>How is Q-learning different from value iteration in reinforcement learning? </p>

<p>I know Q-learning is model-free and training samples are transitions <code>(s, a, s', r)</code>. But since we know the transitions and the reward for every transition in Q-learning, is it not the same as model-based learning where we know the reward for a state and action pair, and the transitions for every action from a state (be it stochastic or deterministic)? I do not understand the difference.</p>
";28955191;562769;90654;Martin Thoma;11;38374964;"<p>Value iteration is used when you have transition probabilities, that means when you know the probability of getting from state x into state x' with action a. In contrast, you might have a black box which allows you to simulate it, but you're not actually given the probability. So you are model-free. This is when you apply Q learning.</p>

<p>Also what is learned is different. With value iteration, you learn the expected cost when you are given a state x. With q-learning, you get the expected discounted cost when you are in state x and apply action a.</p>

<p>Here are the algorithms:</p>

<p><img src=""https://martin-thoma.com/images/2016/07/Value-Iteration.png"" alt=""""></p>

<p><img src=""https://martin-thoma.com/images/2016/07/q-learning.png"" alt=""""></p>

<p>I'm currently writing down quite a bit about reinforcement learning for an exam. You might also be interested in <a href=""https://martin-thoma.com/probabilistische-planung/"" rel=""noreferrer"">my lecture notes</a>. However, they are mostly in German.</p>
"
4609181;397;huskywolf;<machine-learning><artificial-intelligence><reinforcement-learning><q-learning>;28937803;30;What is the difference between Q-learning and Value Iteration?;"<p>How is Q-learning different from value iteration in reinforcement learning? </p>

<p>I know Q-learning is model-free and training samples are transitions <code>(s, a, s', r)</code>. But since we know the transitions and the reward for every transition in Q-learning, is it not the same as model-based learning where we know the reward for a state and action pair, and the transitions for every action from a state (be it stochastic or deterministic)? I do not understand the difference.</p>
";28955191;6142988;31;ZhaoqunZhong;3;51218469;"<p>I don't think the accepted answer captured the essential of difference. To quote the newest version of Richard Sutton's book:</p>

<blockquote>
  <p>""
  <em>Having qâˆ— makes choosing optimal actions even easier. With qâˆ—, the agent does not
  even have to do a one-step-ahead search: for any state s, it can simply find any action that maximizes qâˆ—(s; a). The action-value function effectively caches the results of all one-step-ahead searches. It provides the optimal expected long-term return as a value that is locally and immediately available for each state{action pair. Hence, at the cost of representing a function of state{action pairs, instead of just of states, the optimal action value function allows optimal actions to be selected without having to know anything about possible successor states and their values, that is, without having to know anything
  about the environmentâ€™s dynamics.</em>
  ""</p>
</blockquote>

<p>Usually in real problems the agent doesn't know the world(or the so called transformation) dynamics but we definitely know the rewards, because those are what the environment gives back during the interaction and the reward function is actually defined by us. </p>

<p>The <strong>real difference between q-learning and normal value iteration is that</strong>: 
After you have V*, you still need to do one step action look-ahead to subsequent states to identify the optimal action for that state. And this look-ahead requires the transition dynamic after the action. But if you have q*, the optimal plan is just choosing <em>a</em> from the max <em>q(s,a)</em> pair.</p>
"
4649763;108;John Green;<matlab><machine-learning><gradient-descent>;28942491;1;Gradient descent in linear regression goes wrong;"<p>I actually want to use a linear model to fit a set of 'sin' data, but it turns out the loss function goes larger during each iteration. Is there any problem with my code below ? (gradient descent method)</p>

<p>Here is my code in Matlab</p>

<pre><code>m=20;
rate = 0.1;
x = linspace(0,2*pi,20);
x = [ones(1,length(x));x]
y = sin(x);
w = rand(1,2);
for i=1:500
    h = w*x;
    loss = sum((h-y).^2)/m/2 
    total_loss = [total_loss loss];
    **gradient = (h-y)*x'./m ;**
    w = w - rate.*gradient;
end
</code></pre>

<p>Here is the data I want to fit
<img src=""https://i.stack.imgur.com/st2hL.png"" alt=""y=sin(x)""></p>
";28945531;3250829;96436;rayryeng;1;28945531;"<p>There isn't a problem with your code.  With your current framework, if you can define data in the form of <code>y = m*x + b</code>, then this code is more than adequate.  I actually ran it through a few tests where I define an equation of the line and add some Gaussian random noise to it (amplitude = 0.1, mean = 0, std. dev = 1).  </p>

<p>However, one problem I will mention to you is that if you take a look at your sinusoidal data, you define a domain between <code>[0,2*pi]</code>.  As you can see, you have multiple <code>x</code> values that get mapped to the same <code>y</code> value but of different magnitude.  For example, at <code>x = pi/2</code> we get 1 but at <code>x = -3*pi/2</code> we get -1.  This high variability will not bode well with linear regression, and so one suggestion I have is to restrict your domain... so something like <code>[0, pi]</code>.  Another reason why it probably doesn't converge is the learning rate you chose is too high.  I'd set it to something low like <code>0.01</code>.  As you mentioned in your comments, you already figured that out!</p>

<p>However, if you want to fit non-linear data using linear regression, you're going to have to include higher order terms to account for the variability.  As such, try including second order and/or third order terms.  This can simply be done by modifying your <code>x</code> matrix like so:</p>

<pre><code>x = [ones(1,length(x)); x; x.^2; x.^3];
</code></pre>

<p>If you recall, the hypothesis function can be represented as a summation of linear terms:</p>

<pre><code>h(x) = theta0 + theta1*x1 + theta2*x2 + ... + thetan*xn
</code></pre>

<p>In our case, each <code>theta</code> term would build a higher order term of our polynomial.  <code>x2</code> would be <code>x^2</code> and <code>x3</code> would be <code>x^3</code>.  Therefore, we can still use the definition of gradient descent for linear regression here.</p>

<p>I'm also going to control the random generation seed (via <code>rng</code>) so that you can produce the same results I have gotten:</p>

<pre><code>clear all; 
close all;
rng(123123);
total_loss = [];
m = 20;
x = linspace(0,pi,m); %// Change
y = sin(x);
w = rand(1,4); %// Change
rate = 0.01; %// Change
x = [ones(1,length(x)); x; x.^2; x.^3]; %// Change - Second and third order terms
for i=1:500
    h = w*x;
    loss = sum((h-y).^2)/m/2;
    total_loss = [total_loss loss];
    % gradient is now in a different expression
    gradient = (h-y)*x'./m ; % sum all in each iteration, it's a batch gradient
    w = w - rate.*gradient;
end
</code></pre>

<p>If we try this, we get for <code>w</code> (your parameters):</p>

<pre><code>&gt;&gt; format long g;
&gt;&gt; w


w =

  Columns 1 through 3

         0.128369521905694         0.819533906064327       -0.0944622478526915

  Column 4

       -0.0596638117151464
</code></pre>

<p>My final loss after this point is:</p>

<pre><code>loss =

       0.00154350916582836
</code></pre>

<p>This means that our equation of the line is:</p>

<pre><code>y = 0.12 + 0.819x - 0.094x^2 - 0.059x^3
</code></pre>

<p>If we plot this equation of the line with your sinusoidal data, this is what we get:</p>

<pre><code>xval = x(2,:);
plot(xval, y, xval, polyval(fliplr(w), xval))
legend('Original', 'Fitted');
</code></pre>

<p><img src=""https://i.stack.imgur.com/rrQ8P.png"" alt=""enter image description here""></p>
"
242450;595;bmustata;<node.js><image><opencv><machine-learning><computer-vision>;28944397;0;Image comparison library for node.js;"<p>I'm looking for a basic image comparison library with node.js support or linux standard library. Support for comparing multi-resolutions images and slight color variation will be great. Any advice?</p>
";;4625124;619;Lovell Fuller;2;28944738;"<p>If you're looking for a simple ""distance"" between two images, the <a href=""https://www.npmjs.com/package/dhash-image"" rel=""nofollow"">dhash-image</a> module implements the <a href=""http://www.hackerfactor.com/blog/?/archives/529-Kind-of-Like-That.html"" rel=""nofollow"">dHash</a> algorithm.</p>

<p>It converts to greyscale (ignoring color) and handles images of different dimensions, so meets those needs quite nicely.</p>

<p>I've been using dHash for automated regression testing of an image processing library. It's very fast if you need this sort of relative accuracy, but I would expect the SIFT/SURF features of OpenCV to provide a greater absolute accuracy.</p>
"
242450;595;bmustata;<node.js><image><opencv><machine-learning><computer-vision>;28944397;0;Image comparison library for node.js;"<p>I'm looking for a basic image comparison library with node.js support or linux standard library. Support for comparing multi-resolutions images and slight color variation will be great. Any advice?</p>
";;2802149;901;Jan Bussieck;2;40688632;"<p>Hey RembrandtJS might be exactly what you're looking for.
It's a lightweight library we just released, that does a pixel-wise image comparison using drop-in Node.JS replacement node-canvas.
It accepts both blobs and urls as image sources so you could simply do this:</p>

<pre><code>import Rembrandt from 'rembrandt'

const rembrandt = new Rembrandt({
  // `imageA` and `imageB` can be either Strings (file path on node.js,
  // public url on Browsers) or Buffers
  imageA: '/path/to/imageA',
  imageB: fs.readFileSync('/path/to/imageB'),

  // Needs to be one of Rembrandt.THRESHOLD_PERCENT or Rembrandt.THRESHOLD_PIXELS
  thresholdType: Rembrandt.THRESHOLD_PERCENT,

  // The maximum threshold (0...1 for THRESHOLD_PERCENT, pixel count for THRESHOLD_PIXELS
  maxThreshold: 0.01,

  // Maximum color delta (0...255):
  maxDelta: 20,

  // Maximum surrounding pixel offset
  maxOffset: 0,

  renderComposition: true, // Should Rembrandt render a composition image?
  compositionMaskColor: Rembrandt.Color.RED // Color of unmatched pixels
})

// Run the comparison
rembrandt.compare()
  .then(function (result) {
    console.log('Passed:', result.passed)
    console.log('Difference:', (result.threshold * 100).toFixed(2), '%')
    console.log('Composition image buffer:', result.compositionImage)

    // Note that `compositionImage` is an Image when Rembrandt.js is run in the browser environment
  })
  .catch((e) =&gt; {
    console.error(e)
  })
</code></pre>

<p>As you can see Rembrandt also allows you to introduce threshold values that might offer some support for handling color variations.</p>
"
1680423;23;Siddharth Shah;<linux><authentication><machine-learning><pam><biometrics>;28949169;2;Pam config file conditional execution of statements;"<p>I am working on a project to make a pam module in linux to authenticate via typing behavior of an individual. I have adequately studied and have an idea about the working of the four control flags in the pam config file viz. requisite, required, sufficient and optional. I have the following 2 questions about the PAM config file specific to my project.</p>

<ol>
<li><p>As a second factor of authentication I will like to use google-authenticator.
Currently my config file has the following code:</p>

<pre><code>auth sufficient pam_test.so  
auth required pam_google_authenticator.so
</code></pre></li>
</ol>

<p>The google-authenticator module is invoked correctly if my module fails to authenticate when the password entered is correct but the typing behavior does not match. However, it is also invoked if the password entered itself is incorrect. For the second case I would like to terminate the entire chain. Is there a way to do so? Can I conditionally (based on different pam error status code) invoke the google-authenticator module?</p>

<ol start=""2"">
<li>The other question is that I would like to come back to my module from the google-authenticator module and make decisions based on the google-authenticator module's pam status code. Basically, I would like to use the newest training sample for future training (adaptive algorithm). Therefore, to differentiate between true negative and false negative I would like to come back from the google-authenticator module? Is this possible?</li>
</ol>

<p>This is my first question on stackoverflow. I am sorry if I have made any mistakes in asking the questions.</p>
";28971041;4210387;146;Zach;3;28971041;"<p>See <code>man 5 pam.d</code> for further details about these hijinx.  I'm assuming you're working on Linux, with Linux-PAM.  </p>

<p>I am assuming pam_test.so is something you've written.  As such, you should be returning PAM_AUTH_ERR when the password is invalid.  You can specify different actions based on the different return codes with an ""advanced"" syntax.
For reference, from the (my) man page for pam.d, the ""simple"" actions have the following ""advanced"" syntax:</p>

<pre><code>   required
       [success=ok new_authtok_reqd=ok ignore=ignore default=bad]
   requisite
       [success=ok new_authtok_reqd=ok ignore=ignore default=die]
   sufficient
       [success=done new_authtok_reqd=done default=ignore]
   optional
       [success=ok new_authtok_reqd=ok default=ignore]
</code></pre>

<p>These are of the form retval=action where retval is the PAM_* return code, with the PAM_ removed and converted to lower case (so PAM_SUCCESS becomes success).  The actions bad and die both give failure status, but die exits the stack.  The actions ok and done (and an integer N) indicate a successful status.  The 'done' also stops execution of the stack.  Using an integer N skips that many following modules. The special ""default"" means ""any other return value from the module"".  </p>

<p>You can then do:</p>

<pre><code>auth [success=done new_authtok_reqd=ok auth_err=die default=ignore] pam_test.so
</code></pre>

<p>This will stop execution of the auth stack with failure when the password is wrong, stop execution with success when the module passes, and move along the auth stack in any other scenario.</p>

<p>As for getting back to your code... that's a whole new can of worms.  You can do it, but I can't think of a way that's not a total hack.  For instance, jump through hoops with the control flow by using the code=N to jump around and call your module with special args that then succeed or fail based on the argument.  For completeness, (and a complete HACK) something like:</p>

<pre><code>auth [success=done new_authtok_reqd=ok auth_err=die default=ignore] pam_test.so
auth [success=ok default=1] pam_google_authenticator.so
auth [default=1] pam_test.so wasvalid
auth [default=bad] pam_test.so wasinvalid
</code></pre>

<p>This has the following properties: If the password is invalid or it satisfies the pam_test.so conditions, there is no further authentication processing and finishes with failure or success, respectively.  If pam_test failed for any other reason, it calls google authenticator.  If that succeeds, it moves on to the next line calling <code>pam_test.so wasvalid</code>, and otherwise, it goes to <code>pam_test.so wasinvalid</code>.  Only one of the two subsequent pam_test.so calls are made.  After this snippet, the return code is success or failure, depending on the google authenticator status.  This is effectively ""require pam_google_authenticator.so but also call my module with the appropriate flag"".  If you want auth to end with this no matter what, you can use:</p>

<pre><code>auth [success=done new_authtok_reqd=ok auth_err=die default=ignore] pam_test.so
auth [success=ok default=1] pam_google_authenticator.so
auth [default=done] pam_test.so wasvalid
auth [default=die] pam_test.so wasinvalid
</code></pre>
"
1576464;306;Roy;<machine-learning><neural-network><classification><conv-neural-network>;28953622;7;Do convolutional neural networks suffer from the vanishing gradient?;"<p>I think I read somewhere that convolutional neural networks do not suffer from the vanishing gradient problem as much as standard sigmoid neural networks with increasing number of layers. But I have not been able to find a 'why'.</p>

<p>Does it truly not suffer from the problem or am I wrong and it depends on the activation function?
[I have been using Rectified Linear Units, so I have never tested the Sigmoid Units for Convolutional Neural Networks]</p>
";28954201;3646384;557;Amin Suzani;5;28954201;"<p>Convolutional neural networks (like standard sigmoid neural networks) do suffer from the vanishing gradient problem. The most recommended approaches to overcome the vanishing gradient problem are:</p>

<ul>
<li>Layerwise pre-training</li>
<li>Choice of the activation function</li>
</ul>

<p>You may see that the state-of-the-art deep neural network for computer vision problem (like the <a href=""http://www.image-net.org/"" rel=""noreferrer"">ImageNet</a> winners) have used convolutional layers as the first few layers of the their network, but it is not the key for solving the vanishing gradient. The key is usually training the network greedily layer by layer. Using convolutional layers have several other important benefits of course. Especially in vision problems when the input size is large (the pixels of an image), using convolutional layers for the first layers are recommended because they have fewer parameters than fully-connected layers and you don't end up with billions of parameters for the first layer (which will make your network prone to overfitting). </p>

<p>However, it has been shown (like <a href=""http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf"" rel=""noreferrer"">this paper</a>) for several tasks that using Rectified linear units alleviates the problem of vanishing gradients (as oppose to conventional sigmoid functions). </p>
"
1576464;306;Roy;<machine-learning><neural-network><classification><conv-neural-network>;28953622;7;Do convolutional neural networks suffer from the vanishing gradient?;"<p>I think I read somewhere that convolutional neural networks do not suffer from the vanishing gradient problem as much as standard sigmoid neural networks with increasing number of layers. But I have not been able to find a 'why'.</p>

<p>Does it truly not suffer from the problem or am I wrong and it depends on the activation function?
[I have been using Rectified Linear Units, so I have never tested the Sigmoid Units for Convolutional Neural Networks]</p>
";28954201;3200890;816;dnth;0;34669289;"<p>Recent advances had alleviate the effects of vanishing gradients in deep neural networks. Among contributing advances include:</p>

<ol>
<li>Usage of GPU for training deep neural networks</li>
<li>Usage of better activation functions. (At this point rectified linear units (ReLU) seems to work the best.)</li>
</ol>

<p>With these advances, deep neural networks can be trained even without layerwise pretraining. </p>

<p>Source:
<a href=""http://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-history-training/"" rel=""nofollow"">http://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-history-training/</a></p>
"
1576464;306;Roy;<machine-learning><neural-network><classification><conv-neural-network>;28953622;7;Do convolutional neural networks suffer from the vanishing gradient?;"<p>I think I read somewhere that convolutional neural networks do not suffer from the vanishing gradient problem as much as standard sigmoid neural networks with increasing number of layers. But I have not been able to find a 'why'.</p>

<p>Does it truly not suffer from the problem or am I wrong and it depends on the activation function?
[I have been using Rectified Linear Units, so I have never tested the Sigmoid Units for Convolutional Neural Networks]</p>
";28954201;4668751;41;Faisal Shahbaz;0;60804383;"<p>we do not use Sigmoid and Tanh as Activation functions which causes vanishing Gradient Problems. Mostly nowadays we use RELU based activation functions in training a Deep Neural Network Model to avoid such complications and improve the accuracy.</p>

<p>Itâ€™s because the gradient or slope of RELU activation if itâ€™s over 0, is 1. Sigmoid derivative has a maximum slope of .25, which means that during the backward pass, you are multiplying gradients with values less than 1, and if you have more and more layers, you are multiplying it with values less than 1, making gradients smaller and smaller. RELU activation solves this by having a gradient slope of 1, so during backpropagation, there isnâ€™t gradients passed back that are progressively getting smaller and smaller. but instead they are staying the same, which is how RELU solves the vanishing gradient problem.</p>

<p>One thing to note about RELU however is that if you have a value less than 0, that neuron is dead, and the gradient passed back is 0, meaning that during backpropagation, you will have 0 gradient being passed back if you had a value less than 0.</p>

<p>An alternative is Leaky RELU, which gives some gradient for values less than 0.</p>
"
1576464;306;Roy;<machine-learning><neural-network><classification><conv-neural-network>;28953622;7;Do convolutional neural networks suffer from the vanishing gradient?;"<p>I think I read somewhere that convolutional neural networks do not suffer from the vanishing gradient problem as much as standard sigmoid neural networks with increasing number of layers. But I have not been able to find a 'why'.</p>

<p>Does it truly not suffer from the problem or am I wrong and it depends on the activation function?
[I have been using Rectified Linear Units, so I have never tested the Sigmoid Units for Convolutional Neural Networks]</p>
";28954201;5456268;41;J. Schneider;0;65296305;"<p>The first answer is from 2015 and a bit of age.</p>
<p>Today, CNNs typically also use batchnorm - while there is some debate why this helps: the inventors mention covariate shift: <a href=""https://arxiv.org/abs/1502.03167"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1502.03167</a>
There are other theories like smoothing the loss landscape: <a href=""https://arxiv.org/abs/1805.11604"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1805.11604</a></p>
<p>Either way, it is a method that helps to deal significantly with vanishing/exploding gradient problem that is also relevant for CNNs. In CNNs you also apply the chain rule to get gradients. That is the update of the first layer is proportional to the product of N numbers, where N is the number of inputs. It is very likely that this number is either relatively big or small compared to the update of the last layer. This might be seen by looking at the variance of a product of random variables that quickly grows the more variables are being multiplied: <a href=""https://stats.stackexchange.com/questions/52646/variance-of-product-of-multiple-random-variables"">https://stats.stackexchange.com/questions/52646/variance-of-product-of-multiple-random-variables</a></p>
<p>For recurrent networks that have long sequences of inputs, ie. of length L, the situation is often worse than for CNN, since there the product consists of L numbers. Often the sequence length L in a RNN is much larger than the number of layers N in a CNN.</p>
"
4114372;1915;john doe;<python><python-2.7><matplotlib><machine-learning><scikit-learn>;28954926;1;Problems plotting Receiver Operating Characteristic with scikit-learn?;"<p>I would like to plot the Receiver Operating Characteristic curve, so I do the following:</p>

<pre><code>from sklearn.metrics import roc_curve, auc
predictions = auto_wclf.predict_proba(X_test)
false_positive_rate, recall, thresholds = roc_curve(y_test, predictions[:, 1])
roc_auc = auc(false_positive_rate, recall)
plt.title('Receiver Operating Characteristic')
plt.plot(false_positive_rate, recall, 'b', label='AUC = %0.2f' % roc_auc)
plt.legend(loc='lower right')
plt.plot([0, 1], [0, 1], 'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.ylabel('Recall')
plt.xlabel('Fall-out')
plt.show()
</code></pre>

<p>But I get this exception:</p>

<pre><code>Traceback (most recent call last):
  File ""plot.py"", line 172, in &lt;module&gt;
    false_positive_rate, recall, thresholds = roc_curve(y_test, predictions[:, 1])
  File ""plot.py"", line 890, in roc_curve
    y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)
  File ""/usr/local/lib/python2.7/site-packages/sklearn/metrics/metrics.py"", line 710, in _binary_clf_curve
    raise ValueError(""Data is not binary and pos_label is not specified"")
ValueError: Data is not binary and pos_label is not specified
</code></pre>

<p>I have a multilabel classification problem (5 categories). Any idea of how to plot this?. Thanks in advance guys.</p>
";28961702;4588780;4531;Nikita Astrakhantsev;3;28961702;"<p>Yes, ROC curve ""is a graphical plot that illustrates the performance of a <strong>binary classifier system</strong> as its discrimination threshold is varied""(<a href=""https://en.wikipedia.org/wiki/Receiver_operating_characteristic"" rel=""nofollow"">wiki</a>). </p>

<p>Moreover, ""The extension of ROC curves for classification problems with more than two classes has always been cumbersome, as the degrees of freedom increase quadratically with the number of classes, and the ROC space has c(c-1) dimensions, where c is the number of classes.""(<a href=""https://en.wikipedia.org/wiki/Receiver_operating_characteristic#ROC_curves_beyond_binary_classification"" rel=""nofollow"">same wiki page</a>) Since you have 5 classes and even multilabel, ROC curve isn't suitable for you.</p>

<p>Use metrics like <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html"" rel=""nofollow"">Hamming loss</a>, <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"" rel=""nofollow"">F1-score</a>, <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score"" rel=""nofollow"">accuracy</a>, <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html"" rel=""nofollow"">precision</a>, <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score"" rel=""nofollow"">recall</a> instead - choose the most suitable one for your task.</p>
"
3105909;2651;jfalkson;<python><machine-learning><supervised-learning>;28956303;0;Store Decision Tree Code / Algorithm;"<p>I have an assignment for which I am planning on using machine learning (supervised learning in particular, such as a decision tree). The final code will be run on a Teaching Assistant's pc which doesn't have scikit learn or other external libraries. </p>

<p>As such, I need to either write something like a decision tree classifier from scratch, or use an external library locally, and store the final algorithm. </p>

<p>To summarize: When given a set of labelled training data, how do you store the final algorithm in python code, without relying on external libraries to run the final algorithm in the future? </p>

<p>For example, a decision tree can be broken down to a series of if/then statements, I want to generate those if/then statements and store them so that it can be run on a computer with nothing installed except python. </p>

<p>What is the best recommendation on how to achieve this. If this is in the wrong forum please advise. </p>
";;1863229;383077;Tim Biegeleisen;0;28956398;"<p>You can use the Python random forest package <code>sklearn.ensemble</code> as follows:</p>

<pre><code># Import the random forest package
from sklearn.ensemble import RandomForestClassifier 

# create a random forest object with 100 trees
forest = RandomForestClassifier(n_estimators = 100)

predictors = [[0, 0], [1, 1]]
response = [0, 1]

# fit the model to the training data
forest = forest.fit(predictors, response)

# you can reuse the forest model you built to make predictions
# on other data sets
test_data = [[0, 1], [1, 0]]
output = forest.predict(test_data)
</code></pre>

<p>Note that I imported <code>RandomForestClassifier</code> here, but you could have also used <code>RandomForestRegressor</code> in case you wanted run random forests in regression mode instead.</p>
"
4347611;108;Binny;<machine-learning><weka>;28958200;1;Is the result produced after SMOTE reliable?;"<p>I have a skewed dataset having twitter tweet and Sentiments associated with it.The ratio of positive:negative sentiment is around 1:4(training set). When i ran the training set on Weka(without SOMTE), the results were unsatisfactory. So i used SMOTE to balance the classes.The results i got after that was far better.
I used Libsvm for classification.</p>

<p>How reliable is the model generated by such SMOTE technique?
Can we always use SMOTE for such imbalanced dataset?
I am new to ML and weka, so dont know much about these things.</p>
";;968064;2363;Rushdi Shams;1;28991268;"<p>It depends. There are many pros and cons of oversampling and undersampling be it random or synthetic. The results should be checked by comparing the training and cross validation or test error. Also with the learning curves by plotting error rates at y axis and data size at x axis. This way overly optimistic results, generalization power, etc can be detected. Sometimes we can get good score just because of overfitting. I used SMOTE and have got good results. But then I had to check the processes I mentioned to see how good is that good. The other thing you might try for class imbalance problem is to either keep the dataset as it is and then apply a cost sensitive learner which will be punished for FP and FN based on some weights. Also you can apply a regular algorithm on the imbalanced dataset and then apply cost sensitive evaluation like cost curve. This curve can tell how your model would have performed had it been given a 50-50 balanced dataset. </p>
"
2567081;43;Gonzalo Esteban Maldonado Leiv;<java><machine-learning><classification><weka>;28970357;1;Options in GUI for custom Weka classifier;"<p>I've implemented my own custom classifier for Weka, which inherits from Classifier and implements Serializable and also OptionHandler.</p>

<p>At the moment i've been able to run the classifier from the Weka UI perfectly, but without parameters. I know that i need to implement the following methods in order to see the options in the UI:</p>

<pre><code>public void setOptions(String [] options) throws Exception
public String [] getOptions()
public Enumeration listOptions()
</code></pre>

<p>I've implemented getOptions with some dummy code and it's being perfectly called by the UI (I used a System.out.println to log this). And I also implemented a dummy code for listOptions but it's never being called. </p>

<p>When I try to select the options for my algorithm by the UI i only see the ""Debug"" option. Do you have any ideas how to solve this problem and see my options by the UI?</p>

<p>Basically the thing that i want to do is to run my algorithm from Weka but using custom options/parameters.</p>

<p>I've read this document but i think it's not very useful:</p>

<p><a href=""http://weka.wikispaces.com/Writing+your+own+Classifier+(post+3.5.2)"" rel=""nofollow"">http://weka.wikispaces.com/Writing+your+own+Classifier+(post+3.5.2)</a></p>

<p>Do you have any idea or example to solve this?</p>
";;5579093;1;igrau;0;33793492;"<p>I had the same problem.
I realized by chance :) that if I use the get and set methods instead of just accessing the attributes it works!</p>

<p>More clearly, for an attribute named ""threshold"" represented by ""T"" as an option:</p>

<p>This code doesn't work:</p>

<pre><code>public String[] getOptions() {

    String[] options = new String[3];
    int current = 0;
    options[current++] = ""-T"";
    options[current++] = """" + threshold;

    while (current &lt; options.length) {
        options[current++] = """";
    }
    return options;
}
</code></pre>

<p>And this code works:</p>

<pre><code>public String[] getOptions() {

    String[] options = new String[3];
    int current = 0;
    options[current++] = ""-T"";
    options[current++] = """" + getThreshold();

    while (current &lt; options.length) {
        options[current++] = """";
    }
    return options;
}
</code></pre>

<p>The same for setOptions(String[] options) method.</p>

<p>You only need to use the accessors.</p>
"
2890272;117;Jianxun Lian;<machine-learning>;28979373;-1;what is the best introduction to gaussion process regression?;"<p>i seached and found that there are quite a few documents introducing Gaussion Process Regression. I picked up one but it seems to be too brief and I still could not understand what is GPR. Anyone has experience and direct to one or some good tutorial?</p>
";28983518;2890272;117;Jianxun Lian;0;28983518;"<p>I am currently reading <a href=""http://www.gaussianprocess.org/gpml/chapters/RW2.pdf"" rel=""nofollow"">http://www.gaussianprocess.org/gpml/chapters/RW2.pdf</a>  which seems to be an excellent book introducing gaussian process.  If anyone knows better tutorials, welcome to put them here.</p>
"
4119228;111;cloudybunny;<algorithm><machine-learning><classification><supervised-learning>;28981835;0;What are supervised ML Classification algorithms?;"<p>What I found are:<br>
1. Naive Bayes classifier<br>
2. K nearest neighbors classifier<br>
3. Decision tree Algorithms(C4.5, Random Forest)<br>
4. Kernel Discriminant Analysis<br>
5. Support vector machines</p>

<p>If any other, can someone please help me with the remaining algorithms under this? I need complete list of supervised ML classification algorithms for my academic purpose. Thank you</p>
";28984515;270287;41194;IVlad;1;28984515;"<p>Although this is an active area of research, I wouldn't say new algorithms are invented every day, not good ones anyway. The invention of a new ML algorithm that is better than the rest in even some semi-important particular cases would be pretty big news.</p>

<p>Usually, known algorithms are adapted to a given problem. Adapting one properly can itself be an area of research (spam classification is done with classical ML algorithms, but it's not trivial to perfect, so is digit recognition etc.)</p>

<p>Regardless, it's hard to find a source that lists all the known, classical algorithms. There are a lot, and it's unlikely that an author somewhere lists them all. They usually list the ones they work with, or the ones they consider the most important.</p>

<p>That said, I'm going to try to give you a longer list, and I'm making this community wiki to encourage other people to add more.</p>

<ol>
<li>Naive Bayes classifier</li>
<li>K nearest neighbors classifier</li>
<li>Decision tree Algorithms(C4.5, Random Forest)</li>
<li>Kernel Discriminant Analysis</li>
<li>Support vector machines</li>
<li>Logistic Regression</li>
<li>Passive Aggressive Classifiers</li>
<li>Gaussian Processes</li>
<li>Neural networks</li>
<li>The Winnow algorithm</li>
</ol>
"
4514767;49;SaiLiu;<machine-learning><statistics><classification><random-forest><treemodel>;28983237;3;Does add feature certainly making the model better?;"<p>I have trained a gbdt model for predicting CTR, originally I use 40 features, and then I added some features, but results(auc) is lower than the original. 
1. how could that happen?
2. how to determinate which feature is good for the model?</p>
";;4642538;584;sray;2;28987232;"<p>If adding more features deteriorates performance, this is likely because of overfitting. Your model learning parameters need to be tuned to avoid overly complex(overfitted) models.</p>

<p>In case of random forests, the tree depth is one such parameter. Trees should not be allowed to grow too deep else they can overfit (this can happen in random forests even though there are lot of trees).</p>
"
4514767;49;SaiLiu;<machine-learning><statistics><classification><random-forest><treemodel>;28983237;3;Does add feature certainly making the model better?;"<p>I have trained a gbdt model for predicting CTR, originally I use 40 features, and then I added some features, but results(auc) is lower than the original. 
1. how could that happen?
2. how to determinate which feature is good for the model?</p>
";;1604534;870;miguelmalvarez;0;28991949;"<p>I agree that the most likely reason why adding more features produces worse results is overfitting, and that the main solution is feature selection. </p>

<p>Now, there are different techniques to verify and measure this intuition. One of the best tools is to produce the learning curves for the model given training and validation subsets. </p>

<p>A good example of this can be seen in <a href=""http://www.astroml.org/sklearn_tutorial/practical.html"" rel=""nofollow"">this</a> tutorials for the sklearn library (Python). Also, I strongly recommend you to have a look at the <a href=""https://class.coursera.org/ml-005/lecture/64"" rel=""nofollow"">lecture about Learning Curves</a> from the Machine Learning course by Andrew Ng in Coursera.</p>
"
3141472;363;rusty;<machine-learning><nlp><nltk><lda><text-classification>;28985265;0;Classification of single sentence;"<p>I have 4 different categories and I also have around 3000 words which belong to each of these categories. Now if a new sentence comes, I am able to break the sentence into words and get more words related to it. So say for each new sentence I can get 20-30 words generated from the sentence.
Now what is the best way to classify this sentence in above mentioned category? I know  bag of words works well.
I also looked at LDA, but it works with documents, where as I have a list of words as a training corpus. In LDA it looks at the position of word in document. So I could not get meaningful results from LDA.</p>
";;4337522;1181;Igor;0;28987180;"<p>I'm not sure if I fully understand what your question is exactly. 
Bag of words works well for some purposes, but in a lot of cases it throws away a lot of potentially useful information (which could be taken from word order, for example).
And assuming that you get a grammatical sentence as input, why not use your sentence as document and still use LDA? The position of a word in your sentence can still be verymeaningful.</p>

<p>There are plenty of classification methods available. Which one is best depens largely on your purpose. If you're neew to this area, this may be interesting to have a look at: <a href=""https://www.coursera.org/course/ml"" rel=""nofollow"">https://www.coursera.org/course/ml</a></p>
"
3141472;363;rusty;<machine-learning><nlp><nltk><lda><text-classification>;28985265;0;Classification of single sentence;"<p>I have 4 different categories and I also have around 3000 words which belong to each of these categories. Now if a new sentence comes, I am able to break the sentence into words and get more words related to it. So say for each new sentence I can get 20-30 words generated from the sentence.
Now what is the best way to classify this sentence in above mentioned category? I know  bag of words works well.
I also looked at LDA, but it works with documents, where as I have a list of words as a training corpus. In LDA it looks at the position of word in document. So I could not get meaningful results from LDA.</p>
";;1930402;1175;pnv;0;29070339;"<p>Like, Igor, I am also a bit confused regarding your problem. Be it a document or a sentence, the terms will be part of the feature set for categorization, in some form. You can find out the most relevant terms of each category and using this knowledge, do a better classification of the new sentences.  For example, if your sentence is as follows-"" There is a stray dog near our layout which bites everyone who goes near to it"". If you take the useful keywords from this sentence, removing stopwords, they are a few in number ( stray, dog, layout, bites, near ). You can categorize it into a bucket, ""animals_issue"".  If you train your system with a larger set of example, this bag of words model can help. Otherwise, you can go for LDA/ other topic modelling approaches.</p>
"
3787253;2595;smatthewenglish;<java><machine-learning>;28988732;1;correct implementation of Hinge loss minimization for gradient descent;"<p>I copied the hinge loss function from <a href=""https://code.google.com/p/java-statistical-analysis-tool/source/browse/trunk/JSAT/src/jsat/lossfunctions/HingeLoss.java?r=762"" rel=""nofollow"">here</a> (also LossC and LossFunc upon which it's based. Then I included it in my gradient descent algorithm like so: </p>

<pre><code>  do 
  {
    iteration++;
    error = 0.0;
    cost = 0.0;

    //loop through all instances (complete one epoch)
    for (p = 0; p &lt; number_of_files__train; p++) 
    {

      // 1. Calculate the hypothesis h = X * theta
      hypothesis = calculateHypothesis( theta, feature_matrix__train, p, globo_dict_size );

      // 2. Calculate the loss = h - y and maybe the squared cost (loss^2)/2m
      //cost = hypothesis - outputs__train[p];
      cost = HingeLoss.loss(hypothesis, outputs__train[p]);
      System.out.println( ""cost "" + cost );

      // 3. Calculate the gradient = X' * loss / m
      gradient = calculateGradent( theta, feature_matrix__train, p, globo_dict_size, cost, number_of_files__train);

      // 4. Update the parameters theta = theta - alpha * gradient
      for (int i = 0; i &lt; globo_dict_size; i++) 
      {
          theta[i] = theta[i] - LEARNING_RATE * gradient[i];
      }

    }

    //summation of squared error (error value for all instances)
    error += (cost*cost);       

  /* Root Mean Squared Error */
  //System.out.println(""Iteration "" + iteration + "" : RMSE = "" + Math.sqrt( error/number_of_files__train ) );
  System.out.println(""Iteration "" + iteration + "" : RMSE = "" + Math.sqrt( error/number_of_files__train ) );

  } 
  while( error != 0 );
</code></pre>

<p>But this doesnt work at all. Is that due to the loss function? Maybe how I added the loss function to my code? </p>

<p>I guess it's also possible that my implementation of gradient descent is faulty. </p>

<p>Here are my methods for calculating the gradient and the hypothesis, are these right?</p>

<pre><code>static double calculateHypothesis( double[] theta, double[][] feature_matrix, int file_index, int globo_dict_size )
{
    double hypothesis = 0.0;

     for (int i = 0; i &lt; globo_dict_size; i++) 
     {
         hypothesis += ( theta[i] * feature_matrix[file_index][i] );
     }
     //bias
     hypothesis += theta[ globo_dict_size ];

     return hypothesis;
}

static double[] calculateGradent( double theta[], double[][] feature_matrix, int file_index, int globo_dict_size, double cost, int number_of_files__train)
{
    double m = number_of_files__train;

    double[] gradient = new double[ globo_dict_size];//one for bias?

    for (int i = 0; i &lt; gradient.length; i++) 
    {
        gradient[i] = (1.0/m) * cost * feature_matrix[ file_index ][ i ] ;
    }

    return gradient;
}
</code></pre>

<p>The rest of the code is <a href=""https://github.com/h1395010/gradient_diss-1-ent_1-id"" rel=""nofollow"">here</a> if you're interested to take a look. </p>

<p>Below this sentence is what those loss functions look like. Should I use the <code>loss</code> or <code>deriv</code>, are these even correct?</p>

<pre><code>/**
 * Computes the HingeLoss loss
 *
 * @param pred the predicted value
 * @param y the target value
 * @return the HingeLoss loss
 */
public static double loss(double pred, double y)
{
    return Math.max(0, 1 - y * pred);
}

/**
 * Computes the first derivative of the HingeLoss loss
 *
 * @param pred the predicted value
 * @param y the target value
 * @return the first derivative of the HingeLoss loss
 */
public static double deriv(double pred, double y)
{
    if (pred * y &gt; 1)
        return 0;
    else
        return -y;
}
</code></pre>
";28999444;2658050;56586;lejlot;1;28999444;"<p>The code you provided for gradient does not look like a gradient of Hinge loss. Take a look at a valid equation, for example here:
<a href=""https://stats.stackexchange.com/questions/4608/gradient-of-hinge-loss"">https://stats.stackexchange.com/questions/4608/gradient-of-hinge-loss</a></p>
"
4658896;33;CShor;<algorithm><machine-learning><signal-processing><classification><svm>;28990656;2;Binary classification of sensor data;"<p>My problem is the following: I need to classify a data stream coming from an sensor. I have managed to get a baseline using the 
median of a window and I subtract the values from that baseline (I want to avoid negative peaks, so I only use the absolute value of the difference).</p>

<p>Now I need to distinguish an event (= something triggered the sensor) from the noise near the baseline:</p>

<p><img src=""https://i.stack.imgur.com/Swwq4.png"" alt=""enter image description here""></p>

<p>The problem is that I don't know which method to use. 
There are several approaches of which I thought of:</p>

<ul>
<li>sum up the values in a window, if the sum is above a threshold the class should be EVENT ('Integrate and dump')</li>
<li>sum up the differences of the values in a window and get the mean value (which gives something like the first derivative), if the value is positive and above a threshold set class EVENT, set class NO-EVENT otherwise</li>
<li>combination of both</li>
</ul>

<p>(unfortunately these approaches have the drawback that I need to guess the threshold values and set the window size)</p>

<ul>
<li>using SVM that learns from manually classified data (but I don't know how to set up this algorithm properly: which features should I look at, like median/mean of a window?, integral?, first derivative?...)</li>
</ul>

<p>What would you suggest? Are there better/simpler methods to get this task done?</p>

<p>I know there exist a lot of sophisticated algorithms but I'm confused about what could be the best way - please have a litte patience with a newbie who has no machine learning/DSP background :)</p>

<p>Thank you a lot and best regards. </p>
";28999625;4562911;1530;halfflat;1;28999625;"<p>The key to evaluating your heuristic is to develop a model of the behaviour of the system.</p>

<p>For example, what is the model of the physical process you are monitoring? Do you expect your samples, for example, to be correlated in time?</p>

<p>What is the model for the sensor output? Can it be modelled as, for example, a discretized linear function of the voltage? Is there a noise component? Is the magnitude of the noise known or unknown but constant?</p>

<p>Once you've listed your knowledge of the system that you're monitoring, you can then use that to evaluate and decide upon a good classification system. You may then also get an estimate of its accuracy, which is useful for consumers of the output of your classifier.</p>

<p><strong>Edit:</strong></p>

<p>Given the more detailed description, I'd suggest trying some simple models of behaviour that can be tackled using classical techniques before moving to a generic supervised learning heuristic.</p>

<p>For example, suppose:</p>

<ul>
<li><p>The baseline, event threshold and noise magnitude are all known a priori.</p></li>
<li><p>The underlying process can be modelled as a Markov chain: it has two states (off and on) and the transition times between them are exponentially distributed.</p></li>
</ul>

<p>You could then use a hidden Markov Model approach to determine the most likely underlying state at any given time. Even when the noise parameters and thresholds are unknown, you can use the HMM forward-backward training method to train the parameters (e.g. mean, variance of a Gaussian) associated with the output for each state.</p>

<p>If you know even more about the events, you can get by with simpler approaches: for example, if you knew that the event signal always reached a level above the baseline + noise, and that events were always separated in time by an interval larger than the width of the event itself, you could just do a simple threshold test.</p>

<p><strong>Edit:</strong></p>

<p>The classic intro to HMMs is <a href=""http://dx.doi.org/10.1109/5.18626"" rel=""nofollow"">Rabiner's tutorial</a> (a <a href=""http://www.cs.ubc.ca/~murphyk/Bayes/rabiner.pdf"" rel=""nofollow"">copy can be found here</a>). Relevant also are <a href=""http://xenia.media.mit.edu/~rahimi/rabiner/rabiner-errata/"" rel=""nofollow"">these errata</a>.</p>
"
4658896;33;CShor;<algorithm><machine-learning><signal-processing><classification><svm>;28990656;2;Binary classification of sensor data;"<p>My problem is the following: I need to classify a data stream coming from an sensor. I have managed to get a baseline using the 
median of a window and I subtract the values from that baseline (I want to avoid negative peaks, so I only use the absolute value of the difference).</p>

<p>Now I need to distinguish an event (= something triggered the sensor) from the noise near the baseline:</p>

<p><img src=""https://i.stack.imgur.com/Swwq4.png"" alt=""enter image description here""></p>

<p>The problem is that I don't know which method to use. 
There are several approaches of which I thought of:</p>

<ul>
<li>sum up the values in a window, if the sum is above a threshold the class should be EVENT ('Integrate and dump')</li>
<li>sum up the differences of the values in a window and get the mean value (which gives something like the first derivative), if the value is positive and above a threshold set class EVENT, set class NO-EVENT otherwise</li>
<li>combination of both</li>
</ul>

<p>(unfortunately these approaches have the drawback that I need to guess the threshold values and set the window size)</p>

<ul>
<li>using SVM that learns from manually classified data (but I don't know how to set up this algorithm properly: which features should I look at, like median/mean of a window?, integral?, first derivative?...)</li>
</ul>

<p>What would you suggest? Are there better/simpler methods to get this task done?</p>

<p>I know there exist a lot of sophisticated algorithms but I'm confused about what could be the best way - please have a litte patience with a newbie who has no machine learning/DSP background :)</p>

<p>Thank you a lot and best regards. </p>
";28999625;1858151;3363;stefan;1;29010055;"<p>from your description a correctly parameterized moving average might be sufficient</p>

<ul>
<li>Try to understand the Sensor and its output. Make a model and do a Simulator that provides mock-data that covers expected data with noise and all that stuff</li>
<li>Get lots of real sensor data recorded</li>
<li>visualize the data and verify your assuptions and model</li>
<li>annotate your sensor data i. e. generate ground truth (your simulator shall do that for the mock data)</li>
<li>from what you learned till now propose one or more algorithms</li>
<li>make a test system that can verify your algorithms against ground truth and do regression against previous runs</li>
<li>implement your proposed algorithms and run them against ground truth</li>
<li>try to understand the false positives and false negatives from the recorded data (and try to adapt your simulator to reproduce them)</li>
<li>adapt your algotithm(s)</li>
</ul>

<p>some other tips</p>

<ul>
<li>you may implement hysteresis on thresholds to avoid bouncing</li>
<li>you may implement delays to avoid bouncing</li>
<li>beware of delays if implementing debouncers or low pass filters</li>
<li>you may implement multiple algorithms and voting</li>
<li>for testing relative improvements you may do regression tests on large amounts data not annotated. then you check the flipping detections only to find performance increase/decrease</li>
</ul>
"
668215;604;Amit;<machine-learning><probability><cloudera><sampling><reservoir-sampling>;28993071;1;Test Case for Weighted Reservoir Sampling;"<p>I need to implement Weighted Reservoir Sampling. I have referred to the paper mentioned in this <a href=""http://blog.cloudera.com/blog/2013/04/hadoop-stratified-randosampling-algorithm/"" rel=""nofollow"">blog</a>. I want to write test-cases for unit testing my implementation and am confused as to how to calculate expected-probability of different element to be in reservoir. </p>

<p>I thought it should be propotional to <code>(weight_of_element/weight_of_all_elements)</code>, but the test-case mentioned <a href=""https://github.com/cloudera/ml/blob/master/parallel/src/test/java/com/cloudera/science/ml/parallel/sample/ReservoirSamplingTest.java"" rel=""nofollow"">here</a> computes it differently. How should I do it?</p>
";;5338270;336;Mike Koltsov;0;32712352;"<p>In order to write a test-case, you can indeed estimate the probability of the element being chosen. Suppose you've assigned weights like this:<br>
<strong>weights = [1, 5, 8, 2, 5]</strong>  </p>

<p>Now you are doing Weighted Reservoir Sampling in order to draw <strong>one</strong> element. What are the probabilities of element appearing in the result? They are exactly <code>(weight_of_element/weight_of_all_elements)</code>:<br>
<strong>prob = [0.048, 0.238, 0.381, 0.095, 0.238]</strong>  </p>

<p>In other words, if you draw one element repeatedly 10<sup>6</sup> times, you should have 0.381 * 10<sup>6</sup> instances of the third element, 0.048 *   10<sup>6</sup> instances of the first element, and so on. <strong>Approximately</strong>, of course.  </p>

<p>Therefore, you can look at the percentage of times the first element appeared out of 10<sup>6</sup> trials. This must be approximately <code>(weight_of_first_element/weight_of_all_elements)</code>. Compare those values and see if they are close to each other. </p>

<p>So the test-case may look like this (pseudo-code):  </p>

<pre><code>numTrials = 1000000
histogram = map&lt;int, int&gt;
for i = 1..numTrials:
  element = WeightedReservoir.sample(weights, 1) # draw one element
  histogram[element]++
for i = 1..len(weights):
  real_probability = weights[i] / sum(weights)
  observed_probability = histogram[elements[i]] / numTrials
  assert(abs(real_probability - observed_probability) &lt;= epsilon) # measuring absolute difference, but you can switch to relative difference
</code></pre>

<p>See <a href=""https://github.com/linkedin/datafu/issues/80#issuecomment-31486893"" rel=""nofollow"">this</a> thread for specific implementation in Java.  </p>

<p>As for Cloudera <a href=""https://github.com/cloudera/ml/blob/master/parallel/src/test/java/com/cloudera/science/ml/parallel/sample/ReservoirSamplingTest.java"" rel=""nofollow"">test</a> you've pointed out, it follows different logic (I've seen this in Python package <strong>numpy</strong> tests for numpy.random.choice as well):  </p>

<ol>
<li>The sampling routine is probabilistic in its nature, i.e. <strong>non-deterministic</strong>;</li>
<li>Let's take a fixed seed value for random number generation. Embed this value into test-case. Now it is fully deterministic: invoking the test-case several times yields <strong>the same result</strong>;</li>
<li>Since the result is deterministic, we can obtain it <strong>manually</strong> (for small inputs). Embed the expected result into test.  </li>
</ol>

<p>If you cannot control the seed value, this method is not for you.</p>
"
3443429;852;Siddarth;<python><machine-learning><scikit-learn><cluster-analysis><unsupervised-learning>;28994857;3;Document Clustering in python using SciKit;"<p>I recently started working on Document clustering using SciKit module in python. However I am having a hard time understanding the basics of document clustering. </p>

<p>What I know ?</p>

<ul>
<li>Document clustering is typically done using TF/IDF. Which essentially
converts the words in the documents to vector space model which is
then input to the algorithm.</li>
<li>There are many algorithms like k-means, neural networks, hierarchical
clustering to accomplish this.</li>
</ul>

<p>My Data :</p>

<ul>
<li>I am experimenting with linkedin data, each document would be the
linkedin profile summary, I would like to see if similar job
documents get clustered together.</li>
</ul>

<p>Current Challenges: </p>

<ul>
<li>My data has huge summary descriptions, which end up becoming 10000's
of words when I apply TF/IDF. Is there any proper way to handle this
high dimensional data.</li>
<li>K - means and other algorithms requires I specify the no. of clusters
( centroids ), in my case I do not know the number of clusters
upfront. This I believe is a completely unsupervised learning. Are
there algorithms which can determine the no. of clusters themselves?</li>
<li>I've never worked with document clustering before, if you are aware
of tutorials , textbooks or articles which address this issue, please
feel free to suggest.</li>
</ul>

<p>I went through the code on SciKit webpage, it consists of too many technical words which I donot understand, if you guys have any code with good explanation or comments please share. Thanks in advance. </p>
";;856804;5515;zyxue;0;28997147;"<ol>
<li>For the large matrix after TF/IDF transformation, consider using sparse matrix.</li>
<li>You could try different k values. I am not an expert in unsupervised clustering algorithms, but I bet with such algorithms and different parameters, you could also end up with a varied number of clusters. </li>
</ol>
"
3443429;852;Siddarth;<python><machine-learning><scikit-learn><cluster-analysis><unsupervised-learning>;28994857;3;Document Clustering in python using SciKit;"<p>I recently started working on Document clustering using SciKit module in python. However I am having a hard time understanding the basics of document clustering. </p>

<p>What I know ?</p>

<ul>
<li>Document clustering is typically done using TF/IDF. Which essentially
converts the words in the documents to vector space model which is
then input to the algorithm.</li>
<li>There are many algorithms like k-means, neural networks, hierarchical
clustering to accomplish this.</li>
</ul>

<p>My Data :</p>

<ul>
<li>I am experimenting with linkedin data, each document would be the
linkedin profile summary, I would like to see if similar job
documents get clustered together.</li>
</ul>

<p>Current Challenges: </p>

<ul>
<li>My data has huge summary descriptions, which end up becoming 10000's
of words when I apply TF/IDF. Is there any proper way to handle this
high dimensional data.</li>
<li>K - means and other algorithms requires I specify the no. of clusters
( centroids ), in my case I do not know the number of clusters
upfront. This I believe is a completely unsupervised learning. Are
there algorithms which can determine the no. of clusters themselves?</li>
<li>I've never worked with document clustering before, if you are aware
of tutorials , textbooks or articles which address this issue, please
feel free to suggest.</li>
</ul>

<p>I went through the code on SciKit webpage, it consists of too many technical words which I donot understand, if you guys have any code with good explanation or comments please share. Thanks in advance. </p>
";;270287;41194;IVlad;8;28997633;"<blockquote>
  <p>My data has huge summary descriptions, which end up becoming 10000's of words when I apply TF/IDF. Is there any proper way to handle this high dimensional data.</p>
</blockquote>

<p>My first suggestion is that you don't unless you absolutely have to, due to memory or execution time problems.</p>

<p>If you must handle it, you should use dimensionality reduction (<a href=""http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"" rel=""nofollow noreferrer"">PCA</a> for example) or <a href=""http://scikit-learn.org/stable/modules/feature_selection.html"" rel=""nofollow noreferrer"">feature selection</a> (probably better in your case, see <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html"" rel=""nofollow noreferrer"">chi2</a> for example)</p>

<blockquote>
  <p>K - means and other algorithms requires I specify the no. of clusters ( centroids ), in my case I do not know the number of clusters upfront. This I believe is a completely unsupervised learning. Are there algorithms which can determine the no. of clusters themselves?</p>
</blockquote>

<p>If you look at <a href=""http://scikit-learn.org/stable/modules/clustering.html#dbscan"" rel=""nofollow noreferrer"">the clustering algorithms available in scikit-learn</a>, you'll see that not all of them require that you specify the number of clusters.</p>

<p>Another one that does not is hierarchical clustering, <a href=""http://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html"" rel=""nofollow noreferrer"">implemented in scipy</a>. Also see <a href=""https://stackoverflow.com/questions/10136470/unsupervised-clustering-with-unknown-number-of-clusters"">this answer</a>.</p>

<p>I would also suggest that you use KMeans and try to manually tweak the number of clusters until you are satisfied with the results.</p>

<blockquote>
  <p>I've never worked with document clustering before, if you are aware of tutorials , textbooks or articles which address this issue, please feel free to suggest.</p>
</blockquote>

<p>Scikit has a lot of tutorials for working with text data, just use the ""text data"" search query on their site. One is for KMeans, others are for supervised learning, but I suggest you go over those too to get more familiar with the library. From a coding, style and syntax POV, unsupervised and supervised learning are pretty similar in scikit-learn, in my opinion.</p>

<blockquote>
  <p>Document clustering is typically done using TF/IDF. Which essentially converts the words in the documents to vector space model which is then input to the algorithm.</p>
</blockquote>

<p>Minor correction here: TF-IDF has nothing to do with clustering. It is simply a method for turning text data into numerical data. It does not care what you do with that data (clustering, classification, regression, search engine things etc.) afterwards.</p>

<p>I understand the message you were trying to get across, but it is incorrect to say that ""clustering is done using TF-IDF"". It's done using a clustering algorithm, TF-IDF only plays a preprocessing role in document clustering.</p>
"
3443429;852;Siddarth;<python><machine-learning><scikit-learn><cluster-analysis><unsupervised-learning>;28994857;3;Document Clustering in python using SciKit;"<p>I recently started working on Document clustering using SciKit module in python. However I am having a hard time understanding the basics of document clustering. </p>

<p>What I know ?</p>

<ul>
<li>Document clustering is typically done using TF/IDF. Which essentially
converts the words in the documents to vector space model which is
then input to the algorithm.</li>
<li>There are many algorithms like k-means, neural networks, hierarchical
clustering to accomplish this.</li>
</ul>

<p>My Data :</p>

<ul>
<li>I am experimenting with linkedin data, each document would be the
linkedin profile summary, I would like to see if similar job
documents get clustered together.</li>
</ul>

<p>Current Challenges: </p>

<ul>
<li>My data has huge summary descriptions, which end up becoming 10000's
of words when I apply TF/IDF. Is there any proper way to handle this
high dimensional data.</li>
<li>K - means and other algorithms requires I specify the no. of clusters
( centroids ), in my case I do not know the number of clusters
upfront. This I believe is a completely unsupervised learning. Are
there algorithms which can determine the no. of clusters themselves?</li>
<li>I've never worked with document clustering before, if you are aware
of tutorials , textbooks or articles which address this issue, please
feel free to suggest.</li>
</ul>

<p>I went through the code on SciKit webpage, it consists of too many technical words which I donot understand, if you guys have any code with good explanation or comments please share. Thanks in advance. </p>
";;1645853;8309;Kathirmani Sukumar;0;36760078;"<p>This link might be useful. It provides good amount of explanation for <code>k-means clustering</code> with a visual output <a href=""http://brandonrose.org/clustering"" rel=""nofollow"">http://brandonrose.org/clustering</a></p>
"
2171160;193;Saket;<azure><single-sign-on><office365><microsoft-account><azure-machine-learning-studio>;29000199;1;Can an organizational account (office 365) be used for live/Microsoft services?;"<p>I understand that Office 365 is on separate domain and live id (Microsoft account) is used for consumer applications.</p>

<p>But can an Office 365 account get live/Microsoft services?</p>

<p>The issue is we trying to SSO Office 365 applications and Azure ML (used with Microsoft account) but as the domains are different I am unable to find any proper help or process on the web.</p>

<p>We can create a live account with our company domain but can we create a federation on Live account ? For e.g. on Office 365 we created a @.com federation and were able to SSO it, how can we do the same with a live account ?</p>
";;310446;11142;BenV;2;29000425;"<p>According to the Azure ML <a href=""http://azure.microsoft.com/en-us/pricing/details/machine-learning/"" rel=""nofollow"">pricing page</a> the free tier is standalone, requiring a Live ID.  The Standard tier is associated with your Azure subscription, so you use your org IDs.</p>
"
4431006;111;Edu;<r><machine-learning><statistics><gradient-descent>;29000917;1;Boosted trees and Variable Interactions in R;"<p>How can one see in a Boosted trees classification model for machine learning (adaboost), which variables interact with each other and how much? I would like to make use of this in R gbm package if possible.</p>
";;2145138;4723;MTT;1;29017518;"<p>To extract the interaction between input variables, you can use any package like lm. <a href=""http://www.r-bloggers.com/r-tutorial-series-regression-with-interaction-variables/"" rel=""nofollow"">http://www.r-bloggers.com/r-tutorial-series-regression-with-interaction-variables/</a></p>
"
4431006;111;Edu;<r><machine-learning><statistics><gradient-descent>;29000917;1;Boosted trees and Variable Interactions in R;"<p>How can one see in a Boosted trees classification model for machine learning (adaboost), which variables interact with each other and how much? I would like to make use of this in R gbm package if possible.</p>
";;786220;977;patr1ckm;1;30491011;"<p>You can use ?interact.gbm. See also <a href=""https://stats.stackexchange.com/a/141391/4907"">this</a> cross-validated question, which directs to <a href=""http://cran.r-project.org/web/packages/dismo/vignettes/brt.pdf"" rel=""nofollow noreferrer"">a vignette</a> of a related technique from the package <em>dismo</em>. </p>

<p>In general, these interactions may not necessarily agree with the interaction terms estimated in a linear model.</p>
"
3391077;245;Nikhitha Reddy;<machine-learning><glm><logistic-regression>;29008047;1;Difference between Generalized linear modelling and regular logistic regression;"<p>I am trying to perform logistic regression for my data. I came to know about glm. What is the actual difference between glm and regular logistic regression?
What are the pros and cons of it?</p>
";29017384;1190430;5186;Artem Sobolev;2;29009676;"<p>Logistic Regression is <a href=""http://en.wikipedia.org/wiki/Generalized_linear_model#Binomial_data"" rel=""nofollow"">a special case</a> of Generalized Linear Models. GLMs is a class of models, parametrized by a <em>link function</em>. If you choose logit link function, you'll get Logistic Regression.</p>
"
3391077;245;Nikhitha Reddy;<machine-learning><glm><logistic-regression>;29008047;1;Difference between Generalized linear modelling and regular logistic regression;"<p>I am trying to perform logistic regression for my data. I came to know about glm. What is the actual difference between glm and regular logistic regression?
What are the pros and cons of it?</p>
";29017384;2145138;4723;MTT;2;29017384;"<p>The main benefit of GLM over logistic regression is overfitting avoidance. GLM usually try to extract linearity between input variables and then avoid overfitting of your model. Overfitting means very good performance on training data and poor performance on test data. </p>
"
4303173;63;Physman;<matlab><machine-learning><data-mining><glm><logistic-regression>;29014170;6;MATLAB's glmfit vs fitglm;"<p>I'm trying to perform logistic regression to do classification using MATLAB. There seem to be two different methods in MATLAB's statistics toolbox to build a generalized linear model 'glmfit' and 'fitglm'. I can't figure out what the difference is between the two. Is one preferable over the other?</p>

<p>Here are the links for the function descriptions.</p>

<p><a href=""http://uk.mathworks.com/help/stats/glmfit.html"" rel=""noreferrer"">http://uk.mathworks.com/help/stats/glmfit.html</a>
<a href=""http://uk.mathworks.com/help/stats/fitglm.html"" rel=""noreferrer"">http://uk.mathworks.com/help/stats/fitglm.html</a></p>
";29014332;1011724;43433;Dan;7;29014332;"<p>The difference is what the functions output. <code>glmfit</code> just outputs <strong>a vector</strong> of the regression coefficients (and some other stuff if you ask for it). <code>fitglm</code> outputs a regression <strong>object</strong> that packs all sorts of information and functionality inside (See the docs on <a href=""http://www.mathworks.com/help/stats/generalizedlinearmodel-class.html"" rel=""noreferrer"">GeneralizedLinearModel class</a>). I would assume the <code>fitglm</code> is intended to replace <code>glmfit</code>.</p>
"
4303173;63;Physman;<matlab><machine-learning><data-mining><glm><logistic-regression>;29014170;6;MATLAB's glmfit vs fitglm;"<p>I'm trying to perform logistic regression to do classification using MATLAB. There seem to be two different methods in MATLAB's statistics toolbox to build a generalized linear model 'glmfit' and 'fitglm'. I can't figure out what the difference is between the two. Is one preferable over the other?</p>

<p>Here are the links for the function descriptions.</p>

<p><a href=""http://uk.mathworks.com/help/stats/glmfit.html"" rel=""noreferrer"">http://uk.mathworks.com/help/stats/glmfit.html</a>
<a href=""http://uk.mathworks.com/help/stats/fitglm.html"" rel=""noreferrer"">http://uk.mathworks.com/help/stats/fitglm.html</a></p>
";29014332;2109289;13143;Alex;4;33360214;"<p>In addition to Dan's answer, I would like to add the following.</p>

<p>The function <code>fitglm</code>, like newer functions from the statistics toolbox, accepts more flexible inputs than <code>glmfit</code>. For example, you can use a table as the data source, specifyy a formula of the form <code>Y ~ X1 + X2 + ...</code>, and use categorical variables. </p>

<p>As a side note, the function <code>lassoglm</code> uses (depends on) <code>glmfit</code>.</p>
"
4450744;45;lodeg;<security><machine-learning><cryptography><unsupervised-learning>;29019626;0;Is possible to distinguish strings encrypted with different cryptography algorithms that are in the same set?;"<p>Is possible to distinguish strings encrypted with different cryptography algorithms? </p>

<p>If i have a set of N encrypted strings that comes from different cryptography algorithms (i.e. 100 from AES, 150 from tripleDES, etc... ) i want to know if is possible with a reasonable error that there is a sort of clustering of the strings (i.e. 111 in the AES-cluster, 139 in tripleDES-cluster) also with the simplification that the keys or the strings that are encrypted are the same and obviously without an a priori knowledge (even if there is a training could be interesting). </p>

<p>There are some works, papers, toy-example about that?</p>

<p>Thank you</p>
";29021516;108602;12943;mfanto;1;29019653;"<p>No, there's no way to distinguish one from another without there being some serious flaw in the algorithm. See <a href=""https://en.wikipedia.org/wiki/Ciphertext_indistinguishability"" rel=""nofollow"">here</a> and <a href=""https://crypto.stackexchange.com/questions/1646/is-it-possible-to-distinguish-a-securely-encrypted-ciphertext-from-random-noise"">here</a> for a more detailed explanation. </p>
"
4450744;45;lodeg;<security><machine-learning><cryptography><unsupervised-learning>;29019626;0;Is possible to distinguish strings encrypted with different cryptography algorithms that are in the same set?;"<p>Is possible to distinguish strings encrypted with different cryptography algorithms? </p>

<p>If i have a set of N encrypted strings that comes from different cryptography algorithms (i.e. 100 from AES, 150 from tripleDES, etc... ) i want to know if is possible with a reasonable error that there is a sort of clustering of the strings (i.e. 111 in the AES-cluster, 139 in tripleDES-cluster) also with the simplification that the keys or the strings that are encrypted are the same and obviously without an a priori knowledge (even if there is a training could be interesting). </p>

<p>There are some works, papers, toy-example about that?</p>

<p>Thank you</p>
";29021516;1816580;58134;Artjom B.;1;29021516;"<p>Yes, you can distinguish some ciphers based on their ciphertexts, but this doesn't work for all modes of operation.</p>

<p>The key observation is that AES and Triple DES have different block sizes of 128 bit and 64 bit. Which means that a 7 byte message will be 8 bytes long in 3DES and 16 bytes long in AES. But padding does also have a role in this. PKCS#5 padding will add a whole block of padding if the plaintext size is a multiple of the block size. This means that an 8 byte message will be 16 byte long for 3DES and 16 byte long for AES.</p>

<p>For example: if the lengths of the plaintext messages are distributed uniformly, then there is a 50% chance that you can distinguish between the two, because 3DES can have 24 byte ciphertexts, but AES cannot. Or said differently, you can find out if it is 3DES in 50% of the time, but you cannot say for sure if AES was used. This zero padding the probability is the same, but the matching lengths are slightly different.</p>

<p>This holds true for ECB, CBC and some others. In CTR mode on the other hand the length of the ciphertext cannot be used, because the ciphertext has always the same length as the plaintext. CTR mode is essentially a stream cipher.</p>

<p>If the block sizes are not different, then there is no way to distinguish them, because modern ciphers are designed in a way to be indistinguishable from noise.</p>
"
2757902;463;user2757902;<algorithm><machine-learning><nlp><nltk><knn>;29019794;1;Similarity for arrays of parts of speech;"<p>K-nearest neighbor and natural language processing: How do you test the distance between arrays of parts of speech? eg</p>

<p>('verb','adverb','noun')  and ('adjective','adverb','pronoun')?</p>

<p>A better phrased question would be how do you tell the similarity between the two in the context that they are parts of speech and not just strings?</p>
";;3450064;8034;CentAu;2;29038757;"<p>As a general approach, you can use the <a href=""http://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow"">cosine</a> between POS vectors as a measure of their similarity. Alternative approach would be using the <a href=""http://en.wikipedia.org/wiki/Hamming_distance"" rel=""nofollow"">hamming distance</a> between the two vectors. </p>

<p>There are plenty of other distance functions between vectors. But it really depends on what you want to do and what does your data look like. You should answer questions like does the position matter? How much similarity would you give to these vectors? ('noun', 'verb') and ('verb', 'noun')? Is the distance between ('adverb') and ('adjective') less than distance between ('adverb') and ('noun')? and so on.</p>
"
1294529;853;JLTChiu;<java><machine-learning><text-mining><mallet>;29023108;2;Why MALLET LDA need to keep-sequence?;"<p>In the MALLET documentation, it requires --keep-sequence tag for Topic model training (Detail is at : <a href=""http://mallet.cs.umass.edu/topics.php"" rel=""nofollow"">http://mallet.cs.umass.edu/topics.php</a>)</p>

<p>However, in my knowledge, regular LDA modeling use documents as bag of words, since including bigram will increase the feature space by a lot. I wonder why MALLET requires keep-sequence in LDA training, and how did MALLET actually use that sequential information?</p>

<p>Thank you for reading this post.</p>
";;3702814;15;Dhawal Joharapurkar;1;33340598;"<p>It doesn't ""need"" to keep sequence.</p>

<p>That option is merely so that the words per topic when you do ""--output-topic-keys"" are in the same sequence as they appear in the notes.</p>

<p>It is also useful when you are looking to find phrases in topic models (<a href=""http://www.mimno.org/articles/phrases/"" rel=""nofollow"">http://www.mimno.org/articles/phrases/</a>)</p>
"
2031156;1591;Olivier;<python><machine-learning><scikit-learn><svm>;29025334;2;Predicting with chi squared kernel for multilabel using sklearn;"<p>I'm trying to get predictions for an SVM using a precomputed chi-squared kernel. However, I am getting issues when trying to run clf.predict().</p>

<pre><code>min_max_scaler = preprocessing.MinMaxScaler()
X_train_scaled = min_max_scaler.fit_transform(features_train)
X_test_scaled = min_max_scaler.transform(features_test)

K = chi2_kernel(X_train_scaled)
svm = SVC(kernel='precomputed', cache_size=1000).fit(K, labels_train)
y_pred_chi2 = svm.predict(X_test_scaled)
</code></pre>

<p>The error I am getting is the following:</p>

<pre><code>ValueError: bad input shape (4627L, 20L)
</code></pre>

<p>I am guessing this issue is because of the multi-label, so I trained the classifier for only 1 category by doing the following:</p>

<pre><code>svm = SVC(kernel='precomputed', cache_size=1000).fit(K, labels_train[:, 0])
</code></pre>

<p>However, when trying to run clf.predict(X_test_scaled), I get the error:</p>

<pre><code>ValueError: X.shape[1] = 44604 should be equal to 4627, the number of samples at training time
</code></pre>

<p>Why does the test samples have to be the same number as the training samples?</p>

<p>Here is the shape of the relevant matrices (the features have 44604 dimensions and there are 20 categories):</p>

<pre><code>X_train_scaled.shape    : (4627L, 44604L)
X_test_scaled.shape     : (4637L, 44604L)
K.shape                 : (4627L, 4627L)
labels_train.shape      : (4627L, 20L)
</code></pre>

<p>On a side note, is it normal that there is L next to the shape sizes of these matrices?</p>
";29081702;2031156;1591;Olivier;0;29025664;"<p>The input to clf.predict() must also be passed to the chi2_kernel function.</p>

<pre><code>K_test = chi2_kernel(X_test_scaled)
y_pred = svm.predict(K_test)
</code></pre>
"
2031156;1591;Olivier;<python><machine-learning><scikit-learn><svm>;29025334;2;Predicting with chi squared kernel for multilabel using sklearn;"<p>I'm trying to get predictions for an SVM using a precomputed chi-squared kernel. However, I am getting issues when trying to run clf.predict().</p>

<pre><code>min_max_scaler = preprocessing.MinMaxScaler()
X_train_scaled = min_max_scaler.fit_transform(features_train)
X_test_scaled = min_max_scaler.transform(features_test)

K = chi2_kernel(X_train_scaled)
svm = SVC(kernel='precomputed', cache_size=1000).fit(K, labels_train)
y_pred_chi2 = svm.predict(X_test_scaled)
</code></pre>

<p>The error I am getting is the following:</p>

<pre><code>ValueError: bad input shape (4627L, 20L)
</code></pre>

<p>I am guessing this issue is because of the multi-label, so I trained the classifier for only 1 category by doing the following:</p>

<pre><code>svm = SVC(kernel='precomputed', cache_size=1000).fit(K, labels_train[:, 0])
</code></pre>

<p>However, when trying to run clf.predict(X_test_scaled), I get the error:</p>

<pre><code>ValueError: X.shape[1] = 44604 should be equal to 4627, the number of samples at training time
</code></pre>

<p>Why does the test samples have to be the same number as the training samples?</p>

<p>Here is the shape of the relevant matrices (the features have 44604 dimensions and there are 20 categories):</p>

<pre><code>X_train_scaled.shape    : (4627L, 44604L)
X_test_scaled.shape     : (4637L, 44604L)
K.shape                 : (4627L, 4627L)
labels_train.shape      : (4627L, 20L)
</code></pre>

<p>On a side note, is it normal that there is L next to the shape sizes of these matrices?</p>
";29081702;676634;23217;Andreas Mueller;1;29081702;"<p>You need to give the predict function the kernel between the test data and the training data. The easiest way for that is to give a callable to the kernel parameter <code>kernel=chi2_kernel</code>.
Using</p>

<pre><code>K_test = chi2_kernel(X_test_scaled)
</code></pre>

<p>will not work.It needs to be</p>

<pre><code>K_test = chi2_kernel(X_test_scaled, X_train_scaled)
</code></pre>
"
